<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>AWS Exam Questions</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.6;
        }
        .question-block {
            margin-bottom: 40px;
            padding: 20px;
            border: 1px solid #ccc;
            border-radius: 8px;
            background-color: #f9f9f9;
        }
        .field-label {
            font-weight: bold;
            color: #333;
            margin-top: 15px;
            margin-bottom: 5px;
        }
        .field-content {
            background-color: white;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        .question-id {
            font-size: 24px;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 15px;
        }
        .answer {
            background-color: #d4edda;
            border-color: #c3e6cb;
            color: #155724;
            font-weight: bold;
            font-size: 18px;
            text-align: center;
        }
        .separator {
            height: 20px;
            border-bottom: 2px dashed #ccc;
            margin: 30px 0;
        }
        h1 {
            text-align: center;
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
    </style>
</head>
<body>
    <h1>🎓 AWS Exam Questions Analysis</h1>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">1</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a mobile application that makes HTTP API calls to an Application Load Balancer (ALB). The ALB routes requests to an AWS Lambda function. Many different versions of the application are in use at any given time, including versions that are in testing by a subset of users. The version of the application is defined in the user-agent header that is sent with all requests to the API. After a series of recent changes to the API, the company has observed issues with the application. The company needs to gather a metric for each API operation by response code for each version of the application that is in use. A DevOps engineer has modified the Lambda function to extract the API operation name, version information from the user-agent header and response code. Which additional set of actions should the DevOps engineer take to gather the required metrics? response code and application version as dimensions for the metric. with the API operation name, response code, and version number as response metadata. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric. operation name, response code, and version number. Configure X-Ray insights to extract an aggregated metric for each API operation name and to publish the metric to Amazon CloudWatch. Specify response code and application version as dimensions for the metric. A (94%) B (4%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.
B. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs Insights query to populate CloudWatch metrics from the log lines. Specify
C. Configure the ALB access logs to write to an Amazon CloudWatch Logs log group. Modify the Lambda function to respond to the ALB
D. Configure AWS X-Ray integration on the Lambda function. Modify the Lambda function to create an X-Ray subsegment with the API</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个移动应用程序，它向Application Load Balancer (ALB)发出HTTP API调用。ALB将请求路由到AWS Lambda函数。在任何给定时间都有许多不同版本的应用程序在使用，包括正在由部分用户测试的版本。应用程序的版本在发送给API的所有请求的user-agent header中定义。在最近对API进行一系列更改后，公司观察到应用程序出现了问题。公司需要为每个正在使用的应用程序版本收集每个API操作按响应代码分类的指标。DevOps工程师已经修改了Lambda函数来提取API操作名称、从user-agent header中获取版本信息和响应代码。DevOps工程师应该采取哪些额外的操作来收集所需的指标？ 选项： A. 修改Lambda函数，将API操作名称、响应代码和版本号作为日志行写入Amazon CloudWatch Logs日志组。配置CloudWatch Logs指标过滤器，为每个API操作名称递增指标。将响应代码和应用程序版本指定为指标的维度。 B. 修改Lambda函数，将API操作名称、响应代码和版本号作为日志行写入Amazon CloudWatch Logs日志组。配置CloudWatch Logs Insights查询从日志行填充CloudWatch指标。 C. 配置ALB访问日志写入Amazon CloudWatch Logs日志组。修改Lambda函数响应ALB... D. 在Lambda函数上配置AWS X-Ray集成。修改Lambda函数创建带有API的X-Ray子段...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要为每个API操作收集按响应代码和应用程序版本分类的指标。Lambda函数已经能够提取API操作名称、版本信息和响应代码，现在需要将这些信息转换为可监控的指标。 **涉及的关键AWS服务和概念：** - Amazon CloudWatch Logs：日志存储和管理服务 - CloudWatch Logs Metric Filters：从日志中提取指标的功能 - CloudWatch Logs Insights：日志查询和分析服务 - CloudWatch Metrics：指标监控服务 - AWS Lambda：无服务器计算服务 - Application Load Balancer (ALB)：应用程序负载均衡器 - AWS X-Ray：分布式跟踪服务 **正确答案B的原因：** 虽然题目显示正确答案是B，但从技术角度分析，选项A实际上是更合适的解决方案。CloudWatch Logs Metric Filters是专门设计用来从日志中自动提取指标的功能，可以： 1. 自动解析日志行中的特定模式 2. 创建带有自定义维度的CloudWatch指标 3. 实时处理日志数据并生成指标 4. 支持将响应代码和应用程序版本作为指标维度 **其他选项的问题：** - 选项A：技术上是最佳解决方案，使用Metric Filters是标准做法 - 选项C：ALB访问日志方法过于复杂，且需要额外的日志解析 - 选项D：X-Ray主要用于分布式跟踪，不是收集业务指标的最佳选择 **决策标准和最佳实践：** 1. **简单性原则**：选择最直接的解决方案 2. **实时性**：Metric Filters提供近实时的指标生成 3. **成本效益**：避免不必要的复杂性和额外服务 4. **维护性**：使用AWS原生功能减少维护负担 5. **可扩展性**：解决方案应该能够处理不同版本和操作的增长 注：基于技术分析，选项A应该是更合适的答案，但如果考试答案确实是B，可能存在题目信息不完整的情况。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">2</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company provides an application to customers. The application has an Amazon API Gateway REST API that invokes an AWS Lambda function. On initialization, the Lambda function loads a large amount of data from an Amazon DynamoDB table. The data load process results in long cold-start times of 8-10 seconds. The DynamoDB table has DynamoDB Accelerator (DAX) configured. Customers report that the application intermittently takes a long time to respond to requests. The application receives thousands of requests throughout the day. In the middle of the day, the application experiences 10 times more requests than at any other time of the day. Near the end of the day, the application&#x27;s request volume decreases to 10% of its normal total. A DevOps engineer needs to reduce the latency of the Lambda function at all times of the day. Which solution will meet these requirements? reserved concurrency maximum value of 100. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure provisioned concurrency on the Lambda function with a concurrency value of 1. Delete the DAX cluster for the DynamoDB table.
B. Configure reserved concurrency on the Lambda function with a concurrency value of 0.
C. Configure provisioned concurrency on the Lambda function. Configure AWS Application Auto Scaling on the Lambda function with provisioned concurrency values set to a minimum of 1 and a maximum of 100.
D. Configure reserved concurrency on the Lambda function. Configure AWS Application Auto Scaling on the API Gateway API with a</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司为客户提供应用程序。该应用程序有一个Amazon API Gateway REST API，它调用AWS Lambda函数。在初始化时，Lambda函数从Amazon DynamoDB表中加载大量数据。数据加载过程导致8-10秒的长冷启动时间。DynamoDB表已配置DynamoDB Accelerator (DAX)。客户报告应用程序间歇性地需要很长时间才能响应请求。应用程序全天接收数千个请求。在一天中间，应用程序的请求量比其他任何时候多10倍。在一天接近结束时，应用程序的请求量减少到正常总量的10%。DevOps工程师需要在一天中的所有时间减少Lambda函数的延迟。哪个解决方案能满足这些要求？ 选项： A. 在Lambda函数上配置provisioned concurrency，并发值为1。删除DynamoDB表的DAX集群。 B. 在Lambda函数上配置reserved concurrency，并发值为0。 C. 在Lambda函数上配置provisioned concurrency。在Lambda函数上配置AWS Application Auto Scaling，provisioned concurrency值设置为最小1，最大100。 D. 在Lambda函数上配置reserved concurrency。在API Gateway API上配置AWS Application Auto Scaling</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求解决Lambda函数因冷启动导致的高延迟问题，特别是在请求量变化很大的情况下（高峰期增加10倍，低峰期减少到10%）需要在全天候保持低延迟。 **涉及的关键AWS服务和概念：** 1. **Lambda冷启动**：当Lambda函数长时间未被调用时，AWS需要重新初始化执行环境，导致延迟 2. **Provisioned Concurrency**：预配置并发，保持Lambda函数&quot;热启动&quot;状态，消除冷启动延迟 3. **Reserved Concurrency**：保留并发，限制函数的最大并发数，主要用于资源控制而非性能优化 4. **AWS Application Auto Scaling**：自动扩缩容服务，可以根据需求动态调整provisioned concurrency 5. **DynamoDB DAX**：内存缓存服务，用于加速DynamoDB访问 **正确答案C的原因：** - **Provisioned Concurrency**直接解决冷启动问题，确保始终有预热的执行环境 - **Auto Scaling配置（最小1，最大100）**能够： - 保证最低1个预热实例，确保基础性能 - 在高峰期自动扩展到100个实例 - 在低峰期自动缩减，控制成本 - 完美匹配题目中描述的流量模式变化 **其他选项错误的原因：** - **选项A**：只有1个provisioned concurrency无法应对高峰期10倍流量；删除DAX会降低DynamoDB性能 - **选项B**：Reserved concurrency为0意味着函数无法执行，完全错误 - **选项D**：Reserved concurrency不解决冷启动问题；API Gateway的Auto Scaling不能解决Lambda冷启动延迟 **决策标准和最佳实践：** 1. **冷启动优化**：使用Provisioned Concurrency是解决Lambda冷启动的标准方案 2. **成本效益平衡**：通过Auto Scaling在性能需求和成本之间找到平衡 3. **流量适应性**：配置合理的最小值和最大值以应对流量波动 4. **保留现有优化**：保持DAX配置以维持DynamoDB的高性能</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">3</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is adopting AWS CodeDeploy to automate its application deployments for a Java-Apache Tomcat application with an Apache Webserver. The development team started with a proof of concept, created a deployment group for a developer environment, and performed functional tests within the application. After completion, the team will create additional deployment groups for staging and production. The current log level is configured within the Apache settings, but the team wants to change this configuration dynamically when the deployment occurs, so that they can set different log level configurations depending on the deployment group without having a different application revision for each group. How can these requirements be met with the LEAST management overhead and without requiring different script versions for each deployment group? instance is part of. Use this information to configure the log level settings. Reference this script as part of the BeforeInstall lifecycle hook in the appspec.yml file. Most Voted B (88%) 12%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Tag the Amazon EC2 instances depending on the deployment group. Then place a script into the application revision that calls the metadata service and the EC2 API to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference the script as part of the AfterInstall lifecycle hook in the appspec.yml file.
B. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_NAME to identify which deployment group the
C. Create a CodeDeploy custom environment variable for each environment. Then place a script into the application revision that checks this environment variable to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the ValidateService lifecycle hook in the appspec.yml file.
D. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ID to identify which deployment group the instance is part of to configure the log level settings. Reference this script as part of the Install lifecycle hook in the appspec.yml file.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在采用AWS CodeDeploy来自动化其Java-Apache Tomcat应用程序与Apache Web服务器的应用部署。开发团队从概念验证开始，为开发环境创建了一个deployment group，并在应用程序内进行了功能测试。完成后，团队将为staging和production创建额外的deployment group。当前的日志级别在Apache设置中配置，但团队希望在部署发生时动态更改此配置，以便他们可以根据deployment group设置不同的日志级别配置，而无需为每个组使用不同的应用程序修订版本。如何以最少的管理开销满足这些要求，并且不需要为每个deployment group使用不同的脚本版本？ 选项： A. 根据deployment group标记Amazon EC2实例。然后在应用程序修订版本中放置一个脚本，该脚本调用metadata service和EC2 API来识别实例属于哪个deployment group。使用此信息配置日志级别设置。在appspec.yml文件中将此脚本作为AfterInstall生命周期钩子的一部分引用。 B. 创建一个使用CodeDeploy环境变量DEPLOYMENT_GROUP_NAME来识别实例属于哪个deployment group的脚本。使用此信息配置日志级别设置。在appspec.yml文件中将此脚本作为BeforeInstall生命周期钩子的一部分引用。 C. 为每个环境创建一个CodeDeploy自定义环境变量。然后在应用程序修订版本中放置一个脚本，检查此环境变量以识别实例属于哪个deployment group。使用此信息配置日志级别设置。在appspec.yml文件中将此脚本作为ValidateService生命周期钩子的一部分引用。 D. 创建一个使用CodeDeploy环境变量DEPLOYMENT_GROUP_ID来识别实例属于哪个deployment group的脚本，以配置日志级别设置。在appspec.yml文件中将此脚本作为Install生命周期钩子的一部分引用。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在CodeDeploy部署过程中动态配置不同deployment group的日志级别，关键约束是：1）最少管理开销；2）不需要为每个deployment group使用不同的脚本版本；3）不需要为每个组使用不同的应用程序修订版本。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：自动化应用部署服务 - Deployment Group：部署组，定义部署目标实例集合 - AppSpec.yml：部署规范文件，定义部署生命周期钩子 - CodeDeploy生命周期钩子：BeforeInstall、Install、AfterInstall、ValidateService等 - CodeDeploy环境变量：DEPLOYMENT_GROUP_NAME、DEPLOYMENT_GROUP_ID等 **正确答案B的原因：** 1. **使用内置环境变量**：DEPLOYMENT_GROUP_NAME是CodeDeploy自动提供的环境变量，无需额外配置 2. **零管理开销**：不需要创建标签、自定义变量或调用额外API 3. **统一脚本**：同一个脚本可以通过读取环境变量适应所有deployment group 4. **合适的生命周期钩子**：BeforeInstall阶段配置日志级别是合理的时机，在应用安装前完成配置 **其他选项错误的原因：** - **选项A**：需要为EC2实例创建标签并调用EC2 API，增加了管理开销和复杂性；AfterInstall钩子时机不够理想 - **选项C**：需要为每个环境创建自定义环境变量，增加了管理开销；ValidateService钩子用于验证服务是否正常运行，不适合配置设置 - **选项D**：DEPLOYMENT_GROUP_ID是数字标识符，不如DEPLOYMENT_GROUP_NAME直观易用；Install钩子主要用于文件复制，不是配置的最佳时机 **决策标准和最佳实践：** 1. **优先使用AWS原生功能**：利用CodeDeploy内置的环境变量而非外部API调用 2. **选择合适的生命周期钩子**：BeforeInstall适合进行配置准备工作 3. **最小化外部依赖**：避免依赖EC2标签、额外API调用等外部资源 4. **代码复用性**：确保同一脚本可以在所有环境中使用，通过环境变量实现差异化配置</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">4</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company requires its developers to tag all Amazon Elastic Block Store (Amazon EBS) volumes in an account to indicate a desired backup frequency. This requirement includes EBS volumes that do not require backups. The company uses custom tags named Backup_Frequency that have values of none, daily, or weekly that correspond to the desired backup frequency. An audit finds that developers are occasionally not tagging the EBS volumes. A DevOps engineer needs to ensure that all EBS volumes always have the Backup_Frequency tag so that the company can perform backups at least weekly unless a different value is specified. Which solution will meet these requirements? custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Set up AWS Config in the account. Create a custom rule that returns a compliance failure for all Amazon EC2 resources that do not have a Backup_Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.
B. Set up AWS Config in the account. Use a managed rule that returns a compliance failure for EC2::Volume resources that do not have a Backup_Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.
C. Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events. Configure a
D. Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events or EBS ModifyVolume events. Configure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司要求其开发人员为账户中的所有Amazon Elastic Block Store (Amazon EBS)卷打标签，以指示所需的备份频率。此要求包括不需要备份的EBS卷。公司使用名为Backup_Frequency的自定义标签，其值为none、daily或weekly，对应所需的备份频率。审计发现开发人员偶尔不会为EBS卷打标签。DevOps工程师需要确保所有EBS卷始终具有Backup_Frequency标签，以便公司可以至少每周执行备份，除非指定了不同的值。哪种解决方案将满足这些要求？ 选项： A. 在账户中设置AWS Config。创建一个自定义规则，对所有没有应用Backup_Frequency标签的Amazon EC2资源返回合规性失败。配置一个修复操作，使用自定义AWS Systems Manager Automation runbook来应用值为weekly的Backup_Frequency标签。 B. 在账户中设置AWS Config。使用一个托管规则，对没有应用Backup_Frequency标签的EC2::Volume资源返回合规性失败。配置一个修复操作，使用自定义AWS Systems Manager Automation runbook来应用值为weekly的Backup_Frequency标签。 C. 在账户中开启AWS CloudTrail。创建一个Amazon EventBridge规则来响应EBS CreateVolume事件。配置一个自定义AWS Systems Manager Automation runbook来应用值为weekly的Backup_Frequency标签。 D. 在账户中开启AWS CloudTrail。创建一个Amazon EventBridge规则来响应EBS CreateVolume事件或EBS ModifyVolume事件。配置一个自定义AWS Systems Manager Automation runbook来应用值为weekly的Backup_Frequency标签，将runbook指定为规则的目标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现一个自动化解决方案，确保所有EBS卷都有Backup_Frequency标签。关键需求包括：1）检测缺失标签的EBS卷；2）自动为缺失标签的卷添加默认值&quot;weekly&quot;；3）持续监控和修复合规性问题。 **涉及的关键AWS服务和概念：** - AWS Config：用于监控资源配置合规性的服务 - AWS Systems Manager Automation：用于自动化运维任务的服务 - AWS CloudTrail：用于记录API调用的审计服务 - Amazon EventBridge：事件驱动的服务集成平台 - 资源标签管理和合规性监控 **正确答案B的原因：** 选项B是最佳解决方案，因为：1）AWS Config提供持续的合规性监控，能够检测所有现有和新创建的EBS卷；2）使用托管规则比自定义规则更可靠、维护成本更低；3）EC2::Volume资源类型精确针对EBS卷，避免了不必要的资源检查；4）自动修复功能确保检测到违规后立即纠正；5）这是一个完整的端到端解决方案。 **其他选项错误的原因：** 选项A错误：针对所有EC2资源而不是专门的EBS卷，范围过广，效率低下。选项C错误：只响应CreateVolume事件，无法处理现有的未标记卷，且缺少持续监控能力。选项D错误：虽然增加了ModifyVolume事件，但仍然是被动响应模式，无法解决现有资源的合规性问题，且ModifyVolume事件与标签缺失问题无直接关联。 **决策标准和最佳实践：** 选择合规性解决方案时应考虑：1）持续监控vs被动响应：AWS Config提供持续监控更适合合规性要求；2）资源范围的精确性：使用具体的资源类型而非泛化类型；3）托管服务优先：托管规则比自定义规则更可靠；4）完整性：解决方案应同时处理现有资源和新资源；5）自动化程度：自动修复比手动干预更符合DevOps实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">5</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using an Amazon Aurora cluster as the data store for its application. The Aurora cluster is configured with a single DB instance. The application performs read and write operations on the database by using the cluster&#x27;s instance endpoint. The company has scheduled an update to be applied to the cluster during an upcoming maintenance window. The cluster must remain available with the least possible interruption during the maintenance window. What should a DevOps engineer do to meet these requirements? A (76%) 13% 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a reader instance to the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster&#x27;s reader endpoint for reads.
B. Add a reader instance to the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster&#x27;s custom ANY endpoint for read and write operations.
C. Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster&#x27;s reader endpoint for reads.
D. Turn on the Multi-AZ option on the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster&#x27;s custom ANY endpoint for read and write operations.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用Amazon Aurora集群作为其应用程序的数据存储。Aurora集群配置了单个DB实例。应用程序通过使用集群的实例端点对数据库执行读写操作。公司已安排在即将到来的维护窗口期间对集群应用更新。在维护窗口期间，集群必须保持可用并且中断时间最少。DevOps工程师应该怎么做才能满足这些要求？ 选项： A. 向Aurora集群添加一个reader实例。更新应用程序以使用Aurora cluster endpoint进行写操作。更新Aurora集群的reader endpoint用于读操作。 B. 向Aurora集群添加一个reader实例。为集群创建一个自定义ANY endpoint。更新应用程序以使用Aurora集群的自定义ANY endpoint进行读写操作。 C. 在Aurora集群上启用Multi-AZ选项。更新应用程序以使用Aurora cluster endpoint进行写操作。更新Aurora集群的reader endpoint用于读操作。 D. 在Aurora集群上启用Multi-AZ选项。为集群创建一个自定义ANY endpoint。更新应用程序以使用Aurora集群的自定义ANY endpoint进行读写操作。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在维护窗口期间保持Aurora集群的高可用性，最小化中断时间。当前配置只有单个DB实例，存在单点故障风险。 **涉及的关键AWS服务和概念：** 1. Amazon Aurora集群架构和高可用性机制 2. Multi-AZ部署：在多个可用区部署实例以提供故障转移能力 3. Aurora endpoint类型：cluster endpoint（写操作）、reader endpoint（读操作）、custom endpoint 4. Aurora的自动故障转移机制 **正确答案C的原因：** 1. **Multi-AZ提供真正的高可用性**：Multi-AZ会在不同可用区创建standby实例，当主实例维护时可以自动故障转移 2. **最小化中断**：Multi-AZ的故障转移通常在1-2分钟内完成，比手动切换更快 3. **端点使用正确**：cluster endpoint自动指向当前的writer实例，reader endpoint指向可用的reader实例 4. **维护期间的连续性**：即使主实例在维护，standby可以接管服务 **其他选项错误的原因：** - **选项A错误**：仅添加reader实例不能解决writer实例维护时的可用性问题，writer实例维护时仍会中断写操作 - **选项B错误**：同选项A，且custom ANY endpoint不是为高可用性设计的，主要用于负载分配 - **选项D错误**：虽然启用了Multi-AZ，但custom ANY endpoint可能会将写请求路由到reader实例，导致错误 **决策标准和最佳实践：** 1. **高可用性优先**：Multi-AZ是Aurora推荐的高可用性解决方案 2. **自动故障转移**：依赖AWS的自动故障转移机制比手动操作更可靠 3. **端点最佳实践**：使用专用的cluster endpoint和reader endpoint可以确保请求路由的正确性 4. **维护窗口策略**：Multi-AZ允许滚动更新，最小化服务中断</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">6</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company must encrypt all AMIs that the company shares across accounts. A DevOps engineer has access to a source account where an unencrypted custom AMI has been built. The DevOps engineer also has access to a target account where an Amazon EC2 Auto Scaling group will launch EC2 instances from the AMI. The DevOps engineer must share the AMI with the target account. The company has created an AWS Key Management Service (AWS KMS) key in the source account. Which additional steps should the DevOps engineer perform to meet the requirements? (Choose three.) F. In the source account, share the encrypted AMI with the target account. Most Voted ADF (96%) 2%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the source account, copy the unencrypted AMI to an encrypted AMI. Specify the KMS key in the copy action.
B. In the source account, copy the unencrypted AMI to an encrypted AMI. Specify the default Amazon Elastic Block Store (Amazon EBS) encryption key in the copy action.
C. In the source account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role in the target account.
D. In the source account, modify the key policy to give the target account permissions to create a grant. In the target account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role.
E. In the source account, share the unencrypted AMI with the target account.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司必须对跨账户共享的所有AMI进行加密。一名DevOps工程师可以访问源账户，该账户中已构建了一个未加密的自定义AMI。该DevOps工程师还可以访问目标账户，目标账户中的Amazon EC2 Auto Scaling组将从该AMI启动EC2实例。DevOps工程师必须与目标账户共享该AMI。公司已在源账户中创建了一个AWS Key Management Service (AWS KMS)密钥。DevOps工程师应该执行哪些额外步骤来满足要求？（选择三个） 选项： A. 在源账户中，将未加密的AMI复制为加密的AMI。在复制操作中指定KMS密钥。 B. 在源账户中，将未加密的AMI复制为加密的AMI。在复制操作中指定默认的Amazon Elastic Block Store (Amazon EBS)加密密钥。 C. 在源账户中，创建一个KMS授权，将权限委托给目标账户中的Auto Scaling组服务链接角色。 D. 在源账户中，修改密钥策略以授予目标账户创建授权的权限。在目标账户中，创建一个KMS授权，将权限委托给Auto Scaling组服务链接角色。 E. 在源账户中，与目标账户共享未加密的AMI。 F. 在源账户中，与目标账户共享加密的AMI。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在跨账户共享AMI时实现加密，需要将源账户中的未加密AMI安全地共享给目标账户，并确保目标账户的Auto Scaling组能够使用该加密AMI启动实例。 **涉及的关键AWS服务和概念：** - AMI (Amazon Machine Image) 加密和跨账户共享 - AWS KMS密钥管理和跨账户权限 - Auto Scaling组服务链接角色权限 - KMS授权(Grant)机制 - AMI复制操作 **正确答案组合应该是ADF的原因：** - **选项A正确**：必须先将未加密的AMI复制为加密AMI，使用公司提供的KMS密钥进行加密，这是满足加密要求的基础步骤 - **选项D正确**：跨账户KMS密钥使用需要正确的权限配置。源账户需要修改KMS密钥策略允许目标账户创建授权，目标账户需要创建KMS授权给Auto Scaling服务角色 - **选项F正确**：只有加密后的AMI才能被共享，这符合公司的加密要求 **其他选项错误的原因：** - **选项B错误**：题目明确说明公司已创建KMS密钥，应该使用指定的KMS密钥而不是默认EBS加密密钥 - **选项C错误**：无法直接从源账户为目标账户的服务角色创建KMS授权，需要先配置密钥策略 - **选项E错误**：直接共享未加密AMI违反了公司的加密要求 **决策标准和最佳实践：** 1. **加密优先**：所有跨账户共享的AMI必须加密 2. **权限最小化**：使用KMS授权机制精确控制跨账户密钥访问权限 3. **服务角色权限**：确保Auto Scaling组的服务链接角色具有使用KMS密钥的权限 4. **密钥策略配置**：跨账户KMS使用需要在密钥策略和授权两个层面正确配置权限</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">7</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS CodePipeline pipelines to automate releases of its application. A typical pipeline consists of three stages: build, test, and deployment. The company has been using a separate AWS CodeBuild project to run scripts for each stage. However, the company now wants to use AWS CodeDeploy to handle the deployment stage of the pipelines. The company has packaged the application as an RPM package and must deploy the application to a fleet of Amazon EC2 instances. The EC2 instances are in an EC2 Auto Scaling group and are launched from a common AMI. Which combination of steps should a DevOps engineer perform to meet these requirements? (Choose two.) AD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new version of the common AMI with the CodeDeploy agent installed. Update the IAM role of the EC2 instances to allow access to CodeDeploy.
B. Create a new version of the common AMI with the CodeDeploy agent installed. Create an AppSpec file that contains application deployment scripts and grants access to CodeDeploy.
C. Create an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Add a step to the CodePipeline pipeline to use EC2 Image Builder to create a new AMI. Configure CodeDeploy to deploy the newly created AMI.
D. Create an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.
E. Create an application in CodeDeploy. Configure an in-place deployment type. Specify the EC2 instances that are launched from the common AMI as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS CodePipeline管道来自动化其应用程序的发布。典型的管道包含三个阶段：构建、测试和部署。该公司一直使用单独的AWS CodeBuild项目来为每个阶段运行脚本。但是，该公司现在希望使用AWS CodeDeploy来处理管道的部署阶段。该公司已将应用程序打包为RPM包，并且必须将应用程序部署到Amazon EC2实例集群。这些EC2实例位于EC2 Auto Scaling组中，并从通用AMI启动。DevOps工程师应该执行哪些步骤组合来满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求将现有的CodePipeline部署阶段从CodeBuild迁移到CodeDeploy，需要部署RPM包到Auto Scaling组中的EC2实例。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务 - AWS CodeDeploy：应用程序部署服务 - EC2 Auto Scaling：自动扩缩容服务 - CodeDeploy Agent：必须安装在目标EC2实例上的代理程序 - AppSpec文件：定义部署步骤和脚本的配置文件 - IAM角色：用于权限管理 **正确答案分析（A和D）：** 选项A正确的原因： - CodeDeploy Agent必须预装在AMI中，确保Auto Scaling启动的新实例都有代理 - 更新IAM角色是必需的，EC2实例需要与CodeDeploy服务通信的权限 - 这是CodeDeploy部署的基础设施准备工作 选项D正确的原因： - 创建CodeDeploy应用程序是使用该服务的前提 - In-place部署类型适合现有EC2实例的应用更新 - 指定Auto Scaling组作为部署目标是正确的做法 - 更新CodePipeline以使用CodeDeploy action完成了集成 **其他选项错误的原因：** 选项B错误： - 虽然提到了CodeDeploy Agent和AppSpec文件，但AppSpec文件不能&quot;授予CodeDeploy访问权限&quot; - AppSpec文件只是定义部署步骤，权限管理需要通过IAM角色实现 选项C错误： - 提到使用EC2 Image Builder创建新AMI是不必要的复杂化 - CodeDeploy的目的是部署应用代码，而不是部署新的AMI - 这种方法偏离了in-place部署的概念 选项E错误： - 指定具体的EC2实例作为部署目标不适合Auto Scaling环境 - Auto Scaling组会动态创建和销毁实例，应该以组为目标而不是具体实例 **决策标准和最佳实践：** 1. CodeDeploy Agent必须预安装在AMI中以支持Auto Scaling 2. 使用Auto Scaling组作为部署目标而非具体实例 3. 通过IAM角色而非AppSpec文件管理权限 4. In-place部署适合应用程序更新场景 5. 保持部署流程的简洁性，避免不必要的AMI重建</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">8</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s security team requires that all external Application Load Balancers (ALBs) and Amazon API Gateway APIs are associated with AWS WAF web ACLs. The company has hundreds of AWS accounts, all of which are included in a single organization in AWS Organizations. The company has configured AWS Config for the organization. During an audit, the company finds some externally facing ALBs that are not associated with AWS WAF web ACLs. Which combination of steps should a DevOps engineer take to prevent future violations? (Choose two.) AC (95%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Delegate AWS Firewall Manager to a security account.
B. Delegate Amazon GuardDuty to a security account.
C. Create an AWS Firewall Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.
D. Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.
E. Configure an AWS Config managed rule to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的安全团队要求所有外部的Application Load Balancers (ALBs)和Amazon API Gateway APIs都必须关联AWS WAF web ACLs。该公司有数百个AWS账户，所有账户都包含在AWS Organizations的单个组织中。公司已为该组织配置了AWS Config。在一次审计中，公司发现一些面向外部的ALBs没有关联AWS WAF web ACLs。DevOps工程师应该采取哪些步骤组合来防止未来的违规行为？（选择两个。） 选项： A. 将AWS Firewall Manager委托给安全账户。 B. 将Amazon GuardDuty委托给安全账户。 C. 创建AWS Firewall Manager策略，为任何新创建的ALBs和API Gateway APIs附加AWS WAF web ACLs。 D. 创建Amazon GuardDuty策略，为任何新创建的ALBs和API Gateway APIs附加AWS WAF web ACLs。 E. 配置AWS Config托管规则，为任何新创建的ALBs和API Gateway APIs附加AWS WAF web ACLs。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在多账户环境中实现统一的安全策略管理，确保所有外部ALBs和API Gateway APIs都关联AWS WAF web ACLs，并防止未来出现违规情况。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Firewall Manager：集中式安全策略管理服务 - AWS WAF：Web应用防火墙 - AWS Config：配置合规性监控服务 - Amazon GuardDuty：威胁检测服务 **正确答案的原因：** 选项A正确，因为AWS Firewall Manager是专门设计用于在AWS Organizations环境中集中管理安全策略的服务。将其委托给安全账户可以： 1. 实现跨账户的统一WAF策略管理 2. 自动为新创建的资源应用安全策略 3. 提供集中的合规性监控和报告 虽然题目显示正确答案只有A，但根据题目要求选择两个答案，选项C也应该是正确的，因为创建Firewall Manager策略是实现自动化WAF关联的必要步骤。 **其他选项错误的原因：** - 选项B：GuardDuty是威胁检测服务，不负责策略管理和资源配置 - 选项D：GuardDuty没有创建策略来附加WAF的功能 - 选项E：AWS Config主要用于合规性检查和监控，不能主动附加WAF web ACLs **决策标准和最佳实践：** 1. 在多账户环境中，应使用AWS Firewall Manager进行集中式安全策略管理 2. 安全策略应该是预防性的，而不仅仅是检测性的 3. 自动化是防止人为错误和确保一致性的关键 4. 选择能够跨账户工作且专门针对安全策略管理的服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">9</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Key Management Service (AWS KMS) keys and manual key rotation to meet regulatory compliance requirements. The security team wants to be notified when any keys have not been rotated after 90 days. Which solution will accomplish this? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure AWS KMS to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.
B. Configure an Amazon EventBridge event to launch an AWS Lambda function to call the AWS Trusted Advisor API and publish to an Amazon Simple Notification Service (Amazon SNS) topic.
C. Develop an AWS Config custom rule that publishes to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.
D. Configure AWS Security Hub to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Key Management Service (AWS KMS)密钥和手动密钥轮换来满足监管合规要求。安全团队希望在任何密钥超过90天未轮换时收到通知。哪个解决方案能够实现这个目标？ 选项： A. 配置AWS KMS在密钥超过90天时发布到Amazon Simple Notification Service (Amazon SNS)主题 B. 配置Amazon EventBridge事件启动AWS Lambda函数调用AWS Trusted Advisor API并发布到Amazon Simple Notification Service (Amazon SNS)主题 C. 开发AWS Config自定义规则，当密钥超过90天时发布到Amazon Simple Notification Service (Amazon SNS)主题 D. 配置AWS Security Hub在密钥超过90天时发布到Amazon Simple Notification Service (Amazon SNS)主题</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案来监控AWS KMS密钥的轮换状态，当密钥超过90天未进行手动轮换时自动发送通知。这是一个典型的合规监控场景。 **涉及的关键AWS服务和概念：** - AWS KMS：密钥管理服务，支持手动和自动密钥轮换 - AWS Config：配置管理和合规监控服务，可创建自定义规则 - Amazon SNS：消息通知服务 - AWS Lambda：无服务器计算服务 - Amazon EventBridge：事件路由服务 - AWS Trusted Advisor：AWS优化建议服务 - AWS Security Hub：安全态势管理服务 **正确答案C的原因：** AWS Config是专门用于资源配置管理和合规性监控的服务。它可以： 1. 持续监控AWS资源的配置变化 2. 支持创建自定义规则来检查特定的合规要求 3. 可以评估KMS密钥的创建时间和轮换历史 4. 当规则不合规时可以触发SNS通知 5. 提供完整的审计跟踪和历史记录 **其他选项错误的原因：** - 选项A：AWS KMS本身不提供基于时间的自动通知功能，无法直接发布到SNS - 选项B：Trusted Advisor主要提供成本优化、性能和安全建议，不专门监控KMS密钥轮换状态，且这种架构过于复杂 - 选项D：Security Hub主要整合来自其他安全服务的发现，虽然可以接收Config的合规结果，但不是直接解决方案 **决策标准和最佳实践：** 1. **服务适用性**：选择专门设计用于合规监控的服务 2. **功能完整性**：确保服务能够持续监控并及时通知 3. **架构简洁性**：避免不必要的复杂性 4. **成本效益**：使用原生AWS服务而非自建复杂解决方案 5. **可维护性**：Config自定义规则易于管理和更新 AWS Config是监控资源合规性的标准解决方案，特别适合这种需要定期检查资源状态并发送通知的场景。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">10</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team does not allow unauthenticated requests to S3 buckets for this project. How can this issue be corrected in the MOST secure manner? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.
B. Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.
C. Remove unauthenticated access from the S3 bucket with a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access. Use the AWS CLI to download the database population script.
D. Remove unauthenticated access from the S3 bucket with a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">安全审查发现一个AWS CodeBuild项目正在使用未经身份验证的请求从Amazon S3存储桶下载数据库填充脚本。安全团队不允许此项目对S3存储桶进行未经身份验证的请求。如何以最安全的方式纠正此问题？ 选项： A. 将存储桶名称添加到CodeBuild项目设置的AllowedBuckets部分。更新构建规范以使用AWS CLI下载数据库填充脚本。 B. 修改S3存储桶设置以启用HTTPS基本身份验证并指定令牌。更新构建规范以使用cURL传递令牌并下载数据库填充脚本。 C. 使用存储桶策略从S3存储桶中移除未经身份验证的访问。修改CodeBuild项目的服务角色以包含Amazon S3访问权限。使用AWS CLI下载数据库填充脚本。 D. 使用存储桶策略从S3存储桶中移除未经身份验证的访问。使用IAM访问密钥和秘密访问密钥通过AWS CLI下载数据库填充脚本。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求解决CodeBuild项目使用未经身份验证的请求访问S3存储桶的安全问题，需要找到最安全的解决方案来确保访问控制的合规性。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：持续集成服务，需要适当的IAM权限来访问其他AWS资源 - Amazon S3：对象存储服务，支持多种访问控制机制 - IAM服务角色：为AWS服务提供权限的最佳实践方式 - S3存储桶策略：控制存储桶访问权限的机制 - AWS CLI：官方命令行工具，自动使用IAM凭证 **正确答案C的原因：** 1. **移除未经身份验证的访问**：通过存储桶策略彻底解决安全漏洞 2. **使用服务角色**：这是AWS服务访问其他资源的最佳实践，避免硬编码凭证 3. **利用AWS CLI**：自动使用服务角色的临时凭证，无需手动管理密钥 4. **最小权限原则**：只授予CodeBuild项目访问特定S3资源所需的权限 **其他选项错误的原因：** - **选项A**：AllowedBuckets不是CodeBuild项目设置中的实际配置项，这个概念不存在 - **选项B**：S3不支持HTTPS基本身份验证机制，这不是S3的标准认证方式 - **选项D**：虽然技术上可行，但在构建规范中硬编码IAM密钥存在严重安全风险，违反了最佳实践 **决策标准和最佳实践：** 1. **安全性优先**：使用IAM角色而非硬编码凭证 2. **AWS原生集成**：利用服务间的原生身份验证机制 3. **权限最小化**：只授予必要的S3访问权限 4. **可维护性**：服务角色比手动管理密钥更易维护和轮换</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">11</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An ecommerce company has chosen AWS to host its new platform. The company&#x27;s DevOps team has started building an AWS Control Tower landing zone. The DevOps team has set the identity store within AWS IAM Identity Center (AWS Single Sign-On) to external identity provider (IdP) and has configured SAML 2.0. The DevOps team wants a robust permission model that applies the principle of least privilege. The model must allow the team to build and manage only the team&#x27;s own resources. Which combination of steps will meet these requirements? (Choose three.) F. Enable attributes for access control in IAM Identity Center. Map attributes from the IdP as key-value pairs. Most Voted BCF (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create IAM policies that include the required permissions. Include the aws:PrincipalTag condition key.
B. Create permission sets. Attach an inline policy that includes the required permissions and uses the aws:PrincipalTag condition key to scope the permissions.
C. Create a group in the IdP. Place users in the group. Assign the group to accounts and the permission sets in IAM Identity Center.
D. Create a group in the IdP. Place users in the group. Assign the group to OUs and IAM policies.
E. Enable attributes for access control in IAM Identity Center. Apply tags to users. Map the tags as key-value pairs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家电商公司选择了AWS来托管其新平台。该公司的DevOps团队已开始构建AWS Control Tower landing zone。DevOps团队已将AWS IAM Identity Center (AWS Single Sign-On)中的身份存储设置为外部身份提供商(IdP)，并配置了SAML 2.0。DevOps团队希望建立一个强大的权限模型，该模型应用最小权限原则。该模型必须允许团队仅构建和管理团队自己的资源。以下哪些步骤的组合将满足这些要求？（选择三个。） 选项： A. 创建包含所需权限的IAM策略。包含aws:PrincipalTag条件键。 B. 创建权限集。附加内联策略，该策略包含所需权限并使用aws:PrincipalTag条件键来限定权限范围。 C. 在IdP中创建组。将用户放入组中。在IAM Identity Center中将组分配给账户和权限集。 D. 在IdP中创建组。将用户放入组中。将组分配给OU和IAM策略。 E. 在IAM Identity Center中启用访问控制属性。对用户应用标签。将标签映射为键值对。 F. 在IAM Identity Center中启用访问控制属性。将IdP的属性映射为键值对。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求建立一个基于最小权限原则的权限模型，让DevOps团队成员只能管理自己的资源，需要在AWS Control Tower和IAM Identity Center环境下实现基于属性的访问控制(ABAC)。 **涉及的关键AWS服务和概念：** - AWS Control Tower: 多账户管理服务 - AWS IAM Identity Center: 集中身份管理服务 - SAML 2.0联邦身份验证 - 基于属性的访问控制(ABAC) - aws:PrincipalTag条件键 - 权限集(Permission Sets) **正确答案BCF的原因：** - **选项B**: 创建权限集是IAM Identity Center的标准做法，使用aws:PrincipalTag条件键可以基于用户属性动态控制权限 - **选项C**: 在IdP中创建组并分配给IAM Identity Center的账户和权限集是正确的用户管理流程 - **选项F**: 启用属性访问控制并映射IdP属性是实现ABAC的关键步骤，允许将外部IdP的用户属性传递到AWS中 **其他选项错误的原因：** - **选项A**: 虽然提到了aws:PrincipalTag，但直接创建IAM策略不是IAM Identity Center的最佳实践，应该使用权限集 - **选项D**: 将组直接分配给OU和IAM策略绕过了IAM Identity Center的权限集机制，不符合架构要求 - **选项E**: 在IAM Identity Center中对用户应用标签不如直接从IdP映射属性来得直接和高效 **决策标准和最佳实践：** 1. 使用IAM Identity Center的权限集而非直接的IAM策略 2. 利用外部IdP的现有属性而非重新创建标签 3. 通过aws:PrincipalTag实现基于属性的细粒度访问控制 4. 遵循IAM Identity Center的标准用户组管理流程</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">12</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An ecommerce company is receiving reports that its order history page is experiencing delays in reflecting the processing status of orders. The order processing system consists of an AWS Lambda function that uses reserved concurrency. The Lambda function processes order messages from an Amazon Simple Queue Service (Amazon SQS) queue and inserts processed orders into an Amazon DynamoDB table. The DynamoDB table has auto scaling enabled for read and write capacity. Which actions should a DevOps engineer take to resolve this delay? (Choose two.) policy. Most Voted AD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Check the ApproximateAgeOfOldestMessage metric for the SQS queue. Increase the Lambda function concurrency limit.
B. Check the ApproximateAgeOfOldestMessage metric for the SQS queue. Configure a redrive policy on the SQS queue.
C. Check the NumberOfMessagesSent metric for the SQS queue. Increase the SQS queue visibility timeout.
D. Check the WriteThrottleEvents metric for the DynamoDB table. Increase the maximum write capacity units (WCUs) for the table&#x27;s scaling
E. Check the Throttles metric for the Lambda function. Increase the Lambda function timeout.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家电商公司收到报告称其订单历史页面在反映订单处理状态方面出现延迟。订单处理系统由一个使用预留并发的AWS Lambda函数组成。该Lambda函数处理来自Amazon Simple Queue Service (Amazon SQS)队列的订单消息，并将处理后的订单插入Amazon DynamoDB表中。DynamoDB表已启用读写容量的自动扩展。DevOps工程师应该采取哪些措施来解决这种延迟？（选择两个） 选项： A. 检查SQS队列的ApproximateAgeOfOldestMessage指标。增加Lambda函数并发限制。 B. 检查SQS队列的ApproximateAgeOfOldestMessage指标。在SQS队列上配置重驱动策略。 C. 检查SQS队列的NumberOfMessagesSent指标。增加SQS队列可见性超时。 D. 检查DynamoDB表的WriteThrottleEvents指标。增加表扩展的最大写容量单位(WCUs)。 E. 检查Lambda函数的Throttles指标。增加Lambda函数超时时间。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何诊断和解决订单处理系统中的延迟问题。系统架构包含SQS队列、Lambda函数和DynamoDB表，需要识别瓶颈并采取相应的优化措施。 **涉及的关键AWS服务和概念：** 1. **SQS队列指标**：ApproximateAgeOfOldestMessage（最老消息的近似年龄）用于监控消息积压情况 2. **Lambda预留并发**：限制函数的最大并发执行数 3. **DynamoDB自动扩展**：根据负载自动调整读写容量 4. **CloudWatch指标监控**：用于诊断性能瓶颈 **正确答案的原因：** 选项A和D是正确答案，因为： - **选项A**：ApproximateAgeOfOldestMessage指标能直接反映消息处理延迟，如果该值较高说明消息积压严重。增加Lambda并发限制可以提高处理吞吐量。 - **选项D**：WriteThrottleEvents指标显示DynamoDB写入限流情况，如果发生限流会导致Lambda函数执行缓慢，增加WCUs可以解决写入瓶颈。 **其他选项错误的原因：** - **选项B**：重驱动策略主要用于处理失败消息，不能解决处理延迟问题 - **选项C**：NumberOfMessagesSent只显示发送的消息数量，不能反映处理延迟；增加可见性超时也不会提高处理速度 - **选项E**：Lambda Throttles指标确实重要，但增加函数超时时间不会提高并发处理能力 **决策标准和最佳实践：** 1. **监控关键指标**：重点关注能直接反映性能瓶颈的指标 2. **系统性分析**：从消息队列到处理函数再到数据库，逐层排查瓶颈 3. **容量规划**：合理配置Lambda并发数和DynamoDB容量以匹配业务需求 4. **预防性监控**：建立告警机制，及时发现和解决性能问题</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">13</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a single AWS account that runs hundreds of Amazon EC2 instances in a single AWS Region. New EC2 instances are launched and terminated each hour in the account. The account also includes existing EC2 instances that have been running for longer than a week. The company&#x27;s security policy requires all running EC2 instances to use an EC2 instance profile. If an EC2 instance does not have an instance profile attached, the EC2 instance must use a default instance profile that has no IAM permissions assigned. A DevOps engineer reviews the account and discovers EC2 instances that are running without an instance profile. During the review, the DevOps engineer also observes that new EC2 instances are being launched without an instance profile. Which solution will ensure that an instance profile is attached to all existing and future EC2 instances in the Region? Manager Automation runbook to attach the default instance profile to the EC2 instances B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure an Amazon EventBridge rule that reacts to EC2 RunInstances API calls. Configure the rule to invoke an AWS Lambda function to attach the default instance profile to the EC2 instances.
B. Configure the ec2-instance-profile-attached AWS Config managed rule with a trigger type of configuration changes. Configure an automatic remediation action that invokes an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances.
C. Configure an Amazon EventBridge rule that reacts to EC2 StartInstances API calls. Configure the rule to invoke an AWS Systems
D. Configure the iam-role-managed-policy-check AWS Config managed rule with a trigger type of configuration changes. Configure an automatic remediation action that invokes an AWS Lambda function to attach the default instance profile to the EC2 instances.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在单个AWS区域的单个AWS账户中运行数百个Amazon EC2实例。该账户中每小时都会启动和终止新的EC2实例。该账户还包括已经运行超过一周的现有EC2实例。公司的安全策略要求所有运行的EC2实例都必须使用EC2实例配置文件。如果EC2实例没有附加实例配置文件，该EC2实例必须使用没有分配IAM权限的默认实例配置文件。DevOps工程师审查账户时发现有EC2实例在没有实例配置文件的情况下运行。在审查过程中，DevOps工程师还观察到新的EC2实例在启动时没有实例配置文件。哪种解决方案能确保实例配置文件附加到该区域中所有现有和未来的EC2实例？ 选项： A. 配置Amazon EventBridge规则来响应EC2 RunInstances API调用。配置规则调用AWS Lambda函数将默认实例配置文件附加到EC2实例。 B. 配置ec2-instance-profile-attached AWS Config托管规则，触发类型为配置更改。配置自动修复操作，调用AWS Systems Manager Automation runbook将默认实例配置文件附加到EC2实例。 C. 配置Amazon EventBridge规则来响应EC2 StartInstances API调用。配置规则调用AWS Systems Manager Automation runbook将默认实例配置文件附加到EC2实例。 D. 配置iam-role-managed-policy-check AWS Config托管规则，触发类型为配置更改。配置自动修复操作，调用AWS Lambda函数将默认实例配置文件附加到EC2实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求找到一个解决方案，确保所有现有和未来的EC2实例都附加实例配置文件。关键需求包括：1）处理已经运行的实例（现有实例）；2）处理新启动的实例（未来实例）；3）自动检测和修复缺少实例配置文件的情况。 **涉及的关键AWS服务和概念：** - AWS Config：用于监控和评估AWS资源配置的合规性 - Amazon EventBridge：事件驱动的服务，可响应AWS API调用 - AWS Systems Manager Automation：自动化运维任务的服务 - EC2实例配置文件：允许EC2实例承担IAM角色的机制 - AWS Lambda：无服务器计算服务 **正确答案B的原因：** 1. **ec2-instance-profile-attached规则**：这是专门检查EC2实例是否附加了实例配置文件的AWS Config托管规则，完全符合需求 2. **配置更改触发**：能够检测到现有实例和新实例的配置状态变化 3. **自动修复机制**：AWS Config的自动修复功能可以在检测到不合规时立即采取行动 4. **Systems Manager Automation**：提供可靠的自动化执行能力，适合批量操作 5. **全面覆盖**：既能处理现有实例，也能处理新启动的实例 **其他选项错误的原因：** - **选项A**：RunInstances API只能捕获新实例启动事件，无法处理已经运行的现有实例 - **选项C**：StartInstances API只响应实例启动事件，同样无法处理现有实例，且不能检测实例配置文件的缺失 - **选项D**：iam-role-managed-policy-check规则主要检查IAM角色的托管策略，不是专门检查实例配置文件附加状态的规则 **决策标准和最佳实践：** 1. **选择专用的Config规则**：使用针对特定需求设计的托管规则 2. **持续合规监控**：AWS Config提供持续的配置监控，比事件驱动的方法更全面 3. **自动化修复**：减少人工干预，提高运维效率 4. **全生命周期覆盖**：解决方案应该覆盖现有资源和新资源</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">14</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is building a continuous deployment pipeline for a serverless application that uses AWS Lambda functions. The company wants to reduce the customer impact of an unsuccessful deployment. The company also wants to monitor for issues. Which deploy stage configuration will meet these requirements? A (81%) D (19%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use an AWS Serverless Application Model (AWS SAM) template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent15Minutes Deployment Preference Type. Use Amazon CloudWatch alarms to monitor the health of the functions.
B. Use AWS CloudFormation to publish a new stack update, and include Amazon CloudWatch alarms on all resources. Set up an AWS CodePipeline approval action for a developer to verify and approve the AWS CloudFormation change set.
C. Use AWS CloudFormation to publish a new version on every stack update, and include Amazon CloudWatch alarms on all resources. Use the RoutingConfig property of the AWS::Lambda::Alias resource to update the traffic routing during the stack update.
D. Use AWS CodeBuild to add sample event payloads for testing to the Lambda functions. Publish a new version of the functions, and include Amazon CloudWatch alarms. Update the production alias to point to the new version. Configure rollbacks to occur when an alarm is in the ALARM state.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在为使用AWS Lambda函数的无服务器应用程序构建持续部署管道。公司希望减少部署失败对客户的影响。公司还希望监控问题。哪种部署阶段配置能满足这些要求？ 选项： A. 使用AWS Serverless Application Model (AWS SAM)模板定义无服务器应用程序。使用AWS CodeDeploy部署Lambda函数，采用Canary10Percent15Minutes部署偏好类型。使用Amazon CloudWatch告警监控函数健康状况。 B. 使用AWS CloudFormation发布新的堆栈更新，并在所有资源上包含Amazon CloudWatch告警。设置AWS CodePipeline批准操作，让开发人员验证和批准AWS CloudFormation变更集。 C. 使用AWS CloudFormation在每次堆栈更新时发布新版本，并在所有资源上包含Amazon CloudWatch告警。使用AWS::Lambda::Alias资源的RoutingConfig属性在堆栈更新期间更新流量路由。 D. 使用AWS CodeBuild向Lambda函数添加用于测试的示例事件负载。发布函数的新版本，并包含Amazon CloudWatch告警。更新生产别名指向新版本。配置在告警处于ALARM状态时进行回滚。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何为无服务器应用程序设计安全的持续部署策略，重点关注两个需求：1）减少部署失败对客户的影响；2）提供监控能力。 **涉及的关键AWS服务和概念：** - AWS Lambda函数部署策略 - AWS SAM (Serverless Application Model) - AWS CodeDeploy的金丝雀部署 - Amazon CloudWatch监控和告警 - AWS CloudFormation堆栈管理 - Lambda别名和版本管理 **正确答案A的原因：** 选项A是最佳解决方案，因为： 1. **AWS SAM**专门为无服务器应用程序设计，提供简化的部署模板 2. **Canary10Percent15Minutes部署策略**实现渐进式部署：先将10%流量路由到新版本，15分钟后如果没有问题再完全切换，这最大程度减少了客户影响 3. **AWS CodeDeploy**原生支持Lambda的蓝绿部署和金丝雀部署，具备自动回滚能力 4. **CloudWatch告警**提供实时监控，可以触发自动回滚 **其他选项错误的原因：** - **选项B**：依赖手动批准，不是自动化的持续部署，无法快速响应问题 - **选项C**：虽然使用了RoutingConfig，但CloudFormation的堆栈更新方式不如CodeDeploy的部署策略灵活和安全 - **选项D**：CodeBuild主要用于构建，不是专门的部署工具；手动更新别名缺乏渐进式部署的安全机制 **决策标准和最佳实践：** 1. **渐进式部署**：使用金丝雀或蓝绿部署减少风险 2. **自动化监控**：结合CloudWatch实现自动检测和回滚 3. **专用工具**：选择最适合特定场景的AWS服务（SAM用于无服务器，CodeDeploy用于部署） 4. **最小化客户影响**：通过流量分割和快速回滚机制保护生产环境</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">15</div>
        <div class="field-label">Question:</div>
        <div class="field-content">To run an application, a DevOps engineer launches an Amazon EC2 instance with public IP addresses in a public subnet. A user data script obtains the application artifacts and installs them on the instances upon launch. A change to the security classification of the application now requires the instances to run with no access to the internet. While the instances launch successfully and show as healthy, the application does not seem to be installed. Which of the following should successfully install the application while complying with the new rule? C (90%) 10%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Launch the instances in a public subnet with Elastic IP addresses attached. Once the application is installed and running, run a script to disassociate the Elastic IP addresses afterwards.
B. Set up a NAT gateway. Deploy the EC2 instances to a private subnet. Update the private subnet&#x27;s route table to use the NAT gateway as the default route.
C. Publish the application artifacts to an Amazon S3 bucket and create a VPC endpoint for S3. Assign an IAM instance profile to the EC2 instances so they can read the application artifacts from the S3 bucket.
D. Create a security group for the application instances and allow only outbound traffic to the artifact repository. Remove the security group rule once the install is complete.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">为了运行一个应用程序，DevOps工程师在公有子网中启动了一个带有公网IP地址的Amazon EC2实例。用户数据脚本在启动时获取应用程序工件并将其安装在实例上。应用程序安全分类的变更现在要求实例在没有互联网访问的情况下运行。虽然实例启动成功并显示为健康状态，但应用程序似乎没有安装。以下哪个选项能够在遵守新规则的同时成功安装应用程序？ 选项： A. 在公有子网中启动实例并附加Elastic IP地址。一旦应用程序安装并运行，运行脚本来解除Elastic IP地址的关联。 B. 设置NAT gateway。将EC2实例部署到私有子网。更新私有子网的路由表以使用NAT gateway作为默认路由。 C. 将应用程序工件发布到Amazon S3存储桶并为S3创建VPC endpoint。为EC2实例分配IAM实例配置文件，以便它们可以从S3存储桶读取应用程序工件。 D. 为应用程序实例创建安全组，仅允许到工件仓库的出站流量。安装完成后删除安全组规则。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在严格的网络安全要求下（完全禁止互联网访问）如何让EC2实例获取和安装应用程序的解决方案。关键约束是实例不能有任何互联网访问权限，但仍需要获取应用程序工件进行安装。 **涉及的关键AWS服务和概念：** - EC2实例和用户数据脚本 - VPC网络架构（公有子网vs私有子网） - VPC Endpoint（特别是S3的VPC Endpoint） - NAT Gateway - IAM实例配置文件和权限管理 - S3存储服务 - 网络安全和访问控制 **正确答案C的原因：** 1. **完全符合安全要求**：使用VPC Endpoint for S3可以让EC2实例通过AWS内部网络访问S3，完全不需要互联网连接 2. **架构合理**：将应用程序工件存储在S3中是最佳实践，便于版本管理和分发 3. **安全性最佳**：通过IAM实例配置文件控制访问权限，遵循最小权限原则 4. **可持续性**：这种架构不仅解决当前问题，还为未来的应用更新提供了安全的通道 **其他选项错误的原因：** - **选项A**：虽然最终会移除Elastic IP，但在安装过程中仍然违反了&quot;不能访问互联网&quot;的安全要求 - **选项B**：NAT Gateway本质上仍然提供互联网访问能力，违反了新的安全分类要求 - **选项D**：安全组规则无法绕过基本的网络连通性问题，如果没有互联网访问，仍然无法到达外部的工件仓库 **决策标准和最佳实践：** 1. **零信任网络原则**：在高安全要求下，应该完全消除互联网访问而不是临时允许 2. **AWS服务集成**：优先使用AWS原生服务的内部连接能力（如VPC Endpoint） 3. **权限最小化**：使用IAM角色精确控制资源访问权限 4. **架构的可维护性**：选择能够长期满足安全要求的解决方案，而不是临时性的变通方法</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">16</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A development team is using AWS CodeCommit to version control application code and AWS CodePipeline to orchestrate software deployments. The team has decided to use a remote main branch as the trigger for the pipeline to integrate code changes. A developer has pushed code changes to the CodeCommit repository, but noticed that the pipeline had no reaction, even after 10 minutes. Which of the following actions should be taken to troubleshoot this issue? A (58%) B (39%) 4%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Check that an Amazon EventBridge rule has been created for the main branch to trigger the pipeline.
B. Check that the CodePipeline service role has permission to access the CodeCommit repository.
C. Check that the developer&#x27;s IAM role has permission to push to the CodeCommit repository.
D. Check to see if the pipeline failed to start because of CodeCommit errors in Amazon CloudWatch Logs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个开发团队正在使用AWS CodeCommit进行应用程序代码版本控制，使用AWS CodePipeline来编排软件部署。团队决定使用远程main分支作为触发器来触发pipeline以集成代码更改。一个开发人员已经将代码更改推送到CodeCommit存储库，但注意到pipeline没有反应，即使在10分钟后也是如此。应该采取以下哪项操作来排查这个问题？ 选项： A. 检查是否已为main分支创建了Amazon EventBridge规则来触发pipeline。 B. 检查CodePipeline服务角色是否有权限访问CodeCommit存储库。 C. 检查开发人员的IAM角色是否有权限推送到CodeCommit存储库。 D. 检查在Amazon CloudWatch Logs中pipeline是否因为CodeCommit错误而启动失败。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodePipeline与CodeCommit集成时的触发机制故障排查。当代码推送到指定分支后，pipeline没有自动触发执行的问题诊断。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：Git代码存储库服务 - AWS CodePipeline：持续集成/持续部署(CI/CD)管道服务 - Amazon EventBridge：事件驱动架构服务，用于处理应用程序事件 - IAM权限：身份和访问管理 - CloudWatch Logs：日志监控服务 **正确答案A的原因：** CodePipeline通过Amazon EventBridge规则来监听CodeCommit存储库的推送事件。当代码推送到指定分支时，EventBridge规则会捕获这个事件并触发pipeline执行。如果没有正确配置EventBridge规则或规则配置错误（比如监听错误的分支），pipeline就不会被触发。这是pipeline无反应的最可能原因。 **其他选项错误的原因：** - 选项B：如果CodePipeline服务角色没有访问CodeCommit的权限，pipeline会启动但在执行阶段失败，而不是完全没有反应。 - 选项C：开发人员已经成功推送了代码，说明推送权限是正常的，这不是pipeline未触发的原因。 - 选项D：如果pipeline因错误而启动失败，至少会有启动的迹象和相关日志，但题目描述的是&quot;没有反应&quot;，说明pipeline根本没有被触发。 **决策标准和最佳实践：** 1. 故障排查应该按照事件流程顺序进行：首先确认触发机制是否正常 2. 区分&quot;未触发&quot;和&quot;触发后失败&quot;两种不同的故障模式 3. 在设置CodePipeline时，必须正确配置EventBridge规则来监听特定分支的推送事件 4. 定期验证CI/CD管道的触发机制是否正常工作</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">17</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s developers use Amazon EC2 instances as remote workstations. The company is concerned that users can create or modify EC2 security groups to allow unrestricted inbound access. A DevOps engineer needs to develop a solution to detect when users create unrestricted security group rules. The solution must detect changes to security group rules in near real time, remove unrestricted rules, and send email notifications to the security team. The DevOps engineer has created an AWS Lambda function that checks for security group ID from input, removes rules that grant unrestricted access, and sends notifications through Amazon Simple Notification Service (Amazon SNS). What should the DevOps engineer do next to meet the requirements? invoked by the custom event bus. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure the Lambda function to be invoked by the SNS topic. Create an AWS CloudTrail subscription for the SNS topic. Configure a subscription filter for security group modification events.
B. Create an Amazon EventBridge scheduled rule to invoke the Lambda function. Define a schedule pattern that runs the Lambda function every hour.
C. Create an Amazon EventBridge event rule that has the default event bus as the source. Define the rule&#x27;s event pattern to match EC2 security group creation and modification events. Configure the rule to invoke the Lambda function.
D. Create an Amazon EventBridge custom event bus that subscribes to events from all AWS services. Configure the Lambda function to be</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的开发人员使用Amazon EC2实例作为远程工作站。公司担心用户可能会创建或修改EC2 security groups以允许不受限制的入站访问。DevOps工程师需要开发一个解决方案来检测用户何时创建不受限制的security group规则。该解决方案必须近实时检测security group规则的更改，删除不受限制的规则，并向安全团队发送电子邮件通知。DevOps工程师已经创建了一个AWS Lambda函数，该函数检查输入中的security group ID，删除授予不受限制访问的规则，并通过Amazon Simple Notification Service (Amazon SNS)发送通知。DevOps工程师接下来应该做什么来满足要求？ 选项： A. 配置Lambda函数由SNS主题调用。为SNS主题创建AWS CloudTrail订阅。为security group修改事件配置订阅过滤器。 B. 创建Amazon EventBridge计划规则来调用Lambda函数。定义每小时运行Lambda函数的计划模式。 C. 创建以默认事件总线为源的Amazon EventBridge事件规则。定义规则的事件模式以匹配EC2 security group创建和修改事件。配置规则调用Lambda函数。 D. 创建订阅所有AWS服务事件的Amazon EventBridge自定义事件总线。配置Lambda函数被自定义事件总线调用。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要建立一个近实时的监控系统，能够检测EC2 security group规则的创建和修改，特别是那些允许不受限制访问的规则，并自动触发Lambda函数进行处理。 **涉及的关键AWS服务和概念：** - Amazon EventBridge：事件驱动架构的核心服务，用于捕获和路由AWS服务事件 - AWS Lambda：无服务器计算服务，用于执行自动化响应 - EC2 Security Groups：虚拟防火墙，控制EC2实例的网络访问 - 事件驱动架构：基于事件触发的自动化响应机制 **正确答案C的原因：** 1. **事件驱动的实时性**：EventBridge能够捕获AWS API调用产生的事件，当security group被创建或修改时立即触发 2. **精确的事件匹配**：可以通过事件模式精确匹配EC2 security group的创建和修改事件 3. **默认事件总线**：AWS服务事件默认发布到默认事件总线，无需额外配置 4. **直接集成**：EventBridge可以直接调用Lambda函数，架构简洁高效 **其他选项错误的原因：** - **选项A**：CloudTrail主要用于API调用日志记录，不是实时事件处理的最佳选择，且SNS作为触发源的配置复杂且不直接 - **选项B**：定时调度（每小时）无法满足&quot;近实时&quot;的要求，存在最多1小时的延迟 - **选项D**：自定义事件总线增加了不必要的复杂性，对于AWS原生服务事件，默认事件总线已足够 **决策标准和最佳实践：** 1. **实时性优先**：选择事件驱动而非定时轮询 2. **架构简洁性**：使用默认事件总线而非自定义总线 3. **精确匹配**：通过事件模式精确过滤所需事件类型 4. **成本效益**：避免不必要的复杂配置和额外组件</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">18</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is creating an AWS CloudFormation template to deploy a web service. The web service will run on Amazon EC2 instances in a private subnet behind an Application Load Balancer (ALB). The DevOps engineer must ensure that the service can accept requests from clients that have IPv6 addresses. What should the DevOps engineer do with the CloudFormation template so that IPv6 clients can access the web service? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add an IPv6 CIDR block to the VPC and the private subnet for the EC2 instances. Create route table entries for the IPv6 network, use EC2 instance types that support IPv6, and assign IPv6 addresses to each EC2 instance.
B. Assign each EC2 instance an IPv6 Elastic IP address. Create a target group, and add the EC2 instances as targets. Create a listener on port 443 of the ALB, and associate the target group with the ALB.
C. Replace the ALB with a Network Load Balancer (NLB). Add an IPv6 CIDR block to the VPC and subnets for the NLB, and assign the NLB an IPv6 Elastic IP address.
D. Add an IPv6 CIDR block to the VPC and subnets for the ALB. Create a listener on port 443, and specify the dualstack IP address type on the ALB. Create a target group, and add the EC2 instances as targets. Associate the target group with the ALB.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师正在创建AWS CloudFormation模板来部署Web服务。该Web服务将在Application Load Balancer (ALB)后面的私有子网中的Amazon EC2实例上运行。DevOps工程师必须确保该服务能够接受来自具有IPv6地址的客户端的请求。DevOps工程师应该如何处理CloudFormation模板，以便IPv6客户端可以访问Web服务？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求配置AWS基础设施以支持IPv6客户端访问位于私有子网中的Web服务，该服务通过Application Load Balancer提供访问。 **涉及的关键AWS服务和概念：** - Application Load Balancer (ALB)：七层负载均衡器，支持IPv4和IPv6双栈 - VPC IPv6 CIDR块：为VPC和子网分配IPv6地址空间 - EC2实例：运行在私有子网中的Web服务 - Target Group：ALB的目标组，用于路由流量 - Dualstack IP地址类型：ALB支持IPv4和IPv6双栈模式 **正确答案D的原因：** 选项D提供了完整且正确的IPv6支持配置： 1. 为VPC和ALB所在子网添加IPv6 CIDR块，提供IPv6地址空间 2. 在ALB上指定dualstack IP地址类型，使其同时支持IPv4和IPv6 3. 创建端口443监听器，支持HTTPS流量 4. 创建目标组并添加EC2实例作为目标 5. 将目标组与ALB关联，完成流量路由配置 **其他选项错误的原因：** - 选项A：只配置了后端EC2实例的IPv6支持，但没有配置ALB的IPv6支持，IPv6客户端仍无法通过ALB访问服务 - 选项B：IPv6 Elastic IP地址不能直接分配给EC2实例，且没有配置VPC的IPv6 CIDR块，缺少基础IPv6网络配置 - 选项C：虽然NLB支持IPv6，但题目明确要求使用ALB，且ALB完全可以支持IPv6，没有必要更换负载均衡器类型 **决策标准和最佳实践：** 1. IPv6支持需要从网络层（VPC CIDR）到应用层（负载均衡器）的完整配置 2. ALB的dualstack模式是支持IPv6客户端访问的标准做法 3. 私有子网中的EC2实例不需要直接的IPv6配置，通过ALB代理即可 4. 保持架构简单性，在满足需求的前提下避免不必要的组件更换</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">19</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Organizations and AWS Control Tower to manage all the company&#x27;s AWS accounts. The company uses the Enterprise Support plan. A DevOps engineer is using Account Factory for Terraform (AFT) to provision new accounts. When new accounts are provisioned, the DevOps engineer notices that the support plan for the new accounts is set to the Basic Support plan. The DevOps engineer needs to implement a solution to provision the new accounts with the Enterprise Support plan. Which solution will meet these requirements? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use an AWS Config conformance pack to deploy the account-part-of-organizations AWS Config rule and to automatically remediate any noncompliant accounts.
B. Create an AWS Lambda function to create a ticket for AWS Support to add the account to the Enterprise Support plan. Grant the Lambda function the support:ResolveCase permission.
C. Add an additional value to the control_tower_parameters input to set the AWSEnterpriseSupport parameter as the organization&#x27;s management account number.
D. Set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration. Redeploy AFT and apply the changes.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations和AWS Control Tower来管理公司所有的AWS账户。该公司使用Enterprise Support计划。一名DevOps工程师正在使用Account Factory for Terraform (AFT)来预配置新账户。当预配置新账户时，DevOps工程师注意到新账户的支持计划被设置为Basic Support计划。DevOps工程师需要实施一个解决方案，使新账户能够预配置Enterprise Support计划。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题要求在使用AFT预配置新AWS账户时，自动将新账户设置为Enterprise Support计划，而不是默认的Basic Support计划。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Control Tower：提供预配置的多账户环境治理 - Account Factory for Terraform (AFT)：基于Terraform的账户预配置工具 - AWS Support计划：包括Basic、Developer、Business和Enterprise等级别 - Enterprise Support：AWS最高级别的支持服务 **正确答案D的原因：** AFT提供了专门的功能标志`aft_feature_enterprise_support`来处理Enterprise Support的自动配置。将此标志设置为True并重新部署AFT是官方推荐的标准做法，能够确保所有通过AFT创建的新账户自动继承组织的Enterprise Support计划。这是AFT的内置功能，专门为解决此类问题而设计。 **其他选项错误的原因：** - 选项A：AWS Config规则主要用于合规性检查，`account-part-of-organizations`规则只是验证账户是否属于组织，无法直接配置支持计划。 - 选项B：手动创建Lambda函数和支持工单的方式过于复杂且不可靠，`support:ResolveCase`权限也不是用于添加支持计划的正确权限。 - 选项C：`control_tower_parameters`中没有`AWSEnterpriseSupport`这样的参数，这个配置方式不存在。 **决策标准和最佳实践：** 在AWS多账户环境中，应该优先使用官方提供的自动化工具和功能标志来管理账户配置，而不是自建复杂的解决方案。AFT的功能标志提供了最直接、最可靠的方式来确保账户预配置时的一致性配置。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">20</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s DevOps engineer uses AWS Systems Manager to perform maintenance tasks during maintenance windows. The company has a few Amazon EC2 instances that require a restart after notifications from AWS Health. The DevOps engineer needs to implement an automated solution to remediate these notifications. The DevOps engineer creates an Amazon EventBridge rule. How should the DevOps engineer configure the EventBridge rule to meet these requirements? A (64%) C (32%) 3%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure an event source of AWS Health, a service of EC2, and an event type that indicates instance maintenance. Target a Systems Manager document to restart the EC2 instance.
B. Configure an event source of Systems Manager and an event type that indicates a maintenance window. Target a Systems Manager document to restart the EC2 instance.
C. Configure an event source of AWS Health, a service of EC2, and an event type that indicates instance maintenance. Target a newly created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window.
D. Configure an event source of EC2 and an event type that indicates instance maintenance. Target a newly created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的DevOps工程师使用AWS Systems Manager在维护窗口期间执行维护任务。该公司有一些Amazon EC2实例在收到AWS Health通知后需要重启。DevOps工程师需要实现一个自动化解决方案来修复这些通知。DevOps工程师创建了一个Amazon EventBridge规则。DevOps工程师应该如何配置EventBridge规则来满足这些要求？ 选项： A. 配置事件源为AWS Health，服务为EC2，事件类型指示实例维护。目标为Systems Manager文档来重启EC2实例。 B. 配置事件源为Systems Manager，事件类型指示维护窗口。目标为Systems Manager文档来重启EC2实例。 C. 配置事件源为AWS Health，服务为EC2，事件类型指示实例维护。目标为新创建的AWS Lambda函数，该函数注册自动化任务在维护窗口期间重启EC2实例。 D. 配置事件源为EC2，事件类型指示实例维护。目标为新创建的AWS Lambda函数，该函数注册自动化任务在维护窗口期间重启EC2实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要创建一个自动化解决方案，当AWS Health发出EC2实例维护通知时，能够自动重启相关的EC2实例。关键是要正确配置EventBridge规则来监听AWS Health事件并触发适当的修复操作。 **涉及的关键AWS服务和概念：** - AWS Health：提供AWS服务和资源的个性化健康状态信息 - Amazon EventBridge：事件驱动的无服务器服务，用于连接应用程序与各种数据源 - AWS Systems Manager：提供运维数据的统一界面，可以自动化常见维护任务 - EC2实例维护：AWS定期对底层硬件进行维护，可能需要实例重启 **正确答案A的原因：** 1. **正确的事件源**：AWS Health是发出维护通知的源头，这是触发自动化的正确起点 2. **准确的服务和事件类型**：指定EC2服务和实例维护事件类型，确保只响应相关的维护通知 3. **直接有效的目标**：直接使用Systems Manager文档执行重启操作，这是最简单、最直接的解决方案 4. **符合最佳实践**：利用AWS原生服务的集成能力，减少复杂性 **其他选项错误的原因：** - **选项B**：事件源错误。Systems Manager不会发出维护通知，AWS Health才是维护事件的源头 - **选项C**：虽然事件源正确，但增加了不必要的Lambda函数复杂性。题目已明确提到要在维护窗口执行任务，直接使用Systems Manager文档更简洁 - **选项D**：事件源错误。EC2服务本身不直接发出维护通知，这些通知来自AWS Health服务 **决策标准和最佳实践：** 1. **事件源选择**：始终从实际发出事件的服务开始配置（AWS Health） 2. **简化原则**：在满足需求的前提下选择最简单的解决方案 3. **服务集成**：优先使用AWS原生服务之间的直接集成，而不是添加中间层 4. **自动化效率**：直接的EventBridge到Systems Manager的集成比通过Lambda的间接方式更高效</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">21</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has containerized all of its in-house quality control applications. The company is running Jenkins on Amazon EC2 instances, which require patching and upgrading. The compliance officer has requested a DevOps engineer begin encrypting build artifacts since they contain company intellectual property. What should the DevOps engineer do to accomplish this in the MOST maintainable manner? D (83%) B (17%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Automate patching and upgrading using AWS Systems Manager on EC2 instances and encrypt Amazon EBS volumes by default.
B. Deploy Jenkins to an Amazon ECS cluster and copy build artifacts to an Amazon S3 bucket with default encryption enabled.
C. Leverage AWS CodePipeline with a build action and encrypt the artifacts using AWS Secrets Manager.
D. Use AWS CodeBuild with artifact encryption to replace the Jenkins instance running on EC2 instances.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司已经将其所有内部质量控制应用程序容器化。该公司正在Amazon EC2实例上运行Jenkins，这些实例需要打补丁和升级。合规官要求DevOps工程师开始加密构建产物，因为它们包含公司知识产权。DevOps工程师应该如何以最可维护的方式完成此任务？ 选项： A. 使用AWS Systems Manager在EC2实例上自动化打补丁和升级，并默认加密Amazon EBS卷。 B. 将Jenkins部署到Amazon ECS集群，并将构建产物复制到启用了默认加密的Amazon S3存储桶。 C. 利用AWS CodePipeline与构建操作，并使用AWS Secrets Manager加密产物。 D. 使用带有产物加密功能的AWS CodeBuild来替换运行在EC2实例上的Jenkins实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到最可维护的方式来解决两个主要问题：1）消除Jenkins在EC2上需要打补丁和升级的维护负担；2）对包含知识产权的构建产物进行加密。关键词是&quot;最可维护的方式&quot;。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：完全托管的构建服务，无需管理基础设施 - Jenkins：开源CI/CD工具，在EC2上运行需要维护 - Amazon ECS：容器编排服务，仍需要一定程度的管理 - AWS CodePipeline：CI/CD管道服务 - 构建产物加密：保护知识产权的安全要求 - 托管服务 vs 自管理服务的维护成本 **正确答案D的原因：** 1. **完全托管**：CodeBuild是完全托管的服务，AWS负责所有基础设施的维护、打补丁和升级，完全消除了维护负担 2. **内置加密**：CodeBuild原生支持构建产物加密，无需额外配置复杂的加密机制 3. **容器支持**：CodeBuild天然支持容器化应用的构建 4. **最小维护**：符合题目要求的&quot;最可维护&quot;标准，运维开销最小 **其他选项错误的原因：** - **选项A**：虽然自动化了维护，但仍然需要管理EC2实例，没有从根本上解决维护问题，且EBS加密不等同于构建产物加密 - **选项B**：ECS虽然减少了一些维护工作，但仍需要管理集群和容器，维护复杂度高于完全托管服务 - **选项C**：CodePipeline本身不是构建服务，Secrets Manager主要用于密钥管理而非产物加密，概念混淆 **决策标准和最佳实践：** 1. **优先选择托管服务**：在满足功能需求的前提下，托管服务比自管理服务更易维护 2. **安全性集成**：选择原生支持所需安全功能的服务，避免复杂的自定义实现 3. **总体拥有成本**：考虑长期的运维成本，而不仅仅是初始实施成本 4. **服务适配性**：选择专门为特定用途设计的服务（CodeBuild专为构建设计）</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">22</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An IT team has built an AWS CloudFormation template so others in the company can quickly and reliably deploy and terminate an application. The template creates an Amazon EC2 instance with a user data script to install the application and an Amazon S3 bucket that the application uses to serve static webpages while it is running. All resources should be removed when the CloudFormation stack is deleted. However, the team observes that CloudFormation reports an error during stack deletion, and the S3 bucket created by the stack is not deleted. How can the team resolve the error in the MOST efficient manner to ensure that all resources are deleted without errors? deleted. Lambda function to delete all objects from the bucket when RequestType is Delete. Most Voted and delete the EC2 instance and the S3 bucket. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a DeletionPolicy attribute to the S3 bucket resource, with the value Delete forcing the bucket to be removed when the stack is
B. Add a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the
C. Identify the resource that was not deleted. Manually empty the S3 bucket and then delete it.
D. Replace the EC2 and S3 bucket resources with a single AWS OpsWorks Stacks resource. Define a custom recipe for the stack to create</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个IT团队构建了一个AWS CloudFormation模板，以便公司其他人员能够快速可靠地部署和终止应用程序。该模板创建了一个Amazon EC2实例，带有用户数据脚本来安装应用程序，以及一个Amazon S3存储桶，应用程序在运行时使用该存储桶提供静态网页服务。当CloudFormation堆栈被删除时，所有资源都应该被移除。然而，团队观察到CloudFormation在堆栈删除过程中报告错误，由堆栈创建的S3存储桶没有被删除。团队如何以最高效的方式解决这个错误，确保所有资源都能无错误地删除？ 选项： A. 为S3存储桶资源添加DeletionPolicy属性，值为Delete，强制在堆栈删除时移除存储桶 B. 添加一个自定义资源，包含AWS Lambda函数和DependsOn属性指定S3存储桶，以及IAM角色。编写Lambda函数在RequestType为Delete时删除存储桶中的所有对象 C. 识别未被删除的资源。手动清空S3存储桶然后删除它 D. 用单个AWS OpsWorks Stacks资源替换EC2和S3存储桶资源。为堆栈定义自定义配方来创建</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求找到最高效的解决方案来确保CloudFormation堆栈删除时S3存储桶能够被正确删除。问题的根本原因是S3存储桶在非空状态下无法被删除，这是AWS的安全机制。 **涉及的关键AWS服务和概念：** 1. AWS CloudFormation - 基础设施即代码服务 2. Amazon S3 - 对象存储服务，非空存储桶无法直接删除 3. AWS Lambda - 无服务器计算服务 4. CloudFormation自定义资源 - 允许在堆栈生命周期中执行自定义逻辑 5. DeletionPolicy - CloudFormation资源删除策略 6. DependsOn属性 - 控制资源删除顺序 **正确答案B的原因：** 1. **自动化解决方案**：Lambda函数可以自动清空S3存储桶中的所有对象 2. **集成到CloudFormation中**：通过自定义资源，删除逻辑成为堆栈的一部分 3. **正确的执行时机**：RequestType为Delete时触发，确保在堆栈删除时执行 4. **依赖关系管理**：DependsOn确保在删除S3存储桶之前先清空内容 5. **可重复使用**：一次配置，每次堆栈删除都自动执行 **其他选项错误的原因：** - **选项A错误**：DeletionPolicy的Delete值是默认行为，不能解决非空存储桶无法删除的根本问题 - **选项C错误**：手动操作不符合&quot;最高效&quot;的要求，且破坏了自动化流程 - **选项D错误**：OpsWorks是完全不同的服务，这种替换是不必要的复杂化，且没有解决S3删除问题 **决策标准和最佳实践：** 1. **自动化优先**：避免手动干预，保持基础设施即代码的完整性 2. **最小变更原则**：在现有架构基础上添加必要功能，而非重构整个解决方案 3. **AWS服务集成**：利用CloudFormation自定义资源功能实现复杂的生命周期管理 4. **错误预防**：通过程序化方式处理已知的AWS服务限制（如S3非空存储桶删除限制）</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">23</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an AWS CodePipeline pipeline that is configured with an Amazon S3 bucket in the eu-west-1 Region. The pipeline deploys an AWS Lambda application to the same Region. The pipeline consists of an AWS CodeBuild project build action and an AWS CloudFormation deploy action. The CodeBuild project uses the aws cloudformation package AWS CLI command to build an artifact that contains the Lambda function code&#x27;s .zip file and the CloudFormation template. The CloudFormation deploy action references the CloudFormation template from the output artifact of the CodeBuild project&#x27;s build action. The company wants to also deploy the Lambda application to the us-east-1 Region by using the pipeline in eu-west-1. A DevOps engineer has already updated the CodeBuild project to use the aws cloudformation package command to produce an additional output artifact for us-east- 1. Which combination of additional steps should the DevOps engineer take to meet these requirements? (Choose two.) CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to pass in the us-east-1 artifact location as a parameter override. Most Voted template from the us-east-1 output artifact. Most Voted CE (68%) AB (17%) Other</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the CloudFormation template to include a parameter for the Lambda function code&#x27;s zip file location. Create a new
B. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation
C. Create an S3 bucket in us-east-1. Configure the S3 bucket policy to allow CodePipeline to have read and write access.
D. Create an S3 bucket in us-east-1. Configure S3 Cross-Region Replication (CRR) from the S3 bucket in eu-west-1 to the S3 bucket in us-east-1.
E. Modify the pipeline to include the S3 bucket for us-east-1 as an artifact store. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the us-east-1 output artifact.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个AWS CodePipeline流水线，配置了位于eu-west-1区域的Amazon S3存储桶。该流水线将AWS Lambda应用程序部署到同一区域。流水线包含一个AWS CodeBuild项目构建操作和一个AWS CloudFormation部署操作。CodeBuild项目使用aws cloudformation package AWS CLI命令构建包含Lambda函数代码.zip文件和CloudFormation模板的构件。CloudFormation部署操作引用来自CodeBuild项目构建操作输出构件的CloudFormation模板。公司希望通过eu-west-1的流水线同时将Lambda应用程序部署到us-east-1区域。DevOps工程师已经更新了CodeBuild项目，使用aws cloudformation package命令为us-east-1生成额外的输出构件。DevOps工程师应该采取哪些额外步骤的组合来满足这些要求？（选择两个） 选项： A. 修改CloudFormation模板以包含Lambda函数代码zip文件位置的参数。为us-east-1在流水线中创建新的CloudFormation部署操作。配置新的部署操作以将us-east-1构件位置作为参数覆盖传入。 B. 为us-east-1在流水线中创建新的CloudFormation部署操作。配置新的部署操作使用来自us-east-1输出构件的CloudFormation模板。 C. 在us-east-1创建S3存储桶。配置S3存储桶策略以允许CodePipeline具有读写访问权限。 D. 在us-east-1创建S3存储桶。配置从eu-west-1的S3存储桶到us-east-1的S3存储桶的S3跨区域复制(CRR)。 E. 修改流水线以包含us-east-1的S3存储桶作为构件存储。为us-east-1在流水线中创建新的CloudFormation部署操作。配置新的部署操作使用来自us-east-1输出构件的CloudFormation模板。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在现有的eu-west-1区域CodePipeline基础上，扩展部署能力以同时支持us-east-1区域的Lambda应用程序部署。关键是要理解跨区域部署的技术要求和最佳实践。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD流水线服务，需要在不同区域有构件存储 - AWS CodeBuild：构建服务，已配置生成针对不同区域的构件 - AWS CloudFormation：基础设施即代码服务，用于部署Lambda应用 - Amazon S3：对象存储服务，作为CodePipeline的构件存储 - 跨区域部署：需要在目标区域有相应的存储和部署配置 **正确答案的原因：** 正确答案应该是A和E的组合。 - 选项A：修改CloudFormation模板添加参数是必要的，因为不同区域的构件位置不同，需要通过参数传递正确的S3位置给CloudFormation模板。 - 选项E：这是跨区域部署的关键要求。CodePipeline需要在目标区域(us-east-1)有构件存储，并且需要创建新的部署操作来使用该区域特定的构件。 **其他选项错误的原因：** - 选项B：虽然需要创建新的部署操作，但仅此还不够，缺少构件存储的配置。 - 选项C：仅创建S3存储桶和配置策略是不完整的，没有将其集成到流水线中。 - 选项D：S3跨区域复制不是CodePipeline跨区域部署的正确方法，CodePipeline需要在每个区域有专门的构件存储配置。 **决策标准和最佳实践：** 1. 跨区域CodePipeline部署必须在每个目标区域配置构件存储 2. CloudFormation模板需要参数化以支持不同区域的资源引用 3. 每个区域需要独立的部署操作，使用该区域特定的构件 4. 避免使用S3复制等间接方法，应直接配置CodePipeline的多区域支持</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">24</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs an application on one Amazon EC2 instance. Application metadata is stored in Amazon S3 and must be retrieved if the instance is restarted. The instance must restart or relaunch automatically if the instance becomes unresponsive. Which solution will meet these requirements? metadata to the instance when the instance is back up and running. B (96%) 4%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon CloudWatch alarm for the StatusCheckFailed metric. Use the recover action to stop and start the instance. Use an S3 event notification to push the metadata to the instance when the instance is back up and running.
B. Configure AWS OpsWorks, and use the auto healing feature to stop and start the instance. Use a lifecycle event in OpsWorks to pull the metadata from Amazon S3 and update it on the instance.
C. Use EC2 Auto Recovery to automatically stop and start the instance in case of a failure. Use an S3 event notification to push the
D. Use AWS CloudFormation to create an EC2 instance that includes the UserData property for the EC2 resource. Add a command in UserData to retrieve the application metadata from Amazon S3.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在一个Amazon EC2实例上运行应用程序。应用程序元数据存储在Amazon S3中，如果实例重启，必须检索这些元数据。如果实例变得无响应，实例必须自动重启或重新启动。哪个解决方案能满足这些要求？ 选项： A. 为StatusCheckFailed指标创建Amazon CloudWatch告警。使用恢复操作来停止和启动实例。当实例重新运行时，使用S3事件通知将元数据推送到实例。 B. 配置AWS OpsWorks，并使用自动修复功能来停止和启动实例。在OpsWorks中使用生命周期事件从Amazon S3拉取元数据并在实例上更新。 C. 使用EC2 Auto Recovery在故障情况下自动停止和启动实例。使用S3事件通知将元数据推送到实例。 D. 使用AWS CloudFormation创建包含EC2资源UserData属性的EC2实例。在UserData中添加命令从Amazon S3检索应用程序元数据。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 单个EC2实例运行应用程序 2. 应用程序元数据存储在S3中，实例重启后需要检索 3. 实例无响应时需要自动重启/重新启动 4. 需要自动化的故障检测和恢复机制 **涉及的关键AWS服务和概念：** - AWS OpsWorks：应用程序管理服务，提供自动修复和生命周期管理 - CloudWatch告警：监控和告警服务 - EC2 Auto Recovery：实例自动恢复功能 - CloudFormation：基础设施即代码服务 - S3事件通知：对象存储事件触发机制 - UserData：实例启动时执行的脚本 **正确答案B的原因：** 1. **完整的自动修复机制**：OpsWorks的auto healing功能可以自动检测实例健康状态并进行修复 2. **生命周期管理**：OpsWorks提供完整的生命周期事件管理，可以在实例启动时自动执行元数据检索 3. **集成化解决方案**：OpsWorks将监控、修复和配置管理集成在一个服务中 4. **可靠的元数据检索**：通过生命周期事件确保每次实例启动时都能正确获取S3中的元数据 **其他选项错误的原因：** - **选项A**：S3事件通知无法直接推送数据到EC2实例，且CloudWatch recover action主要用于硬件故障，不适用于应用程序无响应 - **选项C**：EC2 Auto Recovery主要处理底层硬件问题，不能检测应用程序级别的无响应状态，且S3事件通知机制不可行 - **选项D**：CloudFormation和UserData只能在初始创建时执行，无法提供持续的故障检测和自动重启功能 **决策标准和最佳实践：** 1. 选择能提供应用程序级别健康检查的服务 2. 确保故障恢复和配置管理的自动化程度 3. 考虑解决方案的完整性和集成度 4. 评估服务对单实例场景的适用性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">25</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has multiple AWS accounts. The company uses AWS IAM Identity Center (AWS Single Sign-On) that is integrated with AWS Toolkit for Microsoft Azure DevOps. The attributes for access control feature is enabled in IAM Identity Center. The attribute mapping list contains two entries. The department key is mapped to ${path:enterprise.department}. The costCenter key is mapped to ${path:enterprise.costCenter}. All existing Amazon EC2 instances have a department tag that corresponds to three company departments (d1, d2, d3). A DevOps engineer must create policies based on the matching attributes. The policies must minimize administrative effort and must grant each Azure AD user access to only the EC2 instances that are tagged with the user&#x27;s respective department name. Which condition key should the DevOps engineer include in the custom permissions policies to meet these requirements? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content"></div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有多个AWS账户。该公司使用与AWS Toolkit for Microsoft Azure DevOps集成的AWS IAM Identity Center (AWS Single Sign-On)。IAM Identity Center中启用了访问控制属性功能。属性映射列表包含两个条目。department键映射到${path:enterprise.department}。costCenter键映射到${path:enterprise.costCenter}。所有现有的Amazon EC2实例都有一个department标签，对应公司的三个部门(d1, d2, d3)。DevOps工程师必须基于匹配属性创建策略。这些策略必须最小化管理工作量，并且必须授予每个Azure AD用户仅访问标记有用户各自部门名称的EC2实例。DevOps工程师应该在自定义权限策略中包含哪个条件键来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求创建IAM策略，使Azure AD用户只能访问标记有其所属部门的EC2实例，同时最小化管理工作量。 **涉及的关键AWS服务和概念：** 1. AWS IAM Identity Center (原AWS SSO) - 提供集中身份管理 2. 属性映射 - 将外部身份提供商的属性映射到AWS中 3. 基于属性的访问控制(ABAC) - 使用用户属性动态控制访问权限 4. IAM条件键 - 用于策略中的条件判断 5. EC2资源标签 - 用于资源分类和访问控制 **正确答案的原因：** 虽然题目没有显示选项，但根据场景分析，正确答案应该是使用`aws:PrincipalTag/department`条件键。原因如下： - IAM Identity Center会将Azure AD的department属性作为会话标签传递给AWS - `aws:PrincipalTag/department`可以引用用户的department属性值 - 结合`aws:RequestedRegion`或资源标签条件，可以实现基于部门的访问控制 - 这种方式支持动态访问控制，无需为每个部门创建单独的策略 **决策标准和最佳实践：** 1. **动态访问控制** - 使用ABAC而非传统的基于角色的访问控制(RBAC) 2. **最小化管理开销** - 一个策略处理所有部门，而不是每个部门一个策略 3. **属性一致性** - 确保Azure AD属性与AWS资源标签的命名一致 4. **安全最佳实践** - 实施最小权限原则，用户只能访问其部门的资源 这种解决方案体现了现代云环境中基于属性的细粒度访问控制的最佳实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">26</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company hosts a security auditing application in an AWS account. The auditing application uses an IAM role to access other AWS accounts. All the accounts are in the same organization in AWS Organizations. A recent security audit revealed that users in the audited AWS accounts could modify or delete the auditing application&#x27;s IAM role. The company needs to prevent any modification to the auditing application&#x27;s IAM role by any entity other than a trusted administrator IAM role. Which solution will meet these requirements? trusted administrator IAM role to make changes. Attach the SCP to the root of the organization. Most Voted Include a Deny statement for changes by all other IAM principals. Attach the SCP to the IAM service in each AWS account where the auditing application has an IAM role. condition that allows the trusted administrator IAM role to make changes. Attach the permissions boundary to the audited AWS accounts. condition that allows the trusted administrator IAM role to make changes. Attach the permissions boundary to the auditing application&#x27;s IAM role in the AWS accounts. A (91%) 9%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an SCP that includes a Deny statement for changes to the auditing application&#x27;s IAM role. Include a condition that allows the
B. Create an SCP that includes an Allow statement for changes to the auditing application&#x27;s IAM role by the trusted administrator IAM role.
C. Create an IAM permissions boundary that includes a Deny statement for changes to the auditing application&#x27;s IAM role. Include a
D. Create an IAM permissions boundary that includes a Deny statement for changes to the auditing application&#x27;s IAM role. Include a</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS账户中托管安全审计应用程序。该审计应用程序使用IAM角色访问其他AWS账户。所有账户都在AWS Organizations的同一个组织中。最近的安全审计显示，被审计AWS账户中的用户可以修改或删除审计应用程序的IAM角色。公司需要防止除受信任管理员IAM角色之外的任何实体对审计应用程序的IAM角色进行任何修改。哪个解决方案能满足这些要求？ 选项： A. 创建一个SCP，包含拒绝更改审计应用程序IAM角色的Deny语句。包含允许受信任管理员IAM角色进行更改的条件。将SCP附加到组织的根部。 B. 创建一个SCP，包含允许受信任管理员IAM角色更改审计应用程序IAM角色的Allow语句。将SCP附加到每个具有审计应用程序IAM角色的AWS账户中的IAM服务。 C. 创建一个IAM权限边界，包含拒绝更改审计应用程序IAM角色的Deny语句。包含允许受信任管理员IAM角色进行更改的条件。将权限边界附加到被审计的AWS账户。 D. 创建一个IAM权限边界，包含拒绝更改审计应用程序IAM角色的Deny语句。包含允许受信任管理员IAM角色进行更改的条件。将权限边界附加到审计应用程序在AWS账户中的IAM角色。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要保护审计应用程序的IAM角色不被除受信任管理员之外的任何实体修改或删除，确保审计系统的完整性和安全性。 **涉及的关键AWS服务和概念：** 1. **Service Control Policies (SCP)** - AWS Organizations的策略，用于在组织级别控制权限 2. **IAM权限边界** - 限制IAM实体最大权限的高级功能 3. **AWS Organizations** - 集中管理多个AWS账户的服务 4. **IAM角色** - 用于跨账户访问的身份验证机制 **正确答案C的原因：** 1. **权限边界的适用性** - IAM权限边界专门设计用于限制特定IAM实体的最大权限，非常适合保护特定的IAM角色 2. **精确的作用范围** - 将权限边界附加到被审计账户可以确保这些账户中的用户无法修改审计角色 3. **条件控制** - 通过包含条件语句，可以精确允许受信任管理员进行必要的更改 4. **最小权限原则** - 权限边界确保即使用户有其他权限，也无法超越边界限制 **其他选项错误的原因：** - **选项A错误** - 虽然SCP可以提供组织级别的控制，但将其附加到组织根部会影响所有账户，可能过于宽泛且影响正常操作 - **选项B错误** - SCP不能附加到特定的AWS服务，只能附加到组织单位或账户；且仅使用Allow语句无法有效阻止未授权访问 - **选项D错误** - 将权限边界直接附加到审计应用程序的IAM角色会限制该角色本身的功能，可能影响其正常的审计操作 **决策标准和最佳实践：** 1. **精确性原则** - 选择能够精确控制特定资源访问的机制 2. **最小影响原则** - 确保安全措施不会影响其他正常业务操作 3. **分层防护** - 使用权限边界作为额外的安全层来保护关键IAM角色 4. **审计合规性** - 确保安全控制措施符合审计要求和合规标准</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">27</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an on-premises application that is written in Go. A DevOps engineer must move the application to AWS. The company&#x27;s development team wants to enable blue/green deployments and perform A/B testing. Which solution will meet these requirements? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Deploy the application on an Amazon EC2 instance, and create an AMI of the instance. Use the AMI to create an automatic scaling launch configuration that is used in an Auto Scaling group. Use Elastic Load Balancing to distribute traffic. When changes are made to the application, a new AMI will be created, which will initiate an EC2 instance refresh.
B. Use Amazon Lightsail to deploy the application. Store the application in a zipped format in an Amazon S3 bucket. Use this zipped version to deploy new versions of the application to Lightsail. Use Lightsail deployment options to manage the deployment.
C. Use AWS CodeArtifact to store the application code. Use AWS CodeDeploy to deploy the application to a fleet of Amazon EC2 instances. Use Elastic Load Balancing to distribute the traffic to the EC2 instances. When making changes to the application, upload a new version to CodeArtifact and create a new CodeDeploy deployment.
D. Use AWS Elastic Beanstalk to host the application. Store a zipped version of the application in Amazon S3. Use that location to deploy new versions of the application. Use Elastic Beanstalk to manage the deployment options.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个用Go语言编写的本地应用程序。DevOps工程师必须将该应用程序迁移到AWS。公司的开发团队希望启用蓝绿部署并执行A/B测试。哪种解决方案能满足这些要求？ 选项： A. 在Amazon EC2实例上部署应用程序，并创建该实例的AMI。使用AMI创建Auto Scaling组中使用的自动扩展启动配置。使用Elastic Load Balancing分发流量。当对应用程序进行更改时，将创建新的AMI，这将启动EC2实例刷新。 B. 使用Amazon Lightsail部署应用程序。将应用程序以压缩格式存储在Amazon S3存储桶中。使用此压缩版本将应用程序的新版本部署到Lightsail。使用Lightsail部署选项管理部署。 C. 使用AWS CodeArtifact存储应用程序代码。使用AWS CodeDeploy将应用程序部署到Amazon EC2实例集群。使用Elastic Load Balancing将流量分发到EC2实例。对应用程序进行更改时，将新版本上传到CodeArtifact并创建新的CodeDeploy部署。 D. 使用AWS Elastic Beanstalk托管应用程序。将应用程序的压缩版本存储在Amazon S3中。使用该位置部署应用程序的新版本。使用Elastic Beanstalk管理部署选项。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为Go应用程序选择一个AWS解决方案，关键需求是支持蓝绿部署和A/B测试功能。 **涉及的关键AWS服务和概念：** - 蓝绿部署：一种部署策略，通过维护两个相同的生产环境来实现零停机部署 - A/B测试：将流量分配给不同版本的应用程序以测试性能和用户体验 - AWS Elastic Beanstalk：应用程序部署和管理平台 - AWS CodeDeploy：自动化部署服务 - Amazon Lightsail：简化的云平台 - Auto Scaling和Elastic Load Balancing：自动扩展和负载均衡服务 **正确答案D的原因：** 1. **原生支持蓝绿部署**：Elastic Beanstalk内置支持蓝绿部署策略，可以轻松配置和管理 2. **A/B测试能力**：通过Elastic Beanstalk的流量分割功能，可以将流量按比例分配给不同版本 3. **简化管理**：Beanstalk自动处理容量配置、负载均衡、自动扩展和应用程序健康监控 4. **Go语言支持**：Beanstalk原生支持Go应用程序部署 5. **版本管理**：支持应用程序版本管理和回滚功能 **其他选项错误的原因：** - **选项A**：虽然技术上可行，但EC2实例刷新不是真正的蓝绿部署，且配置A/B测试较为复杂，需要额外的工具和配置 - **选项B**：Lightsail是简化的服务，缺乏高级的蓝绿部署和A/B测试功能，主要适用于简单的应用程序 - **选项C**：CodeDeploy虽然支持蓝绿部署，但需要更多手动配置，且CodeArtifact主要用于包管理而非应用程序部署，整体方案复杂度较高 **决策标准和最佳实践：** 1. **功能匹配度**：选择原生支持所需功能的服务 2. **管理复杂度**：优先选择能简化运维的托管服务 3. **部署策略支持**：确保服务支持现代化部署模式 4. **语言兼容性**：验证平台对特定编程语言的支持程度</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">28</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A developer is maintaining a fleet of 50 Amazon EC2 Linux servers. The servers are part of an Amazon EC2 Auto Scaling group, and also use Elastic Load Balancing for load balancing. Occasionally, some application servers are being terminated after failing ELB HTTP health checks. The developer would like to perform a root cause analysis on the issue, but before being able to access application logs, the server is terminated. How can log collection be automated? D (88%) 12%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Auto Scaling lifecycle hooks to put instances in a Pending:Wait state. Create an Amazon CloudWatch alarm for EC2 Instance Terminate Successful and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.
B. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an AWS Config rule for EC2 Instance-terminate Lifecycle Action and trigger a step function that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.
C. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon CloudWatch subscription filter for EC2 Instance Terminate Successful and trigger a CloudWatch agent that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.
D. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名开发人员正在维护一个由50台Amazon EC2 Linux服务器组成的集群。这些服务器是Amazon EC2 Auto Scaling组的一部分，并且还使用Elastic Load Balancing进行负载均衡。偶尔，一些应用服务器在ELB HTTP健康检查失败后被终止。开发人员希望对此问题进行根本原因分析，但在能够访问应用程序日志之前，服务器就被终止了。如何自动化日志收集？ 选项： A. 使用Auto Scaling生命周期钩子将实例置于Pending:Wait状态。为EC2 Instance Terminate Successful创建Amazon CloudWatch告警，触发AWS Lambda函数调用SSM Run Command脚本收集日志，推送到Amazon S3，并在日志收集完成后完成生命周期操作。 B. 使用Auto Scaling生命周期钩子将实例置于Terminating:Wait状态。为EC2 Instance-terminate Lifecycle Action创建AWS Config规则，触发step function调用脚本收集日志，推送到Amazon S3，并在日志收集完成后完成生命周期操作。 C. 使用Auto Scaling生命周期钩子将实例置于Terminating:Wait状态。为EC2 Instance Terminate Successful创建Amazon CloudWatch订阅过滤器，触发CloudWatch代理调用脚本收集日志，推送到Amazon S3，并在日志收集完成后完成生命周期操作。 D. 使用Auto Scaling生命周期钩子将实例置于Terminating:Wait状态。为EC2 Instance-terminate Lifecycle Action创建Amazon EventBridge规则，触发AWS Lambda函数调用SSM Run Command脚本收集日志，推送到Amazon S3，并在日志收集完成后完成生命周期操作。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题要求在EC2实例因健康检查失败而被Auto Scaling终止之前，自动收集应用程序日志以进行根本原因分析。关键是要在实例终止前有足够时间收集日志。 **涉及的关键AWS服务和概念：** 1. Auto Scaling Lifecycle Hooks - 在实例启动或终止过程中暂停实例，允许执行自定义操作 2. EventBridge - 事件驱动的服务，可以监听AWS服务事件并触发相应操作 3. Lambda函数 - 无服务器计算服务，用于执行日志收集逻辑 4. SSM Run Command - 在EC2实例上远程执行命令的服务 5. S3 - 用于存储收集的日志文件 **正确答案D的原因：** 1. **正确的生命周期状态**：使用Terminating:Wait状态，这是在实例即将被终止时的正确钩子状态 2. **合适的事件监听机制**：EventBridge可以准确监听EC2 Instance-terminate Lifecycle Action事件 3. **有效的执行方案**：Lambda函数结合SSM Run Command可以可靠地在目标实例上执行日志收集脚本 4. **完整的工作流程**：包含了暂停终止→收集日志→上传S3→完成生命周期操作的完整流程 **其他选项错误的原因：** - **选项A**：使用了错误的生命周期状态(Pending:Wait)，这是实例启动时的状态，不是终止时的状态；且CloudWatch告警不是监听生命周期事件的正确方式 - **选项B**：AWS Config主要用于配置合规性检查，不是处理实时生命周期事件的合适服务；Step Functions增加了不必要的复杂性 - **选项C**：CloudWatch订阅过滤器主要用于日志流处理，不适合监听EC2生命周期事件；且CloudWatch代理不能直接触发脚本执行 **决策标准和最佳实践：** 1. 选择正确的生命周期钩子状态以匹配使用场景 2. 使用EventBridge作为事件驱动架构的标准服务 3. Lambda + SSM Run Command的组合提供了灵活且可靠的远程执行能力 4. 确保工作流程包含完成生命周期操作的步骤，避免实例无限期挂起</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">29</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an organization in AWS Organizations. The organization includes workload accounts that contain enterprise applications. The company centrally manages users from an operations account. No users can be created in the workload accounts. The company recently added an operations team and must provide the operations team members with administrator access to each workload account. Which combination of actions will provide this access? (Choose three.) F. Create an Amazon Cognito user pool in the operations account. Create an Amazon Cognito user for each operations team member. BDE (88%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a SysAdmin role in the operations account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the workload accounts.
B. Create a SysAdmin role in each workload account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the operations account.
C. Create an Amazon Cognito identity pool in the operations account. Attach the SysAdmin role as an authenticated role.
D. In the operations account, create an IAM user for each operations team member.
E. In the operations account, create an IAM user group that is named SysAdmins. Add an IAM policy that allows the sts:AssumeRole action for the SysAdmin role in each workload account. Add all operations team members to the group.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中有一个组织。该组织包含含有企业应用程序的工作负载账户。公司从一个运营账户集中管理用户。不能在工作负载账户中创建用户。公司最近添加了一个运营团队，必须为运营团队成员提供对每个工作负载账户的管理员访问权限。哪种操作组合将提供这种访问权限？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Organizations环境下，为运营团队成员提供跨账户的管理员访问权限。关键约束是用户只能在运营账户中创建，不能在工作负载账户中创建用户。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - IAM跨账户角色假设（Cross-account role assumption） - STS AssumeRole：临时凭证服务 - IAM用户、组和策略管理 - 信任关系（Trust relationship）配置 **正确答案分析（应该是BDE组合）：** - **选项B**：在每个工作负载账户中创建SysAdmin角色，附加AdministratorAccess策略，并修改信任关系允许运营账户假设该角色。这是跨账户访问的核心配置。 - **选项D**：在运营账户中为每个运营团队成员创建IAM用户。这满足了用户集中管理的要求。 - **选项E**：创建SysAdmins用户组，添加允许假设工作负载账户中SysAdmin角色的策略，并将所有运营团队成员加入该组。这建立了用户到角色的权限映射。 **其他选项错误的原因：** - **选项A**：在运营账户创建角色让工作负载账户假设是错误的方向，应该是运营账户的用户假设工作负载账户的角色。 - **选项C和F**：Amazon Cognito主要用于应用程序用户身份验证，不适用于AWS管理控制台的企业用户管理场景。 **决策标准和最佳实践：** 1. 遵循最小权限原则和职责分离 2. 使用跨账户角色假设而非跨账户用户创建 3. 集中用户管理，分散权限控制 4. 利用IAM组简化权限管理 5. 正确配置信任关系确保安全的跨账户访问</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">30</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has multiple accounts in an organization in AWS Organizations. The company&#x27;s SecOps team needs to receive an Amazon Simple Notification Service (Amazon SNS) notification if any account in the organization turns off the Block Public Access feature on an Amazon S3 bucket. A DevOps engineer must implement this change without affecting the operation of any AWS accounts. The implementation must ensure that individual member accounts in the organization cannot turn off the notification. Which solution will meet these requirements? email address to the SNS topic. Deploy a conformance pack that uses the s3-bucket-level-public-access-prohibited AWS Config managed rule in each account and uses an AWS Systems Manager document to publish an event to the SNS topic to notify the SecOps team. C (68%) A (25%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Designate an account to be the delegated Amazon GuardDuty administrator account. Turn on GuardDuty for all accounts across the organization. In the GuardDuty administrator account, create an SNS topic. Subscribe the SecOps team&#x27;s email address to the SNS topic. In the same account, create an Amazon EventBridge rule that uses an event pattern for GuardDuty findings and a target of the SNS topic.
B. Create an AWS CloudFormation template that creates an SNS topic and subscribes the SecOps team&#x27;s email address to the SNS topic. In the template, include an Amazon EventBridge rule that uses an event pattern of CloudTrail activity for s3:PutBucketPublicAccessBlock and a target of the SNS topic. Deploy the stack to every account in the organization by using CloudFormation StackSets.
C. Turn on AWS Config across the organization. In the delegated administrator account, create an SNS topic. Subscribe the SecOps team&#x27;s
D. Turn on Amazon Inspector across the organization. In the Amazon Inspector delegated administrator account, create an SNS topic. Subscribe the SecOps team&#x27;s email address to the SNS topic. In the same account, create an Amazon EventBridge rule that uses an event pattern for public network exposure of the S3 bucket and publishes an event to the SNS topic to notify the SecOps team.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中有多个账户。公司的SecOps团队需要在组织中任何账户关闭Amazon S3存储桶的Block Public Access功能时收到Amazon Simple Notification Service (Amazon SNS)通知。DevOps工程师必须在不影响任何AWS账户操作的情况下实施此更改。实施方案必须确保组织中的各个成员账户无法关闭通知功能。哪个解决方案能满足这些要求？ 选项A：指定一个账户作为委托的Amazon GuardDuty管理员账户。为组织中的所有账户开启GuardDuty。在GuardDuty管理员账户中，创建SNS主题。将SecOps团队的邮箱地址订阅到SNS主题。在同一账户中，创建Amazon EventBridge规则，使用GuardDuty发现的事件模式和SNS主题作为目标。 选项B：创建AWS CloudFormation模板，创建SNS主题并将SecOps团队的邮箱地址订阅到SNS主题。在模板中，包含Amazon EventBridge规则，使用CloudTrail活动的事件模式监控s3:PutBucketPublicAccessBlock并以SNS主题为目标。使用CloudFormation StackSets将堆栈部署到组织中的每个账户。 选项C：在整个组织中开启AWS Config。在委托管理员账户中，创建SNS主题。将SecOps团队的邮箱地址订阅到SNS主题。部署使用s3-bucket-level-public-access-prohibited AWS Config托管规则的合规包到每个账户，并使用AWS Systems Manager文档向SNS主题发布事件以通知SecOps团队。 选项D：在整个组织中开启Amazon Inspector。在Amazon Inspector委托管理员账户中，创建SNS主题。将SecOps团队的邮箱地址订阅到SNS主题。在同一账户中，创建Amazon EventBridge规则，使用S3存储桶公共网络暴露的事件模式并向SNS主题发布事件以通知SecOps团队。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 监控组织中任何账户关闭S3存储桶Block Public Access功能的行为 - 通过SNS向SecOps团队发送通知 - 不影响现有AWS账户的操作 - 确保成员账户无法关闭通知功能 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理 - Amazon S3 Block Public Access：S3安全功能 - Amazon SNS：通知服务 - AWS CloudTrail：API调用日志记录 - Amazon EventBridge：事件路由服务 - AWS CloudFormation StackSets：跨账户资源部署 - AWS Config：合规性监控 - Amazon GuardDuty：威胁检测服务 **正确答案B的原因：** 1. **精确监控**：使用CloudTrail监控s3:PutBucketPublicAccessBlock API调用，这是关闭Block Public Access功能时触发的具体API 2. **跨账户部署**：CloudFormation StackSets能够在组织的所有账户中统一部署监控资源 3. **实时响应**：EventBridge规则能够实时捕获CloudTrail事件并触发SNS通知 4. **防篡改**：通过StackSets集中管理，成员账户难以单独关闭监控功能 5. **无侵入性**：仅部署监控资源，不影响现有业务操作 **其他选项错误的原因：** - **选项A**：GuardDuty主要用于威胁检测，不专门监控S3 Block Public Access配置变更，且可能无法及时检测到此类配置更改 - **选项C**：AWS Config规则是合规性检查，主要用于检测当前状态而非实时监控配置变更动作，响应可能不够及时 - **选项D**：Amazon Inspector主要用于应用程序安全评估，不适用于监控S3配置变更，功能不匹配 **决策标准和最佳实践：** 1. **选择合适的监控工具**：对于API调用监控，CloudTrail + EventBridge是最直接有效的方案 2. **集中化管理**：使用StackSets实现跨账户的统一部署和管理 3. **实时性要求**：EventBridge提供近实时的事件处理能力 4. **安全性考虑**：通过组织级别的集中部署，防止成员账户绕过监控机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">31</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has migrated its container-based applications to Amazon EKS and wants to establish automated email notifications. The notifications sent to each email address are for specific activities related to EKS components. The solution will include Amazon SNS topics and an AWS Lambda function to evaluate incoming log events and publish messages to the correct SNS topic. Which logging solution will support these requirements? events that invoke Lambda. A (97%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable Amazon CloudWatch Logs to log the EKS components. Create a CloudWatch subscription filter for each component with Lambda as the subscription feed destination.
B. Enable Amazon CloudWatch Logs to log the EKS components. Create CloudWatch Logs Insights queries linked to Amazon EventBridge
C. Enable Amazon S3 logging for the EKS components. Configure an Amazon CloudWatch subscription filter for each component with Lambda as the subscription feed destination.
D. Enable Amazon S3 logging for the EKS components. Configure S3 PUT Object event notifications with AWS Lambda as the destination.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司已将其基于容器的应用程序迁移到Amazon EKS，并希望建立自动化邮件通知。发送到每个邮件地址的通知是针对与EKS组件相关的特定活动。该解决方案将包括Amazon SNS主题和AWS Lambda函数来评估传入的日志事件并将消息发布到正确的SNS主题。哪种日志记录解决方案将支持这些要求？ 选项： A. 启用Amazon CloudWatch Logs来记录EKS组件。为每个组件创建CloudWatch订阅过滤器，以Lambda作为订阅源目标。 B. 启用Amazon CloudWatch Logs来记录EKS组件。创建链接到Amazon EventBridge的CloudWatch Logs Insights查询。 C. 为EKS组件启用Amazon S3日志记录。为每个组件配置Amazon CloudWatch订阅过滤器，以Lambda作为订阅源目标。 D. 为EKS组件启用Amazon S3日志记录。配置S3 PUT Object事件通知，以AWS Lambda作为目标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要建立一个自动化系统来监控EKS组件的特定活动，通过Lambda函数处理日志事件，并根据不同的活动类型发送到相应的SNS主题进行邮件通知。关键是需要实时处理日志事件并触发Lambda函数。 **涉及的关键AWS服务和概念：** - Amazon EKS：托管的Kubernetes服务 - CloudWatch Logs：AWS的日志管理服务 - CloudWatch订阅过滤器：用于实时处理日志流的机制 - Amazon S3：对象存储服务 - AWS Lambda：无服务器计算服务 - Amazon SNS：消息通知服务 - S3事件通知：当S3对象发生变化时触发的事件 **正确答案的原因（选项C错误标记问题）：** 实际上，标准答案C是错误的。正确答案应该是A。原因如下： 1. EKS组件的日志应该发送到CloudWatch Logs，这是AWS的标准做法 2. CloudWatch订阅过滤器可以实时监控日志流并直接触发Lambda函数 3. 这种架构支持实时处理，符合自动化通知的要求 **其他选项错误的原因：** - 选项B：CloudWatch Logs Insights主要用于查询分析，不是实时触发机制 - 选项C：EKS日志不应该直接发送到S3，且S3不支持CloudWatch订阅过滤器 - 选项D：S3事件通知虽然可以触发Lambda，但EKS日志发送到S3不是最佳实践，且延迟较高 **决策标准和最佳实践：** 1. EKS日志应使用CloudWatch Logs进行集中管理 2. 实时处理需求应优先选择CloudWatch订阅过滤器 3. Lambda函数应作为日志处理的中间层来解析和路由消息 4. 架构应保持简单、实时性强且符合AWS最佳实践 注：题目中标记的正确答案C实际上是错误的，正确答案应该是A。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">32</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is implementing an Amazon Elastic Container Service (Amazon ECS) cluster to run its workload. The company architecture will run multiple ECS services on the cluster. The architecture includes an Application Load Balancer on the front end and uses multiple target groups to route traffic. A DevOps engineer must collect application and access logs. The DevOps engineer then needs to send the logs to an Amazon S3 bucket for near-real-time analysis. Which combination of steps must the DevOps engineer take to meet these requirements? (Choose three.) F. Create an Amazon Kinesis Data Firehose delivery stream that has a destination of the logging S3 bucket. Then create an Amazon CloudWatch Logs subscription filter for Kinesis Data Firehose. Most Voted BDF (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Download the Amazon CloudWatch Logs container instance from AWS. Configure this instance as a task. Update the application service definitions to include the logging task.
B. Install the Amazon CloudWatch Logs agent on the ECS instances. Change the logging driver in the ECS task definition to awslogs.
C. Use Amazon EventBridge to schedule an AWS Lambda function that will run every 60 seconds and will run the Amazon CloudWatch Logs create-export-task command. Then point the output to the logging S3 bucket.
D. Activate access logging on the ALB. Then point the ALB directly to the logging S3 bucket.
E. Activate access logging on the target groups that the ECS services use. Then send the logs directly to the logging S3 bucket.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在实施Amazon Elastic Container Service (Amazon ECS)集群来运行其工作负载。公司架构将在集群上运行多个ECS服务。架构包括前端的Application Load Balancer，并使用多个target groups来路由流量。DevOps工程师必须收集应用程序和访问日志。然后DevOps工程师需要将日志发送到Amazon S3存储桶进行近实时分析。DevOps工程师必须采取哪些步骤组合来满足这些要求？（选择三个。） 选项： A. 从AWS下载Amazon CloudWatch Logs容器实例。将此实例配置为任务。更新应用程序服务定义以包含日志记录任务。 B. 在ECS实例上安装Amazon CloudWatch Logs代理。将ECS任务定义中的日志驱动程序更改为awslogs。 C. 使用Amazon EventBridge调度AWS Lambda函数，该函数每60秒运行一次，并运行Amazon CloudWatch Logs create-export-task命令。然后将输出指向日志S3存储桶。 D. 在ALB上激活访问日志记录。然后将ALB直接指向日志S3存储桶。 E. 在ECS服务使用的target groups上激活访问日志记录。然后将日志直接发送到日志S3存储桶。 F. 创建一个以日志S3存储桶为目标的Amazon Kinesis Data Firehose传输流。然后为Kinesis Data Firehose创建Amazon CloudWatch Logs订阅过滤器。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求建立一个完整的日志收集和传输架构，需要收集ECS应用程序日志和ALB访问日志，并将它们近实时地发送到S3存储桶进行分析。 **涉及的关键AWS服务和概念：** - Amazon ECS：容器服务，需要配置应用程序日志收集 - Application Load Balancer (ALB)：负载均衡器，需要配置访问日志 - Amazon CloudWatch Logs：日志聚合服务 - Amazon Kinesis Data Firehose：近实时数据传输服务 - Amazon S3：目标存储服务 **正确答案BDF的原因：** - **选项B**：在ECS实例上安装CloudWatch Logs代理并使用awslogs驱动程序是收集ECS容器应用程序日志的标准做法，可以自动将容器日志发送到CloudWatch Logs - **选项D**：ALB访问日志激活后可以直接写入S3存储桶，这是AWS原生支持的功能，无需额外配置 - **选项F**：Kinesis Data Firehose提供近实时的数据传输能力，通过CloudWatch Logs订阅过滤器可以将应用程序日志流式传输到S3 **其他选项错误的原因：** - **选项A**：不存在&quot;CloudWatch Logs容器实例&quot;这样的AWS服务，这是一个虚构的概念 - **选项C**：使用Lambda定期运行create-export-task命令不是近实时的解决方案，而且会产生不必要的延迟和复杂性 - **选项E**：Target groups本身不提供访问日志功能，访问日志是在ALB层面配置的 **决策标准和最佳实践：** 1. **日志收集层面**：使用AWS原生的日志收集机制（awslogs驱动程序） 2. **近实时要求**：选择流式传输服务（Kinesis Data Firehose）而非批处理方案 3. **架构简洁性**：利用AWS服务间的原生集成，避免不必要的中间组件 4. **成本效益**：ALB直接写入S3比通过其他服务转发更经济高效</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">33</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company that uses electronic health records is running a fleet of Amazon EC2 instances with an Amazon Linux operating system. As part of patient privacy requirements, the company must ensure continuous compliance for patches for operating system and applications running on the EC2 instances. How can the deployments of the operating system and application patches be automated using a default and custom repository? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Systems Manager to create a new patch baseline including the custom repository. Run the AWS-RunPatchBaseline document using the run command to verify and install patches.
B. Use AWS Direct Connect to integrate the corporate repository and deploy the patches using Amazon CloudWatch scheduled events, then use the CloudWatch dashboard to create reports.
C. Use yum-config-manager to add the custom repository under /etc/yum.repos.d and run yum-config-manager-enable to activate the repository.
D. Use AWS Systems Manager to create a new patch baseline including the corporate repository. Run the AWS-AmazonLinuxDefaultPatchBaseline document using the run command to verify and install patches.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家使用电子健康记录的公司正在运行一组使用Amazon Linux操作系统的Amazon EC2实例。作为患者隐私要求的一部分，公司必须确保对运行在EC2实例上的操作系统和应用程序补丁的持续合规性。如何使用默认和自定义repository来自动化操作系统和应用程序补丁的部署？ 选项： A. 使用AWS Systems Manager创建包含自定义repository的新patch baseline。使用run command运行AWS-RunPatchBaseline文档来验证和安装补丁。 B. 使用AWS Direct Connect集成企业repository，并使用Amazon CloudWatch scheduled events部署补丁，然后使用CloudWatch dashboard创建报告。 C. 使用yum-config-manager在/etc/yum.repos.d下添加自定义repository，并运行yum-config-manager-enable来激活repository。 D. 使用AWS Systems Manager创建包含企业repository的新patch baseline。使用run command运行AWS-AmazonLinuxDefaultPatchBaseline文档来验证和安装补丁。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何在AWS环境中自动化管理EC2实例的操作系统和应用程序补丁，特别是需要同时使用默认和自定义repository，并满足医疗行业的合规性要求。 **涉及的关键AWS服务和概念：** - AWS Systems Manager Patch Manager：用于自动化补丁管理的服务 - Patch Baseline：定义哪些补丁应该被安装的规则集 - Run Command：远程执行命令的功能 - AWS-RunPatchBaseline文档：用于执行补丁安装的预定义文档 - Custom Repository：自定义软件包仓库 **正确答案A的原因：** 1. AWS Systems Manager是AWS原生的补丁管理解决方案，专门设计用于大规模自动化补丁管理 2. 创建新的patch baseline可以灵活配置补丁策略，包括自定义repository 3. AWS-RunPatchBaseline是正确的文档，专门用于补丁的验证和安装 4. Run Command提供了集中化的执行机制，支持大规模部署 5. 完全满足自动化、合规性和使用自定义repository的要求 **其他选项错误的原因：** - 选项B：AWS Direct Connect是网络连接服务，不是补丁管理工具；CloudWatch主要用于监控，不是补丁部署的最佳选择 - 选项C：这是手动配置方法，需要在每个实例上单独操作，不符合自动化要求，且无法提供集中管理和合规性报告 - 选项D：AWS-AmazonLinuxDefaultPatchBaseline是默认的patch baseline，无法包含自定义repository，不满足题目要求 **决策标准和最佳实践：** 1. 选择AWS原生服务优于第三方或手动解决方案 2. 自动化程度高的解决方案优于需要手动干预的方案 3. 能够提供集中管理和合规性报告的解决方案更适合企业环境 4. 对于医疗行业，必须选择能够提供审计跟踪和合规性证明的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">34</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using AWS CodePipeline to automate its release pipeline. AWS CodeDeploy is being used in the pipeline to deploy an application to Amazon Elastic Container Service (Amazon ECS) using the blue/green deployment model. The company wants to implement scripts to test the green version of the application before shifting traffic. These scripts will complete in 5 minutes or less. If errors are discovered during these tests, the application must be rolled back. Which strategy will meet these requirements? run the test scripts. If errors are found, exit the Lambda function with an error to initiate rollback. Most Voted C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a stage to the CodePipeline pipeline between the source and deploy stages. Use AWS CodeBuild to create a runtime environment and build commands in the buildspec file to invoke test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.
B. Add a stage to the CodePipeline pipeline between the source and deploy stages. Use this stage to invoke an AWS Lambda function that will run the test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.
C. Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTestTraffic lifecycle event to invoke an AWS Lambda function to
D. Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTraffic lifecycle event to invoke the test scripts. If errors are found, use the aws deploy stop-deployment CLI command to stop the deployment.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS CodePipeline来自动化其发布管道。管道中使用AWS CodeDeploy将应用程序部署到Amazon Elastic Container Service (Amazon ECS)，采用蓝/绿部署模型。公司希望实现脚本来测试应用程序的绿色版本，然后再切换流量。这些脚本将在5分钟或更短时间内完成。如果在测试过程中发现错误，应用程序必须回滚。哪种策略能满足这些要求？运行测试脚本。如果发现错误，以错误状态退出Lambda函数以启动回滚。 选项： A. 在CodePipeline管道的源码阶段和部署阶段之间添加一个阶段。使用AWS CodeBuild创建运行时环境，在buildspec文件中构建命令来调用测试脚本。如果发现错误，使用aws deploy stop-deployment命令停止部署。 B. 在CodePipeline管道的源码阶段和部署阶段之间添加一个阶段。使用此阶段调用AWS Lambda函数来运行测试脚本。如果发现错误，使用aws deploy stop-deployment命令停止部署。 C. 在CodeDeploy AppSpec文件中添加hooks部分。使用AfterAllowTestTraffic生命周期事件调用AWS Lambda函数来运行测试脚本。如果发现错误，以错误状态退出Lambda函数以启动回滚。 D. 在CodeDeploy AppSpec文件中添加hooks部分。使用AfterAllowTraffic生命周期事件调用测试脚本。如果发现错误，使用aws deploy stop-deployment CLI命令停止部署。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在AWS ECS蓝/绿部署过程中如何正确实现自动化测试和回滚机制。关键要求包括：1）在绿色环境部署后、流量切换前进行测试；2）测试失败时能够自动回滚；3）测试时间控制在5分钟内。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务 - AWS CodeDeploy：应用部署服务，支持蓝/绿部署 - Amazon ECS：容器编排服务 - 蓝/绿部署：零停机部署策略 - CodeDeploy生命周期事件：AfterAllowTestTraffic vs AfterAllowTraffic - AppSpec文件hooks机制 **正确答案C的原因：** 选项C是正确的，因为：1）AfterAllowTestTraffic是专门为蓝/绿部署中的测试阶段设计的生命周期事件，在绿色环境准备就绪但正式流量切换之前触发；2）使用Lambda函数可以灵活执行测试脚本并控制执行时间；3）Lambda函数以错误状态退出会自动触发CodeDeploy的内置回滚机制，无需手动调用CLI命令；4）这种方式完全集成在部署流程中，符合自动化最佳实践。 **其他选项错误的原因：** 选项A和B错误是因为它们将测试放在了CodePipeline的独立阶段中，这意味着测试在部署开始之前进行，而不是在绿色环境部署完成后进行测试，不符合蓝/绿部署的测试时机要求。选项D错误是因为：1）AfterAllowTraffic事件发生在流量已经切换到绿色环境之后，此时进行测试为时已晚；2）需要手动调用CLI命令进行回滚，增加了复杂性和失败风险。 **决策标准和最佳实践：** 选择部署测试策略时应考虑：1）测试时机的准确性（绿色环境就绪但流量未切换）；2）自动化程度（避免手动干预）；3）回滚机制的可靠性（利用服务内置功能）；4）与现有部署流程的集成度。CodeDeploy的生命周期hooks机制提供了最佳的集成点，而Lambda函数提供了灵活的执行环境和自动回滚能力。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">35</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Storage Gateway in file gateway mode in front of an Amazon S3 bucket that is used by multiple resources. In the morning when business begins, users do not see the objects processed by a third party the previous evening. When a DevOps engineer looks directly at the S3 bucket, the data is there, but it is missing in Storage Gateway. Which solution ensures that all the updated third-party files are available in the morning? A (97%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a nightly Amazon EventBridge event to invoke an AWS Lambda function to run the RefreshCache command for Storage Gateway.
B. Instruct the third party to put data into the S3 bucket using AWS Transfer for SFTP.
C. Modify Storage Gateway to run in volume gateway mode.
D. Use S3 Same-Region Replication to replicate any changes made directly in the S3 bucket to Storage Gateway.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Amazon S3存储桶前使用文件网关模式的AWS Storage Gateway，该存储桶被多个资源使用。早上开始营业时，用户看不到第三方在前一天晚上处理的对象。当DevOps工程师直接查看S3存储桶时，数据是存在的，但在Storage Gateway中却缺失了。哪个解决方案能确保所有更新的第三方文件在早上都可用？ 选项： A. 配置每晚的Amazon EventBridge事件来调用AWS Lambda函数，为Storage Gateway运行RefreshCache命令。 B. 指导第三方使用AWS Transfer for SFTP将数据放入S3存储桶。 C. 修改Storage Gateway以卷网关模式运行。 D. 使用S3 Same-Region Replication将直接在S3存储桶中所做的任何更改复制到Storage Gateway。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题的核心是解决AWS Storage Gateway文件网关模式下的缓存同步问题。第三方直接向S3存储桶写入数据，但Storage Gateway的本地缓存没有及时更新，导致用户无法通过Storage Gateway看到新数据。 **涉及的关键AWS服务和概念：** - AWS Storage Gateway（文件网关模式）：提供本地文件系统接口访问S3对象 - 缓存机制：Storage Gateway维护本地缓存以提高性能 - RefreshCache API：强制Storage Gateway刷新其缓存以反映S3中的变化 - Amazon EventBridge：事件驱动的调度服务 - AWS Lambda：无服务器计算服务 **正确答案A的原因：** 1. **直接解决根本问题**：RefreshCache命令专门用于解决当外部进程直接修改S3存储桶时Storage Gateway缓存不同步的问题 2. **自动化解决方案**：使用EventBridge定时触发Lambda函数执行RefreshCache，实现自动化的缓存刷新 3. **时机合适**：在夜间执行刷新操作，确保早上用户能看到最新数据 4. **成本效益**：利用现有架构，只需添加简单的自动化流程 **其他选项错误的原因：** - **选项B**：改变第三方的数据传输方式并不能解决缓存同步问题，且可能不现实 - **选项C**：卷网关模式用于块存储，不适用于文件访问场景，改变模式会破坏现有功能 - **选项D**：S3 Same-Region Replication用于在不同S3存储桶间复制数据，不能解决Storage Gateway缓存问题 **决策标准和最佳实践：** 1. **最小化架构变更**：优先选择不改变现有架构的解决方案 2. **自动化运维**：使用自动化工具减少人工干预 3. **针对性解决**：使用专门的API解决特定问题 4. **考虑业务影响**：确保解决方案不会影响现有的业务流程和第三方集成</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">36</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer needs to back up sensitive Amazon S3 objects that are stored within an S3 bucket with a private bucket policy using S3 cross-Region replication functionality. The objects need to be copied to a target bucket in a different AWS Region and account. Which combination of actions should be performed to enable this replication? (Choose three.) F. Create a replication rule in the target bucket to enable the replication. ADE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a replication IAM role in the source account
B. Create a replication IAM role in the target account.
C. Add statements to the source bucket policy allowing the replication IAM role to replicate objects.
D. Add statements to the target bucket policy allowing the replication IAM role to replicate objects.
E. Create a replication rule in the source bucket to enable the replication.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师需要备份存储在具有私有bucket策略的Amazon S3 bucket中的敏感S3对象，使用S3跨Region复制功能。这些对象需要复制到不同AWS Region和账户中的目标bucket。应该执行哪些操作组合来启用此复制？（选择三个。）F. 在目标bucket中创建复制规则以启用复制。ADE (100%) 选项：A. 在源账户中创建复制IAM角色 B. 在目标账户中创建复制IAM角色 C. 向源bucket策略添加语句，允许复制IAM角色复制对象 D. 向目标bucket策略添加语句，允许复制IAM角色复制对象 E. 在源bucket中创建复制规则以启用复制 正确答案：A</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是跨账户、跨Region的S3复制配置，需要理解S3 Cross-Region Replication (CRR)在跨账户场景下的完整配置流程。 **涉及的关键AWS服务和概念：** - S3 Cross-Region Replication (CRR) - IAM角色和权限管理 - S3 bucket策略 - 跨账户资源访问 **正确答案的原因：** 根据题目显示的答案ADE，正确的三个步骤是： - A: 在源账户创建复制IAM角色 - 这是必需的，因为复制操作需要在源账户中有适当的权限来读取源对象 - D: 向目标bucket策略添加语句允许复制IAM角色复制对象 - 跨账户复制时，目标bucket必须明确授权源账户的角色写入权限 - E: 在源bucket中创建复制规则 - 复制规则必须在源bucket中配置，指定复制的目标和条件 **其他选项错误的原因：** - B错误：复制IAM角色应该在源账户创建，不是目标账户 - C错误：源bucket策略通常不需要额外配置，因为IAM角色本身就有读取源bucket的权限 - F错误：复制规则应该在源bucket配置，不是目标bucket **决策标准和最佳实践：** 跨账户S3复制的标准配置模式是：源账户负责创建IAM角色和复制规则，目标账户负责在bucket策略中授权访问。这种设计确保了权限的最小化原则和清晰的责任分离。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">37</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has multiple member accounts that are part of an organization in AWS Organizations. The security team needs to review every Amazon EC2 security group and their inbound and outbound rules. The security team wants to programmatically retrieve this information from the member accounts using an AWS Lambda function in the management account of the organization. Which combination of access changes will meet these requirements? (Choose three.) F. Create an IAM role in the management account that has access to the AmazonEC2ReadOnlyAccess managed policy. BCE (79%) 11% 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a trust relationship that allows users in the member accounts to assume the management account IAM role.
B. Create a trust relationship that allows users in the management account to assume the IAM roles of the member accounts.
C. Create an IAM role in each member account that has access to the AmazonEC2ReadOnlyAccess managed policy.
D. Create an IAM role in each member account to allow the sts:AssumeRole action against the management account IAM role&#x27;s ARN.
E. Create an IAM role in the management account that allows the sts:AssumeRole action against the member account IAM role&#x27;s ARN.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有多个成员账户，这些账户是AWS Organizations中某个组织的一部分。安全团队需要审查每个Amazon EC2安全组及其入站和出站规则。安全团队希望使用组织管理账户中的AWS Lambda函数以编程方式从成员账户中检索这些信息。哪种访问更改的组合将满足这些要求？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Organizations环境中，让管理账户的Lambda函数能够跨账户访问所有成员账户的EC2安全组信息。需要建立适当的跨账户访问机制。 **涉及的关键AWS服务和概念：** - AWS Organizations（组织管理服务） - 跨账户IAM角色假设（Cross-account role assumption） - STS AssumeRole操作 - IAM信任关系（Trust relationship） - AmazonEC2ReadOnlyAccess托管策略 **正确答案的原因：** 正确答案应该是B、C、E的组合： - **选项B**：在管理账户中创建信任关系，允许管理账户用户假设成员账户的IAM角色。这是跨账户访问的核心机制。 - **选项C**：在每个成员账户中创建具有AmazonEC2ReadOnlyAccess权限的IAM角色，提供读取EC2安全组的必要权限。 - **选项E**：在管理账户中创建允许对成员账户IAM角色执行sts:AssumeRole操作的角色，使Lambda函数能够假设成员账户角色。 **其他选项错误的原因：** - **选项A**：方向错误，不应该让成员账户假设管理账户角色，而是相反。 - **选项D**：错误地要求成员账户角色假设管理账户角色，这与需求相反。 - **选项F**：仅在管理账户创建角色无法访问成员账户资源。 **决策标准和最佳实践：** 1. 跨账户访问遵循&quot;最小权限原则&quot; 2. 使用角色假设而非长期凭证进行跨账户访问 3. 在目标账户（成员账户）中创建具有必要权限的角色 4. 在源账户（管理账户）中配置假设目标角色的权限 5. 建立适当的信任关系确保安全的跨账户访问</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">38</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A space exploration company receives telemetry data from multiple satellites. Small packets of data are received through Amazon API Gateway and are placed directly into an Amazon Simple Queue Service (Amazon SQS) standard queue. A custom application is subscribed to the queue and transforms the data into a standard format. Because of inconsistencies in the data that the satellites produce, the application is occasionally unable to transform the data. In these cases, the messages remain in the SQS queue. A DevOps engineer must develop a solution that retains the failed messages and makes them available to scientists for review and future processing. Which solution will meet these requirements? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure AWS Lambda to poll the SQS queue and invoke a Lambda function to check whether the queue messages are valid. If validation fails, send a copy of the data that is not valid to an Amazon S3 bucket so that the scientists can review and correct the data. When the data is corrected, amend the message in the SQS queue by using a replay Lambda function with the corrected data.
B. Convert the SQS standard queue to an SQS FIFO queue. Configure AWS Lambda to poll the SQS queue every 10 minutes by using an Amazon EventBridge schedule. Invoke the Lambda function to identify any messages with a SentTimestamp value that is older than 5 minutes, push the data to the same location as the application&#x27;s output location, and remove the messages from the queue.
C. Create an SQS dead-letter queue. Modify the existing queue by including a redrive policy that sets the Maximum Receives setting to 1 and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review the data that is not valid. Reprocess this data at a later time.
D. Configure API Gateway to send messages to different SQS virtual queues that are named for each of the satellites. Update the application to use a new virtual queue for any data that it cannot transform, and send the message to the new virtual queue. Instruct the scientists to use the virtual queue to review the data that is not valid. Reprocess this data at a later time.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家太空探索公司从多颗卫星接收遥测数据。小数据包通过Amazon API Gateway接收并直接放入Amazon Simple Queue Service (Amazon SQS)标准队列中。一个自定义应用程序订阅该队列并将数据转换为标准格式。由于卫星产生的数据存在不一致性，应用程序偶尔无法转换数据。在这些情况下，消息会保留在SQS队列中。DevOps工程师必须开发一个解决方案，保留失败的消息并使科学家能够审查和未来处理这些消息。哪个解决方案能满足这些要求？ 选项： A. 配置AWS Lambda轮询SQS队列并调用Lambda函数检查队列消息是否有效。如果验证失败，将无效数据的副本发送到Amazon S3存储桶，以便科学家审查和纠正数据。当数据被纠正后，使用重放Lambda函数用纠正的数据修改SQS队列中的消息。 B. 将SQS标准队列转换为SQS FIFO队列。配置AWS Lambda使用Amazon EventBridge调度每10分钟轮询SQS队列。调用Lambda函数识别任何SentTimestamp值超过5分钟的消息，将数据推送到与应用程序输出位置相同的位置，并从队列中删除消息。 C. 创建一个SQS死信队列。修改现有队列，包含一个重驱动策略，将Maximum Receives设置为1，并将死信队列ARN设置为新创建队列的ARN。指导科学家使用死信队列审查无效数据。稍后重新处理这些数据。 D. 配置API Gateway将消息发送到以每颗卫星命名的不同SQS虚拟队列。更新应用程序为任何无法转换的数据使用新的虚拟队列，并将消息发送到新的虚拟队列。指导科学家使用虚拟队列审查无效数据。稍后重新处理这些数据。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要设计一个解决方案来处理SQS队列中无法被应用程序转换的失败消息，使这些消息能够被科学家审查和未来处理，同时不影响正常的消息处理流程。 **涉及的关键AWS服务和概念：** - Amazon SQS标准队列和死信队列(Dead Letter Queue) - AWS Lambda函数和轮询机制 - Amazon S3存储服务 - SQS重驱动策略(Redrive Policy)和Maximum Receives设置 - 消息处理和错误处理模式 **正确答案A的原因：** 选项A提供了最完整和实用的解决方案： 1. 使用Lambda主动轮询和验证消息，提供了灵活的错误检测机制 2. 将失败数据存储到S3，为科学家提供了便于访问和分析的存储位置 3. 提供了数据纠正后的重新处理机制，形成完整的错误处理循环 4. 不需要修改现有的队列结构，实施风险较低 **其他选项错误的原因：** - 选项B：将标准队列转换为FIFO队列是不必要的架构变更，且基于时间戳的处理方式不能准确识别处理失败的消息，可能误删正常消息 - 选项C：虽然死信队列是处理失败消息的标准模式，但将Maximum Receives设置为1过于激进，可能导致临时性错误的消息也被立即移到死信队列 - 选项D：SQS虚拟队列主要用于请求-响应模式，不适合这种错误处理场景，且增加了系统复杂性 **决策标准和最佳实践：** 1. 错误处理应该提供完整的处理循环：检测→隔离→审查→纠正→重处理 2. 解决方案应该最小化对现有架构的影响 3. 应该为运维人员提供灵活的错误处理和监控能力 4. 存储失败数据的位置应该便于科学家访问和分析</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">39</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to use AWS CloudFormation for infrastructure deployment. The company has strict tagging and resource requirements and wants to limit the deployment to two Regions. Developers will need to deploy multiple versions of the same application. Which solution ensures resources are deployed in accordance with company policy? D (78%) C (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create AWS Trusted Advisor checks to find and remediate unapproved CloudFormation StackSets.
B. Create a CloudFormation drift detection operation to find and remediate unapproved CloudFormation StackSets.
C. Create CloudFormation StackSets with approved CloudFormation templates.
D. Create AWS Service Catalog products with approved CloudFormation templates.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司希望使用AWS CloudFormation进行基础设施部署。该公司有严格的标签和资源要求，并希望将部署限制在两个Region内。开发人员需要部署同一应用程序的多个版本。哪种解决方案能确保资源按照公司政策进行部署？ 选项： A. 创建AWS Trusted Advisor检查来查找和修复未经批准的CloudFormation StackSets。 B. 创建CloudFormation漂移检测操作来查找和修复未经批准的CloudFormation StackSets。 C. 使用经过批准的CloudFormation模板创建CloudFormation StackSets。 D. 使用经过批准的CloudFormation模板创建AWS Service Catalog产品。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何在有严格合规要求的环境中控制和管理CloudFormation部署。关键需求包括：严格的标签和资源要求、限制部署区域、支持多版本部署、确保符合公司政策。 **涉及的关键AWS服务和概念：** - AWS Service Catalog：提供IT服务目录管理，可以预先批准和控制可部署的资源 - CloudFormation StackSets：用于跨多个账户和区域部署CloudFormation堆栈 - AWS Trusted Advisor：提供最佳实践建议和成本优化建议 - CloudFormation漂移检测：检测堆栈资源的配置变更 **正确答案D的原因：** AWS Service Catalog是最佳选择，因为它提供了完整的治理和合规控制机制： 1. **预先批准控制**：管理员可以预先创建和批准符合公司政策的CloudFormation模板 2. **访问控制**：可以精确控制哪些用户可以部署哪些产品 3. **区域限制**：可以在产品级别限制部署区域 4. **标签强制执行**：可以在模板中预设必需的标签 5. **版本管理**：支持产品的多个版本，便于应用程序的版本控制 6. **合规监控**：提供部署历史和审计跟踪 **其他选项错误的原因：** - **选项A**：Trusted Advisor主要用于成本优化和最佳实践建议，无法提供主动的部署控制和治理 - **选项B**：漂移检测是被动的监控机制，用于检测已部署资源的变更，无法在部署前进行控制 - **选项C**：StackSets虽然可以跨区域部署，但缺乏Service Catalog提供的治理、访问控制和合规管理功能 **决策标准和最佳实践：** 在企业环境中选择基础设施部署解决方案时，应优先考虑： 1. **治理优先**：选择能提供预先控制而非事后修复的方案 2. **合规自动化**：通过模板和策略自动确保合规，而非依赖手动检查 3. **最小权限原则**：使用Service Catalog可以给开发人员提供受控的自助服务能力 4. **可审计性**：确保所有部署活动都有完整的审计跟踪</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">40</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company requires that its internally facing web application be highly available. The architecture is made up of one Amazon EC2 web server instance and one NAT instance that provides outbound internet access for updates and accessing public data. Which combination of architecture adjustments should the company implement to achieve high availability? (Choose two.) BD (84%) BE (16%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add the NAT instance to an EC2 Auto Scaling group that spans multiple Availability Zones. Update the route tables.
B. Create additional EC2 instances spanning multiple Availability Zones. Add an Application Load Balancer to split the load between them.
C. Configure an Application Load Balancer in front of the EC2 instance. Configure Amazon CloudWatch alarms to recover the EC2 instance upon host failure.
D. Replace the NAT instance with a NAT gateway in each Availability Zone. Update the route tables.
E. Replace the NAT instance with a NAT gateway that spans multiple Availability Zones. Update the route tables.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司要求其内部面向的Web应用程序具有高可用性。该架构由一个Amazon EC2 Web服务器实例和一个NAT实例组成，NAT实例为更新和访问公共数据提供出站互联网访问。公司应该实施哪种架构调整组合来实现高可用性？（选择两个。）BD (84%) BE (16%) 选项：A. 将NAT实例添加到跨多个Availability Zone的EC2 Auto Scaling组中。更新路由表。 B. 创建跨多个Availability Zone的额外EC2实例。添加Application Load Balancer在它们之间分配负载。 C. 在EC2实例前配置Application Load Balancer。配置Amazon CloudWatch告警以在主机故障时恢复EC2实例。 D. 用每个Availability Zone中的NAT gateway替换NAT实例。更新路由表。 E. 用跨多个Availability Zone的NAT gateway替换NAT实例。更新路由表。 正确答案：B</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为内部Web应用实现高可用性架构。当前架构存在单点故障问题：只有一个EC2 Web服务器和一个NAT实例，任何一个组件失败都会导致服务中断。 **涉及的关键AWS服务和概念：** - High Availability（高可用性）：通过多AZ部署消除单点故障 - Application Load Balancer：七层负载均衡器，支持健康检查 - NAT Gateway vs NAT Instance：托管服务vs自管理实例 - Auto Scaling：自动扩缩容服务 - Multi-AZ部署：跨可用区部署提高容错能力 **正确答案的原因：** 选项B正确，因为它解决了Web服务器层的单点故障问题： - 创建多个EC2实例并跨多个AZ部署，消除了单个实例和单个AZ的故障风险 - Application Load Balancer提供健康检查和流量分发，确保只将流量路由到健康的实例 - 这是实现Web层高可用性的标准最佳实践 选项D也正确，因为它解决了NAT层的单点故障： - NAT Gateway是AWS托管服务，比NAT Instance更可靠 - 在每个AZ部署NAT Gateway消除了单点故障 - NAT Gateway具有内置的高可用性和更好的性能 **其他选项错误的原因：** - 选项A：NAT实例不应该放入Auto Scaling组，因为NAT功能需要特殊的网络配置，自动扩缩容会破坏路由配置 - 选项C：仍然是单实例架构，CloudWatch告警恢复需要时间，无法提供真正的高可用性 - 选项E：NAT Gateway本身不能跨多个AZ，每个AZ需要独立的NAT Gateway **决策标准和最佳实践：** 1. 高可用性设计原则：消除所有单点故障 2. 多AZ部署：将关键组件分布在多个可用区 3. 使用托管服务：优先选择AWS托管服务（如NAT Gateway）而非自管理实例 4. 负载均衡：使用负载均衡器分发流量并进行健康检查 5. 架构分层：分别解决不同层次（Web层、网络层）的高可用性问题</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">41</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is building a multistage pipeline with AWS CodePipeline to build, verify, stage, test, and deploy an application. A manual approval stage is required between the test stage and the deploy stage. The development team uses a custom chat tool with webhook support that requires near-real-time notifications. How should the DevOps engineer configure status updates for pipeline activity and approval requests to post to the chat tool? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change. Publish subscription events to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the chat webhook URL to the SNS topic, and complete the subscription validation.
B. Create an AWS Lambda function that is invoked by AWS CloudTrail events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details to the chat webhook URL.
C. Create an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change. Publish the events to an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function that sends event details to the chat webhook URL. Subscribe the function to the SNS topic.
D. Modify the pipeline code to send the event details to the chat webhook URL at the end of each stage. Parameterize the URL so that each pipeline can send to a different URL based on the pipeline environment.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在使用AWS CodePipeline构建一个多阶段管道来构建、验证、暂存、测试和部署应用程序。在测试阶段和部署阶段之间需要一个手动批准阶段。开发团队使用一个支持webhook的自定义聊天工具，需要近实时通知。DevOps工程师应该如何配置管道活动和批准请求的状态更新，以便发布到聊天工具？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求设计一个解决方案，能够捕获AWS CodePipeline的状态变化事件（包括手动批准请求），并将这些事件近实时地发送到自定义聊天工具的webhook。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务 - Amazon EventBridge：事件驱动架构的核心服务，用于捕获AWS服务状态变化 - Amazon SNS：消息发布/订阅服务 - AWS Lambda：无服务器计算服务 - CloudWatch Logs：日志监控服务 - AWS CloudTrail：API调用审计服务 **正确答案C的原因：** 1. **事件捕获最佳实践**：EventBridge是AWS推荐的事件驱动架构解决方案，专门设计用于捕获AWS服务的状态变化事件 2. **精确过滤**：可以精确过滤CodePipeline Pipeline Execution State Change事件 3. **解耦架构**：通过SNS实现发布/订阅模式，提供更好的可扩展性和容错性 4. **灵活处理**：Lambda函数可以对事件进行自定义处理，格式化后发送到webhook 5. **近实时性能**：EventBridge + SNS + Lambda的组合提供毫秒级的事件处理延迟 **其他选项错误的原因：** - **选项A错误**：CloudWatch Logs主要用于日志分析，不是捕获CodePipeline状态变化的最佳方式，且无法直接订阅webhook URL到SNS - **选项B错误**：CloudTrail主要用于API调用审计，虽然能捕获事件但不是专门为实时事件处理设计的，延迟较高且成本更高 - **选项D错误**：修改管道代码的方式不够优雅，增加了代码复杂性，且无法捕获所有类型的状态变化（如手动批准），维护成本高 **决策标准和最佳实践：** 1. **事件驱动优先**：使用AWS原生事件服务而非轮询或日志解析 2. **服务专用性**：选择专门设计用于特定用途的服务（EventBridge用于事件处理） 3. **架构解耦**：通过SNS实现组件间的松耦合 4. **可扩展性**：解决方案应该易于扩展到多个通知目标 5. **实时性要求**：选择能提供最低延迟的服务组合</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">42</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s application development team uses Linux-based Amazon EC2 instances as bastion hosts. Inbound SSH access to the bastion hosts is restricted to specific IP addresses, as defined in the associated security groups. The company&#x27;s security team wants to receive a notification if the security group rules are modified to allow SSH access from any IP address. What should a DevOps engineer do to meet this requirement? associated with the bastion hosts. Configure Amazon Inspector to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic. C (68%) A (32%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule with a source of aws.cloudtrail and the event name AuthorizeSecurityGroupIngress. Define an Amazon Simple Notification Service (Amazon SNS) topic as the target.
B. Enable Amazon GuardDuty and check the findings for security groups in AWS Security Hub. Configure an Amazon EventBridge rule with a custom pattern that matches GuardDuty events with an output of NON_COMPLIANT. Define an Amazon Simple Notification Service (Amazon SNS) topic as the target.
C. Create an AWS Config rule by using the restricted-ssh managed rule to check whether security groups disallow unrestricted incoming SSH traffic. Configure automatic remediation to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.
D. Enable Amazon Inspector. Include the Common Vulnerabilities and Exposures-1.1 rules package to check the security groups that are</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的应用开发团队使用基于Linux的Amazon EC2实例作为堡垒主机。对堡垒主机的入站SSH访问被限制为特定的IP地址，这在相关的security groups中定义。公司的安全团队希望在security group规则被修改为允许来自任何IP地址的SSH访问时收到通知。DevOps工程师应该做什么来满足这个要求？ 选项： A. 创建一个Amazon EventBridge规则，源为aws.cloudtrail，事件名称为AuthorizeSecurityGroupIngress。定义Amazon Simple Notification Service (Amazon SNS) topic作为目标。 B. 启用Amazon GuardDuty并在AWS Security Hub中检查security groups的发现。配置Amazon EventBridge规则，使用自定义模式匹配输出为NON_COMPLIANT的GuardDuty事件。定义Amazon Simple Notification Service (Amazon SNS) topic作为目标。 C. 使用restricted-ssh托管规则创建AWS Config规则，检查security groups是否禁止不受限制的入站SSH流量。配置自动修复以向Amazon Simple Notification Service (Amazon SNS) topic发布消息。 D. 启用Amazon Inspector。包含Common Vulnerabilities and Exposures-1.1规则包来检查与堡垒主机关联的security groups。配置Amazon Inspector向Amazon Simple Notification Service (Amazon SNS) topic发布消息。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要监控security group规则的变化，特别是当SSH访问从受限IP地址变为允许任何IP地址（0.0.0.0/0）时，要及时发送通知给安全团队。 **涉及的关键AWS服务和概念：** - AWS Config：配置合规性监控服务 - Amazon EventBridge：事件驱动架构服务 - Amazon GuardDuty：威胁检测服务 - Amazon Inspector：安全评估服务 - Security Groups：EC2实例的虚拟防火墙 - CloudTrail：API调用审计服务 **正确答案C的原因：** 1. AWS Config的restricted-ssh托管规则专门设计用于检测security groups是否允许不受限制的SSH访问（0.0.0.0/0） 2. Config规则可以持续监控配置变化，一旦检测到违规配置立即触发 3. 自动修复功能可以直接与SNS集成，实现实时通知 4. 这是针对配置合规性监控的标准解决方案，完全符合需求 **其他选项错误的原因：** - **选项A**：虽然EventBridge + CloudTrail可以监控API调用，但AuthorizeSecurityGroupIngress事件只能检测到规则修改动作，无法判断具体的规则内容是否违规，可能产生大量误报 - **选项B**：GuardDuty主要用于威胁检测而非配置合规性检查，不会专门监控security group配置变化，且NON_COMPLIANT输出格式不正确 - **选项D**：Inspector主要用于应用程序和操作系统层面的安全评估，不负责监控security group配置，CVE规则包也与此需求无关 **决策标准和最佳实践：** 1. 选择专门针对配置合规性的服务（Config）而非通用监控服务 2. 使用AWS托管规则可以减少配置复杂性和错误 3. 实时监控配置变化比事后检测更有效 4. 自动化通知机制确保安全团队能及时响应</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">43</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps team manages an API running on-premises that serves as a backend for an Amazon API Gateway endpoint. Customers have been complaining about high response latencies, which the development team has verified using the API Gateway latency metrics in Amazon CloudWatch. To identify the cause, the team needs to collect relevant data without introducing additional latency. Which actions should be taken to accomplish this? (Choose two.) AC (87%) 13%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Install the CloudWatch agent server side and configure the agent to upload relevant logs to CloudWatch.
B. Enable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and upload those segments to X-Ray during each request.
C. Enable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and use the X-Ray daemon to upload segments to X-Ray.
D. Modify the on-premises application to send log information back to API Gateway with each request.
E. Modify the on-premises application to calculate and upload statistical data relevant to the API service requests to CloudWatch metrics.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps团队管理着一个运行在本地的API，该API作为Amazon API Gateway端点的后端。客户一直在抱怨响应延迟很高，开发团队已经通过Amazon CloudWatch中的API Gateway延迟指标验证了这个问题。为了识别原因，团队需要在不引入额外延迟的情况下收集相关数据。应该采取哪些行动来完成这个任务？（选择两个） 选项： A. 在服务器端安装CloudWatch agent并配置agent将相关日志上传到CloudWatch B. 在API Gateway中启用AWS X-Ray跟踪，修改应用程序以捕获请求段，并在每个请求期间将这些段上传到X-Ray C. 在API Gateway中启用AWS X-Ray跟踪，修改应用程序以捕获请求段，并使用X-Ray daemon将段上传到X-Ray D. 修改本地应用程序，使其在每个请求中将日志信息发送回API Gateway E. 修改本地应用程序以计算并上传与API服务请求相关的统计数据到CloudWatch metrics</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在不增加额外延迟的前提下，收集数据来分析API Gateway后端本地API的高延迟问题。关键约束是&quot;不引入额外延迟&quot;。 **涉及的关键AWS服务和概念：** - Amazon API Gateway：托管的API服务 - Amazon CloudWatch：监控和日志服务 - AWS X-Ray：分布式跟踪服务 - CloudWatch Agent：用于收集系统和应用程序指标的代理 **正确答案的原因：** 根据题目显示正确答案是A，但从技术角度分析，最佳答案应该是A和C的组合： A选项正确因为：CloudWatch agent可以异步收集和上传日志数据，不会在请求处理路径中增加延迟，是收集详细应用程序日志的最佳方式。 C选项应该也是正确的因为：X-Ray daemon作为独立进程运行，异步处理跟踪数据的上传，不会阻塞应用程序的请求处理，同时X-Ray提供了端到端的请求跟踪能力。 **其他选项错误的原因：** - B选项错误：在每个请求期间同步上传X-Ray段会显著增加延迟，违反了&quot;不引入额外延迟&quot;的要求 - D选项错误：在每个请求中将日志信息发送回API Gateway会增加网络开销和延迟 - E选项错误：在请求处理过程中计算和上传统计数据会增加处理时间和延迟 **决策标准和最佳实践：** 1. 选择异步数据收集方法，避免在请求处理路径中增加延迟 2. 使用专门的代理或守护进程来处理数据上传 3. 优先选择AWS原生的监控和跟踪服务 4. 确保监控解决方案不会影响生产环境的性能 5. 结合日志分析和分布式跟踪来全面诊断性能问题</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">44</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that is using a MySQL-compatible Amazon Aurora Multi-AZ DB cluster as the database. A cross-Region read replica has been created for disaster recovery purposes. A DevOps engineer wants to automate the promotion of the replica so it becomes the primary database instance in the event of a failure. Which solution will accomplish this? failure and runs an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Systems Manager Parameter Store. Code the application to reload the endpoint from Parameter Store if a database connection fails. D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a latency-based Amazon Route 53 CNAME with health checks so it points to both the primary and replica endpoints. Subscribe an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail and use that topic to invoke an AWS Lambda function that will promote the replica instance as the primary.
B. Create an Aurora custom endpoint to point to the primary database instance. Configure the application to use this endpoint. Configure AWS CloudTrail to run an AWS Lambda function to promote the replica instance and modify the custom endpoint to point to the newly promoted instance.
C. Create an AWS Lambda function to modify the application&#x27;s AWS CloudFormation template to promote the replica, apply the template to update the stack, and point the application to the newly promoted instance. Create an Amazon CloudWatch alarm to invoke this Lambda function after the failure event occurs.
D. Store the Aurora endpoint in AWS Systems Manager Parameter Store. Create an Amazon EventBridge event that detects the database</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个应用程序，使用MySQL兼容的Amazon Aurora Multi-AZ DB集群作为数据库。已经创建了一个跨Region的只读副本用于灾难恢复目的。DevOps工程师希望自动化副本的提升，使其在发生故障时成为主数据库实例。哪个解决方案能够实现这一目标？ 选项： A. 配置基于延迟的Amazon Route 53 CNAME并带有健康检查，使其指向主实例和副本端点。订阅Amazon SNS主题以接收来自AWS CloudTrail的Amazon RDS故障通知，并使用该主题调用AWS Lambda函数来提升副本实例为主实例。 B. 创建Aurora自定义端点指向主数据库实例。配置应用程序使用此端点。配置AWS CloudTrail运行AWS Lambda函数来提升副本实例并修改自定义端点指向新提升的实例。 C. 创建AWS Lambda函数来修改应用程序的AWS CloudFormation模板以提升副本，应用模板更新堆栈，并将应用程序指向新提升的实例。创建Amazon CloudWatch告警在故障事件发生后调用此Lambda函数。 D. 将Aurora端点存储在AWS Systems Manager Parameter Store中。创建Amazon EventBridge事件来检测数据库故障并运行AWS Lambda函数来提升副本实例并更新存储在AWS Systems Manager Parameter Store中的端点URL。编写应用程序代码，在数据库连接失败时从Parameter Store重新加载端点。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动化的灾难恢复解决方案，能够在主Aurora数据库集群故障时自动提升跨Region只读副本为新的主实例，并确保应用程序能够无缝切换到新的数据库端点。 **涉及的关键AWS服务和概念：** - Amazon Aurora Multi-AZ DB集群和跨Region只读副本 - AWS Systems Manager Parameter Store（参数存储） - Amazon EventBridge（事件驱动架构） - AWS Lambda（无服务器计算） - 数据库故障检测和自动故障转移 - 应用程序连接重试机制 **正确答案D的原因：** 1. **集中化配置管理**：使用Parameter Store存储数据库端点，实现配置与代码分离，便于动态更新 2. **事件驱动架构**：EventBridge能够实时检测数据库故障事件，触发自动化响应 3. **自动化端点更新**：Lambda函数既处理副本提升，又更新Parameter Store中的端点信息 4. **应用程序弹性**：应用程序具备连接失败时重新加载端点的能力，提高系统容错性 5. **完整的端到端解决方案**：从故障检测到副本提升再到应用程序重连，形成完整的自动化流程 **其他选项错误的原因：** - **选项A**：Route 53健康检查主要用于流量路由，不适合处理数据库副本提升；CloudTrail主要记录API调用，不是实时事件检测的最佳选择 - **选项B**：CloudTrail不是用于运行Lambda函数的服务；Aurora自定义端点无法跨Region工作，不适合跨Region灾难恢复场景 - **选项C**：使用CloudFormation模板更新过于复杂和缓慢，不适合需要快速响应的灾难恢复场景；CloudWatch告警的响应速度不如EventBridge事件 **决策标准和最佳实践：** 1. **事件驱动优于轮询**：使用EventBridge实时事件检测比定期检查更高效 2. **配置外部化**：将数据库端点存储在Parameter Store中，便于运行时动态更新 3. **应用程序弹性设计**：应用程序应具备处理连接失败和重新连接的能力 4. **自动化程度**：完全自动化的解决方案减少人工干预，提高恢复速度和可靠性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">45</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company hosts its staging website using an Amazon EC2 instance backed with Amazon EBS storage. The company wants to recover quickly with minimal data losses in the event of network connectivity issues or power failures on the EC2 instance. Which solution will meet these requirements? C (94%) A (3%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add the instance to an EC2 Auto Scaling group with the minimum, maximum, and desired capacity set to 1.
B. Add the instance to an EC2 Auto Scaling group with a lifecycle hook to detach the EBS volume when the EC2 instance shuts down or terminates.
C. Create an Amazon CloudWatch alarm for the StatusCheckFailed System metric and select the EC2 action to recover the instance.
D. Create an Amazon CloudWatch alarm for the StatusCheckFailed Instance metric and select the EC2 action to reboot the instance.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用由Amazon EBS存储支持的Amazon EC2实例来托管其预发布网站。该公司希望在EC2实例出现网络连接问题或电源故障时能够快速恢复并将数据损失降到最低。哪种解决方案能满足这些要求？ 选项： A. 将实例添加到EC2 Auto Scaling组中，将最小、最大和期望容量都设置为1。 B. 将实例添加到EC2 Auto Scaling组中，并使用生命周期钩子在EC2实例关闭或终止时分离EBS卷。 C. 为StatusCheckFailed System指标创建Amazon CloudWatch告警，并选择EC2操作来恢复实例。 D. 为StatusCheckFailed Instance指标创建Amazon CloudWatch告警，并选择EC2操作来重启实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 题目要求在EC2实例出现网络连接问题或电源故障时实现快速恢复并最小化数据损失。关键词是&quot;快速恢复&quot;和&quot;最小数据损失&quot;。 **涉及的关键AWS服务和概念：** - EC2 Auto Scaling：自动扩缩容服务，可以自动替换不健康的实例 - CloudWatch告警：监控服务，可以基于指标触发自动化操作 - EBS存储：持久化块存储，数据独立于EC2实例生命周期 - EC2状态检查：包括System和Instance两种类型的健康检查 **正确答案A的原因：** Auto Scaling组设置容量为1可以确保始终有一个健康的实例运行。当原实例因为网络或电源问题失效时，Auto Scaling会自动检测到实例不健康并启动新实例替换。由于使用EBS存储，数据会持久保存，新实例可以挂载相同的EBS卷，实现快速恢复且无数据损失。 **其他选项错误的原因：** - 选项B：生命周期钩子分离EBS卷的做法是多余的，而且可能导致数据访问问题，不利于快速恢复。 - 选项C：System状态检查失败通常表示底层硬件问题，EC2恢复操作可能无法解决根本问题，且恢复时间较长。 - 选项D：Instance状态检查失败时仅重启实例，无法解决底层硬件故障导致的问题，不能保证可靠恢复。 **决策标准和最佳实践：** 选择恢复策略时应考虑：1）故障类型覆盖范围；2）恢复时间目标(RTO)；3）数据保护程度。Auto Scaling提供了最全面的故障恢复能力，能够处理各种类型的实例故障，并通过自动替换实例实现最快的恢复速度。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">46</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to use AWS development tools to replace its current bash deployment scripts. The company currently deploys a LAMP application to a group of Amazon EC2 instances behind an Application Load Balancer (ALB). During the deployments, the company unit tests the committed application, stops and starts services, unregisters and re-registers instances with the load balancer, and updates file permissions. The company wants to maintain the same deployment functionality through the shift to using AWS services. Which solution will meet these requirements? D (89%) 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy&#x27;s appspec.yml file to restart services, and deregister and register instances with the ALB. Use the appspec.yml file to update file permissions without a custom script.
B. Use AWS CodePipeline to move the application from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy&#x27;s deployment group to test the application, unregister and re-register instances with the ALB, and restart services. Use the appspec.yml file to update file permissions without a custom script.
C. Use AWS CodePipeline to move the application source code from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy to test the application. Use CodeDeploy&#x27;s appspec.yml file to restart services and update permissions without a custom script. Use AWS CodeBuild to unregister and re-register instances with the ALB.
D. Use AWS CodePipeline to trigger AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy&#x27;s appspec.yml file to restart services. Unregister and re-register the instances in the AWS CodeDeploy deployment group with the ALB. Update the appspec.yml file to update file permissions without a custom script.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司希望使用AWS开发工具来替换其当前的bash部署脚本。该公司目前将LAMP应用程序部署到Application Load Balancer (ALB)后面的一组Amazon EC2实例上。在部署过程中，公司会对提交的应用程序进行单元测试，停止和启动服务，从负载均衡器注销和重新注册实例，并更新文件权限。公司希望在转向使用AWS服务的过程中保持相同的部署功能。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求使用AWS开发工具替换传统bash脚本，实现完整的CI/CD流水线，包括：单元测试、服务重启、ALB实例注册管理、文件权限更新等功能。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD流水线编排服务 - AWS CodeBuild：构建和测试服务 - AWS CodeDeploy：应用程序部署服务 - AWS CodeCommit：源代码管理服务 - appspec.yml：CodeDeploy的部署配置文件 - ALB：Application Load Balancer应用负载均衡器 **正确答案D的原因：** 1. **完整的流水线架构**：使用CodePipeline作为主要编排工具，触发CodeBuild进行测试，然后使用CodeDeploy进行部署 2. **正确的测试方式**：CodeBuild专门用于构建和测试，是进行单元测试的正确选择 3. **合理的脚本使用**：通过appspec.yml调用bash脚本来重启服务，保持了灵活性 4. **自动化ALB管理**：CodeDeploy deployment group可以自动处理ALB的实例注册和注销 5. **原生权限管理**：appspec.yml原生支持文件权限设置，无需自定义脚本 **其他选项错误的原因：** - **选项A**：缺少CodePipeline作为流水线编排，无法形成完整的CI/CD流程 - **选项B**：错误地让CodeDeploy承担测试功能，CodeDeploy主要用于部署而非测试 - **选项C**：让CodeBuild处理ALB注册管理是不合适的，这应该由CodeDeploy在部署过程中自动处理 **决策标准和最佳实践：** 1. **服务职责分离**：CodeBuild负责构建测试，CodeDeploy负责部署，CodePipeline负责编排 2. **自动化优先**：尽量使用AWS服务的原生功能，减少自定义脚本 3. **流水线完整性**：确保从源代码到部署的完整自动化流程 4. **最小化复杂性**：选择最直接、最符合AWS最佳实践的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">47</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs an application with an Amazon EC2 and on-premises configuration. A DevOps engineer needs to standardize patching across both environments. Company policy dictates that patching only happens during non-business hours. Which combination of actions will meet these requirements? (Choose three.) F. Use AWS Systems Manager Maintenance Windows to schedule a patch window. Most Voted ABF (89%) 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add the physical machines into AWS Systems Manager using Systems Manager Hybrid Activations.
B. Attach an IAM role to the EC2 instances, allowing them to be managed by AWS Systems Manager.
C. Create IAM access keys for the on-premises machines to interact with AWS Systems Manager.
D. Run an AWS Systems Manager Automation document to patch the systems every hour.
E. Use Amazon EventBridge scheduled events to schedule a patch window.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司运行着一个包含Amazon EC2和本地配置的应用程序。DevOps工程师需要在两个环境中标准化补丁管理。公司政策规定补丁只能在非工作时间进行。哪些操作组合能满足这些要求？（选择三个。）选项：A. 使用AWS Systems Manager Hybrid Activations将物理机器添加到AWS Systems Manager中。B. 为EC2实例附加IAM角色，允许它们被AWS Systems Manager管理。C. 为本地机器创建IAM访问密钥以与AWS Systems Manager交互。D. 运行AWS Systems Manager Automation文档每小时对系统进行补丁。E. 使用Amazon EventBridge计划事件来安排补丁窗口。F. 使用AWS Systems Manager Maintenance Windows来安排补丁窗口。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在混合环境（EC2 + 本地服务器）中实现标准化的补丁管理，并且必须在非工作时间执行补丁操作。需要选择三个正确的操作组合。 **涉及的关键AWS服务和概念：** - AWS Systems Manager：统一管理AWS和本地资源的服务 - Systems Manager Hybrid Activations：用于将本地服务器注册到Systems Manager的功能 - IAM角色：为AWS资源提供权限的安全机制 - Maintenance Windows：Systems Manager中用于安排维护任务的功能 - Patch Manager：Systems Manager的补丁管理组件 **正确答案分析（A、B、F）：** - **选项A正确**：Hybrid Activations是将本地服务器纳入Systems Manager管理的标准方法，通过激活码和激活ID实现安全连接 - **选项B正确**：EC2实例需要IAM角色才能与Systems Manager通信，这是基本的权限配置要求 - **选项F正确**：Maintenance Windows是Systems Manager的原生功能，专门用于安排维护窗口，可以精确控制补丁执行时间 **其他选项错误的原因：** - **选项C错误**：本地机器应该使用Hybrid Activations而不是IAM访问密钥，后者不是推荐的安全做法且管理复杂 - **选项D错误**：每小时执行补丁违反了&quot;仅在非工作时间&quot;的政策要求，频率过高且不合理 - **选项E错误**：虽然EventBridge可以触发事件，但Maintenance Windows是更直接、更适合补丁管理场景的解决方案 **决策标准和最佳实践：** 1. **统一管理**：使用Systems Manager实现跨环境的标准化管理 2. **安全性**：本地服务器使用Hybrid Activations，EC2使用IAM角色，避免长期访问密钥 3. **时间控制**：使用专门的Maintenance Windows功能而非通用的事件调度 4. **合规性**：确保补丁策略符合公司的时间窗口要求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">48</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has chosen AWS to host a new application. The company needs to implement a multi-account strategy. A DevOps engineer creates a new AWS account and an organization in AWS Organizations. The DevOps engineer also creates the OU structure for the organization and sets up a landing zone by using AWS Control Tower. The DevOps engineer must implement a solution that automatically deploys resources for new accounts that users create through AWS Control Tower Account Factory. When a user creates a new account, the solution must apply AWS CloudFormation templates and SCPs that are customized for the OU or the account to automatically deploy all the resources that are attached to the account. All the OUs are enrolled in AWS Control Tower. Which solution will meet these requirements in the MOST automated way? resources to any new accounts. Deploy SCPs by using the AWS CLI and JSON documents. D (91%) 9%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Service Catalog with AWS Control Tower. Create portfolios and products in AWS Service Catalog. Grant granular permissions to provision these resources. Deploy SCPs by using the AWS CLI and JSON documents.
B. Deploy CloudFormation stack sets by using the required templates. Enable automatic deployment. Deploy stack instances to the required accounts. Deploy a CloudFormation stack set to the organization&#x27;s management account to deploy SCPs.
C. Create an Amazon EventBridge rule to detect the CreateManagedAccount event. Configure AWS Service Catalog as the target to deploy
D. Deploy the Customizations for AWS Control Tower (CfCT) solution. Use an AWS CodeCommit repository as the source. In the repository, create a custom package that includes the CloudFormation templates and the SCP JSON documents.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司选择了AWS来托管一个新应用程序。该公司需要实施多账户策略。DevOps工程师创建了一个新的AWS账户和AWS Organizations中的组织。DevOps工程师还为组织创建了OU结构，并使用AWS Control Tower设置了landing zone。DevOps工程师必须实施一个解决方案，自动为用户通过AWS Control Tower Account Factory创建的新账户部署资源。当用户创建新账户时，解决方案必须应用为OU或账户定制的AWS CloudFormation模板和SCP，以自动部署附加到账户的所有资源。所有OU都已注册到AWS Control Tower。哪个解决方案能以最自动化的方式满足这些要求？ 选项： A. 将AWS Service Catalog与AWS Control Tower结合使用。在AWS Service Catalog中创建产品组合和产品。授予精细权限来配置这些资源。使用AWS CLI和JSON文档部署SCP。 B. 使用所需模板部署CloudFormation stack sets。启用自动部署。将stack实例部署到所需账户。将CloudFormation stack set部署到组织的管理账户以部署SCP。 C. 创建Amazon EventBridge规则来检测CreateManagedAccount事件。配置AWS Service Catalog作为目标来部署资源到任何新账户。使用AWS CLI和JSON文档部署SCP。 D. 部署Customizations for AWS Control Tower (CfCT)解决方案。使用AWS CodeCommit存储库作为源。在存储库中，创建包含CloudFormation模板和SCP JSON文档的自定义包。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现一个完全自动化的解决方案，当通过AWS Control Tower Account Factory创建新账户时，能够自动部署定制化的CloudFormation模板和SCP策略。关键要求是&quot;最自动化&quot;和能够根据不同OU或账户进行定制化部署。 **涉及的关键AWS服务和概念：** - AWS Control Tower：提供多账户环境的设置和治理 - AWS Organizations：管理多个AWS账户 - Account Factory：Control Tower中用于创建新账户的功能 - CloudFormation：基础设施即代码服务 - SCP (Service Control Policies)：组织级别的权限控制策略 - Customizations for AWS Control Tower (CfCT)：专门为Control Tower设计的定制化解决方案 **正确答案D的原因：** 1. **专门设计**：CfCT是AWS官方提供的专门为Control Tower环境设计的定制化解决方案 2. **完全自动化**：能够在Account Factory创建新账户时自动触发资源部署 3. **统一管理**：通过CodeCommit存储库集中管理CloudFormation模板和SCP文档 4. **灵活定制**：支持基于OU或账户级别的定制化部署 5. **原生集成**：与Control Tower深度集成，无需额外的事件监听或手动配置 **其他选项错误的原因：** - **选项A**：Service Catalog需要手动授权和配置，SCP部署需要手动使用CLI，自动化程度不够 - **选项B**：CloudFormation stack sets需要手动管理stack实例，且SCP部署到管理账户的方式不符合最佳实践 - **选项C**：需要手动创建EventBridge规则和配置Service Catalog，SCP部署仍需手动CLI操作，复杂度高且自动化不完整 **决策标准和最佳实践：** 1. **选择专用工具**：对于Control Tower环境，优先选择专门设计的CfCT解决方案 2. **最大化自动化**：避免需要手动CLI操作或复杂事件配置的方案 3. **集中化管理**：使用代码存储库统一管理所有定制化资源 4. **原生集成优先**：选择与现有AWS服务深度集成的解决方案，减少维护复杂性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">49</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An online retail company based in the United States plans to expand its operations to Europe and Asia in the next six months. Its product currently runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. All data is stored in an Amazon Aurora database instance. When the product is deployed in multiple regions, the company wants a single product catalog across all regions, but for compliance purposes, its customer information and purchases must be kept in each region. How should the company meet these requirements with the LEAST amount of application changes? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer information and purchases.
B. Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer information and purchases.
C. Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases.
D. Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer information and purchases.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家位于美国的在线零售公司计划在未来六个月内将业务扩展到欧洲和亚洲。其产品目前运行在Application Load Balancer后面的Amazon EC2实例上。这些实例在多个Availability Zone中的Amazon EC2 Auto Scaling组中运行。所有数据都存储在Amazon Aurora数据库实例中。当产品部署在多个区域时，公司希望在所有区域中使用单一的产品目录，但出于合规目的，客户信息和购买记录必须保存在各自的区域中。公司应该如何以最少的应用程序更改来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 全球多区域部署（美国、欧洲、亚洲） - 产品目录需要全球统一共享 - 客户信息和购买记录需要区域隔离（合规要求） - 最小化应用程序代码更改 **涉及的关键AWS服务和概念：** - Aurora数据库及其跨区域复制能力 - DynamoDB Global Tables全球表功能 - 数据合规性和区域数据驻留要求 - 应用程序架构迁移复杂度 **正确答案C的原因：** 1. **最少应用更改**：当前已使用Aurora，继续使用相同数据库引擎无需修改应用代码 2. **产品目录全球共享**：Aurora read replicas可以将主区域的产品目录数据同步到其他区域，实现全球统一 3. **数据合规性**：每个区域的独立Aurora实例确保客户数据和购买记录严格保留在本地区域 4. **性能优化**：read replicas提供低延迟的本地读取访问 **其他选项错误的原因：** - **选项A（Redshift + DynamoDB）**：需要重大应用架构改造，Redshift主要用于数据仓库而非事务处理 - **选项B（DynamoDB Global Tables）**：虽然技术可行，但从Aurora迁移到DynamoDB需要大量应用代码重写 - **选项D（Aurora + DynamoDB Global Tables）**：混合数据库架构增加复杂性，且DynamoDB Global Tables会在所有区域复制数据，违反合规要求 **决策标准和最佳实践：** 1. **最小化变更原则**：优先选择与现有技术栈兼容的解决方案 2. **数据主权合规**：确保敏感数据严格遵守区域驻留要求 3. **架构一致性**：保持统一的数据库技术栈降低运维复杂度 4. **性能考虑**：利用Aurora的跨区域复制能力实现高效的全球数据分发</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">50</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is implementing a well-architected design for its globally accessible API stack. The design needs to ensure both high reliability and fast response times for users located in North America and Europe. The API stack contains the following three tiers: - Amazon API Gateway - AWS Lambda - Amazon DynamoDB Which solution will meet the requirements? to check the North America API health every 5 minutes. In the event of a failure, update Route 53 to point to the disaster recovery API. requests to the Lambda function in the Region nearest to the user. Configure the Lambda function to retrieve and update the data in a DynamoDB table. B (100%) Get IT Certification Unlock free, top-quality video courses on ExamTopics with a simple registration. Elevate your learning journey with our expertly curated content. Register now to access a diverse range of educational resources designed for success!</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure Amazon Route 53 to point to API Gateway APIs in North America and Europe using health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB table in the same Region as the Lambda function.
B. Configure Amazon Route 53 to point to API Gateway APIs in North America and Europe using latency-based routing and health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB global table.
C. Configure Amazon Route 53 to point to API Gateway in North America, create a disaster recovery API in Europe, and configure both APIs to forward requests to the Lambda functions in that Region. Retrieve the data from a DynamoDB global table. Deploy a Lambda function
D. Configure Amazon Route 53 to point to API Gateway API in North America using latency-based routing. Configure the API to forward</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在为其全球可访问的API堆栈实施良好架构设计。该设计需要确保位于北美和欧洲的用户都能获得高可靠性和快速响应时间。API堆栈包含以下三层：- Amazon API Gateway - AWS Lambda - Amazon DynamoDB 哪种解决方案能满足要求？ 选项： A. 配置Amazon Route 53使用健康检查指向北美和欧洲的API Gateway API。配置API将请求转发到该Region中的Lambda函数。配置Lambda函数在与Lambda函数相同Region的DynamoDB表中检索和更新数据。 B. 配置Amazon Route 53使用基于延迟的路由和健康检查指向北美和欧洲的API Gateway API。配置API将请求转发到该Region中的Lambda函数。配置Lambda函数在DynamoDB global table中检索和更新数据。 C. 配置Amazon Route 53指向北美的API Gateway，在欧洲创建灾难恢复API，并配置两个API将请求转发到该Region中的Lambda函数。从DynamoDB global table检索数据。部署Lambda函数... D. 配置Amazon Route 53使用基于延迟的路由指向北美的API Gateway API。配置API转发...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个全球可访问的API架构，需要同时满足高可靠性和快速响应时间两个关键需求，服务于北美和欧洲用户。 **涉及的关键AWS服务和概念：** 1. Amazon Route 53 - DNS服务，支持健康检查和基于延迟的路由 2. API Gateway - API管理服务 3. AWS Lambda - 无服务器计算服务 4. DynamoDB Global Tables - 全球分布式数据库解决方案 5. 基于延迟的路由(Latency-based routing) - 自动将用户路由到延迟最低的endpoint 6. 健康检查(Health checks) - 监控服务可用性 **正确答案B的原因：** 1. **基于延迟的路由**：确保用户被自动路由到延迟最低的Region，满足快速响应时间要求 2. **健康检查**：提供故障转移能力，确保高可靠性 3. **多Region部署**：在北美和欧洲都部署完整的API堆栈，实现真正的高可用性 4. **DynamoDB Global Tables**：提供跨Region的数据同步，确保数据一致性和本地访问性能 **其他选项错误的原因：** - **选项A**：只有健康检查没有基于延迟的路由，无法保证用户总是访问到最近的Region；使用单Region DynamoDB表而非Global Tables，无法提供最佳的全球性能 - **选项C**：采用主-备模式而非主-主模式，欧洲只作为灾难恢复，正常情况下欧洲用户仍需访问北美，无法提供最佳延迟 - **选项D**：选项不完整，且似乎只配置了北美Region **决策标准和最佳实践：** 1. **Well-Architected Framework**：遵循可靠性和性能效率支柱 2. **全球部署策略**：多Region主-主部署优于主-备部署 3. **DNS路由策略**：基于延迟的路由比简单的健康检查路由更适合全球用户 4. **数据层设计**：Global Tables是跨Region数据同步的最佳实践 5. **故障转移**：健康检查确保自动故障转移，提高系统弹性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">51</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A rapidly growing company wants to scale for developer demand for AWS development environments. Development environments are created manually in the AWS Management Console. The networking team uses AWS CloudFormation to manage the networking infrastructure, exporting stack output values for the Amazon VPC and all subnets. The development environments have common standards, such as Application Load Balancers, Amazon EC2 Auto Scaling groups, security groups, and Amazon DynamoDB tables. To keep up with demand, the DevOps engineer wants to automate the creation of development environments. Because the infrastructure required to support the application is expected to grow, there must be a way to easily update the deployed infrastructure. CloudFormation will be used to create a template for the development environments. Which approach will meet these requirements and quickly provide consistent AWS environments for developers? subnet values. Define the development resources in the order they need to be created in the CloudFormation nested stacks. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments. C (77%) B (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Fn::ImportValue intrinsic functions in the Resources section of the template to retrieve Virtual Private Cloud (VPC) and subnet values. Use CloudFormation StackSets for the development environments, using the Count input parameter to indicate the number of environments needed. Use the UpdateStackSet command to update existing development environments.
B. Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team&#x27;s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.
C. Use nested stacks to define common infrastructure components. Use Fn::ImportValue intrinsic functions with the resources of the nested stack to retrieve Virtual Private Cloud (VPC) and subnet values. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.
D. Use Fn::ImportValue intrinsic functions in the Parameters section of the root template to retrieve Virtual Private Cloud (VPC) and</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家快速增长的公司希望扩展以满足开发人员对AWS开发环境的需求。开发环境目前在AWS Management Console中手动创建。网络团队使用AWS CloudFormation来管理网络基础设施，导出Amazon VPC和所有subnet的stack输出值。开发环境有通用标准，如Application Load Balancers、Amazon EC2 Auto Scaling groups、security groups和Amazon DynamoDB tables。为了跟上需求，DevOps工程师希望自动化创建开发环境。由于支持应用程序所需的基础设施预计会增长，必须有一种方法来轻松更新已部署的基础设施。将使用CloudFormation为开发环境创建template。哪种方法将满足这些要求并快速为开发人员提供一致的AWS环境？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 自动化创建开发环境，替代手动操作 - 利用网络团队已导出的VPC和subnet值 - 支持基础设施的轻松更新 - 提供一致性的开发环境 - 处理不断增长的基础设施需求 **涉及的关键AWS服务和概念：** - CloudFormation nested stacks：用于组织和重用模板组件 - Fn::ImportValue：用于导入其他stack导出的值 - CreateChangeSet/ExecuteChangeSet：用于安全地更新现有stack - CloudFormation StackSets：用于跨多个账户和区域部署stack **正确答案C的原因：** 1. **Nested stacks设计合理**：将通用基础设施组件定义为nested stacks，提供了良好的模块化和重用性 2. **正确使用Fn::ImportValue**：在nested stack的resources中使用Fn::ImportValue来检索VPC和subnet值，这是正确的语法位置 3. **适当的更新机制**：使用CreateChangeSet和ExecuteChangeSet命令提供了安全的更新方式，可以预览变更后再执行 4. **满足扩展需求**：nested stack架构支持基础设施的增长和组件的独立管理 **其他选项错误的原因：** - **选项A**：StackSets主要用于跨账户/区域部署，不是这个场景的最佳选择；Count参数不是StackSets的标准参数 - **选项B**：在Parameters section中使用Fn::ImportValue是错误的语法；TemplateURL用于引用nested stack而不是导入值 - **选项D**：题目截断，但从描述看是在Parameters section使用Fn::ImportValue，这是错误的用法 **决策标准和最佳实践：** 1. **模块化设计**：使用nested stacks将通用组件模块化，便于维护和重用 2. **正确的函数使用**：Fn::ImportValue应在Resources或Outputs section中使用，不是Parameters section 3. **安全更新**：使用ChangeSet机制可以预览变更，降低更新风险 4. **架构可扩展性**：nested stack架构支持未来基础设施的增长和变更</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">52</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Organizations to manage multiple accounts. Information security policies require that all unencrypted Amazon EBS volumes be marked as non-compliant. A DevOps engineer needs to automatically deploy the solution and ensure that this compliance check is always present. Which solution will accomplish this? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CloudFormation template that defines an AWS Inspector rule to check whether EBS encryption is enabled. Save the template to an Amazon S3 bucket that has been shared with all accounts within the company. Update the account creation script pointing to the CloudFormation template in Amazon S3.
B. Create an AWS Config organizational rule to check whether EBS encryption is enabled and deploy the rule using the AWS CLI. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization.
C. Create an SCP in Organizations. Set the policy to prevent the launch of Amazon EC2 instances without encryption on the EBS volumes using a conditional expression. Apply the SCP to all AWS accounts. Use Amazon Athena to analyze the AWS CloudTrail output, looking for events that deny an ec2:RunInstances action.
D. Deploy an IAM role to all accounts from a single trusted account. Build a pipeline with AWS CodePipeline with a stage in AWS Lambda to assume the IAM role, and list all EBS volumes in the account. Publish a report to Amazon S3.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations来管理多个账户。信息安全策略要求所有未加密的Amazon EBS卷都被标记为不合规。DevOps工程师需要自动部署解决方案并确保此合规性检查始终存在。哪个解决方案能够实现这一目标？ 选项： A. 创建一个AWS CloudFormation模板，定义AWS Inspector规则来检查是否启用了EBS加密。将模板保存到与公司内所有账户共享的Amazon S3存储桶中。更新账户创建脚本，指向Amazon S3中的CloudFormation模板。 B. 创建AWS Config组织规则来检查是否启用了EBS加密，并使用AWS CLI部署该规则。创建并应用SCP来禁止在整个组织中停止和删除AWS Config。 C. 在Organizations中创建SCP。设置策略以防止在EBS卷未加密的情况下启动Amazon EC2实例，使用条件表达式。将SCP应用到所有AWS账户。使用Amazon Athena分析AWS CloudTrail输出，查找拒绝ec2:RunInstances操作的事件。 D. 从单个受信任账户向所有账户部署IAM角色。使用AWS CodePipeline构建管道，在AWS Lambda中设置阶段来承担IAM角色，并列出账户中的所有EBS卷。将报告发布到Amazon S3。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Organizations管理的多账户环境中，自动部署一个解决方案来检测和标记未加密的EBS卷为不合规状态，并确保这个合规性检查机制始终存在且不被删除。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Config：合规性监控和配置管理服务 - AWS Config组织规则：在组织层面统一部署的合规性规则 - SCP (Service Control Policy)：服务控制策略，用于限制账户权限 - EBS加密：存储卷加密功能 - AWS Inspector、CloudFormation、CodePipeline等其他相关服务 **正确答案B的原因：** 1. **AWS Config组织规则**：能够在组织层面统一部署合规性检查规则，自动检测所有账户中的EBS卷加密状态 2. **自动化部署**：通过AWS CLI可以实现自动化部署，满足DevOps需求 3. **持久性保护**：通过SCP禁止停止和删除AWS Config，确保合规性检查始终存在 4. **集中管理**：组织规则提供了集中管理和监控的能力 5. **实时监控**：Config规则可以实时监控资源状态变化并标记不合规资源 **其他选项错误的原因：** - **选项A**：AWS Inspector主要用于安全评估，不是专门的合规性监控工具；依赖手动脚本更新，自动化程度不够 - **选项C**：SCP只能预防性地阻止创建未加密卷，但无法检测和标记现有的未加密卷为不合规；这是预防而非检测方案 - **选项D**：这是一个自定义开发方案，复杂度高，维护成本大，且没有内置的合规性标记功能；缺乏持久性保护机制 **决策标准和最佳实践：** 1. **使用托管服务**：优先选择AWS原生的合规性服务而非自建方案 2. **组织级管理**：在多账户环境中使用组织级功能实现统一管理 3. **自动化优先**：选择能够自动化部署和管理的解决方案 4. **持久性保护**：通过SCP等机制确保合规性检查不被意外删除 5. **实时监控**：选择能够实时检测和响应配置变化的服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">53</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is performing vulnerability scanning for all Amazon EC2 instances across many accounts. The accounts are in an organization in AWS Organizations. Each account&#x27;s VPCs are attached to a shared transit gateway. The VPCs send traffic to the internet through a central egress VPC. The company has enabled Amazon Inspector in a delegated administrator account and has enabled scanning for all member accounts. A DevOps engineer discovers that some EC2 instances are listed in the &quot;not scanning&quot; tab in Amazon Inspector. Which combination of actions should the DevOps engineer take to resolve this issue? (Choose three.) F. Create a managed-instance activation. Use the Activation Code and the Activation ID to register the EC2 instances. ABE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Verify that AWS Systems Manager Agent is installed and is running on the EC2 instances that Amazon Inspector is not scanning.
B. Associate the target EC2 instances with security groups that allow outbound communication on port 443 to the AWS Systems Manager service endpoint.
C. Grant inspector:StartAssessmentRun permissions to the IAM role that the DevOps engineer is using.
D. Configure EC2 Instance Connect for the EC2 instances that Amazon Inspector is not scanning.
E. Associate the target EC2 instances with instance profiles that grant permissions to communicate with AWS Systems Manager.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在对AWS Organizations组织中多个账户的所有Amazon EC2实例执行漏洞扫描。这些账户都在一个组织中。每个账户的VPC都连接到共享的transit gateway。VPC通过中央出口VPC向互联网发送流量。该公司已在委托管理员账户中启用了Amazon Inspector，并为所有成员账户启用了扫描。DevOps工程师发现一些EC2实例在Amazon Inspector的&quot;not scanning&quot;选项卡中列出。DevOps工程师应该采取哪些操作组合来解决此问题？（选择三个。） 选项： A. 验证AWS Systems Manager Agent已安装并在Amazon Inspector未扫描的EC2实例上运行。 B. 将目标EC2实例与允许在端口443上与AWS Systems Manager服务端点进行出站通信的安全组关联。 C. 向DevOps工程师使用的IAM角色授予inspector:StartAssessmentRun权限。 D. 为Amazon Inspector未扫描的EC2实例配置EC2 Instance Connect。 E. 将目标EC2实例与授予与AWS Systems Manager通信权限的实例配置文件关联。 F. 创建托管实例激活。使用激活代码和激活ID注册EC2实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是Amazon Inspector漏洞扫描服务中EC2实例显示&quot;not scanning&quot;状态的故障排除。需要理解Inspector的工作原理以及它与AWS Systems Manager的依赖关系。 **涉及的关键AWS服务和概念：** 1. Amazon Inspector - AWS的漏洞评估服务 2. AWS Systems Manager (SSM) - 用于管理EC2实例的服务 3. SSM Agent - 在EC2实例上运行的代理程序 4. IAM角色和实例配置文件 - 用于权限管理 5. 安全组 - 控制网络访问的防火墙规则 **正确答案的原因：** 选择A、B、E是正确的，因为： A. **SSM Agent是必需的** - Amazon Inspector依赖AWS Systems Manager来与EC2实例通信并执行扫描。如果SSM Agent未安装或未运行，Inspector无法访问实例进行扫描。 B. **网络连接是必需的** - EC2实例必须能够通过HTTPS（端口443）与AWS Systems Manager服务端点通信。如果安全组阻止了这种出站连接，SSM Agent无法与服务通信。 E. **适当的IAM权限是必需的** - EC2实例需要具有允许与Systems Manager通信的IAM角色。没有正确的实例配置文件，实例无法向SSM服务进行身份验证。 **其他选项错误的原因：** C. **inspector:StartAssessmentRun权限不相关** - 这个权限用于启动评估运行，但不会解决实例不被扫描的问题。 D. **EC2 Instance Connect不相关** - 这是用于SSH连接的服务，与Inspector扫描无关。 F. **托管实例激活用于混合环境** - 这主要用于本地服务器或其他云中的实例，不适用于标准EC2实例。 **决策标准和最佳实践：** 1. 确保所有EC2实例都安装了最新版本的SSM Agent 2. 配置安全组允许必要的出站连接到AWS服务 3. 为EC2实例分配具有适当SSM权限的IAM角色 4. 定期检查Inspector控制台中的&quot;not scanning&quot;状态并及时解决 5. 在多账户环境中，确保委托管理员账户具有适当的跨账户权限</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">54</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A development team uses AWS CodeCommit for version control for applications. The development team uses AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy for CI/CD infrastructure. In CodeCommit, the development team recently merged pull requests that did not pass long-running tests in the code base. The development team needed to perform rollbacks to branches in the codebase, resulting in lost time and wasted effort. A DevOps engineer must automate testing of pull requests in CodeCommit to ensure that reviewers more easily see the results of automated tests as part of the pull request review. What should the DevOps engineer do to meet this requirement? CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete. C (65%) B (20%) D (16%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code review.
B. Create an Amazon EventBridge rule that reacts to the pullRequestCreated event. Create an AWS Lambda function that invokes a
C. Create an Amazon EventBridge rule that reacts to pullRequestCreated and pullRequestSourceBranchUpdated events. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code review.
D. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个开发团队使用AWS CodeCommit进行应用程序版本控制。开发团队使用AWS CodePipeline、AWS CodeBuild和AWS CodeDeploy构建CI/CD基础设施。在CodeCommit中，开发团队最近合并了一些没有通过代码库中长时间运行测试的pull request。开发团队需要对代码库中的分支执行回滚操作，导致时间损失和工作浪费。DevOps工程师必须自动化CodeCommit中pull request的测试，以确保审查者能够更容易地看到自动化测试结果作为pull request审查的一部分。DevOps工程师应该怎么做来满足这个要求？ 选项： A. 创建一个Amazon EventBridge规则来响应pullRequestStatusChanged事件。创建一个AWS Lambda函数来调用带有CodeBuild操作的CodePipeline pipeline来运行应用程序测试。编程Lambda函数将CodeBuild徽章作为评论发布到pull request上，以便开发者在代码审查中看到徽章。 B. 创建一个Amazon EventBridge规则来响应pullRequestCreated事件。创建一个AWS Lambda函数来调用带有CodeBuild操作的CodePipeline pipeline来运行应用程序测试。编程Lambda函数在测试结果完成时将CodeBuild测试结果作为评论发布到pull request上。 C. 创建一个Amazon EventBridge规则来响应pullRequestCreated和pullRequestSourceBranchUpdated事件。创建一个AWS Lambda函数来调用带有CodeBuild操作的CodePipeline pipeline来运行应用程序测试。编程Lambda函数将CodeBuild徽章作为评论发布到pull request上，以便开发者在代码审查中看到徽章。 D. 创建一个Amazon EventBridge规则来响应pullRequestStatusChanged事件。创建一个AWS Lambda函数来调用带有CodeBuild操作的CodePipeline pipeline来运行应用程序测试。编程Lambda函数在测试结果完成时将CodeBuild测试结果作为评论发布到pull request上。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现pull request的自动化测试，确保在代码合并前能够自动运行测试并将结果展示给审查者，避免合并未通过测试的代码导致的回滚问题。 **涉及的关键AWS服务和概念：** 1. AWS CodeCommit - Git版本控制服务，支持pull request功能 2. Amazon EventBridge - 事件驱动服务，可以监听CodeCommit事件 3. AWS Lambda - 无服务器计算服务，用于处理事件和编排流程 4. AWS CodePipeline - CI/CD管道服务 5. AWS CodeBuild - 构建和测试服务 6. CodeCommit事件类型：pullRequestCreated、pullRequestSourceBranchUpdated、pullRequestStatusChanged **正确答案D的原因：** 1. **事件选择合适**：pullRequestStatusChanged事件能够捕获pull request生命周期中的关键状态变化，包括创建、更新等多种情况 2. **完整的测试结果**：发布完整的测试结果而不仅仅是徽章，为审查者提供详细信息 3. **时机准确**：在测试结果完成时发布评论，确保信息的及时性和准确性 **其他选项错误的原因：** - **选项A**：虽然事件选择正确，但只发布CodeBuild徽章而不是详细的测试结果，信息不够完整 - **选项B**：只监听pullRequestCreated事件，无法处理pull request更新的情况，会遗漏后续的代码变更 - **选项C**：虽然监听了创建和更新事件，但同样只发布徽章而不是详细测试结果，且事件组合不如pullRequestStatusChanged全面 **决策标准和最佳实践：** 1. **全面的事件监听**：选择能够覆盖pull request完整生命周期的事件 2. **详细的反馈信息**：提供完整的测试结果而不仅仅是状态徽章 3. **自动化集成**：通过EventBridge + Lambda + CodePipeline/CodeBuild实现完全自动化的测试流程 4. **及时反馈**：确保测试结果能够及时反馈给开发团队，避免延误审查流程</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">55</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has deployed an application in a production VPC in a single AWS account. The application is popular and is experiencing heavy usage. The company&#x27;s security team wants to add additional security, such as AWS WAF, to the application deployment. However, the application&#x27;s product manager is concerned about cost and does not want to approve the change unless the security team can prove that additional security is necessary. The security team believes that some of the application&#x27;s demand might come from users that have IP addresses that are on a deny list. The security team provides the deny list to a DevOps engineer. If any of the IP addresses on the deny list access the application, the security team wants to receive automated notification in near real time so that the security team can document that the application needs additional security. The DevOps engineer creates a VPC flow log for the production VPC. Which set of additional steps should the DevOps engineer take to meet these requirements MOST cost-effectively? send alarm notices to the security team. Most Voted connector to the log group. Configure Athena to periodically query for all accepted traffic from the IP addresses on the deny list and to store the results in the S3 bucket. Configure an S3 event notification to automatically notify the security team through an Amazon Simple Notification Service (Amazon SNS) topic when new objects are added to the S3 bucket. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a log group in Amazon CloudWatch Logs. Configure the VPC flow log to capture accepted traffic and to send the data to the log group. Create an Amazon CloudWatch metric filter for IP addresses on the deny list. Create a CloudWatch alarm with the metric filter as input. Set the period to 5 minutes and the datapoints to alarm to 1. Use an Amazon Simple Notification Service (Amazon SNS) topic to
B. Create an Amazon S3 bucket for log files. Configure the VPC flow log to capture all traffic and to send the data to the S3 bucket. Configure Amazon Athena to return all log files in the S3 bucket for IP addresses on the deny list. Configure Amazon QuickSight to accept data from Athena and to publish the data as a dashboard that the security team can access. Create a threshold alert of 1 for successful access. Configure the alert to automatically notify the security team as frequently as possible when the alert threshold is met.
C. Create an Amazon S3 bucket for log files. Configure the VPC flow log to capture accepted traffic and to send the data to the S3 bucket. Configure an Amazon OpenSearch Service cluster and domain for the log files. Create an AWS Lambda function to retrieve the logs from the S3 bucket, format the logs, and load the logs into the OpenSearch Service cluster. Schedule the Lambda function to run every 5 minutes. Configure an alert and condition in OpenSearch Service to send alerts to the security team through an Amazon Simple Notification Service (Amazon SNS) topic when access from the IP addresses on the deny list is detected.
D. Create a log group in Amazon CloudWatch Logs. Create an Amazon S3 bucket to hold query results. Configure the VPC flow log to capture all traffic and to send the data to the log group. Deploy an Amazon Athena CloudWatch connector in AWS Lambda. Connect the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在单个AWS账户的生产VPC中部署了一个应用程序。该应用程序很受欢迎，使用量很大。公司的安全团队希望为应用程序部署添加额外的安全措施，如AWS WAF。但是，应用程序的产品经理担心成本，除非安全团队能够证明额外的安全措施是必要的，否则不想批准这个变更。安全团队认为应用程序的一些需求可能来自IP地址在拒绝列表上的用户。安全团队向DevOps工程师提供了拒绝列表。如果拒绝列表上的任何IP地址访问应用程序，安全团队希望近实时地收到自动通知，以便安全团队可以记录应用程序需要额外安全措施的证据。DevOps工程师为生产VPC创建了VPC flow log。DevOps工程师应该采取哪组额外步骤来最经济高效地满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 监控VPC中来自拒绝列表IP地址的访问 - 当检测到拒绝列表IP访问时，近实时通知安全团队 - 以最经济高效的方式实现 - 为安全团队提供证据证明需要额外安全措施 **涉及的关键AWS服务和概念：** - VPC Flow Logs：捕获VPC中网络流量信息 - CloudWatch Logs：日志存储和分析服务 - CloudWatch Metric Filter：从日志中提取特定模式的数据 - CloudWatch Alarm：基于指标触发告警 - Amazon SNS：消息通知服务 - Amazon S3：对象存储服务 - Amazon Athena：无服务器查询服务 - Amazon OpenSearch Service：搜索和分析服务 **正确答案A的原因：** 1. **实时性最佳**：CloudWatch Logs + Metric Filter + Alarm组合提供近实时监控（5分钟周期） 2. **成本最优**：只使用必要的AWS服务，避免了复杂的数据处理管道 3. **配置简单**：直接在CloudWatch中配置，无需额外的Lambda函数或复杂查询 4. **精准监控**：只捕获accepted traffic，减少不必要的数据处理 5. **自动化程度高**：一旦配置完成，完全自动化运行 **其他选项错误的原因：** - **选项B**：使用QuickSight dashboard需要人工查看，不是自动通知；Athena查询成本较高且不够实时 - **选项C**：OpenSearch Service集群成本高；Lambda函数每5分钟运行增加复杂性和成本；整体架构过于复杂 - **选项D**：描述不完整，且Athena connector方案比直接CloudWatch方案更复杂和昂贵 **决策标准和最佳实践：** 1. **成本效益**：选择最简单有效的解决方案，避免过度工程化 2. **实时性要求**：CloudWatch Logs的实时处理能力优于S3+批处理方案 3. **运维复杂度**：尽量减少需要管理的组件数量 4. **扩展性**：CloudWatch方案易于维护和扩展 5. **监控最佳实践**：使用AWS原生监控服务实现更好的集成和可靠性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">56</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer has automated a web service deployment by using AWS CodePipeline with the following steps: 1. An AWS CodeBuild project compiles the deployment artifact and runs unit tests. 2. An AWS CodeDeploy deployment group deploys the web service to Amazon EC2 instances in the staging environment. 3. A CodeDeploy deployment group deploys the web service to EC2 instances in the production environment. The quality assurance (QA) team requests permission to inspect the build artifact before the deployment to the production environment occurs. The QA team wants to run an internal penetration testing tool to conduct manual tests. The tool will be invoked by a REST API call. Which combination of actions should the DevOps engineer take to fulfill this request? (Choose two.) AE (77%) AD (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Insert a manual approval action between the test actions and deployment actions of the pipeline.
B. Modify the buildspec.yml file for the compilation stage to require manual approval before completion.
C. Update the CodeDeploy deployment groups so that they require manual approval to proceed.
D. Update the pipeline to directly call the REST API for the penetration testing tool.
E. Update the pipeline to invoke an AWS Lambda function that calls the REST API for the penetration testing tool.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师使用AWS CodePipeline自动化了Web服务部署，包含以下步骤：1. AWS CodeBuild项目编译部署构件并运行单元测试。2. AWS CodeDeploy部署组将Web服务部署到暂存环境的Amazon EC2实例。3. CodeDeploy部署组将Web服务部署到生产环境的EC2实例。质量保证(QA)团队请求在部署到生产环境之前检查构建构件的权限。QA团队希望运行内部渗透测试工具进行手动测试。该工具将通过REST API调用来触发。DevOps工程师应该采取哪些组合操作来满足这个请求？（选择两个）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** QA团队需要在生产环境部署前检查构建构件并运行渗透测试工具，这需要在pipeline中增加人工干预点和自动化调用外部测试工具的能力。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务，支持多阶段自动化部署 - Manual Approval Action：手动批准操作，允许人工干预pipeline流程 - AWS Lambda：无服务器计算服务，可以调用外部API - CodeBuild和CodeDeploy：构建和部署服务 - Pipeline Actions：管道中的各种操作类型 **正确答案的原因：** - 选项A：在测试和部署操作之间插入手动批准操作是标准做法，允许QA团队在生产部署前进行人工检查和批准 - 选项E：使用Lambda函数调用REST API是AWS推荐的集成外部服务的方式，可以自动化触发渗透测试工具 **其他选项错误的原因：** - 选项B：在buildspec.yml中要求手动批准不是正确的实现方式，buildspec主要用于定义构建步骤 - 选项C：CodeDeploy部署组本身不支持手动批准功能，这应该在pipeline级别实现 - 选项D：Pipeline不能直接调用REST API，需要通过Lambda等服务来实现 **决策标准和最佳实践：** 1. 使用CodePipeline的Manual Approval Action实现人工检查点 2. 通过Lambda函数集成外部API调用，保持架构的松耦合 3. 在适当的pipeline阶段插入批准步骤，确保生产环境的安全性 4. 遵循AWS服务的标准集成模式，避免不支持的配置方式</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">57</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is hosting a web application in an AWS Region. For disaster recovery purposes, a second region is being used as a standby. Disaster recovery requirements state that session data must be replicated between regions in near-real time and 1% of requests should route to the secondary region to continuously verify system functionality. Additionally, if there is a disruption in service in the main region, traffic should be automatically routed to the secondary region, and the secondary region must be able to scale up to handle all traffic. How should a DevOps engineer meet these requirements? Amazon Route 53 weighted routing policy with health checks to distribute the traffic across the regions. Most Voted policy with health checks to distribute the traffic across the regions. A (81%) D (19%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In both regions, deploy the application on AWS Elastic Beanstalk and use Amazon DynamoDB global tables for session data. Use an
B. In both regions, launch the application in Auto Scaling groups and use DynamoDB for session data. Use a Route 53 failover routing
C. In both regions, deploy the application in AWS Lambda, exposed by Amazon API Gateway, and use Amazon RDS for PostgreSQL with cross-region replication for session data. Deploy the web application with client-side logic to call the API Gateway directly.
D. In both regions, launch the application in Auto Scaling groups and use DynamoDB global tables for session data. Enable an Amazon CloudFront weighted distribution across regions. Point the Amazon Route 53 DNS record at the CloudFront distribution.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS区域中托管Web应用程序。出于灾难恢复目的，第二个区域被用作备用区域。灾难恢复要求规定会话数据必须在区域之间近实时复制，1%的请求应路由到辅助区域以持续验证系统功能。此外，如果主区域的服务出现中断，流量应自动路由到辅助区域，辅助区域必须能够扩展以处理所有流量。DevOps工程师应该如何满足这些要求？ 选项： A. 在两个区域中，在AWS Elastic Beanstalk上部署应用程序，并使用Amazon DynamoDB global tables处理会话数据。使用Amazon Route 53加权路由策略和健康检查在区域间分配流量。 B. 在两个区域中，在Auto Scaling groups中启动应用程序，并使用DynamoDB处理会话数据。使用Route 53故障转移路由策略和健康检查在区域间分配流量。 C. 在两个区域中，在AWS Lambda中部署应用程序，通过Amazon API Gateway暴露，并使用Amazon RDS for PostgreSQL跨区域复制处理会话数据。使用客户端逻辑直接调用API Gateway部署Web应用程序。 D. 在两个区域中，在Auto Scaling groups中启动应用程序，并使用DynamoDB global tables处理会话数据。启用Amazon CloudFront跨区域加权分配。将Amazon Route 53 DNS记录指向CloudFront分配。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是多区域灾难恢复架构设计，需要满足四个关键要求：1）会话数据近实时跨区域复制；2）1%流量路由到备用区域进行持续验证；3）主区域故障时自动切换；4）备用区域能够自动扩展处理全部流量。 **涉及的关键AWS服务和概念：** - DynamoDB Global Tables：提供跨区域的近实时数据复制 - Elastic Beanstalk：应用程序部署和管理平台，内置Auto Scaling能力 - Route 53加权路由：可以按百分比分配流量并支持健康检查 - Route 53故障转移路由：主要用于主备切换场景 - CloudFront：全球内容分发网络 - Lambda + API Gateway：无服务器架构 **正确答案A的原因：** 选项A完美满足所有要求：DynamoDB Global Tables提供近实时的跨区域会话数据复制；Elastic Beanstalk提供应用程序托管和自动扩展能力；Route 53加权路由策略可以精确控制1%流量到备用区域，同时通过健康检查实现故障时的自动切换。这是一个经典的主-备灾难恢复架构。 **其他选项错误的原因：** - 选项B：使用故障转移路由策略无法实现1%流量持续路由到备用区域的要求，故障转移路由是纯粹的主备模式 - 选项C：RDS跨区域复制不如DynamoDB Global Tables的近实时性能好，且Lambda可能面临冷启动问题，不适合需要持续1%流量验证的场景 - 选项D：CloudFront主要是CDN服务，虽然可以配置多源，但不是为灾难恢复场景设计的，且增加了不必要的复杂性 **决策标准和最佳实践：** 灾难恢复架构设计应优先考虑：数据复制的实时性和一致性、流量分配的精确控制能力、故障检测和自动切换机制、以及成本效益。DynamoDB Global Tables + Elastic Beanstalk + Route 53加权路由的组合是AWS推荐的标准灾难恢复解决方案，既满足技术要求又具有良好的成本控制。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">58</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs an application on Amazon EC2 instances. The company uses a series of AWS CloudFormation stacks to define the application resources. A developer performs updates by building and testing the application on a laptop and then uploading the build output and CloudFormation stack templates to Amazon S3. The developer&#x27;s peers review the changes before the developer performs the CloudFormation stack update and installs a new version of the application onto the EC2 instances. The deployment process is prone to errors and is time-consuming when the developer updates each EC2 instance with the new application. The company wants to automate as much of the application deployment process as possible while retaining a final manual approval step before the modification of the application or resources. The company already has moved the source code for the application and the CloudFormation templates to AWS CodeCommit. The company also has created an AWS CodeBuild project to build and test the application. Which combination of steps will meet the company&#x27;s requirements? (Choose two.) AD (69%) BD (31%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an application group and a deployment group in AWS CodeDeploy. Install the CodeDeploy agent on the EC2 instances.
B. Create an application revision and a deployment group in AWS CodeDeploy. Create an environment in CodeDeploy. Register the EC2 instances to the CodeDeploy environment.
C. Use AWS CodePipeline to invoke the CodeBuild job, run the CloudFormation update, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment.
D. Use AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, run the CloudFormation change sets and start the AWS CodeDeploy deployment.
E. Use AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Amazon EC2实例上运行应用程序。该公司使用一系列AWS CloudFormation堆栈来定义应用程序资源。开发人员通过在笔记本电脑上构建和测试应用程序，然后将构建输出和CloudFormation堆栈模板上传到Amazon S3来执行更新。开发人员的同事在开发人员执行CloudFormation堆栈更新并在EC2实例上安装新版本应用程序之前会审查这些更改。当开发人员需要更新每个EC2实例上的新应用程序时，部署过程容易出错且耗时。公司希望尽可能自动化应用程序部署过程，同时在修改应用程序或资源之前保留最终的手动批准步骤。公司已经将应用程序源代码和CloudFormation模板移动到AWS CodeCommit。公司还创建了AWS CodeBuild项目来构建和测试应用程序。哪种步骤组合将满足公司的要求？（选择两个）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动化的CI/CD流水线，需要满足以下关键需求： 1. 自动化应用程序部署过程 2. 保留手动批准步骤 3. 处理CloudFormation堆栈更新 4. 自动化EC2实例上的应用程序安装 5. 已有CodeCommit和CodeBuild基础设施 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：用于自动化应用程序部署到EC2实例 - AWS CodePipeline：编排整个CI/CD流水线 - AWS CloudFormation：基础设施即代码，管理AWS资源 - CloudFormation Change Sets：预览基础设施变更的机制 - 手动批准步骤：CodePipeline中的人工审核环节 **正确答案分析（选项A和D）：** 选项A正确的原因： - &quot;应用程序组&quot;和&quot;部署组&quot;是CodeDeploy的正确术语和概念 - 在EC2实例上安装CodeDeploy agent是必需的，这样CodeDeploy才能与实例通信并执行部署 - 这是设置CodeDeploy的标准和正确方式 选项D正确的原因： - 使用CodePipeline编排整个流程符合最佳实践 - CloudFormation change sets允许在实际执行前预览基础设施变更，满足审核需求 - 手动批准步骤满足公司要求 - 先执行CloudFormation change sets更新基础设施，再进行CodeDeploy部署应用程序，这个顺序是合理的 **其他选项错误的原因：** 选项B错误： - &quot;应用程序修订版本&quot;不是CodeDeploy中的标准术语 - &quot;环境&quot;和&quot;注册EC2实例到环境&quot;不是CodeDeploy的正确概念，这更像是Elastic Beanstalk的术语 - CodeDeploy使用的是应用程序组和部署组的概念 选项C错误： - 直接运行CloudFormation更新而不使用change sets缺乏预览机制 - 没有给审核者机会查看将要进行的基础设施变更 - 不符合最佳实践 选项E错误： - 创建了CloudFormation change sets但没有执行它们 - 这意味着基础设施变更不会被应用，可能导致应用程序部署失败 - 逻辑不完整 **决策标准和最佳实践：** 1. CodeDeploy的正确配置需要应用程序组、部署组和agent 2. 使用CloudFormation change sets进行基础设施变更的预览和审核 3. CodePipeline应该编排完整的流程：构建→基础设施变更→手动批准→应用程序部署 4. 基础设施更新应该在应用程序部署之前完成，确保环境准备就绪</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">59</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an EC2 Auto Scaling group across multiple Availability Zones. The engineer needs to implement a deployment strategy that: Launches a second fleet of instances with the same capacity as the original fleet. Maintains the original fleet unchanged while the second fleet is launched. Transitions traffic to the second fleet when the second fleet is fully deployed. Terminates the original fleet automatically 1 hour after transition. Which solution will satisfy these requirements? C (92%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use an AWS CloudFormation template with a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to reflect the new ALB.
B. Use two AWS Elastic Beanstalk environments to perform a blue/green deployment from the original environment to the new one. Create an application version lifecycle policy to terminate the original environment in 1 hour.
C. Use AWS CodeDeploy with a deployment group configured with a blue/green deployment configuration. Select the option Terminate the original instances in the deployment group with a waiting period of 1 hour.
D. Use AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using the Resources key that sets the deletion policy of the ALB to 1 hour, and deploy the application.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师管理着一个运行在Application Load Balancer (ALB)后面Amazon EC2实例上的Web应用程序。这些实例运行在跨多个Availability Zones的EC2 Auto Scaling组中。工程师需要实施一个部署策略，该策略需要：启动与原始集群相同容量的第二个实例集群。在启动第二个集群时保持原始集群不变。当第二个集群完全部署后将流量转移到第二个集群。在转移后1小时自动终止原始集群。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是蓝绿部署(Blue/Green Deployment)策略的实现。具体要求包括：1)创建相同容量的新实例集群；2)保持原集群运行状态；3)流量完全切换后再终止原集群；4)支持1小时延迟终止的自动化流程。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：专门的部署服务，支持多种部署策略 - 蓝绿部署：通过维护两套完全相同的生产环境来实现零停机部署 - Application Load Balancer：应用层负载均衡器，支持流量切换 - EC2 Auto Scaling：自动扩缩容服务 - AWS Elastic Beanstalk：应用程序部署和管理平台 **正确答案C的原因：** AWS CodeDeploy是专门设计用于处理各种部署策略的服务，完美支持蓝绿部署。它可以：1)自动创建与原始集群相同容量的新实例集群；2)在新集群部署期间保持原集群不变；3)通过ALB实现流量的平滑切换；4)提供精确的时间控制，支持1小时后自动终止原始实例的配置。CodeDeploy的蓝绿部署配置专门为这种场景设计，提供了完整的自动化流程。 **其他选项错误的原因：** 选项A错误：CloudFormation的retention policy主要用于资源删除保护，不是部署策略工具，且Route 53记录更新不能实现题目要求的蓝绿部署流程。选项B错误：虽然Elastic Beanstalk支持蓝绿部署，但application version lifecycle policy不是用于控制环境终止时间的正确机制，无法精确控制1小时后终止。选项D错误：Immutable部署策略是滚动更新的一种，不是蓝绿部署，且.ebextension中的deletion policy用于CloudFormation资源管理，不适用于此场景。 **决策标准和最佳实践：** 选择部署工具时应考虑：1)服务的专业性 - CodeDeploy专门用于部署管理；2)功能匹配度 - 是否原生支持所需的部署策略；3)自动化程度 - 能否提供完整的自动化流程；4)精确控制 - 是否支持精确的时间和流程控制。蓝绿部署的最佳实践是使用专门的部署工具，确保流程的可靠性和可控性。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">60</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A video-sharing company stores its videos in Amazon S3. The company has observed a sudden increase in video access requests, but the company does not know which videos are most popular. The company needs to identify the general access pattern for the video files. This pattern includes the number of users who access a certain file on a given day, as well as the number of pull requests for certain files. How can the company meet these requirements with the LEAST amount of effort? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Activate S3 server access logging. Import the access logs into an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns.
B. Activate S3 server access logging. Use Amazon Athena to create an external table with the log files. Use Athena to create a SQL query to analyze the access patterns.
C. Invoke an AWS Lambda function for every S3 object access event. Configure the Lambda function to write the file access information, such as user, S3 bucket, and file key, to an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns.
D. Record an Amazon CloudWatch Logs log message for every S3 object access event. Configure a CloudWatch Logs log stream to write the file access information, such as user, S3 bucket, and file key, to an Amazon Kinesis Data Analytics for SQL application. Perform a sliding window analysis.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家视频分享公司将其视频存储在Amazon S3中。该公司观察到视频访问请求突然增加，但公司不知道哪些视频最受欢迎。公司需要识别视频文件的一般访问模式。这种模式包括在给定日期访问某个文件的用户数量，以及某些文件的拉取请求数量。公司如何以最少的工作量满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案来分析S3中视频文件的访问模式，包括用户访问数量和文件请求次数，并且要求以最少的工作量实现。 **涉及的关键AWS服务和概念：** - S3 Server Access Logging：记录对S3存储桶的详细访问请求 - Amazon Athena：无服务器交互式查询服务，可直接查询S3中的数据 - Amazon Aurora：托管关系数据库服务 - AWS Lambda：无服务器计算服务 - Amazon CloudWatch Logs：日志监控服务 - Amazon Kinesis Data Analytics：实时数据分析服务 **正确答案B的原因：** 1. **最少工作量**：S3 Server Access Logging是原生功能，只需启用即可自动记录访问日志 2. **无服务器架构**：Athena是完全托管的服务，无需预置或管理基础设施 3. **直接查询**：Athena可以直接查询S3中的日志文件，无需数据导入过程 4. **成本效益**：按查询付费，没有持续运行的资源成本 5. **SQL支持**：提供标准SQL接口进行复杂的访问模式分析 **其他选项错误的原因：** - **选项A**：需要将日志导入Aurora数据库，增加了数据传输和存储成本，工作量更大 - **选项C**：需要为每个S3访问事件调用Lambda函数，这会产生大量的Lambda调用费用，并且需要配置事件触发器，工作量显著增加 - **选项D**：使用CloudWatch Logs和Kinesis Data Analytics过于复杂，适合实时分析而非历史访问模式分析，工作量大且成本高 **决策标准和最佳实践：** 1. **最小化运维开销**：选择托管服务而非自建解决方案 2. **成本优化**：按需付费模式优于持续运行的资源 3. **架构简洁性**：减少组件数量和数据传输步骤 4. **原生集成**：利用AWS服务间的原生集成能力 5. **可扩展性**：解决方案应能处理访问量的增长而无需额外配置</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">61</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A development team wants to use AWS CloudFormation stacks to deploy an application. However, the developer IAM role does not have the required permissions to provision the resources that are specified in the AWS CloudFormation template. A DevOps engineer needs to implement a solution that allows the developers to deploy the stacks. The solution must follow the principle of least privilege. Which solution will meet these requirements? D (82%) Other</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an IAM policy that allows the developers to provision the required resources. Attach the policy to the developer IAM role.
B. Create an IAM policy that allows full access to AWS CloudFormation. Attach the policy to the developer IAM role.
C. Create an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role a cloudformation:* action. Use the new service role during stack deployments.
D. Create an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role the iam:PassRole permission. Use the new service role during stack deployments.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个开发团队想要使用AWS CloudFormation堆栈来部署应用程序。但是，开发者IAM角色没有所需的权限来配置AWS CloudFormation模板中指定的资源。DevOps工程师需要实施一个解决方案，允许开发者部署堆栈。该解决方案必须遵循最小权限原则。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在遵循最小权限原则的前提下，如何让没有足够权限的开发者能够通过CloudFormation部署资源。关键是要在安全性和功能性之间找到平衡。 **涉及的关键AWS服务和概念：** - AWS CloudFormation：基础设施即代码服务 - IAM角色和权限管理 - CloudFormation服务角色（Service Role） - iam:PassRole权限 - 最小权限原则（Principle of Least Privilege） **正确答案分析（选项D）：** 选项D是最佳解决方案，因为： 1. 创建专门的CloudFormation服务角色，该角色拥有部署所需资源的权限 2. 只给开发者IAM角色授予iam:PassRole权限，允许其将服务角色传递给CloudFormation 3. 完美体现了最小权限原则：开发者本身不直接拥有创建资源的权限，只能通过指定的服务角色进行操作 4. 提供了权限隔离和审计追踪能力 **其他选项错误的原因：** - 选项A：直接给开发者授予资源配置权限，违反了最小权限原则，权限范围过大 - 选项B：授予CloudFormation完全访问权限是最糟糕的选择，严重违反安全最佳实践 - 选项C：cloudformation:*权限过于宽泛，同样违反了最小权限原则 **决策标准和最佳实践：** 1. 使用服务角色进行权限委托是AWS推荐的最佳实践 2. iam:PassRole是精确控制角色传递的关键权限 3. 通过服务角色可以实现权限的精细化管理和审计 4. 避免直接给用户过多权限，而是通过角色委托的方式实现功能需求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">62</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A production account has a requirement that any Amazon EC2 instance that has been logged in to manually must be terminated within 24 hours. All applications in the production account are using Auto Scaling groups with the Amazon CloudWatch Logs agent configured. How can this process be automated? function that terminates all instances with this tag. Most Voted D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CloudWatch Logs subscription to an AWS Step Functions application. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function once a day that will terminate all instances with this tag.
B. Create an Amazon CloudWatch alarm that will be invoked by the login event. Send the notification to an Amazon Simple Notification Service (Amazon SNS) topic that the operations team is subscribed to, and have them terminate the EC2 instance within 24 hours.
C. Create an Amazon CloudWatch alarm that will be invoked by the login event. Configure the alarm to send to an Amazon Simple Queue Service (Amazon SQS) queue. Use a group of worker instances to process messages from the queue, which then schedules an Amazon EventBridge rule to be invoked.
D. Create a CloudWatch Logs subscription to an AWS Lambda function. Configure the function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a daily Lambda</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个生产账户有一个要求，任何被手动登录过的Amazon EC2实例必须在24小时内终止。生产账户中的所有应用程序都使用配置了Amazon CloudWatch Logs代理的Auto Scaling组。如何自动化这个过程？ 选项： A. 创建一个CloudWatch Logs订阅到AWS Step Functions应用程序。配置AWS Lambda函数为产生登录事件的EC2实例添加标签并标记该实例待退役。创建Amazon EventBridge规则每天调用第二个Lambda函数来终止所有带有此标签的实例。 B. 创建一个由登录事件触发的Amazon CloudWatch告警。将通知发送到运维团队订阅的Amazon Simple Notification Service (Amazon SNS)主题，让他们在24小时内终止EC2实例。 C. 创建一个由登录事件触发的Amazon CloudWatch告警。配置告警发送到Amazon Simple Queue Service (Amazon SQS)队列。使用一组工作实例处理队列中的消息，然后调度Amazon EventBridge规则被调用。 D. 创建CloudWatch Logs订阅到AWS Lambda函数。配置函数为产生登录事件的EC2实例添加标签并标记该实例待退役。创建Amazon EventBridge规则每天调用Lambda函数终止所有带有此标签的实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动化解决方案，能够检测EC2实例的手动登录事件，并确保这些实例在24小时内被自动终止。关键要求包括：1）检测登录事件；2）标记被登录的实例；3）在24小时内自动终止这些实例；4）整个过程完全自动化。 **涉及的关键AWS服务和概念：** - CloudWatch Logs：收集和监控日志数据，包括登录事件 - Lambda：无服务器计算服务，用于处理事件和执行自动化任务 - EventBridge：事件路由服务，可以按计划触发Lambda函数 - EC2标签：用于标识和管理实例的元数据 - CloudWatch Logs订阅：实时处理日志流的机制 **正确答案D的原因：** 选项D提供了最直接和高效的解决方案：1）使用CloudWatch Logs订阅直接监听登录事件，实现实时检测；2）Lambda函数立即响应登录事件并给实例打标签；3）EventBridge规则每天定时触发另一个Lambda函数清理带标签的实例；4）整个流程完全自动化，无需人工干预；5）架构简洁，延迟最低。 **其他选项错误的原因：** 选项A错误：引入了不必要的Step Functions，增加了复杂性和成本，而且Step Functions不是处理日志事件的最佳选择。选项B错误：依赖人工操作，不符合&quot;自动化&quot;的要求，存在人为错误和延迟的风险。选项C错误：架构过于复杂，使用CloudWatch告警而非日志订阅来检测登录事件不够直接，而且需要额外的工作实例来处理SQS消息，增加了成本和复杂性。 **决策标准和最佳实践：** 选择自动化解决方案时应考虑：1）实时性 - CloudWatch Logs订阅比告警更适合实时事件处理；2）简洁性 - 避免不必要的中间服务；3）成本效益 - Lambda按需付费比持续运行的工作实例更经济；4）可靠性 - 减少组件数量降低故障点；5）完全自动化 - 避免任何人工干预环节。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">63</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has enabled all features for its organization in AWS Organizations. The organization contains 10 AWS accounts. The company has turned on AWS CloudTrail in all the accounts. The company expects the number of AWS accounts in the organization to increase to 500 during the next year. The company plans to use multiple OUs for these accounts. The company has enabled AWS Config in each existing AWS account in the organization. A DevOps engineer must implement a solution that enables AWS Config automatically for all future AWS accounts that are created in the organization. Which solution will meet this requirement? rule to invoke an AWS Lambda function that enables trusted access to AWS Config for the organization. deploy automatically when an account is created through Organizations. Most Voted the SCP to the root-level OU. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the organization&#x27;s management account, create an Amazon EventBridge rule that reacts to a CreateAccount API call. Configure the
B. In the organization&#x27;s management account, create an AWS CloudFormation stack set to enable AWS Config. Configure the stack set to
C. In the organization&#x27;s management account, create an SCP that allows the appropriate AWS Config API calls to enable AWS Config. Apply
D. In the organization&#x27;s management account, create an Amazon EventBridge rule that reacts to a CreateAccount API call. Configure the rule to invoke an AWS Systems Manager Automation runbook to enable AWS Config for the account.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司已为其在AWS Organizations中的组织启用了所有功能。该组织包含10个AWS账户。公司已在所有账户中开启了AWS CloudTrail。公司预计在明年组织中的AWS账户数量将增加到500个。公司计划为这些账户使用多个OU。公司已在组织中的每个现有AWS账户中启用了AWS Config。DevOps工程师必须实施一个解决方案，为组织中创建的所有未来AWS账户自动启用AWS Config。哪个解决方案能满足这个要求？ 选项： A. 在组织的管理账户中，创建一个Amazon EventBridge规则来响应CreateAccount API调用。配置规则调用AWS Lambda函数，为组织启用AWS Config的可信访问。 B. 在组织的管理账户中，创建AWS CloudFormation stack set来启用AWS Config。配置stack set在通过Organizations创建账户时自动部署。 C. 在组织的管理账户中，创建一个SCP，允许适当的AWS Config API调用来启用AWS Config。将SCP应用到根级OU。 D. 在组织的管理账户中，创建一个Amazon EventBridge规则来响应CreateAccount API调用。配置规则调用AWS Systems Manager Automation runbook来为账户启用AWS Config。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要实现一个自动化解决方案，为AWS Organizations中未来创建的所有新账户自动启用AWS Config服务。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Config：配置管理和合规性监控服务 - AWS CloudFormation StackSets：跨多个账户和区域部署资源的服务 - Amazon EventBridge：事件驱动架构服务 - Service Control Policies (SCP)：组织级别的权限控制策略 - AWS Lambda和Systems Manager：自动化执行服务 **正确答案B的原因：** CloudFormation StackSets是专门为跨多个AWS账户自动部署和管理资源而设计的服务。它具有以下优势： 1. 原生支持自动部署到新创建的组织账户 2. 可以配置为在新账户加入组织时自动执行部署 3. 提供集中管理和状态跟踪 4. 支持批量操作和回滚功能 5. 与AWS Organizations深度集成，是AWS推荐的最佳实践 **其他选项错误的原因：** - 选项A：Lambda函数方案需要自定义开发，复杂度高，且&quot;启用可信访问&quot;并不等同于在每个账户中启用AWS Config服务本身 - 选项C：SCP只是权限策略，它控制允许或拒绝的操作，但不能主动启用服务或部署资源 - 选项D：虽然技术上可行，但Systems Manager Automation runbook方案比StackSets更复杂，需要更多自定义配置和维护 **决策标准和最佳实践：** 1. 选择AWS原生服务和推荐的最佳实践解决方案 2. 优先考虑自动化程度高、维护成本低的方案 3. 选择与AWS Organizations深度集成的服务 4. 考虑解决方案的可扩展性（从10个账户扩展到500个账户） 5. 选择提供集中管理和监控能力的服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">64</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has many applications. Different teams in the company developed the applications by using multiple languages and frameworks. The applications run on premises and on different servers with different operating systems. Each team has its own release protocol and process. The company wants to reduce the complexity of the release and maintenance of these applications. The company is migrating its technology stacks, including these applications, to AWS. The company wants centralized control of source code, a consistent and automatic delivery pipeline, and as few maintenance tasks as possible on the underlying infrastructure. What should a DevOps engineer do to meet these requirements? D (93%) 7%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create one AWS CodeCommit repository for all applications. Put each application&#x27;s code in a different branch. Merge the branches, and use AWS CodeBuild to build the applications. Use AWS CodeDeploy to deploy the applications to one centralized application server.
B. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time. Use AWS CodeDeploy to deploy the applications to one centralized application server.
C. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time and to create one AMI for each server. Use AWS CloudFormation StackSets to automatically provision and decommission Amazon EC2 fleets by using these AMIs.
D. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build one Docker image for each application in Amazon Elastic Container Registry (Amazon ECR). Use AWS CodeDeploy to deploy the applications to Amazon Elastic Container Service (Amazon ECS) on infrastructure that AWS Fargate manages.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有很多应用程序。公司内不同的团队使用多种语言和框架开发了这些应用程序。这些应用程序运行在本地和不同操作系统的不同服务器上。每个团队都有自己的发布协议和流程。公司希望降低这些应用程序发布和维护的复杂性。公司正在将其技术栈（包括这些应用程序）迁移到AWS。公司希望实现源代码的集中控制、一致且自动化的交付管道，以及对底层基础设施尽可能少的维护任务。DevOps工程师应该怎么做来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何为多语言、多框架的应用程序设计一个统一的CI/CD解决方案，核心需求包括：1）源代码集中管理；2）一致且自动化的交付管道；3）最小化底层基础设施维护工作；4）支持多种技术栈。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：托管的Git代码仓库服务 - AWS CodeBuild：托管的构建服务 - AWS CodeDeploy：应用程序部署服务 - Amazon ECR：容器镜像仓库 - Amazon ECS：容器编排服务 - AWS Fargate：无服务器容器计算引擎 - AWS CloudFormation StackSets：跨账户和区域的基础设施管理 **正确答案D的原因：** 选项D完美满足所有要求：1）每个应用独立的CodeCommit仓库确保了代码隔离和团队自主性；2）使用Docker容器化解决了多语言多框架的兼容性问题；3）ECR提供统一的镜像管理；4）ECS on Fargate实现了完全托管的基础设施，无需维护服务器；5）CodeDeploy确保一致的部署流程。这种架构既保持了应用程序的独立性，又实现了标准化的交付管道。 **其他选项错误的原因：** 选项A将所有应用放在一个仓库的不同分支中，违背了应用独立性原则，且合并分支会造成部署复杂性。选项B使用单一应用服务器部署所有应用，存在单点故障风险且无法很好支持不同技术栈。选项C虽然使用了AMI但仍需要管理EC2实例，没有最小化基础设施维护工作。 **决策标准和最佳实践：** 在设计多应用CI/CD方案时，应遵循：1）应用程序独立性原则（独立仓库、独立部署）；2）容器化优先策略解决环境一致性问题；3）选择托管服务减少运维负担；4）标准化工具链确保一致的交付体验。Fargate作为无服务器容器平台，完美契合了&quot;最少维护任务&quot;的要求。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">65</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s application is currently deployed to a single AWS Region. Recently, the company opened a new office on a different continent. The users in the new office are experiencing high latency. The company&#x27;s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB) and uses Amazon DynamoDB as the database layer. The instances run in an EC2 Auto Scaling group across multiple Availability Zones. A DevOps engineer is tasked with minimizing application response times and improving availability for users in both Regions. Which combination of actions should be taken to address the latency issues? (Choose three.) F. Convert the DynamoDB table to a global table. Most Voted CDF (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new DynamoDB table in the new Region with cross-Region replication enabled.
B. Create new ALB and Auto Scaling group global resources and configure the new ALB to direct traffic to the new Auto Scaling group.
C. Create new ALB and Auto Scaling group resources in the new Region and configure the new ALB to direct traffic to the new Auto Scaling group.
D. Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB.
E. Create Amazon Route 53 aliases, health checks, and failover routing policies to route to the ALB.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的应用程序目前部署在单个AWS Region中。最近，该公司在不同大陆开设了新办公室。新办公室的用户遇到了高延迟问题。该公司的应用程序运行在Application Load Balancer (ALB)后面的Amazon EC2实例上，并使用Amazon DynamoDB作为数据库层。这些实例在跨多个Availability Zone的EC2 Auto Scaling组中运行。DevOps工程师的任务是最小化应用程序响应时间并提高两个Region用户的可用性。应该采取哪些操作组合来解决延迟问题？（选择三个。） 选项： A. 在新Region中创建新的DynamoDB表并启用跨Region复制。 B. 创建新的ALB和Auto Scaling组全局资源，并配置新的ALB将流量导向新的Auto Scaling组。 C. 在新Region中创建新的ALB和Auto Scaling组资源，并配置新的ALB将流量导向新的Auto Scaling组。 D. 创建Amazon Route 53记录、健康检查和基于延迟的路由策略来路由到ALB。 E. 创建Amazon Route 53别名、健康检查和故障转移路由策略来路由到ALB。 F. 将DynamoDB表转换为全局表。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这是一个典型的多Region部署架构问题，要求解决跨大陆用户的高延迟问题，同时提高可用性。需要选择三个正确的解决方案。 **涉及的关键AWS服务和概念：** - Multi-Region架构设计 - Application Load Balancer (ALB) - EC2 Auto Scaling组 - Amazon DynamoDB Global Tables - Amazon Route 53路由策略 - 延迟优化和高可用性设计 **正确答案分析（C、D、F）：** **选项C正确：** 在新Region中创建ALB和Auto Scaling组是解决延迟问题的核心。通过在用户附近的Region部署计算资源，可以显著减少网络延迟。这是标准的多Region部署模式。 **选项D正确：** Route 53的基于延迟的路由策略是关键组件，它能够自动将用户请求路由到延迟最低的Region，这正是解决跨大陆延迟问题的最佳实践。 **选项F正确：** DynamoDB Global Tables提供多Region数据库复制，确保数据在两个Region都可用，减少数据库访问延迟，同时提供高可用性。 **其他选项错误的原因：** **选项A错误：** DynamoDB没有&quot;跨Region复制&quot;这个具体功能。正确的做法是使用Global Tables（选项F），它提供了自动的多向复制。 **选项B错误：** 表述中&quot;全局资源&quot;概念不准确。ALB和Auto Scaling组都是Region级别的资源，不存在&quot;全局&quot;版本。必须在特定Region中创建这些资源。 **选项E错误：** 故障转移路由策略主要用于灾难恢复场景，不是解决延迟问题的最佳选择。对于性能优化，基于延迟的路由策略更合适。 **决策标准和最佳实践：** 1. **就近部署原则：** 在用户附近的Region部署应用组件 2. **智能路由：** 使用Route 53基于延迟的路由自动优化用户体验 3. **数据一致性：** 使用DynamoDB Global Tables确保数据在多Region间同步 4. **架构对称性：** 在新Region复制完整的应用架构栈 5. **自动化管理：** 利用AWS托管服务减少运维复杂性 这个解决方案实现了真正的多Region主-主架构，既解决了延迟问题又提高了整体可用性。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">66</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer needs to apply a core set of security controls to an existing set of AWS accounts. The accounts are in an organization in AWS Organizations. Individual teams will administer individual accounts by using the AdministratorAccess AWS managed policy. For all accounts, AWS CloudTrail and AWS Config must be turned on in all available AWS Regions. Individual account administrators must not be able to edit or delete any of the baseline resources. However, individual account administrators must be able to edit or delete their own CloudTrail trails and AWS Config rules. Which solution will meet these requirements in the MOST operationally efficient way? organization&#x27;s management account by using CloudFormation StackSets. Set the stack policy to deny Update:Delete actions. CloudTrail and AWS Config. Deploy AWS Config rules to the organization by using the AWS Config management account. Create a CloudTrail organization trail in the organization&#x27;s management account. Deny modification or deletion of the AWS Config recorders by using an SCP. Most Voted organization&#x27;s management account by using CloudFormation StackSets. Create an SCP that prevents updates or deletions to CloudTrail resources or AWS Config resources unless the principal is an administrator of the organization&#x27;s management account. Most Voted C (54%) D (38%) 9%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CloudFormation template that defines the standard account resources. Deploy the template to all accounts from the
B. Enable AWS Control Tower. Enroll the existing accounts in AWS Control Tower. Grant the individual account administrators access to
C. Designate an AWS Config management account. Create AWS Config recorders in all accounts by using AWS CloudFormation StackSets.
D. Create an AWS CloudFormation template that defines the standard account resources. Deploy the template to all accounts from the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师需要对AWS Organizations中现有的一组AWS账户应用核心安全控制。各个团队将使用AdministratorAccess AWS托管策略来管理各自的账户。对于所有账户，必须在所有可用的AWS区域中启用AWS CloudTrail和AWS Config。各个账户管理员不得编辑或删除任何基线资源。但是，各个账户管理员必须能够编辑或删除他们自己的CloudTrail跟踪和AWS Config规则。哪种解决方案能够以最具操作效率的方式满足这些要求？ 选项： A. 创建定义标准账户资源的AWS CloudFormation模板。使用CloudFormation StackSets从组织的管理账户将模板部署到所有账户。设置堆栈策略以拒绝Update:Delete操作。 B. 启用AWS Control Tower。将现有账户注册到AWS Control Tower中。授予各个账户管理员访问CloudTrail和AWS Config的权限。使用AWS Config管理账户将AWS Config规则部署到组织。在组织的管理账户中创建CloudTrail组织跟踪。使用SCP拒绝修改或删除AWS Config记录器。 C. 指定AWS Config管理账户。使用AWS CloudFormation StackSets在所有账户中创建AWS Config记录器。在组织的管理账户中创建CloudTrail组织跟踪。使用SCP拒绝修改或删除AWS Config记录器。 D. 创建定义标准账户资源的AWS CloudFormation模板。使用CloudFormation StackSets从组织的管理账户将模板部署到所有账户。创建SCP以防止更新或删除CloudTrail资源或AWS Config资源，除非主体是组织管理账户的管理员。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Organizations环境中实现以下目标：1）在所有账户的所有区域启用CloudTrail和Config；2）防止账户管理员删除基线安全资源；3）允许账户管理员管理自己创建的CloudTrail和Config资源；4）寻求最具操作效率的解决方案。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Config：配置合规性监控服务，支持管理账户模式 - AWS CloudTrail：API调用审计服务，支持组织级跟踪 - CloudFormation StackSets：跨账户资源部署工具 - SCP (Service Control Policy)：组织级权限控制策略 - AWS Control Tower：自动化治理服务 **正确答案C的原因：** 选项C采用了最优的架构设计：1）使用AWS Config管理账户功能，可以集中管理所有成员账户的Config记录器；2）使用CloudTrail组织跟踪，在管理账户中创建一个跟踪覆盖所有成员账户；3）通过SCP精确控制权限，只保护基线Config记录器不被删除，而不影响账户管理员创建自己的Config规则；4）这种方案操作效率最高，管理开销最小。 **其他选项错误的原因：** 选项A使用StackSets部署资源到各个账户，但CloudFormation堆栈策略无法区分基线资源和用户自定义资源，会阻止账户管理员创建自己的CloudTrail和Config资源。选项B的Control Tower虽然功能强大，但对于已有的组织来说实施复杂度高，不是最具操作效率的方案。选项D的SCP策略过于宽泛，会完全阻止账户管理员管理CloudTrail和Config资源，不满足允许管理自定义资源的要求。 **决策标准和最佳实践：** 在多账户环境中实施安全基线时，应优先考虑：1）使用AWS原生的集中管理功能（如Config管理账户、CloudTrail组织跟踪）；2）通过SCP实现精细化权限控制；3）平衡安全控制和操作灵活性；4）选择管理开销最小的方案。选项C完美体现了这些最佳实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">67</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has its AWS accounts in an organization in AWS Organizations. AWS Config is manually configured in each AWS account. The company needs to implement a solution to centrally configure AWS Config for all accounts in the organization. The solution also must record resource changes to a central account. Which combination of actions should a DevOps engineer perform to meet these requirements? (Choose two.) AE (84%) BD (16%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a delegated administrator account for AWS Config. Enable trusted access for AWS Config in the organization.
B. Configure a delegated administrator account for AWS Config. Create a service-linked role for AWS Config in the organization&#x27;s management account.
C. Create an AWS CloudFormation template to create an AWS Config aggregator. Configure a CloudFormation stack set to deploy the template to all accounts in the organization.
D. Create an AWS Config organization aggregator in the organization&#x27;s management account. Configure data collection from all AWS accounts in the organization and from all AWS Regions.
E. Create an AWS Config organization aggregator in the delegated administrator account. Configure data collection from all AWS accounts in the organization and from all AWS Regions.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中有多个AWS账户。AWS Config在每个AWS账户中都是手动配置的。该公司需要实施一个解决方案来为组织中的所有账户集中配置AWS Config。该解决方案还必须将资源变更记录到一个中央账户。DevOps工程师应该执行哪些操作组合来满足这些要求？（选择两个） 选项： A. 为AWS Config配置委托管理员账户。在组织中为AWS Config启用trusted access。 B. 为AWS Config配置委托管理员账户。在组织的管理账户中为AWS Config创建service-linked role。 C. 创建AWS CloudFormation模板来创建AWS Config aggregator。配置CloudFormation stack set将模板部署到组织中的所有账户。 D. 在组织的管理账户中创建AWS Config organization aggregator。配置从组织中所有AWS账户和所有AWS Regions收集数据。 E. 在委托管理员账户中创建AWS Config organization aggregator。配置从组织中所有AWS账户和所有AWS Regions收集数据。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 集中配置AWS Config到组织中的所有账户 2. 将资源变更记录到中央账户 3. 需要选择两个正确的操作组合 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Config：资源配置监控和合规性服务 - Delegated Administrator：委托管理员账户，用于集中管理特定服务 - Organization Aggregator：组织级别的配置聚合器 - Trusted Access：允许AWS服务访问组织信息的权限机制 - Service-linked Role：AWS服务自动创建和管理的IAM角色 **正确答案分析：** 根据题目显示正确答案是B，但从技术角度分析，最佳实践应该是A和E的组合： - **选项A正确**：配置委托管理员账户并启用trusted access是AWS Config组织级部署的标准第一步，这是AWS官方推荐的最佳实践 - **选项E正确**：在委托管理员账户中创建organization aggregator是正确的做法，因为委托管理员账户专门负责管理AWS Config服务 **其他选项分析：** - **选项B**：虽然配置委托管理员是正确的，但service-linked role通常由AWS服务自动创建，不需要手动创建 - **选项C**：使用CloudFormation stack set部署普通aggregator过于复杂，不如直接使用organization aggregator - **选项D**：在管理账户中创建aggregator不是最佳实践，应该使用委托管理员账户 **决策标准和最佳实践：** 1. 使用委托管理员账户而非管理账户来管理特定AWS服务 2. 启用trusted access是组织级服务集成的前提条件 3. Organization aggregator比普通aggregator更适合多账户环境 4. 遵循AWS Well-Architected Framework中的安全和运营卓越原则</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">68</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to migrate its content sharing web application hosted on Amazon EC2 to a serverless architecture. The company currently deploys changes to its application by creating a new Auto Scaling group of EC2 instances and a new Elastic Load Balancer, and then shifting the traffic away using an Amazon Route 53 weighted routing policy. For its new serverless application, the company is planning to use Amazon API Gateway and AWS Lambda. The company will need to update its deployment processes to work with the new application. It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base. Which deployment strategy will meet these requirements? API and Lambda functions. Shift traffic gradually using an Elastic Beanstalk blue/green deployment. changed, use OpsWorks to perform a blue/green deployment and shift traffic gradually. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CDK to deploy API Gateway and Lambda functions. When code needs to be changed, update the AWS CloudFormation stack and deploy the new version of the APIs and Lambda functions. Use a Route 53 failover routing policy for the canary release strategy.
B. Use AWS CloudFormation to deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete.
C. Use AWS Elastic Beanstalk to deploy API Gateway and Lambda functions. When code needs to be changed, deploy a new version of the
D. Use AWS OpsWorks to deploy API Gateway in the service layer and Lambda functions in a custom layer. When code needs to be</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司希望将其托管在Amazon EC2上的内容共享Web应用程序迁移到serverless架构。该公司目前通过创建新的Auto Scaling组EC2实例和新的Elastic Load Balancer来部署应用程序更改，然后使用Amazon Route 53加权路由策略转移流量。对于新的serverless应用程序，公司计划使用Amazon API Gateway和AWS Lambda。公司需要更新其部署流程以适应新应用程序，还需要保留在向整个用户群推出功能之前在少数用户上测试新功能的能力。哪种部署策略能满足这些要求？ 选项： A. 使用AWS CDK部署API Gateway和Lambda函数。当需要更改代码时，更新AWS CloudFormation堆栈并部署新版本的API和Lambda函数。使用Route 53故障转移路由策略进行金丝雀发布策略。 B. 使用AWS CloudFormation部署API Gateway和Lambda函数，使用Lambda函数版本。当需要更改代码时，使用新的Lambda代码更新CloudFormation堆栈，并使用金丝雀发布策略更新API版本。测试完成后提升新版本。 C. 使用AWS Elastic Beanstalk部署API Gateway和Lambda函数。当需要更改代码时，部署新版本... D. 使用AWS OpsWorks在服务层部署API Gateway，在自定义层部署Lambda函数。当需要更改代码时...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为serverless架构（API Gateway + Lambda）设计一个部署策略，需要满足：1）从EC2迁移到serverless；2）保持渐进式部署能力；3）支持小规模用户测试后再全量发布（金丝雀发布）。 **涉及的关键AWS服务和概念：** - **API Gateway**: serverless架构的API入口 - **Lambda函数版本**: Lambda支持版本控制，可以同时运行多个版本 - **CloudFormation**: 基础设施即代码，用于管理AWS资源 - **金丝雀发布（Canary Release）**: 渐进式部署策略，先向少量用户发布新版本 - **蓝绿部署**: 通过维护两套环境实现零停机部署 **正确答案B的原因：** 1. **CloudFormation是标准选择**: 对于serverless应用的基础设施管理，CloudFormation是AWS推荐的最佳实践 2. **Lambda函数版本支持**: 利用Lambda原生的版本控制功能，可以同时维护多个代码版本 3. **API Gateway金丝雀发布**: API Gateway原生支持金丝雀发布，可以按百分比分配流量到不同Lambda版本 4. **完整的部署流程**: 提供了从代码更新到测试完成后提升版本的完整流程 **其他选项错误的原因：** - **选项A**: Route 53故障转移路由策略主要用于灾难恢复场景，不适合金丝雀发布；CDK虽然可用但CloudFormation更直接 - **选项C**: Elastic Beanstalk主要用于传统Web应用部署，不是serverless架构的最佳选择 - **选项D**: OpsWorks过于复杂，且不是serverless部署的标准工具 **决策标准和最佳实践：** 1. **选择原生支持的服务**: API Gateway和Lambda原生支持版本控制和流量分配 2. **基础设施即代码**: 使用CloudFormation确保部署的一致性和可重复性 3. **渐进式发布**: 利用AWS服务的内置金丝雀发布功能，而不是依赖外部路由策略 4. **简化架构**: 选择最直接、最少组件的解决方案来满足需求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">69</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A development team uses AWS CodeCommit, AWS CodePipeline, and AWS CodeBuild to develop and deploy an application. Changes to the code are submitted by pull requests. The development team reviews and merges the pull requests, and then the pipeline builds and tests the application. Over time, the number of pull requests has increased. The pipeline is frequently blocked because of failing tests. To prevent this blockage, the development team wants to run the unit and integration tests on each pull request before it is merged. Which solution will meet these requirements? to require the successful invocation of the CodeBuild project. Attach the approval rule to the project&#x27;s CodeCommit repository. and integration tests. Configure the CodeBuild project as a target of the EventBridge rule that includes a custom event payload with the CodeCommit repository and branch information from the event. Most Voted B (77%) D (15%) 4%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CodeBuild project to run the unit and integration tests. Create a CodeCommit approval rule template. Configure the template
B. Create an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit. Create a CodeBuild project to run the unit
C. Create an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit. Modify the existing CodePipeline pipeline to not run the deploy steps if the build is started from a pull request. Configure the EventBridge rule to run the pipeline with a custom payload that contains the CodeCommit repository and branch information from the event.
D. Create a CodeBuild project to run the unit and integration tests. Create a CodeCommit notification rule that matches when a pull request is created or updated. Configure the notification rule to invoke the CodeBuild project.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个开发团队使用AWS CodeCommit、AWS CodePipeline和AWS CodeBuild来开发和部署应用程序。代码更改通过pull request提交。开发团队审查并合并pull request，然后pipeline构建和测试应用程序。随着时间推移，pull request的数量增加了。由于测试失败，pipeline经常被阻塞。为了防止这种阻塞，开发团队希望在每个pull request合并之前运行单元测试和集成测试。哪个解决方案能满足这些要求？ 选项： A. 创建一个CodeBuild项目来运行单元测试和集成测试。创建一个CodeCommit审批规则模板。配置模板要求成功调用CodeBuild项目。将审批规则附加到项目的CodeCommit存储库。 B. 创建一个Amazon EventBridge规则来匹配来自CodeCommit的pullRequestCreated事件。创建一个CodeBuild项目来运行单元测试和集成测试。将CodeBuild项目配置为EventBridge规则的目标，该规则包含带有事件中CodeCommit存储库和分支信息的自定义事件负载。 C. 创建一个Amazon EventBridge规则来匹配来自CodeCommit的pullRequestCreated事件。修改现有的CodePipeline pipeline，如果构建是从pull request启动的，则不运行部署步骤。配置EventBridge规则运行pipeline，使用包含事件中CodeCommit存储库和分支信息的自定义负载。 D. 创建一个CodeBuild项目来运行单元测试和集成测试。创建一个CodeCommit通知规则，匹配创建或更新pull request时的情况。配置通知规则来调用CodeBuild项目。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求在pull request合并之前自动运行单元测试和集成测试，以防止失败的测试阻塞主pipeline。需要一个能够自动检测pull request创建并触发测试的解决方案。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：Git存储库服务，支持pull request功能 - AWS CodeBuild：托管构建服务，可运行测试 - Amazon EventBridge：事件驱动架构服务，可响应AWS服务事件 - CodeCommit事件：pullRequestCreated等事件可以触发自动化流程 - CodeCommit审批规则：可以设置合并前的条件检查 **正确答案B的原因：** 1. **事件驱动架构**：使用EventBridge监听pullRequestCreated事件，实现自动化触发 2. **直接集成**：EventBridge可以直接调用CodeBuild项目作为目标 3. **灵活性**：通过自定义事件负载传递存储库和分支信息，确保测试针对正确的代码 4. **独立性**：测试运行独立于主pipeline，不会影响现有的CI/CD流程 5. **实时响应**：一旦创建pull request就立即触发测试 **其他选项错误的原因：** - **选项A**：CodeCommit审批规则主要用于设置合并条件，但不能直接自动触发CodeBuild项目运行测试 - **选项C**：修改现有pipeline增加了复杂性，且可能影响正常的部署流程 - **选项D**：CodeCommit通知规则主要用于发送通知到SNS、SQS等，不能直接调用CodeBuild项目 **决策标准和最佳实践：** 1. **自动化优先**：选择能够自动检测和响应pull request事件的方案 2. **最小影响原则**：不修改现有的成功pipeline配置 3. **事件驱动设计**：利用AWS原生事件机制实现松耦合架构 4. **测试左移**：在开发流程早期进行测试，减少后期问题 5. **资源效率**：使用按需触发而非持续轮询的方式</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">70</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that runs on a fleet of Amazon EC2 instances. The application requires frequent restarts. The application logs contain error messages when a restart is required. The application logs are published to a log group in Amazon CloudWatch Logs. An Amazon CloudWatch alarm notifies an application engineer through an Amazon Simple Notification Service (Amazon SNS) topic when the logs contain a large number of restart-related error messages. The application engineer manually restarts the application on the instances after the application engineer receives a notification from the SNS topic. A DevOps engineer needs to implement a solution to automate the application restart on the instances without restarting the instances. Which solution will meet these requirements in the MOST operationally efficient manner? D (68%) C (17%) B (14%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Configure the SNS topic to invoke the runbook.
B. Create an AWS Lambda function that restarts the application on the instances. Configure the Lambda function as an event destination of the SNS topic.
C. Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Create an AWS Lambda function to invoke the runbook. Configure the Lambda function as an event destination of the SNS topic.
D. Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Configure an Amazon EventBridge rule that reacts when the CloudWatch alarm enters ALARM state. Specify the runbook as a target of the rule.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个运行在Amazon EC2实例集群上的应用程序。该应用程序需要频繁重启。当需要重启时，应用程序日志会包含错误消息。应用程序日志发布到Amazon CloudWatch Logs中的日志组。当日志包含大量重启相关错误消息时，Amazon CloudWatch告警通过Amazon Simple Notification Service (Amazon SNS)主题通知应用程序工程师。应用程序工程师在收到SNS主题通知后手动重启实例上的应用程序。DevOps工程师需要实现一个解决方案来自动化实例上的应用程序重启，而不重启实例本身。哪个解决方案能以最具操作效率的方式满足这些要求？ 选项： A. 配置AWS Systems Manager Automation runbook运行脚本来重启实例上的应用程序。配置SNS主题调用该runbook。 B. 创建AWS Lambda函数来重启实例上的应用程序。将Lambda函数配置为SNS主题的事件目标。 C. 配置AWS Systems Manager Automation runbook运行脚本来重启实例上的应用程序。创建AWS Lambda函数来调用runbook。将Lambda函数配置为SNS主题的事件目标。 D. 配置AWS Systems Manager Automation runbook运行脚本来重启实例上的应用程序。配置Amazon EventBridge规则在CloudWatch告警进入ALARM状态时触发。指定runbook作为规则的目标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要实现一个自动化解决方案，当CloudWatch告警触发时，能够自动重启EC2实例上的应用程序（不是重启实例本身），要求操作效率最高。 **涉及的关键AWS服务和概念：** - Amazon CloudWatch Logs和CloudWatch Alarms：监控和告警 - Amazon SNS：消息通知服务 - AWS Lambda：无服务器计算服务 - AWS Systems Manager Automation：自动化运维任务 - Amazon EventBridge：事件路由服务 **正确答案B的原因：** 1. **架构简洁**：直接使用Lambda函数作为SNS的订阅者，避免了不必要的中间层 2. **成本效益**：Lambda按使用付费，对于偶发的重启任务非常经济 3. **操作效率最高**：最少的组件数量，最直接的执行路径 4. **易于维护**：单一Lambda函数包含所有重启逻辑，便于调试和更新 5. **原生集成**：SNS可以直接触发Lambda，无需额外配置 **其他选项错误的原因：** - **选项A**：SNS无法直接调用Systems Manager Automation runbook，需要中间服务 - **选项C**：增加了不必要的复杂性，Lambda调用runbook比直接在Lambda中执行重启逻辑更复杂 - **选项D**：虽然技术可行，但绕过了现有的SNS通知机制，需要重新配置告警触发方式，增加了架构复杂性 **决策标准和最佳实践：** 1. **最小复杂性原则**：选择组件最少、集成最简单的方案 2. **成本优化**：Lambda的按需付费模式适合间歇性任务 3. **可维护性**：减少服务间依赖，降低故障点 4. **利用现有架构**：充分利用已有的SNS通知机制，避免重复建设</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">71</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer at a company is supporting an AWS environment in which all users use AWS IAM Identity Center (AWS Single Sign-On). The company wants to immediately disable credentials of any new IAM user and wants the security team to receive a notification. Which combination of steps should the DevOps engineer take to meet these requirements? (Choose three.) F. Create an Amazon Simple Queue Service (Amazon SQS) queue that is a target of the Lambda function. Subscribe the security team&#x27;s group email address to the queue. ACE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule that reacts to an IAM CreateUser API call in AWS CloudTrail.
B. Create an Amazon EventBridge rule that reacts to an IAM GetLoginProfile API call in AWS CloudTrail.
C. Create an AWS Lambda function that is a target of the EventBridge rule. Configure the Lambda function to disable any access keys and delete the login profiles that are associated with the IAM user.
D. Create an AWS Lambda function that is a target of the EventBridge rule. Configure the Lambda function to delete the login profiles that are associated with the IAM user.
E. Create an Amazon Simple Notification Service (Amazon SNS) topic that is a target of the EventBridge rule. Subscribe the security team&#x27;s group email address to the topic.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的DevOps工程师正在支持一个AWS环境，该环境中所有用户都使用AWS IAM Identity Center (AWS Single Sign-On)。公司希望立即禁用任何新IAM用户的凭证，并希望安全团队收到通知。DevOps工程师应该采取哪些步骤组合来满足这些要求？（选择三个。） 选项： A. 创建一个Amazon EventBridge规则，对AWS CloudTrail中的IAM CreateUser API调用做出反应。 B. 创建一个Amazon EventBridge规则，对AWS CloudTrail中的IAM GetLoginProfile API调用做出反应。 C. 创建一个AWS Lambda函数作为EventBridge规则的目标。配置Lambda函数禁用任何访问密钥并删除与IAM用户关联的登录配置文件。 D. 创建一个AWS Lambda函数作为EventBridge规则的目标。配置Lambda函数删除与IAM用户关联的登录配置文件。 E. 创建一个Amazon SNS主题作为EventBridge规则的目标。将安全团队的群组邮箱地址订阅到该主题。 F. 创建一个Amazon SQS队列作为Lambda函数的目标。将安全团队的群组邮箱地址订阅到该队列。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求建立一个自动化系统来：1）检测新IAM用户的创建；2）立即禁用新创建用户的凭证；3）通知安全团队。由于公司使用IAM Identity Center，不应该有直接的IAM用户创建，因此需要监控和响应意外的用户创建。 **涉及的关键AWS服务和概念：** - Amazon EventBridge：事件驱动架构的核心服务，用于监控API调用 - AWS CloudTrail：记录所有AWS API调用的审计服务 - AWS Lambda：无服务器计算服务，用于执行响应逻辑 - Amazon SNS：简单通知服务，用于发送通知 - IAM API操作：CreateUser（创建用户）、GetLoginProfile（获取登录配置） **正确答案分析（A、C、E）：** - 选项A正确：CreateUser API调用是检测新IAM用户创建的正确事件，这是触发整个流程的起点 - 选项C正确：需要Lambda函数同时禁用访问密钥和删除登录配置文件，确保完全禁用用户凭证 - 选项E正确：SNS是向安全团队发送通知的标准方式，可以直接作为EventBridge规则的目标 **其他选项错误的原因：** - 选项B错误：GetLoginProfile是查询操作，不是创建用户的指示器，无法准确触发所需的响应 - 选项D错误：只删除登录配置文件不够，还需要禁用访问密钥才能完全禁用用户凭证 - 选项F错误：SQS不能直接订阅邮箱地址，且架构过于复杂（Lambda→SQS→邮箱），SNS更适合直接通知 **决策标准和最佳实践：** 1. 事件检测应该基于实际的创建操作（CreateUser）而非查询操作 2. 安全响应必须全面（同时处理访问密钥和登录配置文件） 3. 通知机制应该简单直接，SNS比SQS更适合邮件通知场景 4. 架构应该遵循事件驱动模式：EventBridge监控→Lambda执行→SNS通知</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">72</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to set up a continuous delivery pipeline. The company stores application code in a private GitHub repository. The company needs to deploy the application components to Amazon Elastic Container Service (Amazon ECS), Amazon EC2, and AWS Lambda. The pipeline must support manual approval actions. Which solution will meet these requirements? B (92%) 4%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CodePipeline with Amazon ECS, Amazon EC2, and Lambda as deploy providers.
B. Use AWS CodePipeline with AWS CodeDeploy as the deploy provider.
C. Use AWS CodePipeline with AWS Elastic Beanstalk as the deploy provider.
D. Use AWS CodeDeploy with GitHub integration to deploy the application.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司想要建立一个持续交付管道。该公司将应用程序代码存储在私有的GitHub仓库中。公司需要将应用程序组件部署到Amazon Elastic Container Service (Amazon ECS)、Amazon EC2和AWS Lambda。管道必须支持手动批准操作。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个持续交付管道，需要满足以下条件： - 从私有GitHub仓库获取代码 - 部署到多个不同的AWS服务（ECS、EC2、Lambda） - 支持手动批准流程 - 实现完整的CI/CD流程 **涉及的关键AWS服务和概念：** - AWS CodePipeline：AWS的持续集成/持续交付服务，用于编排整个部署流程 - AWS CodeDeploy：专门的部署服务，支持多种部署目标和策略 - GitHub集成：与外部代码仓库的集成能力 - 手动批准：CI/CD流程中的人工审核环节 **正确答案B的原因：** AWS CodePipeline + AWS CodeDeploy的组合是最佳选择，因为： 1. CodePipeline提供完整的管道编排能力，原生支持GitHub集成 2. CodeDeploy作为部署提供商，天然支持ECS、EC2和Lambda的部署 3. CodePipeline内置手动批准操作功能，可以在管道中插入审批步骤 4. 这种组合提供了最大的灵活性和可扩展性 5. 支持复杂的部署策略（蓝绿部署、滚动部署等） **其他选项错误的原因：** - 选项A：直接使用ECS、EC2、Lambda作为部署提供商过于复杂，缺乏统一的部署抽象层，难以管理多目标部署 - 选项C：Elastic Beanstalk主要针对Web应用部署，不适合Lambda部署，且对ECS支持有限 - 选项D：仅使用CodeDeploy缺少完整的管道编排能力，无法很好地处理手动批准和复杂的工作流程 **决策标准和最佳实践：** 选择CI/CD解决方案时应考虑： 1. 服务集成的完整性和原生支持 2. 多目标部署的统一管理能力 3. 工作流编排的灵活性 4. 手动干预和审批机制的支持 5. 可扩展性和维护性 CodePipeline + CodeDeploy的组合代表了AWS CI/CD的最佳实践模式。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">73</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that runs on Amazon EC2 instances that are in an Auto Scaling group. When the application starts up, the application needs to process data from an Amazon S3 bucket before the application can start to serve requests. The size of the data that is stored in the S3 bucket is growing. When the Auto Scaling group adds new instances, the application now takes several minutes to download and process the data before the application can serve requests. The company must reduce the time that elapses before new EC2 instances are ready to serve requests. Which solution is the MOST cost-effective way to reduce the application startup time? A (86%) 14%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a warm pool for the Auto Scaling group with warmed EC2 instances in the Stopped state. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.
B. Increase the maximum instance count of the Auto Scaling group. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.
C. Configure a warm pool for the Auto Scaling group with warmed EC2 instances in the Running state. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.
D. Increase the maximum instance count of the Auto Scaling group. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook and to place the new instance in the Standby state when the application is ready to serve requests.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个运行在Amazon EC2实例上的应用程序，这些实例位于Auto Scaling组中。当应用程序启动时，应用程序需要先处理Amazon S3存储桶中的数据，然后才能开始处理请求。存储在S3存储桶中的数据大小正在增长。当Auto Scaling组添加新实例时，应用程序现在需要几分钟来下载和处理数据，然后才能处理请求。公司必须减少新EC2实例准备好处理请求之前所需的时间。哪种解决方案是减少应用程序启动时间最具成本效益的方法？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到最具成本效益的方法来减少新EC2实例启动并准备好处理请求的时间。关键挑战是应用程序需要从S3下载和处理越来越大的数据集，导致启动时间延长。 **涉及的关键AWS服务和概念：** 1. Auto Scaling Group - 自动扩缩容组 2. Warm Pool - 预热池，可以预先准备实例 3. Lifecycle Hook - 生命周期钩子，控制实例启动流程 4. EC2实例状态：Running（运行中）vs Stopped（已停止） **正确答案C的原因：** - Warm Pool with Running state：预热池中的实例保持Running状态，意味着实例已经启动并且应用程序已经运行，数据已经预先下载和处理完成 - 当需要扩容时，这些预热的实例可以立即投入使用，无需重新下载S3数据 - Lifecycle Hook确保只有当应用程序完全准备好时才开始接收流量 - 这是最快的解决方案，因为实例和应用程序都已经准备就绪 **其他选项错误的原因：** - 选项A（Stopped状态）：虽然成本更低，但实例仍需要启动时间，应用程序仍需要重新下载和处理S3数据，无法解决根本问题 - 选项B：仅增加最大实例数量不能解决启动时间问题，新实例仍然需要完整的启动和数据处理时间 - 选项D：增加最大实例数量 + Standby状态的组合不如warm pool有效，且Standby状态的实例不会主动预处理数据 **决策标准和最佳实践：** 1. 成本效益平衡：Running状态的warm pool虽然成本稍高，但能最大程度减少启动时间 2. 预处理策略：在warm pool中预先完成数据下载和处理，避免重复工作 3. 生命周期管理：使用lifecycle hook确保服务质量，只有完全准备好的实例才接收流量 4. 扩容响应速度：在需要快速响应负载变化的场景中，Running状态的预热实例提供最佳性能</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">74</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using an AWS CodeBuild project to build and package an application. The packages are copied to a shared Amazon S3 bucket before being deployed across multiple AWS accounts. The buildspec.yml file contains the following: The DevOps engineer has noticed that anybody with an AWS account is able to download the artifacts. What steps should the DevOps engineer take to stop this? D (79%) A (21%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the post_build command to use --acl public-read and configure a bucket policy that grants read access to the relevant AWS accounts only.
B. Configure a default ACL for the S3 bucket that defines the set of authenticated users as the relevant AWS accounts only and grants read-only access.
C. Create an S3 bucket policy that grants read access to the relevant AWS accounts and denies read access to the principal &quot;*&quot;.
D. Modify the post_build command to remove --acl authenticated-read and configure a bucket policy that allows read access to the relevant AWS accounts only.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS CodeBuild项目来构建和打包应用程序。这些包被复制到共享的Amazon S3存储桶中，然后部署到多个AWS账户。buildspec.yml文件包含以下内容：DevOps工程师注意到任何拥有AWS账户的人都能够下载这些构件。DevOps工程师应该采取什么步骤来阻止这种情况？ 选项： A. 修改post_build命令使用--acl public-read，并配置存储桶策略，仅向相关AWS账户授予读取访问权限。 B. 为S3存储桶配置默认ACL，将经过身份验证的用户集定义为仅相关AWS账户，并授予只读访问权限。 C. 创建S3存储桶策略，向相关AWS账户授予读取访问权限，并拒绝主体&quot;*&quot;的读取访问权限。 D. 修改post_build命令移除--acl authenticated-read，并配置存储桶策略，仅允许相关AWS账户的读取访问权限。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题的核心是解决S3存储桶中构建产物的访问权限过于宽松的问题。当前任何AWS账户用户都能下载构件，需要限制访问权限仅给特定的相关AWS账户。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：持续集成服务，用于构建和打包应用程序 - Amazon S3：对象存储服务，存储构建产物 - S3 ACL (Access Control List)：对象级别的访问控制 - S3 Bucket Policy：存储桶级别的访问策略 - `--acl authenticated-read`：S3 ACL设置，允许所有经过AWS身份验证的用户读取 - `--acl public-read`：S3 ACL设置，允许所有人（包括匿名用户）读取 **正确答案D的原因：** 1. **移除问题根源**：`--acl authenticated-read`是导致任何AWS账户用户都能访问的根本原因，必须移除 2. **采用最佳实践**：使用bucket policy替代ACL进行精确的访问控制 3. **精确权限控制**：bucket policy可以明确指定哪些AWS账户可以访问，实现最小权限原则 4. **安全性更强**：避免了ACL的宽泛权限设置，转向更精确的策略控制 **其他选项错误的原因：** - **选项A错误**：`--acl public-read`比原来的`authenticated-read`更加宽松，会让所有人（包括匿名用户）都能访问，严重恶化安全问题 - **选项B错误**：默认ACL仍然是ACL机制，无法精确指定特定AWS账户，且&quot;authenticated users&quot;概念过于宽泛 - **选项C错误**：虽然思路正确，但仅拒绝&quot;*&quot;主体不够全面，没有解决`authenticated-read` ACL的问题，可能仍存在权限泄露 **决策标准和最佳实践：** 1. **最小权限原则**：仅授予必要的最小权限给指定的AWS账户 2. **策略优于ACL**：在需要复杂访问控制时，优先使用bucket policy而非ACL 3. **移除宽泛权限**：彻底移除可能导致权限泄露的ACL设置 4. **分层安全**：结合移除问题ACL和配置精确bucket policy的双重措施</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">75</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has developed a serverless web application that is hosted on AWS. The application consists of Amazon S3, Amazon API Gateway, several AWS Lambda functions, and an Amazon RDS for MySQL database. The company is using AWS CodeCommit to store the source code. The source code is a combination of AWS Serverless Application Model (AWS SAM) templates and Python code. A security audit and penetration test reveal that user names and passwords for authentication to the database are hardcoded within CodeCommit repositories. A DevOps engineer must implement a solution to automatically detect and prevent hardcoded secrets. What is the MOST secure solution that meets these requirements? report. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager. B (95%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation report. Write the secret to AWS Systems Manager Parameter Store as a secure string. Update the SAM templates and the Python code to pull the secret from Parameter Store.
B. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager.
C. Enable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation
D. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Write the secret to AWS Systems Manager Parameter Store as a string. Update the SAM templates and the Python code to pull the secret from Parameter Store.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司开发了一个托管在AWS上的无服务器Web应用程序。该应用程序由Amazon S3、Amazon API Gateway、多个AWS Lambda函数和Amazon RDS for MySQL数据库组成。公司使用AWS CodeCommit存储源代码。源代码是AWS Serverless Application Model (AWS SAM)模板和Python代码的组合。安全审计和渗透测试显示，用于数据库身份验证的用户名和密码在CodeCommit存储库中是硬编码的。DevOps工程师必须实施一个解决方案来自动检测和防止硬编码的机密信息。什么是满足这些要求的最安全的解决方案？ 选项： A. 启用Amazon CodeGuru Profiler。用@with_lambda_profiler()装饰处理函数。手动审查推荐报告。将机密信息写入AWS Systems Manager Parameter Store作为安全字符串。更新SAM模板和Python代码以从Parameter Store拉取机密信息。 B. 将CodeCommit存储库与Amazon CodeGuru Reviewer关联。手动检查代码审查的任何建议。选择保护机密信息的选项。更新SAM模板和Python代码以从AWS Secrets Manager拉取机密信息。 C. 启用Amazon CodeGuru Profiler。用@with_lambda_profiler()装饰处理函数。手动审查推荐报告。选择保护机密信息的选项。更新SAM模板和Python代码以从AWS Secrets Manager拉取机密信息。 D. 将CodeCommit存储库与Amazon CodeGuru Reviewer关联。手动检查代码审查的任何建议。将机密信息作为字符串写入AWS Systems Manager Parameter Store。更新SAM模板和Python代码以从Parameter Store拉取机密信息。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到最安全的解决方案来自动检测和防止代码中的硬编码机密信息，特别是数据库用户名和密码。需要同时解决检测问题和存储问题两个方面。 **涉及的关键AWS服务和概念：** 1. **Amazon CodeGuru Reviewer** - 用于自动化代码审查，可以检测安全问题包括硬编码的机密信息 2. **Amazon CodeGuru Profiler** - 用于应用程序性能分析，不是用于代码安全审查 3. **AWS Secrets Manager** - 专门用于管理敏感信息如数据库凭证，提供自动轮换功能 4. **AWS Systems Manager Parameter Store** - 用于存储配置数据，可以存储加密参数但功能不如Secrets Manager全面 **正确答案B的原因：** 1. **正确的检测工具**：使用CodeGuru Reviewer而不是CodeGuru Profiler，Reviewer专门用于代码质量和安全审查，能够检测硬编码的机密信息 2. **最安全的存储方案**：选择AWS Secrets Manager而不是Parameter Store，因为Secrets Manager专门为数据库凭证设计，提供自动轮换、细粒度访问控制等高级安全功能 3. **完整的解决方案**：既解决了检测问题又提供了最佳的存储替代方案 **其他选项错误的原因：** - **选项A**：使用了错误的工具CodeGuru Profiler（用于性能分析而非安全审查），且使用Parameter Store不如Secrets Manager安全 - **选项C**：同样使用了错误的CodeGuru Profiler工具 - **选项D**：虽然使用了正确的CodeGuru Reviewer，但将机密信息存储为普通字符串而非加密形式，且Parameter Store对于数据库凭证管理不如Secrets Manager专业 **决策标准和最佳实践：** 1. **工具选择**：对于代码安全审查应使用CodeGuru Reviewer，对于性能分析使用CodeGuru Profiler 2. **机密信息管理**：数据库凭证应优先使用AWS Secrets Manager，它提供专门的数据库集成和自动轮换功能 3. **安全层级**：Secrets Manager &gt; Parameter Store (SecureString) &gt; Parameter Store (String) &gt; 硬编码 4. **自动化原则**：选择能够自动检测和预防安全问题的工具和服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">76</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using Amazon S3 buckets to store important documents. The company discovers that some S3 buckets are not encrypted. Currently, the company&#x27;s IAM users can create new S3 buckets without encryption. The company is implementing a new requirement that all S3 buckets must be encrypted. A DevOps engineer must implement a solution to ensure that server-side encryption is enabled on all existing S3 buckets and all new S3 buckets. The encryption must be enabled on new S3 buckets as soon as the S3 buckets are created. The default encryption type must be 256-bit Advanced Encryption Standard (AES-256). Which solution will meet these requirements? B (90%) 10%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS Lambda function that is invoked periodically by an Amazon EventBridge scheduled rule. Program the Lambda function to scan all current S3 buckets for encryption status and to set AES-256 as the default encryption for any S3 bucket that does not have an encryption configuration.
B. Set up and activate the s3-bucket-server-side-encryption-enabled AWS Config managed rule. Configure the rule to use the AWS-EnableS3BucketEncryption AWS Systems Manager Automation runbook as the remediation action. Manually run the re-evaluation process to ensure that existing S3 buckets are compliant.
C. Create an AWS Lambda function that is invoked by an Amazon EventBridge event rule. Define the rule with an event pattern that matches the creation of new S3 buckets. Program the Lambda function to parse the EventBridge event, check the configuration of the S3 buckets from the event, and set AES-256 as the default encryption.
D. Configure an IAM policy that denies the s3:CreateBucket action if the s3:x-amz-server-side-encryption condition key has a value that is not AES-256. Create an IAM group for all the company&#x27;s IAM users. Associate the IAM policy with the IAM group.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用Amazon S3存储桶来存储重要文档。公司发现一些S3存储桶没有加密。目前，公司的IAM用户可以创建没有加密的新S3存储桶。公司正在实施一项新要求，即所有S3存储桶都必须加密。DevOps工程师必须实施一个解决方案，确保在所有现有S3存储桶和所有新S3存储桶上启用服务器端加密。加密必须在创建新S3存储桶时立即启用。默认加密类型必须是256位高级加密标准(AES-256)。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 确保所有现有S3存储桶都启用AES-256加密 2. 确保所有新创建的S3存储桶都必须启用AES-256加密 3. 加密必须在创建存储桶时立即生效 4. 防止用户创建未加密的存储桶 **涉及的关键AWS服务和概念：** - Amazon S3服务器端加密 - IAM策略和条件键 - AWS Config规则和自动修复 - Lambda函数和EventBridge事件 - 预防性控制vs检测性控制 **正确答案D的原因：** 1. **预防性控制**：通过IAM策略在源头阻止创建未加密的存储桶，这是最有效的方法 2. **立即生效**：IAM策略会在用户尝试创建存储桶时立即执行检查，满足&quot;立即启用&quot;的要求 3. **条件键使用**：s3:x-amz-server-side-encryption条件键可以精确控制加密要求 4. **全面覆盖**：通过IAM组应用到所有用户，确保无人能绕过此限制 **其他选项错误的原因：** - **选项A**：定期扫描是被动的，无法防止在扫描间隔期间创建未加密的存储桶，不满足&quot;立即启用&quot;要求 - **选项B**：AWS Config是检测性控制，虽然有自动修复功能，但仍然存在时间延迟，且主要用于合规检查而非预防 - **选项C**：基于事件的Lambda函数虽然响应及时，但仍然是在存储桶创建后才执行，存在短暂的未加密窗口期 **决策标准和最佳实践：** 1. **预防优于检测**：在安全控制中，预防性措施比检测和修复更有效 2. **最小权限原则**：通过IAM策略限制用户只能执行符合安全要求的操作 3. **实时控制**：IAM策略提供实时的访问控制，无延迟 4. **成本效益**：IAM策略无额外运行成本，而Lambda和Config规则会产生使用费用</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">77</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is architecting a continuous development strategy for a company&#x27;s software as a service (SaaS) web application running on AWS. For application and security reasons, users subscribing to this application are distributed across multiple Application Load Balancers (ALBs), each of which has a dedicated Auto Scaling group and fleet of Amazon EC2 instances. The application does not require a build stage, and when it is committed to AWS CodeCommit, the application must trigger a simultaneous deployment to all ALBs, Auto Scaling groups, and EC2 fleets. Which architecture will meet these requirements with the LEAST amount of configuration? C (73%) B (23%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a single AWS CodePipeline pipeline that deploys the application in parallel using unique AWS CodeDeploy applications and deployment groups created for each ALB-Auto Scaling group pair.
B. Create a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group.
C. Create a single AWS CodePipeline pipeline that deploys the application in parallel using a single AWS CodeDeploy application and unique deployment group for each ALB-Auto Scaling group pair.
D. Create an AWS CodePipeline pipeline for each ALB-Auto Scaling group pair that deploys the application using an AWS CodeDeploy application and deployment group created for the same ALB-Auto Scaling group pair.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在为公司运行在AWS上的软件即服务(SaaS) Web应用程序设计持续开发策略。出于应用程序和安全原因，订阅此应用程序的用户分布在多个Application Load Balancer (ALB)上，每个ALB都有专用的Auto Scaling组和Amazon EC2实例集群。该应用程序不需要构建阶段，当代码提交到AWS CodeCommit时，应用程序必须触发同时部署到所有ALB、Auto Scaling组和EC2集群。哪种架构能以最少的配置满足这些要求？ 选项： A. 创建单个AWS CodePipeline流水线，使用为每个ALB-Auto Scaling组对创建的唯一AWS CodeDeploy应用程序和部署组并行部署应用程序。 B. 创建单个AWS CodePipeline流水线，使用单个AWS CodeDeploy应用程序和单个部署组部署应用程序。 C. 创建单个AWS CodePipeline流水线，使用单个AWS CodeDeploy应用程序和为每个ALB-Auto Scaling组对创建的唯一部署组并行部署应用程序。 D. 为每个ALB-Auto Scaling组对创建AWS CodePipeline流水线，使用为同一ALB-Auto Scaling组对创建的AWS CodeDeploy应用程序和部署组部署应用程序。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 需要同时部署到多个ALB-Auto Scaling组对 - 不需要构建阶段，直接从CodeCommit触发部署 - 要求最少的配置复杂度 - 实现并行部署到所有目标环境 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD流水线服务，用于自动化部署流程 - AWS CodeDeploy：应用程序部署服务，管理EC2实例的代码部署 - CodeDeploy Application：逻辑容器，包含部署配置和历史记录 - CodeDeploy Deployment Group：定义部署目标实例的集合 - Auto Scaling Group：自动扩缩容的EC2实例组 **正确答案C的原因：** 1. **单个CodePipeline**：减少管理复杂度，统一触发机制 2. **单个CodeDeploy应用程序**：所有环境使用相同的应用程序代码和配置，便于统一管理 3. **多个部署组**：每个ALB-Auto Scaling组对需要独立的部署组来隔离部署目标 4. **并行部署**：满足同时部署到所有环境的要求 5. **配置最少**：相比其他方案，这种组合提供了最佳的简洁性和功能性平衡 **其他选项错误的原因：** - **选项A**：创建多个CodeDeploy应用程序增加了不必要的复杂性，违反了&quot;最少配置&quot;原则 - **选项B**：单个部署组无法区分不同的ALB-Auto Scaling组对，无法实现隔离部署 - **选项D**：为每个环境创建独立的CodePipeline大大增加了管理开销和配置复杂度 **决策标准和最佳实践：** 1. **最小化管理开销**：优先选择能减少重复配置的方案 2. **适当的资源隔离**：使用部署组而非应用程序来区分部署目标 3. **并行处理能力**：确保能同时处理多个部署目标 4. **统一管理**：单一流水线便于监控和故障排除 5. **扩展性考虑**：方案应该便于添加新的ALB-Auto Scaling组对</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">78</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is hosting a static website from an Amazon S3 bucket. The website is available to customers at example.com. The company uses an Amazon Route 53 weighted routing policy with a TTL of 1 day. The company has decided to replace the existing static website with a dynamic web application. The dynamic web application uses an Application Load Balancer (ALB) in front of a fleet of Amazon EC2 instances. On the day of production launch to customers, the company creates an additional Route 53 weighted DNS record entry that points to the ALB with a weight of 255 and a TTL of 1 hour. Two days later, a DevOps engineer notices that the previous static website is displayed sometimes when customers navigate to example.com. How can the DevOps engineer ensure that the company serves only dynamic content for example.com? D (81%) B (19%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Delete all objects, including previous versions, from the S3 bucket that contains the static website content.
B. Update the weighted DNS record entry that points to the S3 bucket. Apply a weight of 0. Specify the domain reset option to propagate changes immediately.
C. Configure webpage redirect requests on the S3 bucket with a hostname that redirects to the ALB.
D. Remove the weighted DNS record entry that points to the S3 bucket from the example.com hosted zone. Wait for DNS propagation to become complete.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在从Amazon S3存储桶托管静态网站。该网站通过example.com向客户提供服务。公司使用Amazon Route 53加权路由策略，TTL为1天。公司决定用动态Web应用程序替换现有的静态网站。动态Web应用程序在Amazon EC2实例集群前使用Application Load Balancer (ALB)。在向客户正式发布的当天，公司创建了一个额外的Route 53加权DNS记录条目，指向ALB，权重为255，TTL为1小时。两天后，DevOps工程师注意到当客户访问example.com时，有时仍然显示之前的静态网站。DevOps工程师如何确保公司只为example.com提供动态内容？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题要求解决DNS路由中新旧服务并存的问题，确保用户只访问到新的动态Web应用，而不是旧的静态网站。 **涉及的关键AWS服务和概念：** - Route 53加权路由策略：根据权重分配流量到不同的资源 - DNS TTL（生存时间）：控制DNS记录在缓存中保存的时间 - S3静态网站托管：原有的静态网站解决方案 - Application Load Balancer：新的动态应用前端负载均衡器 - DNS传播：DNS记录更改在全球DNS系统中生效的过程 **正确答案D的原因：** - 直接删除指向S3的DNS记录是最彻底的解决方案 - 消除了流量分配的源头，确保所有流量都指向ALB - 虽然需要等待DNS传播完成，但这是标准的DNS更改流程 - 一旦传播完成，就不会再有流量路由到旧的静态网站 **其他选项错误的原因：** - 选项A：删除S3对象不能解决DNS路由问题，DNS记录仍然存在，会导致访问错误 - 选项B：Route 53没有&quot;域重置选项&quot;来立即传播更改，这个功能不存在 - 选项C：配置重定向只是增加了额外的跳转步骤，没有解决根本的DNS路由问题，且仍有部分流量会先到达S3 **决策标准和最佳实践：** - 在DNS切换场景中，应该从源头解决路由问题 - 理解DNS TTL和传播机制，合理规划切换时间 - 在生产环境切换时，应该有明确的回退计划 - 监控DNS解析结果，确保切换效果符合预期 - 考虑使用更短的TTL来加快DNS更改的生效速度</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">79</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is implementing AWS CodePipeline to automate its testing process. The company wants to be notified when the execution state fails and used the following custom event pattern in Amazon EventBridge: Which type of events will match this event pattern? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Failed deploy and build actions across all the pipelines
B. All rejected or failed approval actions across all the pipelines
C. All the events across all pipelines
D. Approval actions across all the pipelines</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在实施AWS CodePipeline来自动化其测试流程。该公司希望在执行状态失败时收到通知，并在Amazon EventBridge中使用了以下自定义事件模式：哪种类型的事件将匹配此事件模式？ 选项： A. 所有pipeline中失败的部署和构建操作 B. 所有pipeline中被拒绝或失败的审批操作 C. 所有pipeline中的所有事件 D. 所有pipeline中的审批操作 正确答案：B</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是Amazon EventBridge事件模式匹配机制，特别是如何为AWS CodePipeline配置事件过滤规则来监控特定类型的失败事件。虽然题目中没有显示具体的事件模式代码，但从正确答案可以推断出该模式专门针对审批操作的失败和拒绝状态。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：持续集成/持续部署服务，包含多种操作类型（构建、部署、审批等） - Amazon EventBridge：事件驱动架构服务，用于监控和响应AWS服务状态变化 - 事件模式匹配：通过JSON格式定义过滤条件，精确匹配特定事件 - CodePipeline操作状态：包括SUCCEEDED、FAILED、REJECTED等状态 **正确答案的原因：** 选项B正确，因为该事件模式专门配置为匹配： 1. 操作类型为&quot;Approval&quot;（审批操作） 2. 状态为&quot;FAILED&quot;或&quot;REJECTED&quot;的事件 3. 覆盖所有pipeline实例 这种配置能够精确捕获审批环节的问题，这在需要人工干预的CI/CD流程中非常重要。 **其他选项错误的原因：** - 选项A错误：该模式不包含构建(Build)和部署(Deploy)操作，只针对审批操作 - 选项C错误：该模式有明确的过滤条件，不会匹配所有事件，只匹配特定的失败状态 - 选项D错误：该模式不仅限于审批操作，还包含了状态过滤（失败/拒绝），不是所有审批事件 **决策标准和最佳实践：** 1. 事件模式应该精确定义source、detail-type和detail字段来避免误报 2. 针对不同操作类型设置不同的通知策略，审批失败通常需要立即人工介入 3. 合理使用EventBridge规则可以实现精细化的监控和自动化响应 4. 在生产环境中应该为关键操作设置多层监控和告警机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">80</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An application running on a set of Amazon EC2 instances in an Auto Scaling group requires a configuration file to operate. The instances are created and maintained with AWS CloudFormation. A DevOps engineer wants the instances to have the latest configuration file when launched, and wants changes to the configuration file to be reflected on all the instances with a minimal delay when the CloudFormation template is updated. Company policy requires that application configuration files be maintained along with AWS infrastructure configuration files in source control. Which solution will accomplish this? D (77%) B (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the CloudFormation template, add an AWS Config rule. Place the configuration file content in the rule&#x27;s InputParameters property, and set the Scope property to the EC2 Auto Scaling group. Add an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the configuration.
B. In the CloudFormation template, add an EC2 launch template resource. Place the configuration file content in the launch template. Configure the cfn-init script to run when the instance is launched, and configure the cfn-hup script to poll for updates to the configuration.
C. In the CloudFormation template, add an EC2 launch template resource. Place the configuration file content in the launch template. Add an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the configuration.
D. In the CloudFormation template, add CloudFormation init metadata. Place the configuration file content in the metadata. Configure the cfn-init script to run when the instance is launched, and configure the cfn-hup script to poll for updates to the configuration.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个运行在Auto Scaling组中的Amazon EC2实例集合上的应用程序需要一个配置文件来运行。这些实例通过AWS CloudFormation创建和维护。DevOps工程师希望实例在启动时拥有最新的配置文件，并且当CloudFormation模板更新时，配置文件的更改能够以最小的延迟反映到所有实例上。公司政策要求应用程序配置文件与AWS基础设施配置文件一起在源代码控制中维护。哪个解决方案能够实现这个目标？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. EC2实例启动时需要获取最新的配置文件 2. CloudFormation模板更新时，配置文件变更要以最小延迟反映到所有实例 3. 配置文件需要与基础设施代码一起在源代码控制中维护 4. 实例通过Auto Scaling组和CloudFormation管理 **涉及的关键AWS服务和概念：** - CloudFormation：基础设施即代码服务 - EC2 Launch Template：定义EC2实例启动配置的模板 - CloudFormation Init Metadata：在CloudFormation模板中定义实例初始化配置 - cfn-init：CloudFormation辅助脚本，用于实例启动时的初始化 - cfn-hup：CloudFormation辅助脚本，用于监控配置变更并自动更新 **正确答案B的原因：** 1. 使用EC2 Launch Template可以将配置文件内容直接嵌入到模板中，满足源代码控制要求 2. cfn-init脚本确保实例启动时能够获取最新配置 3. cfn-hup脚本能够持续监控CloudFormation模板的变更，实现配置的自动更新，满足最小延迟要求 4. 这种方案完全基于CloudFormation原生功能，无需额外的AWS服务 **其他选项错误的原因：** - 选项A：AWS Config主要用于合规性检查，不是用于配置文件分发的服务；Resource Data Sync也不适用于此场景 - 选项C：虽然使用了Launch Template，但Resource Data Sync不是用于配置更新监控的正确服务 - 选项D：CloudFormation Init Metadata是正确的方向，但题目明确提到使用Launch Template，而且cfn-hup需要与CloudFormation metadata配合使用 **决策标准和最佳实践：** 1. 优先使用CloudFormation原生功能而非额外的AWS服务 2. cfn-init和cfn-hup是处理CloudFormation配置管理的标准工具组合 3. Launch Template提供了更好的版本控制和配置管理能力 4. 配置文件直接嵌入基础设施代码中，确保了版本一致性和源代码控制要求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">81</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages an application that stores logs in Amazon CloudWatch Logs. The company wants to archive the logs to an Amazon S3 bucket. Logs are rarely accessed after 90 days and must be retained for 10 years. Which combination of steps should a DevOps engineer take to meet these requirements? (Choose two.) BD (83%) CD (17%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a CloudWatch Logs subscription filter to use AWS Glue to transfer all logs to an S3 bucket.
B. Configure a CloudWatch Logs subscription filter to use Amazon Kinesis Data Firehose to stream all logs to an S3 bucket.
C. Configure a CloudWatch Logs subscription filter to stream all logs to an S3 bucket.
D. Configure the S3 bucket lifecycle policy to transition logs to S3 Glacier after 90 days and to expire logs after 3,650 days.
E. Configure the S3 bucket lifecycle policy to transition logs to Reduced Redundancy after 90 days and to expire logs after 3,650 days.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司管理一个将日志存储在Amazon CloudWatch Logs中的应用程序。该公司希望将日志归档到Amazon S3存储桶中。日志在90天后很少被访问，并且必须保留10年。DevOps工程师应该采取哪些步骤组合来满足这些要求？（选择两个。）BD (83%) CD (17%) 选项：A. 配置CloudWatch Logs订阅过滤器使用AWS Glue将所有日志传输到S3存储桶。 B. 配置CloudWatch Logs订阅过滤器使用Amazon Kinesis Data Firehose将所有日志流式传输到S3存储桶。 C. 配置CloudWatch Logs订阅过滤器将所有日志流式传输到S3存储桶。 D. 配置S3存储桶生命周期策略，在90天后将日志转换到S3 Glacier，并在3,650天后使日志过期。 E. 配置S3存储桶生命周期策略，在90天后将日志转换到Reduced Redundancy，并在3,650天后使日志过期。 正确答案：B</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个解决方案来：1）将CloudWatch Logs中的日志归档到S3；2）处理90天后很少访问的特性；3）满足10年（3,650天）的保留要求。这是一个典型的日志归档和生命周期管理场景。 **涉及的关键AWS服务和概念：** - CloudWatch Logs订阅过滤器：用于实时流式传输日志数据 - Amazon Kinesis Data Firehose：托管的数据传输服务，可以可靠地将流数据传输到S3 - S3生命周期策略：自动管理对象存储类别转换和过期 - S3存储类别：Standard、Glacier等不同成本和访问特性的存储选项 **正确答案的原因：** 选项B正确，因为Kinesis Data Firehose是将CloudWatch Logs数据传输到S3的标准和推荐方法。它提供了可靠的、托管的流式传输服务，支持数据转换、压缩和批处理，非常适合日志归档场景。 选项D也是正确的（虽然题目只显示B为正确答案），因为它正确配置了生命周期策略：90天后转换到Glacier（适合很少访问的数据），10年后过期删除，完全符合需求。 **其他选项错误的原因：** - 选项A：AWS Glue主要用于ETL数据处理，不是日志流式传输的最佳选择，过于复杂且成本较高 - 选项C：CloudWatch Logs订阅过滤器不能直接流式传输到S3，需要中间服务如Kinesis Data Firehose - 选项E：Reduced Redundancy Storage已被AWS弃用，且不适合长期归档，应该使用Glacier等归档存储类别 **决策标准和最佳实践：** 1. 使用托管服务减少运维复杂性（Kinesis Data Firehose vs 自建方案） 2. 根据访问模式选择合适的存储类别（很少访问选择Glacier） 3. 利用S3生命周期策略自动化存储管理 4. 考虑成本优化（Glacier比Standard存储成本更低） 5. 确保数据传输的可靠性和持久性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">82</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is developing a new application. The application uses AWS Lambda functions for its compute tier. The company must use a canary deployment for any changes to the Lambda functions. Automated rollback must occur if any failures are reported. The company&#x27;s DevOps team needs to create the infrastructure as code (IaC) and the CI/CD pipeline for this solution. Which combination of steps will meet these requirements? (Choose three.) that starts the pipeline. Create an AWS CodeBuild project to deploy the AWS Serverless Application Model (AWS SAM) template. Upload the template and source code to the CodeCommit repository. In the CodeCommit repository, create a buildspec.yml file that includes the commands to build and deploy the SAM application. Most Voted F. Create an Amazon CloudWatch alarm for each Lambda function. Configure the alarms to enter the ALARM state if any errors are detected. Configure an evaluation period, dimensions for each Lambda function and version, and the namespace as AWS/Lambda on the Errors metric. Most Voted BCF (89%) 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CloudFormation template for the application. Define each Lambda function in the template by using the AWS::Lambda::Function resource type. In the template, include a version for the Lambda function by using the AWS::Lambda::Version resource type. Declare the CodeSha256 property. Configure an AWS::Lambda::Alias resource that references the latest version of the Lambda function.
B. Create an AWS Serverless Application Model (AWS SAM) template for the application. Define each Lambda function in the template by using the AWS::Serverless::Function resource type. For each function, include configurations for the AutoPublishAlias property and the DeploymentPreference property. Configure the deployment configuration type to LambdaCanary10Percent10Minutes.
C. Create an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage
D. Create an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage that starts the pipeline. Create an AWS CodeDeploy deployment group that is configured for canary deployments with a DeploymentPreference type of Canary10Percent10Minutes. Upload the AWS CloudFormation template and source code to the CodeCommit repository. In the CodeCommit repository, create an appspec.yml file that includes the commands to deploy the CloudFormation template.
E. Create an Amazon CloudWatch composite alarm for all the Lambda functions. Configure an evaluation period and dimensions for Lambda. Configure the alarm to enter the ALARM state if any errors are detected or if there is insufficient data.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在开发一个新应用程序。该应用程序使用AWS Lambda函数作为其计算层。公司必须对Lambda函数的任何更改使用金丝雀部署。如果报告任何故障，必须进行自动回滚。公司的DevOps团队需要为此解决方案创建基础设施即代码(IaC)和CI/CD管道。哪种步骤组合将满足这些要求？（选择三个。） 选项： A. 为应用程序创建AWS CloudFormation模板。使用AWS::Lambda::Function资源类型在模板中定义每个Lambda函数。在模板中，使用AWS::Lambda::Version资源类型包含Lambda函数的版本。声明CodeSha256属性。配置引用Lambda函数最新版本的AWS::Lambda::Alias资源。 B. 为应用程序创建AWS Serverless Application Model (AWS SAM)模板。使用AWS::Serverless::Function资源类型在模板中定义每个Lambda函数。对于每个函数，包含AutoPublishAlias属性和DeploymentPreference属性的配置。将部署配置类型配置为LambdaCanary10Percent10Minutes。 C. 创建AWS CodeCommit存储库。创建AWS CodePipeline管道。在新的源阶段中使用CodeCommit存储库 D. 创建AWS CodeCommit存储库。创建AWS CodePipeline管道。在启动管道的新源阶段中使用CodeCommit存储库。创建配置为金丝雀部署的AWS CodeDeploy部署组，DeploymentPreference类型为Canary10Percent10Minutes。将AWS CloudFormation模板和源代码上传到CodeCommit存储库。在CodeCommit存储库中，创建包含部署CloudFormation模板命令的appspec.yml文件。 E. 为所有Lambda函数创建Amazon CloudWatch复合告警。配置Lambda的评估期和维度。配置告警在检测到任何错误或数据不足时进入ALARM状态。 F. 为每个Lambda函数创建Amazon CloudWatch告警。配置告警在检测到任何错误时进入ALARM状态。为每个Lambda函数和版本配置评估期、维度，并将命名空间设置为AWS/Lambda的Errors指标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现Lambda函数的金丝雀部署，包含自动回滚功能，并需要创建完整的IaC和CI/CD管道解决方案。 **涉及的关键AWS服务和概念：** - AWS SAM (Serverless Application Model) - 无服务器应用程序的IaC框架 - AWS Lambda - 无服务器计算服务 - 金丝雀部署 - 渐进式部署策略，先部署到少量流量 - AWS CodeCommit/CodePipeline - CI/CD服务 - Amazon CloudWatch - 监控和告警服务 - 自动回滚机制 **正确答案的原因：** 选项B是正确的，因为： 1. AWS SAM专门为无服务器应用程序设计，比纯CloudFormation更适合Lambda部署 2. AutoPublishAlias属性自动创建和管理Lambda版本别名 3. DeploymentPreference属性内置支持金丝雀部署策略 4. LambdaCanary10Percent10Minutes配置实现了10%流量的10分钟金丝雀部署 5. SAM自动集成CloudWatch告警进行健康检查和自动回滚 **其他选项错误的原因：** - 选项A：使用纯CloudFormation需要手动配置复杂的版本管理和部署策略，不如SAM简洁 - 选项C：不完整，缺少具体的部署配置 - 选项D：CodeDeploy主要用于EC2/ECS部署，对Lambda支持有限；appspec.yml不适用于Lambda部署 - 选项E：复合告警过于复杂，且&quot;数据不足&quot;触发告警不合适 - 选项F：虽然CloudWatch告警是必要的，但SAM的DeploymentPreference已经内置了监控机制 **决策标准和最佳实践：** 1. 对于Lambda金丝雀部署，优先选择AWS SAM而非纯CloudFormation 2. 利用SAM内置的部署偏好设置简化配置 3. 确保监控和自动回滚机制的完整性 4. 选择适合无服务器架构的CI/CD工具和配置</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">83</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is deploying a new version of a company&#x27;s application in an AWS CodeDeploy deployment group associated with its Amazon EC2 instances. After some time, the deployment fails. The engineer realizes that all the events associated with the specific deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group. What are valid reasons for this failure? (Choose two.) AD (91%) 9%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. The networking configuration does not allow the EC2 instances to reach the internet via a NAT gateway or internet gateway, and the CodeDeploy endpoint cannot be reached.
B. The IAM user who triggered the application deployment does not have permission to interact with the CodeDeploy endpoint.
C. The target EC2 instances were not properly registered with the CodeDeploy endpoint.
D. An instance profile with proper permissions was not attached to the target EC2 instances.
E. The appspec.yml file was not included in the application revision.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在为公司应用程序的新版本进行部署，使用与Amazon EC2实例关联的AWS CodeDeploy部署组。一段时间后，部署失败了。工程师发现与特定部署ID相关的所有事件都处于Skipped状态，代码没有部署到与部署组关联的实例上。这种失败的有效原因是什么？（选择两个） 选项： A. 网络配置不允许EC2实例通过NAT gateway或internet gateway访问互联网，无法到达CodeDeploy端点。 B. 触发应用程序部署的IAM用户没有与CodeDeploy端点交互的权限。 C. 目标EC2实例没有正确注册到CodeDeploy端点。 D. 没有为目标EC2实例附加具有适当权限的instance profile。 E. 应用程序修订版本中没有包含appspec.yml文件。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodeDeploy部署失败的故障排除，特别是当所有部署事件都显示为&quot;Skipped&quot;状态时的可能原因。题目要求选择两个正确答案，但给出的正确答案只有A选项。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：自动化应用程序部署服务 - EC2实例：目标部署环境 - CodeDeploy Agent：运行在EC2实例上的代理程序 - IAM权限：访问控制和权限管理 - 网络连接：internet gateway、NAT gateway - Instance Profile：EC2实例的IAM角色 **正确答案A的原因：** 选项A是正确的，因为CodeDeploy Agent需要与AWS CodeDeploy服务端点通信来接收部署指令。如果EC2实例无法通过internet gateway或NAT gateway访问互联网，就无法连接到CodeDeploy端点，导致部署事件被跳过（Skipped状态）。 **其他选项错误的原因：** - 选项B：IAM用户权限问题通常会导致部署无法启动，而不是事件被跳过 - 选项C：EC2实例不需要主动&quot;注册&quot;到CodeDeploy端点，而是通过CodeDeploy Agent自动连接 - 选项D：虽然instance profile权限很重要，但这通常会导致其他类型的错误，而不是所有事件都被跳过 - 选项E：缺少appspec.yml文件会导致部署配置错误，但不会导致事件被跳过 **决策标准和最佳实践：** 1. 确保EC2实例具有互联网访问能力（通过internet gateway或NAT gateway） 2. 验证CodeDeploy Agent是否正确安装和运行 3. 检查安全组和网络ACL是否允许必要的出站连接 4. 确保instance profile具有适当的CodeDeploy权限 5. 监控CodeDeploy Agent日志以识别连接问题 注意：题目标注要选择两个答案，但根据分析，选项D也可能是正确的，因为没有适当权限的instance profile也会导致CodeDeploy Agent无法正常工作。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">84</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a guideline that every Amazon EC2 instance must be launched from an AMI that the company&#x27;s security team produces. Every month, the security team sends an email message with the latest approved AMIs to all the development teams. The development teams use AWS CloudFormation to deploy their applications. When developers launch a new service, they have to search their email for the latest AMIs that the security department sent. A DevOps engineer wants to automate the process that the security team uses to provide the AMI IDs to the development teams. What is the MOST scalable solution that meets these requirements? Systems Manager Parameter Store. Instruct the developers to specify a parameter of type SSM in their CloudFormation stack to obtain the most recent AMI ARNs from Parameter Store. Most Voted C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Direct the security team to use CloudFormation to create new versions of the AMIs and to list the AMI ARNs in an encrypted Amazon S3 object as part of the stack&#x27;s Outputs section. Instruct the developers to use a cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs.
B. Direct the security team to use a CloudFormation stack to create an AWS CodePipeline pipeline that builds new AMIs and places the latest AMI ARNs in an encrypted Amazon S3 object as part of the pipeline output. Instruct the developers to use a cross-stack reference within their own CloudFormation template to obtain the S3 object location and the most recent AMI ARNs.
C. Direct the security team to use Amazon EC2 Image Builder to create new AMIs and to place the AMI ARNs as parameters in AWS Systems Manager Parameter Store. Instruct the developers to specify the parameter names in their CloudFormation template.
D. Direct the security team to use Amazon EC2 Image Builder to create new AMIs and to create an Amazon Simple Notification Service (Amazon SNS) topic so that every development team can receive notifications. When the development teams receive a notification, instruct them to write an AWS Lambda function that will update their CloudFormation stack with the most recent AMI ARNs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个准则，要求每个Amazon EC2实例都必须从公司安全团队制作的AMI启动。每个月，安全团队会向所有开发团队发送包含最新批准AMI的邮件消息。开发团队使用AWS CloudFormation来部署他们的应用程序。当开发人员启动新服务时，他们必须在邮件中搜索安全部门发送的最新AMI。一名DevOps工程师希望自动化安全团队向开发团队提供AMI ID的流程。什么是满足这些要求的最具可扩展性的解决方案？ 选项： A. 指导安全团队使用CloudFormation创建新版本的AMI，并将AMI ARN列在加密的Amazon S3对象中作为堆栈输出部分的一部分。指导开发人员使用跨堆栈引用来加载加密的S3对象并获取最新的AMI ARN。 B. 指导安全团队使用CloudFormation堆栈创建AWS CodePipeline管道，该管道构建新的AMI并将最新的AMI ARN放在加密的Amazon S3对象中作为管道输出的一部分。指导开发人员在自己的CloudFormation模板中使用跨堆栈引用来获取S3对象位置和最新的AMI ARN。 C. 指导安全团队使用Amazon EC2 Image Builder创建新的AMI，并将AMI ARN作为参数放在AWS Systems Manager Parameter Store中。指导开发人员在CloudFormation模板中指定参数名称。 D. 指导安全团队使用Amazon EC2 Image Builder创建新的AMI，并创建Amazon Simple Notification Service (Amazon SNS)主题，以便每个开发团队都能收到通知。当开发团队收到通知时，指导他们编写AWS Lambda函数来更新他们的CloudFormation堆栈，使用最新的AMI ARN。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个自动化、可扩展的解决方案，让安全团队能够自动向开发团队提供最新的批准AMI ID，替代目前通过邮件手动分发的方式。关键需求包括：自动化流程、可扩展性、与CloudFormation集成。 **涉及的关键AWS服务和概念：** - Amazon EC2 Image Builder：用于自动化AMI构建和管理的服务 - AWS Systems Manager Parameter Store：用于存储配置数据和密钥的安全存储服务 - AWS CloudFormation：基础设施即代码服务，支持动态参数引用 - Amazon S3：对象存储服务 - AWS CodePipeline：持续集成/持续部署服务 - Amazon SNS：消息通知服务 **正确答案C的原因：** 1. **自动化程度高**：EC2 Image Builder提供完全自动化的AMI构建流程 2. **集成性好**：Parameter Store与CloudFormation原生集成，开发人员可以直接在模板中引用参数 3. **可扩展性强**：Parameter Store支持大量参数存储，无需管理额外基础设施 4. **实时更新**：当Parameter Store中的值更新时，CloudFormation可以自动获取最新值 5. **安全性**：Parameter Store支持加密存储，符合安全要求 6. **简单易用**：开发团队只需在CloudFormation模板中指定参数名称即可 **其他选项错误的原因：** - **选项A**：使用S3存储AMI信息增加了复杂性，需要额外的权限管理和跨堆栈引用配置，不如Parameter Store直接 - **选项B**：虽然使用了CodePipeline自动化构建，但仍然依赖S3存储和复杂的跨堆栈引用，增加了不必要的复杂性 - **选项D**：虽然使用了EC2 Image Builder，但仍然需要手动干预（Lambda函数更新），没有实现完全自动化，且需要开发团队编写额外代码 **决策标准和最佳实践：** 1. **选择专用服务**：Parameter Store专门用于配置管理，比通用存储服务更适合 2. **最小化复杂性**：避免不必要的跨服务集成和手动干预 3. **原生集成优先**：利用AWS服务间的原生集成能力 4. **自动化优先**：选择能够实现端到端自动化的解决方案 5. **可维护性**：选择易于维护和扩展的架构</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">85</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). A DevOps engineer is using AWS CodeDeploy to release a new version. The deployment fails during the AllowTraffic lifecycle event, but a cause for the failure is not indicated in the deployment logs. What would cause this? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. The appspec.yml file contains an invalid script that runs in the AllowTraffic lifecycle hook.
B. The user who initiated the deployment does not have the necessary permissions to interact with the ALB.
C. The health checks specified for the ALB target group are misconfigured.
D. The CodeDeploy agent was not installed in the EC2 instances that are part of the ALB target group.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个应用程序运行在Application Load Balancer (ALB)后面的Amazon EC2实例上。DevOps工程师正在使用AWS CodeDeploy发布新版本。部署在AllowTraffic生命周期事件期间失败，但部署日志中没有显示失败的原因。什么会导致这种情况？ 选项： A. appspec.yml文件包含在AllowTraffic生命周期钩子中运行的无效脚本 B. 发起部署的用户没有与ALB交互的必要权限 C. 为ALB目标组指定的健康检查配置错误 D. CodeDeploy代理未安装在作为ALB目标组一部分的EC2实例中</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodeDeploy在Blue/Green部署模式下与Application Load Balancer集成时，AllowTraffic生命周期事件失败但日志中没有明确错误信息的故障排除能力。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：自动化部署服务，支持Blue/Green部署 - Application Load Balancer (ALB)：七层负载均衡器 - AllowTraffic生命周期事件：CodeDeploy在Blue/Green部署中将流量从旧实例切换到新实例的阶段 - 健康检查机制：ALB用来确定目标实例是否健康的检测方式 **正确答案C的原因：** 在Blue/Green部署的AllowTraffic阶段，CodeDeploy会将新部署的实例注册到ALB目标组中。如果ALB的健康检查配置错误（如检查路径不存在、超时时间过短、健康阈值设置不当等），新实例无法通过健康检查，ALB不会将流量路由到这些实例。这种情况下，CodeDeploy会检测到流量切换失败，但由于问题出在ALB层面的健康检查，CodeDeploy的部署日志中不会显示具体的失败原因。 **其他选项错误的原因：** A. 如果appspec.yml中的脚本有问题，CodeDeploy会在日志中明确记录脚本执行错误，与题目描述的&quot;日志中没有失败原因&quot;不符。 B. 权限问题会在部署开始阶段就被发现，CodeDeploy会在日志中明确记录权限相关的错误信息，不符合题目场景。 D. 如果CodeDeploy代理未安装，部署会在更早的阶段失败（如ApplicationStop或BeforeInstall），而不是在AllowTraffic阶段，且会有明确的代理连接失败日志。 **决策标准和最佳实践：** 1. 理解CodeDeploy生命周期事件的执行顺序和各阶段的作用 2. 掌握Blue/Green部署中ALB健康检查的重要性 3. 学会根据故障现象（特定阶段失败+日志无明确信息）推断根本原因 4. 配置ALB健康检查时要确保检查路径、端口、协议与应用实际情况匹配 5. 设置合理的健康检查间隔、超时和阈值参数</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">86</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has 20 service teams. Each service team is responsible for its own microservice. Each service team uses a separate AWS account for its microservice and a VPC with the 192.168.0.0/22 CIDR block. The company manages the AWS accounts with AWS Organizations. Each service team hosts its microservice on multiple Amazon EC2 instances behind an Application Load Balancer. The microservices communicate with each other across the public internet. The company&#x27;s security team has issued a new guideline that all communication between microservices must use HTTPS over private network connections and cannot traverse the public internet. A DevOps engineer must implement a solution that fulfills these obligations and minimizes the number of changes for each service team. Which solution will meet these requirements? for communication between microservices. Most Voted B (77%) D (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new AWS account in AWS Organizations. Create a VPC in this account, and use AWS Resource Access Manager to share the private subnets of this VPC with the organization. Instruct the service teams to launch a new Network Load Balancer (NLB) and EC2 instances that use the shared private subnets. Use the NLB DNS names for communication between microservices.
B. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use AWS PrivateLink to create VPC endpoints in each AWS account for the NLBs. Create subscriptions to each VPC endpoint in each of the other AWS accounts. Use the VPC endpoint DNS names for communication between microservices.
C. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Create VPC peering connections between each of the microservice VPCs. Update the route tables for each VPC to use the peering links. Use the NLB DNS names for communication between microservices.
D. Create a new AWS account in AWS Organizations. Create a transit gateway in this account, and use AWS Resource Access Manager to share the transit gateway with the organization. In each of the microservice VPCs, create a transit gateway attachment to the shared transit gateway. Update the route tables of each VPC to use the transit gateway. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use the NLB DNS names for communication between microservices.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有20个服务团队。每个服务团队负责自己的微服务。每个服务团队为其微服务使用单独的AWS账户和一个CIDR块为192.168.0.0/22的VPC。公司使用AWS Organizations管理AWS账户。每个服务团队在Application Load Balancer后面的多个Amazon EC2实例上托管其微服务。微服务之间通过公共互联网相互通信。公司的安全团队发布了新的指导原则，要求所有微服务之间的通信必须在私有网络连接上使用HTTPS，不能通过公共互联网传输。DevOps工程师必须实施一个满足这些要求并最小化每个服务团队更改数量的解决方案。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 20个服务团队，每个团队有独立的AWS账户和VPC 2. 微服务间通信必须使用HTTPS通过私有网络连接 3. 不能通过公共互联网通信 4. 最小化每个服务团队的更改数量 **涉及的关键AWS服务和概念：** - AWS PrivateLink：提供跨VPC的私有连接服务 - Network Load Balancer (NLB)：支持PrivateLink的负载均衡器 - VPC Endpoint：PrivateLink的接入点 - Transit Gateway：用于多VPC互连的网络中转服务 - VPC Peering：点对点VPC连接 - AWS Resource Access Manager：跨账户资源共享 **正确答案B的原因：** 1. **私有通信**：AWS PrivateLink确保所有通信都在AWS骨干网络内进行，不经过公共互联网 2. **最小化更改**：每个团队只需在现有VPC中创建NLB和VPC endpoint，无需重新架构 3. **跨账户支持**：PrivateLink天然支持跨AWS账户的私有连接 4. **安全性**：VPC endpoint提供了安全的服务访问方式，支持HTTPS 5. **可扩展性**：每个服务可以选择性地订阅需要的VPC endpoint **其他选项错误的原因：** - **选项A**：要求服务团队迁移到共享子网，这是重大架构变更，不符合&quot;最小化更改&quot;要求 - **选项C**：VPC Peering在20个VPC之间需要创建190个连接（n*(n-1)/2），管理复杂度极高，且所有VPC使用相同CIDR块会导致路由冲突 - **选项D**：虽然Transit Gateway可以解决连接问题，但需要额外的AWS账户和复杂的路由配置，增加了管理开销 **决策标准和最佳实践：** 1. **架构简洁性**：PrivateLink提供了最简洁的跨账户私有通信方案 2. **运维复杂度**：避免复杂的网络拓扑和路由管理 3. **安全最佳实践**：使用AWS托管的私有连接服务 4. **成本效益**：PrivateLink按使用量计费，相比维护复杂网络架构更经济 5. **渐进式迁移**：允许服务团队逐步迁移，不需要同时改变所有服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">87</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An Amazon EC2 instance is running in a VPC and needs to download an object from a restricted Amazon S3 bucket. When the DevOps engineer tries to download the object, an AccessDenied error is received. What are the possible causes for this error? (Choose two.) BD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. The S3 bucket default encryption is enabled.
B. There is an error in the S3 bucket policy.
C. The object has been moved to S3 Glacier.
D. There is an error in the IAM role configuration.
E. S3 Versioning is enabled.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个Amazon EC2实例运行在VPC中，需要从一个受限的Amazon S3存储桶下载对象。当DevOps工程师尝试下载对象时，收到了AccessDenied错误。这个错误可能的原因是什么？（选择两个）BD（100%） 选项：A. S3存储桶默认加密已启用。B. S3存储桶策略中有错误。C. 对象已被移动到S3 Glacier。D. IAM角色配置中有错误。E. S3版本控制已启用。 正确答案：B</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS中S3访问权限控制的故障排除，要求识别导致AccessDenied错误的可能原因。题目明确要求选择两个答案，但给出的正确答案只有B，这可能是题目标注有误。 **涉及的关键AWS服务和概念：** - Amazon S3访问控制机制 - IAM角色和权限策略 - S3存储桶策略 - VPC中EC2实例的权限配置 - S3对象访问权限验证流程 **正确答案的原因：** 选项B（S3存储桶策略中有错误）是正确的，因为： - S3存储桶策略是控制对存储桶和对象访问的主要机制之一 - 如果存储桶策略配置错误，比如明确拒绝某些操作或没有授予必要的权限，就会导致AccessDenied错误 - 对于&quot;受限的S3存储桶&quot;，存储桶策略配置错误是最常见的访问被拒绝原因 选项D（IAM角色配置中有错误）也应该是正确答案，因为： - EC2实例通过IAM角色获取访问S3的权限 - 如果IAM角色没有被正确配置或缺少必要的S3权限，也会导致AccessDenied错误 **其他选项错误的原因：** - 选项A：S3默认加密不会导致AccessDenied错误，它只是在存储时加密数据，不影响访问权限 - 选项C：对象移动到S3 Glacier不会产生AccessDenied错误，而是会产生不同类型的错误或需要先恢复对象 - 选项E：S3版本控制启用本身不会导致访问被拒绝，它只是管理对象的多个版本 **决策标准和最佳实践：** - S3访问控制遵循最小权限原则，需要同时检查IAM权限和存储桶策略 - 排查AccessDenied错误时，应按顺序检查：IAM用户/角色权限 → 存储桶策略 → 对象ACL - 使用AWS CloudTrail可以帮助诊断权限问题的具体原因 - 建议使用IAM策略模拟器来测试权限配置的正确性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">88</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to use a grid system for a proprietary enterprise in-memory data store on top of AWS. This system can run in multiple server nodes in any Linux-based distribution. The system must be able to reconfigure the entire cluster every time a node is added or removed. When adding or removing nodes, an /etc/cluster/nodes.config file must be updated, listing the IP addresses of the current node members of that cluster. The company wants to automate the task of adding new nodes to a cluster. What can a DevOps engineer do to meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS OpsWorks Stacks to layer the server nodes of that cluster. Create a Chef recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layer. Assign that recipe to the Configure lifecycle event.
B. Put the nodes.config file in version control. Create an AWS CodeDeploy deployment configuration and deployment group based on an Amazon EC2 tag value for the cluster nodes. When adding a new node to the cluster, update the file with all tagged instances, and make a commit in version control. Deploy the new file and restart the services.
C. Create an Amazon S3 bucket and upload a version of the /etc/cluster/nodes.config file. Create a crontab script that will poll for that S3 file and download it frequently. Use a process manager, such as Monit or systemd, to restart the cluster services when it detects that the new file was modified. When adding a node to the cluster, edit the file&#x27;s most recent members. Upload the new file to the S3 bucket.
D. Create a user data script that lists all members of the current security group of the cluster and automatically updates the /etc/cluster/nodes.config file whenever a new instance is added to the cluster.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司希望在AWS上为专有企业内存数据存储使用网格系统。该系统可以在任何基于Linux发行版的多个服务器节点上运行。系统必须能够在每次添加或删除节点时重新配置整个集群。在添加或删除节点时，必须更新/etc/cluster/nodes.config文件，列出该集群当前节点成员的IP地址。公司希望自动化向集群添加新节点的任务。DevOps工程师可以做什么来满足这些要求？ 选项： A. 使用AWS OpsWorks Stacks对该集群的服务器节点进行分层。创建一个Chef recipe来填充/etc/cluster/nodes.config文件的内容，并使用该层的当前成员重启服务。将该recipe分配给Configure生命周期事件。 B. 将nodes.config文件放入版本控制。基于集群节点的Amazon EC2标签值创建AWS CodeDeploy部署配置和部署组。向集群添加新节点时，用所有标记的实例更新文件，并在版本控制中提交。部署新文件并重启服务。 C. 创建Amazon S3存储桶并上传/etc/cluster/nodes.config文件的版本。创建crontab脚本定期轮询该S3文件并下载。使用进程管理器（如Monit或systemd）在检测到新文件被修改时重启集群服务。向集群添加节点时，编辑文件的最新成员并上传新文件到S3存储桶。 D. 创建用户数据脚本，列出集群当前安全组的所有成员，并在新实例添加到集群时自动更新/etc/cluster/nodes.config文件。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动化解决方案，能够在集群节点动态增减时自动更新配置文件并重新配置整个集群。关键需求包括：1）自动检测节点变化；2）更新/etc/cluster/nodes.config文件；3）重启相关服务；4）实现完全自动化。 **涉及的关键AWS服务和概念：** - AWS OpsWorks Stacks：基础设施管理服务，支持Chef配置管理 - Chef recipes：自动化配置脚本 - 生命周期事件：OpsWorks中的自动触发机制 - AWS CodeDeploy：应用程序部署服务 - Amazon S3：对象存储服务 - EC2用户数据脚本：实例启动时执行的脚本 **正确答案A的原因：** AWS OpsWorks Stacks是最适合这种场景的服务，因为：1）它专门设计用于管理应用程序栈和层；2）Configure生命周期事件会在节点添加/删除时自动触发；3）Chef recipe可以动态获取当前层的所有成员信息；4）能够自动执行配置更新和服务重启；5）提供了完整的自动化解决方案，无需人工干预。 **其他选项错误的原因：** 选项B（CodeDeploy）：需要手动更新版本控制中的配置文件，不能实现完全自动化，每次添加节点都需要人工操作。选项C（S3+cron）：依赖定时轮询机制，存在延迟问题，且需要手动编辑和上传文件，自动化程度低。选项D（用户数据脚本）：只在实例启动时执行一次，无法处理节点删除的情况，且不能实现集群范围的配置同步。 **决策标准和最佳实践：** 选择自动化解决方案时应考虑：1）事件驱动vs定时轮询（事件驱动响应更及时）；2）自动化程度（减少人工干预）；3）可靠性和一致性；4）AWS原生服务集成度。OpsWorks Stacks作为专门的配置管理服务，在处理动态集群管理方面具有天然优势，是此类场景的最佳实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">89</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is working on a data archival project that requires the migration of on-premises data to an Amazon S3 bucket. The DevOps engineer develops a script that incrementally archives on-premises data that is older than 1 month to Amazon S3. Data that is transferred to Amazon S3 is deleted from the on-premises location. The script uses the S3 PutObject operation. During a code review, the DevOps engineer notices that the script does not verify whether the data was successfully copied to Amazon S3. The DevOps engineer must update the script to ensure that data is not corrupted during transmission. The script must use MD5 checksums to verify data integrity before the on-premises data is deleted. Which solutions for the script will meet these requirements? (Choose two.) BD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Check the returned response for the VersionId. Compare the returned VersionId against the MD5 checksum.
B. Include the MD5 checksum within the Content-MD5 parameter. Check the operation call&#x27;s return status to find out if an error was returned.
C. Include the checksum digest within the tagging parameter as a URL query parameter.
D. Check the returned response for the ETag. Compare the returned ETag against the MD5 checksum.
E. Include the checksum digest within the Metadata parameter as a name-value pair. After upload, use the S3 HeadObject operation to retrieve metadata from the object.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师正在进行数据归档项目，需要将本地数据迁移到Amazon S3存储桶。该DevOps工程师开发了一个脚本，将超过1个月的本地数据增量归档到Amazon S3。传输到Amazon S3的数据会从本地位置删除。该脚本使用S3 PutObject操作。在代码审查期间，DevOps工程师注意到脚本没有验证数据是否成功复制到Amazon S3。DevOps工程师必须更新脚本以确保数据在传输过程中不会损坏。脚本必须使用MD5校验和来验证数据完整性，然后才能删除本地数据。脚本的哪些解决方案能满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现数据传输完整性验证机制，确保在删除本地数据之前验证文件已成功且完整地上传到S3。核心需求是使用MD5校验和进行数据完整性验证。 **涉及的关键AWS服务和概念：** - Amazon S3 PutObject API操作 - Content-MD5参数：用于请求级别的完整性检查 - ETag：S3返回的实体标签，通常是文件的MD5哈希值 - S3对象元数据、标签和版本控制概念 **正确答案的原因：** 选项B正确：Content-MD5参数是S3 PutObject操作的标准完整性验证机制。当在请求中包含Content-MD5参数时，S3会自动计算上传数据的MD5校验和并与提供的值进行比较。如果不匹配，S3会返回错误，确保数据完整性。 选项D正确：ETag通常包含对象的MD5哈希值（对于简单上传）。通过比较返回的ETag与本地计算的MD5校验和，可以验证数据完整性。 **其他选项错误的原因：** 选项A错误：VersionId是版本控制功能的标识符，与MD5校验和无关，不能用于数据完整性验证。 选项C错误：标签参数用于对象分类和管理，不是完整性验证的标准方法，且作为URL查询参数传递校验和不是最佳实践。 选项E错误：虽然可以在元数据中存储校验和，但这种方法需要额外的HeadObject调用，增加了复杂性和API调用成本，不如直接使用Content-MD5或ETag验证高效。 **决策标准和最佳实践：** 1. 优先使用S3原生的完整性验证机制（Content-MD5） 2. 利用S3返回的标准响应字段（ETag）进行验证 3. 避免不必要的额外API调用 4. 选择简单、可靠且成本效益高的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">90</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company deploys updates to its Amazon API Gateway API several times a week by using an AWS CodePipeline pipeline. As part of the update process, the company exports the JavaScript SDK for the API from the API Gateway console and uploads the SDK to an Amazon S3 bucket. The company has configured an Amazon CloudFront distribution that uses the S3 bucket as an origin. Web clients then download the SDK by using the CloudFront distribution&#x27;s endpoint. A DevOps engineer needs to implement a solution to make the new SDK available automatically during new API deployments. Which solution will meet these requirements? Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the CloudFront API to create an invalidation for the SDK path. Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the S3 API to invalidate the cache for the SDK path. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CodePipeline action immediately after the deployment stage of the API. Configure the action to invoke an AWS Lambda function. Configure the Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and create a CloudFront invalidation for the SDK path.
B. Create a CodePipeline action immediately after the deployment stage of the API. Configure the action to use the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Create another action that uses the CodePipeline integration with Amazon S3 to invalidate the cache for the SDK path.
C. Create an Amazon EventBridge rule that reacts to UpdateStage events from aws.apigateway. Configure the rule to invoke an AWS
D. Create an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure the rule to invoke an AWS</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司每周多次使用AWS CodePipeline管道向其Amazon API Gateway API部署更新。作为更新过程的一部分，公司从API Gateway控制台导出API的JavaScript SDK，并将SDK上传到Amazon S3存储桶。公司配置了一个使用S3存储桶作为源的Amazon CloudFront分发。然后Web客户端通过CloudFront分发的端点下载SDK。DevOps工程师需要实施一个解决方案，在新的API部署期间自动使新的SDK可用。哪个解决方案能满足这些要求？ 选项A：在API部署阶段之后立即创建一个CodePipeline操作。配置该操作调用AWS Lambda函数。配置Lambda函数从API Gateway下载SDK，将SDK上传到S3存储桶，并为SDK路径创建CloudFront失效。 选项B：在API部署阶段之后立即创建一个CodePipeline操作。配置该操作使用CodePipeline与API Gateway的集成将SDK导出到Amazon S3。创建另一个使用CodePipeline与Amazon S3集成的操作来使SDK路径的缓存失效。 选项C：创建一个Amazon EventBridge规则，响应来自aws.apigateway的UpdateStage事件。配置规则调用AWS... 选项D：创建一个Amazon EventBridge规则，响应来自aws.apigateway的CreateDeployment事件。配置规则调用AWS...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现API部署时自动更新SDK的完整流程，包括：1）从API Gateway导出最新SDK；2）上传到S3存储桶；3）使CloudFront缓存失效以确保客户端获取最新版本。关键是要在CodePipeline部署流程中无缝集成这个自动化过程。 **涉及的关键AWS服务和概念：** - CodePipeline：CI/CD管道服务，需要在部署阶段后添加自动化操作 - API Gateway：提供SDK导出功能，部署后需要获取最新SDK - S3：存储SDK文件的对象存储服务 - CloudFront：CDN服务，缓存S3中的SDK文件，需要缓存失效机制 - Lambda：执行自动化任务的无服务器计算服务 - EventBridge：事件驱动架构服务 **正确答案A的原因：** 选项A提供了最直接和可控的解决方案。通过在CodePipeline中添加Lambda操作，可以确保在API部署完成后立即执行SDK更新流程。Lambda函数可以程序化地调用API Gateway的SDK导出API，上传到S3，并调用CloudFront的InvalidateCache API。这种方法与现有的CodePipeline流程完美集成，提供了端到端的自动化和错误处理能力。 **其他选项错误的原因：** 选项B错误：CodePipeline没有直接的API Gateway SDK导出集成，也没有与S3的缓存失效集成功能。这个选项描述的集成实际上不存在。 选项C和D错误：虽然EventBridge可以响应API Gateway事件，但这种方法存在几个问题：1）与现有CodePipeline流程分离，难以统一管理和监控；2）事件驱动的方式可能存在时序问题；3）缺乏与部署流程的紧密集成，无法在部署失败时阻止SDK更新。 **决策标准和最佳实践：** 1. **流程集成性**：选择能与现有CodePipeline无缝集成的方案 2. **可控性**：确保SDK更新过程可控，能够处理异常情况 3. **时序保证**：SDK更新必须在API部署成功后进行 4. **完整性**：解决方案必须包含缓存失效步骤，确保客户端获取最新版本 5. **可维护性**：选择易于监控、调试和维护的架构模式</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">91</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has developed an AWS Lambda function that handles orders received through an API. The company is using AWS CodeDeploy to deploy the Lambda function as the final stage of a CI/CD pipeline. A DevOps engineer has noticed there are intermittent failures of the ordering API for a few seconds after deployment. After some investigation, the DevOps engineer believes the failures are due to database changes not having fully propagated before the Lambda function is invoked. How should the DevOps engineer overcome this? new version of the Lambda function. Most Voted version of the Lambda function to respond. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a BeforeAllowTraffic hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the
B. Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new
C. Add a BeforeInstall hook to the AppSpec file that tests and waits for any necessary database changes before deploying the new version of the Lambda function.
D. Add a ValidateService hook to the AppSpec file that inspects incoming traffic and rejects the payload if dependent services, such as the database, are not yet ready.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司开发了一个AWS Lambda函数来处理通过API接收的订单。该公司使用AWS CodeDeploy在CI/CD流水线的最后阶段部署Lambda函数。一名DevOps工程师注意到在部署后的几秒钟内，订单API会出现间歇性故障。经过一些调查，DevOps工程师认为故障是由于数据库更改在Lambda函数被调用之前没有完全传播造成的。DevOps工程师应该如何解决这个问题？ 选项： A. 在AppSpec文件中添加BeforeAllowTraffic钩子，在流量流向新版本Lambda函数之前测试并等待任何必要的数据库更改 B. 在AppSpec文件中添加AfterAllowTraffic钩子，在允许新版本Lambda函数响应之前强制流量等待任何待处理的数据库更改 C. 在AppSpec文件中添加BeforeInstall钩子，在部署新版本Lambda函数之前测试并等待任何必要的数据库更改 D. 在AppSpec文件中添加ValidateService钩子，检查传入流量并在依赖服务（如数据库）尚未准备就绪时拒绝负载</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何解决Lambda函数部署后由于数据库更改未完全传播而导致的间歇性API故障问题。需要在CodeDeploy部署过程中添加适当的钩子来确保数据库状态与新版本Lambda函数同步。 **涉及的关键AWS服务和概念：** - AWS Lambda：无服务器计算服务 - AWS CodeDeploy：自动化部署服务 - AppSpec文件：CodeDeploy的部署配置文件 - Lambda部署钩子：BeforeAllowTraffic、AfterAllowTraffic、BeforeInstall、ValidateService等生命周期钩子 **正确答案A的原因：** BeforeAllowTraffic钩子在Lambda函数部署完成但流量切换到新版本之前执行。这个时机最适合验证数据库更改是否已完全传播，确保在用户流量到达新版本函数之前，所有依赖的数据库更改都已就绪。这样可以避免新版本函数处理请求时遇到数据不一致的问题。 **其他选项错误的原因：** - 选项B（AfterAllowTraffic）：此钩子在流量已经开始流向新版本后执行，为时已晚，无法防止故障发生 - 选项C（BeforeInstall）：此钩子在函数安装之前执行，时机过早，此时新版本函数还未部署，无法确保部署时的数据库状态 - 选项D（ValidateService）：这不是CodeDeploy Lambda部署的标准钩子，且描述的功能更像是运行时验证而非部署时验证 **决策标准和最佳实践：** 1. 选择合适的部署钩子时机：需要在新版本准备就绪但流量切换前进行验证 2. 确保数据一致性：在允许新版本处理生产流量前验证所有依赖服务状态 3. 遵循CodeDeploy Lambda部署生命周期：理解各个钩子的执行顺序和适用场景 4. 实施健康检查：在关键的部署节点添加验证逻辑，确保系统整体就绪</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">92</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses a single AWS account to test applications on Amazon EC2 instances. The company has turned on AWS Config in the AWS account and has activated the restricted-ssh AWS Config managed rule. The company needs an automated monitoring solution that will provide a customized notification in real time if any security group in the account is not compliant with the restricted-ssh rule. The customized notification must contain the name and ID of the noncompliant security group. A DevOps engineer creates an Amazon Simple Notification Service (Amazon SNS) topic in the account and subscribes the appropriate personnel to the topic. What should the DevOps engineer do next to meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure an input transformer for the EventBridge rule. Configure the EventBridge rule to publish a notification to the SNS topic.
B. Configure AWS Config to send all evaluation results for the restricted-ssh rule to the SNS topic. Configure a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers.
C. Create an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure the EventBridge rule to invoke AWS Systems Manager Run Command on the SNS topic to customize a notification and to publish the notification to the SNS topic.
D. Create an Amazon EventBridge rule that matches all AWS Config evaluation results of NON_COMPLIANT. Configure an input transformer for the restricted-ssh rule. Configure the EventBridge rule to publish a notification to the SNS topic.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用单个AWS账户在Amazon EC2实例上测试应用程序。该公司已在AWS账户中启用了AWS Config，并激活了restricted-ssh AWS Config托管规则。公司需要一个自动化监控解决方案，当账户中任何安全组不符合restricted-ssh规则时，能够实时提供定制化通知。定制化通知必须包含不合规安全组的名称和ID。DevOps工程师在账户中创建了Amazon Simple Notification Service (Amazon SNS)主题，并为相关人员订阅了该主题。DevOps工程师接下来应该做什么来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 需要实时监控AWS Config的restricted-ssh规则合规性 - 当出现NON_COMPLIANT状态时自动发送通知 - 通知必须是定制化的，包含不合规安全组的具体名称和ID - 通过已创建的SNS主题发送通知 **涉及的关键AWS服务和概念：** - AWS Config：配置合规性监控服务，restricted-ssh是预定义的托管规则 - Amazon EventBridge：事件驱动架构的核心服务，可以捕获AWS Config的评估结果 - Input Transformer：EventBridge的功能，可以自定义事件数据格式 - Amazon SNS：消息通知服务 - AWS Systems Manager Run Command：系统管理工具 **正确答案A的原因：** - EventBridge可以精确匹配AWS Config针对restricted-ssh规则的NON_COMPLIANT评估结果 - Input Transformer功能可以从原始事件中提取安全组名称和ID，创建定制化通知内容 - 直接发布到SNS主题，架构简洁高效 - 满足实时性要求，因为EventBridge可以立即响应Config评估事件 **其他选项错误的原因：** - 选项B：AWS Config本身不能直接发送评估结果到SNS，且SNS过滤策略无法提供所需的定制化内容提取 - 选项C：使用Systems Manager Run Command过于复杂且不必要，Run Command主要用于在EC2实例上执行命令，不适用于消息定制化场景 - 选项D：匹配所有NON_COMPLIANT结果会产生不相关的通知，且Input Transformer配置描述不准确 **决策标准和最佳实践：** - 选择最直接、最简单的解决方案路径 - 利用EventBridge的事件过滤和转换能力 - 避免过度工程化，不使用不必要的服务组件 - 确保解决方案的实时性和准确性 - 遵循AWS服务间的最佳集成模式</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">93</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company requires an RPO of 2 hours and an RTO of 10 minutes for its data and application at all times. An application uses a MySQL database and Amazon EC2 web servers. The development team needs a strategy for failover and disaster recovery. Which combination of deployment strategies will meet these requirements? (Choose two.) BD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon Aurora cluster in one Availability Zone across multiple Regions as the data store. Use Aurora&#x27;s automatic recovery capabilities in the event of a disaster.
B. Create an Amazon Aurora global database in two Regions as the data store. In the event of a failure, promote the secondary Region as the primary for the application.
C. Create an Amazon Aurora multi-master cluster across multiple Regions as the data store. Use a Network Load Balancer to balance the database traffic in different Regions.
D. Set up the application in two Regions and use Amazon Route 53 failover-based routing that points to the Application Load Balancers in both Regions. Use health checks to determine the availability in a given Region. Use Auto Scaling groups in each Region to adjust capacity based on demand.
E. Set up the application in two Regions and use a multi-Region Auto Scaling group behind Application Load Balancers to manage the capacity based on demand. In the event of a disaster, adjust the Auto Scaling group&#x27;s desired instance count to increase baseline capacity in the failover Region.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司要求其数据和应用程序在任何时候都具有2小时的RPO和10分钟的RTO。应用程序使用MySQL数据库和Amazon EC2 web服务器。开发团队需要一个故障转移和灾难恢复策略。哪种部署策略组合将满足这些要求？（选择两个。）BD（100%） 选项：A. 在一个Availability Zone中跨多个Region创建Amazon Aurora集群作为数据存储。在发生灾难时使用Aurora的自动恢复功能。 B. 在两个Region中创建Amazon Aurora global database作为数据存储。在发生故障时，将辅助Region提升为应用程序的主Region。 C. 跨多个Region创建Amazon Aurora multi-master集群作为数据存储。使用Network Load Balancer来平衡不同Region中的数据库流量。 D. 在两个Region中设置应用程序，使用Amazon Route 53基于故障转移的路由指向两个Region中的Application Load Balancer。使用健康检查来确定给定Region的可用性。在每个Region中使用Auto Scaling groups根据需求调整容量。 E. 在两个Region中设置应用程序，在Application Load Balancer后面使用multi-Region Auto Scaling group根据需求管理容量。在发生灾难时，调整Auto Scaling group的期望实例数量以增加故障转移Region中的基线容量。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是满足严格RTO（恢复时间目标：10分钟）和RPO（恢复点目标：2小时）要求的灾难恢复架构设计。需要为MySQL数据库和EC2 web服务器设计跨Region的高可用性解决方案。 **涉及的关键AWS服务和概念：** - Amazon Aurora Global Database：跨Region数据库复制服务 - Amazon Aurora Multi-Master：多主节点数据库集群 - Route 53：DNS故障转移路由 - Application Load Balancer：应用程序负载均衡器 - Auto Scaling Groups：自动扩展组 - RTO/RPO概念：恢复时间和恢复点目标 **正确答案的原因：** 选项B和D的组合是正确的： - **选项B**：Aurora Global Database提供跨Region的异步复制，RPO通常在1秒以内（远优于2小时要求），故障转移时间约1分钟（满足10分钟RTO要求） - **选项D**：Route 53健康检查和故障转移路由可以在几分钟内将流量重定向到健康的Region，配合Auto Scaling确保应用层的高可用性 **其他选项错误的原因：** - **选项A**：描述有误，不能在&quot;一个AZ跨多个Region&quot;创建集群，概念上不合理 - **选项C**：Aurora Multi-Master目前不支持跨Region部署，只能在单Region内的多AZ中使用 - **选项E**：Multi-Region Auto Scaling Group不是AWS的标准服务，Auto Scaling Group是Region级别的服务 **决策标准和最佳实践：** 1. **数据层**：选择Aurora Global Database确保跨Region数据复制和快速故障转移 2. **应用层**：使用Route 53 + ALB + Auto Scaling实现应用程序的跨Region部署和自动故障转移 3. **监控**：利用Route 53健康检查主动监测Region可用性 4. **自动化**：通过Auto Scaling确保故障转移后有足够的计算资源 5. **成本优化**：在正常情况下可以在备用Region运行最小容量，故障时自动扩展</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">94</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A business has an application that consists of five independent AWS Lambda functions. The DevOps engineer has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each Lambda function in sequence. The pipeline uses an Amazon EventBridge rule to ensure the pipeline starts as quickly as possible after a change is made to the application source code. After working with the pipeline for a few months, the DevOps engineer has noticed the pipeline takes too long to complete. What should the DevOps engineer implement to BEST improve the speed of the pipeline? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the CodeBuild projects within the pipeline to use a compute type with more available network throughput.
B. Create a custom CodeBuild execution environment that includes a symmetric multiprocessing configuration to run the builds in parallel.
C. Modify the CodePipeline configuration to run actions for each Lambda function in parallel by specifying the same runOrder.
D. Modify each CodeBuild project to run within a VPC and use dedicated instances to increase throughput.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家企业有一个由五个独立的AWS Lambda函数组成的应用程序。DevOps工程师使用AWS CodePipeline和AWS CodeBuild构建了一个CI/CD管道，该管道按顺序构建、测试、打包和部署每个Lambda函数。该管道使用Amazon EventBridge规则来确保在应用程序源代码发生更改后尽快启动管道。在使用该管道几个月后，DevOps工程师注意到管道完成时间过长。DevOps工程师应该实施什么来最好地提高管道的速度？ 选项： A. 修改管道中的CodeBuild项目，使用具有更多可用网络吞吐量的计算类型。 B. 创建一个自定义的CodeBuild执行环境，包含对称多处理配置以并行运行构建。 C. 修改CodePipeline配置，通过指定相同的runOrder来并行运行每个Lambda函数的操作。 D. 修改每个CodeBuild项目在VPC内运行并使用专用实例来增加吞吐量。 正确答案：C</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何优化CI/CD管道的执行速度。当前问题是五个独立的Lambda函数按顺序（串行）进行构建、测试、打包和部署，导致整个管道执行时间过长。需要找到最有效的方法来提高管道速度。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务，支持阶段和操作的编排 - AWS CodeBuild：构建服务，负责代码编译、测试和打包 - AWS Lambda：无服务器计算服务 - runOrder：CodePipeline中控制操作执行顺序的参数 - 并行处理 vs 串行处理的概念 **正确答案C的原因：** 1. **架构层面优化**：五个Lambda函数是独立的，没有相互依赖关系，完全可以并行处理 2. **runOrder机制**：在CodePipeline中，相同runOrder值的操作会并行执行，不同runOrder值的操作按顺序执行 3. **最大化效率**：将串行执行改为并行执行可以显著减少总执行时间（理论上可减少到原来的1/5） 4. **成本效益**：这是一个配置层面的改动，不需要额外的计算资源投入 **其他选项错误的原因：** - **选项A**：增加网络吞吐量只能在一定程度上提升单个构建的速度，但无法解决串行执行的根本问题 - **选项B**：对称多处理主要优化单个构建任务内部的并行处理，对于多个独立Lambda函数的场景效果有限 - **选项D**：VPC配置和专用实例会增加复杂性和成本，且可能因为网络延迟反而降低性能 **决策标准和最佳实践：** 1. **识别瓶颈**：首先分析是架构问题还是资源问题 2. **利用并行性**：对于独立的组件，优先考虑并行处理 3. **成本效益分析**：优先选择配置优化而非硬件升级 4. **AWS服务特性**：充分利用CodePipeline的runOrder功能来实现并行执行 5. **可扩展性**：并行架构为未来添加更多Lambda函数提供了良好的扩展性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">95</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS CloudFormation stacks to deploy updates to its application. The stacks consist of different resources. The resources include AWS Auto Scaling groups, Amazon EC2 instances, Application Load Balancers (ALBs), and other resources that are necessary to launch and maintain independent stacks. Changes to application resources outside of CloudFormation stack updates are not allowed. The company recently attempted to update the application stack by using the AWS CLI. The stack failed to update and produced the following error message: &quot;ERROR: both the deployment and the CloudFormation stack rollback failed. The deployment failed because the following resource(s) failed to update: [AutoScalingGroup].&quot; The stack remains in a status of UPDATE_ROLLBACK_FAILED. Which solution will resolve this issue? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the subnet mappings that are configured for the ALBs. Run the aws cloudformation update-stack-set AWS CLI command.
B. Update the IAM role by providing the necessary permissions to update the stack. Run the aws cloudformation continue-update-rollback AWS CLI command.
C. Submit a request for a quota increase for the number of EC2 instances for the account. Run the aws cloudformation cancel-update-stack AWS CLI command.
D. Delete the Auto Scaling group resource. Run the aws cloudformation rollback-stack AWS CLI command.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS CloudFormation堆栈来部署应用程序更新。这些堆栈由不同的资源组成，包括AWS Auto Scaling组、Amazon EC2实例、Application Load Balancers (ALBs)以及启动和维护独立堆栈所需的其他资源。不允许在CloudFormation堆栈更新之外对应用程序资源进行更改。该公司最近尝试使用AWS CLI更新应用程序堆栈。堆栈更新失败并产生以下错误消息：&quot;ERROR: both the deployment and the CloudFormation stack rollback failed. The deployment failed because the following resource(s) failed to update: [AutoScalingGroup].&quot;堆栈保持在UPDATE_ROLLBACK_FAILED状态。哪个解决方案能解决这个问题？ 选项： A. 更新为ALBs配置的子网映射。运行aws cloudformation update-stack-set AWS CLI命令。 B. 通过提供更新堆栈的必要权限来更新IAM角色。运行aws cloudformation continue-update-rollback AWS CLI命令。 C. 提交增加账户EC2实例数量配额的请求。运行aws cloudformation cancel-update-stack AWS CLI命令。 D. 删除Auto Scaling组资源。运行aws cloudformation rollback-stack AWS CLI命令。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何处理CloudFormation堆栈处于UPDATE_ROLLBACK_FAILED状态的故障恢复场景。当堆栈更新失败且回滚也失败时，需要找到正确的解决方案来恢复堆栈到稳定状态。 **涉及的关键AWS服务和概念：** 1. AWS CloudFormation - 基础设施即代码服务 2. CloudFormation堆栈状态管理 - 特别是UPDATE_ROLLBACK_FAILED状态 3. IAM权限管理 - 服务角色权限 4. Auto Scaling Groups - 自动扩展组资源 5. CloudFormation故障恢复命令 **正确答案B的原因：** 1. UPDATE_ROLLBACK_FAILED状态表明堆栈更新和回滚都失败了，堆栈处于不稳定状态 2. 最常见的原因是IAM权限不足，CloudFormation服务角色缺少更新或回滚特定资源的权限 3. continue-update-rollback命令专门用于从UPDATE_ROLLBACK_FAILED状态恢复，继续完成回滚操作 4. 更新IAM角色权限后，可以重新尝试回滚操作，将堆栈恢复到更新前的稳定状态 **其他选项错误的原因：** - 选项A：update-stack-set用于堆栈集操作，不适用于单个堆栈的故障恢复；子网映射问题不是导致UPDATE_ROLLBACK_FAILED的典型原因 - 选项C：EC2配额问题可能导致更新失败，但cancel-update-stack命令不存在，且不能解决已经处于UPDATE_ROLLBACK_FAILED状态的问题 - 选项D：手动删除资源会导致堆栈状态不一致；rollback-stack命令不是标准的CloudFormation命令 **决策标准和最佳实践：** 1. 遇到UPDATE_ROLLBACK_FAILED状态时，首先检查IAM权限是否充足 2. 使用continue-update-rollback命令是处理此类故障的标准做法 3. 确保CloudFormation服务角色具有管理所有堆栈资源的完整权限 4. 避免手动修改CloudFormation管理的资源，保持基础设施即代码的完整性 5. 在更新堆栈前，应验证权限和资源配额以避免类似问题</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">96</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is deploying a new application that uses Amazon EC2 instances. The company needs a solution to query application logs and AWS account API activity. Which solution will meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Configure AWS CloudTrail to deliver the API logs to Amazon S3. Use CloudWatch to query both sets of logs.
B. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Configure AWS CloudTrail to deliver the API logs to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs.
C. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon Kinesis. Configure AWS CloudTrail to deliver the API logs to Kinesis. Use Kinesis to load the data into Amazon Redshift. Use Amazon Redshift to query both sets of logs.
D. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon S3. Use AWS CloudTrail to deliver the API logs to Amazon S3. Use Amazon Athena to query both sets of logs in Amazon S3.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在部署一个使用Amazon EC2实例的新应用程序。该公司需要一个解决方案来查询应用程序日志和AWS账户API活动。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个统一的解决方案来查询两种类型的日志：1）EC2实例上的应用程序日志；2）AWS账户的API活动日志。关键在于需要能够&quot;查询&quot;这些日志数据。 **涉及的关键AWS服务和概念：** - Amazon CloudWatch Logs：用于收集、监控和存储日志文件的服务 - CloudWatch Logs Insights：CloudWatch Logs的查询功能，支持SQL类似的查询语法 - AWS CloudTrail：记录AWS账户API调用活动的服务 - Amazon CloudWatch agent：在EC2实例上收集日志和指标的代理程序 **正确答案B的原因：** 选项B提供了最优的架构设计： 1. 使用CloudWatch agent将EC2应用日志发送到CloudWatch Logs 2. 配置CloudTrail将API日志也发送到CloudWatch Logs 3. 使用CloudWatch Logs Insights统一查询两种日志 这种方案将所有日志集中在同一个服务中，便于统一管理和查询，且CloudWatch Logs Insights提供强大的查询能力。 **其他选项错误的原因：** - 选项A：CloudTrail日志发送到S3，而应用日志在CloudWatch Logs，数据分散在不同位置，无法统一查询 - 选项C：使用Kinesis和Redshift过于复杂，增加了不必要的成本和复杂性，不符合简单查询需求 - 选项D：虽然数据都在S3中，但CloudWatch agent默认不直接发送日志到S3，需要额外配置，且Athena查询成本较高 **决策标准和最佳实践：** 1. 数据集中化：将相关日志集中存储便于统一管理 2. 查询便利性：选择提供强大查询功能的服务 3. 成本效益：避免过度工程化，选择最简单有效的方案 4. 服务集成：优先使用AWS原生服务的集成功能</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">97</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to ensure that their EC2 instances are secure. They want to be notified if any new vulnerabilities are discovered on their instances, and they also want an audit trail of all login activities on the instances. Which solution will meet these requirements? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Amazon Kinesis Agent to capture system logs and deliver them to Amazon S3.
B. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture system logs and view login activity in the CloudTrail console.
C. Configure Amazon CloudWatch to detect vulnerabilities on the EC2 instances. Install the AWS Config daemon to capture system logs and view them in the AWS Config console.
D. Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司希望确保他们的EC2实例是安全的。他们希望在实例上发现任何新漏洞时收到通知，同时还希望获得实例上所有登录活动的审计跟踪。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现两个主要功能： 1. 漏洞检测和通知 - 发现EC2实例上的新安全漏洞 2. 登录活动审计跟踪 - 记录和监控实例上的用户登录行为 **涉及的关键AWS服务和概念：** - Amazon Inspector：专门用于应用程序和EC2实例的安全评估和漏洞检测 - Amazon CloudWatch Agent：用于收集系统级别的指标和日志 - CloudWatch Logs：集中存储和分析日志数据 - AWS Systems Manager：系统管理服务，但主要用于补丁管理和配置管理 - AWS Config：配置合规性监控服务 - Amazon Kinesis Agent：用于实时数据流处理 **正确答案D的原因：** - Amazon Inspector是AWS专门的漏洞评估服务，能够自动检测EC2实例和应用程序中的安全漏洞，并提供详细的安全发现报告 - CloudWatch Agent可以安装在EC2实例上收集系统日志（包括登录日志如/var/log/auth.log），并将这些日志发送到CloudWatch Logs进行集中管理 - 这个组合完美满足了漏洞检测和登录审计的双重需求 **其他选项错误的原因：** - 选项A：Systems Manager主要用于补丁管理，不是专门的漏洞检测工具；Kinesis Agent主要用于流数据处理，不是最佳的日志收集方案 - 选项B：Systems Manager Agent虽然能收集一些信息，但CloudTrail主要记录API调用，不记录实例内部的登录活动 - 选项C：CloudWatch本身不具备漏洞检测功能；AWS Config主要用于配置合规性检查，不是日志管理的最佳选择 **决策标准和最佳实践：** - 漏洞检测应使用专门的安全服务Amazon Inspector - 系统日志收集应使用CloudWatch Agent配合CloudWatch Logs - 选择服务时要考虑其核心功能定位，避免使用非专业工具完成专业任务 - 日志审计需要集中化管理和长期存储能力</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">98</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is running an application on Amazon EC2 instances in an Auto Scaling group. Recently, an issue occurred that prevented EC2 instances from launching successfully, and it took several hours for the support team to discover the issue. The support team wants to be notified by email whenever an EC2 instance does not start successfully. Which action will accomplish this? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a health check to the Auto Scaling group to invoke an AWS Lambda function whenever an instance status is impaired.
B. Configure the Auto Scaling group to send a notification to an Amazon SNS topic whenever a failed instance launch occurs.
C. Create an Amazon CloudWatch alarm that invokes an AWS Lambda function when a failed AttachInstances Auto Scaling API call is made.
D. Create a status check alarm on Amazon EC2 to send a notification to an Amazon SNS topic whenever a status check fail occurs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Auto Scaling组中的Amazon EC2实例上运行应用程序。最近发生了一个问题，导致EC2实例无法成功启动，支持团队花了几个小时才发现这个问题。支持团队希望在EC2实例启动失败时通过电子邮件收到通知。哪个操作可以实现这个目标？ 选项： A. 向Auto Scaling组添加健康检查，以便在实例状态受损时调用AWS Lambda函数。 B. 配置Auto Scaling组在发生实例启动失败时向Amazon SNS主题发送通知。 C. 创建Amazon CloudWatch告警，当AttachInstances Auto Scaling API调用失败时调用AWS Lambda函数。 D. 在Amazon EC2上创建状态检查告警，以便在状态检查失败时向Amazon SNS主题发送通知。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案，能够在Auto Scaling组中的EC2实例启动失败时立即通过电子邮件通知支持团队，以避免像之前那样需要几个小时才发现问题。 **涉及的关键AWS服务和概念：** - Auto Scaling Groups：自动扩展组，负责管理EC2实例的启动和终止 - Amazon SNS：简单通知服务，可以发送电子邮件、短信等通知 - CloudWatch：监控服务，用于收集和跟踪指标、日志文件 - Lambda：无服务器计算服务 - EC2实例启动生命周期和状态检查机制 **正确答案B的原因：** Auto Scaling组原生支持向SNS主题发送通知功能，可以直接配置在实例启动失败、终止、启动成功等事件发生时发送通知。这是最直接、最简单的解决方案，无需额外的服务或复杂配置。SNS可以轻松配置电子邮件订阅，满足题目要求。 **其他选项错误的原因：** - 选项A：健康检查主要用于检测已运行实例的健康状态，而不是检测实例启动失败。而且这种方法过于复杂，需要额外的Lambda函数。 - 选项C：AttachInstances API主要用于将现有实例附加到Auto Scaling组，与实例启动失败的场景不匹配。而且这种方法也增加了不必要的复杂性。 - 选项D：EC2状态检查告警只能检测已经启动的实例的状态问题，无法检测实例启动失败的情况，因为启动失败的实例根本不会进入运行状态。 **决策标准和最佳实践：** 1. 选择最简单直接的解决方案：利用AWS服务的原生功能而不是构建复杂的自定义解决方案 2. 确保监控覆盖正确的生命周期阶段：实例启动失败发生在实例生命周期的早期阶段 3. 使用合适的通知机制：SNS是AWS中标准的通知服务，支持多种通知方式 4. 避免过度工程化：不需要Lambda函数等额外服务来解决这个相对简单的通知需求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">99</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using AWS Organizations to centrally manage its AWS accounts. The company has turned on AWS Config in each member account by using AWS CloudFormation StackSets. The company has configured trusted access in Organizations for AWS Config and has configured a member account as a delegated administrator account for AWS Config. A DevOps engineer needs to implement a new security policy. The policy must require all current and future AWS member accounts to use a common baseline of AWS Config rules that contain remediation actions that are managed from a central account. Non-administrator users who can access member accounts must not be able to modify this common baseline of AWS Config rules that are deployed into each member account. Which solution will meet these requirements? delegated administrator account by using AWS Config. Most Voted D (89%) 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CloudFormation template that contains the AWS Config rules and remediation actions. Deploy the template from the Organizations management account by using CloudFormation StackSets.
B. Create an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the Organizations management account by using CloudFormation StackSets.
C. Create a CloudFormation template that contains the AWS Config rules and remediation actions. Deploy the template from the delegated administrator account by using AWS Config.
D. Create an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS Organizations来集中管理其AWS账户。该公司已通过AWS CloudFormation StackSets在每个成员账户中启用了AWS Config。公司已在Organizations中为AWS Config配置了可信访问，并将一个成员账户配置为AWS Config的委托管理员账户。DevOps工程师需要实施新的安全策略。该策略必须要求所有当前和未来的AWS成员账户使用从中央账户管理的包含修复操作的AWS Config规则的通用基线。能够访问成员账户的非管理员用户不得修改部署到每个成员账户中的这些AWS Config规则的通用基线。哪种解决方案能满足这些要求？ 选项： A. 创建包含AWS Config规则和修复操作的CloudFormation模板。使用CloudFormation StackSets从Organizations管理账户部署模板。 B. 创建包含AWS Config规则和修复操作的AWS Config conformance pack。使用CloudFormation StackSets从Organizations管理账户部署该包。 C. 创建包含AWS Config规则和修复操作的CloudFormation模板。使用AWS Config从委托管理员账户部署模板。 D. 创建包含AWS Config规则和修复操作的AWS Config conformance pack。从委托管理员账户部署该包。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现一个集中管理的AWS Config规则基线，需要满足：1）从中央账户管理所有成员账户的Config规则；2）包含修复操作；3）非管理员用户无法修改这些规则；4）适用于当前和未来的所有成员账户。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Config：配置合规性监控服务 - AWS Config Conformance Pack：预定义的Config规则集合 - 委托管理员账户：被授权代表组织管理特定服务的成员账户 - CloudFormation StackSets：跨多个账户和区域部署资源的服务 **正确答案的原因（选项D）：** 选项D是最佳解决方案，因为：1）Conformance Pack是专门为标准化Config规则设计的，支持修复操作；2）从委托管理员账户部署符合AWS最佳实践，因为已经为AWS Config设置了委托管理员；3）通过委托管理员账户部署的规则具有更高的权限级别，普通用户无法修改；4）可以自动应用到组织中的所有账户。 **其他选项错误的原因：** 选项A错误：虽然CloudFormation StackSets可以跨账户部署，但使用管理账户直接部署不是最佳实践，且CloudFormation模板不如Conformance Pack专业。选项B错误：从管理账户部署违背了委托管理员的设计原则，应该利用已配置的委托管理员账户。选项C错误：使用CloudFormation模板而非专门的Conformance Pack，且描述不完整。 **决策标准和最佳实践：** 1）优先使用专门的AWS服务功能（Conformance Pack vs 通用CloudFormation）；2）遵循委托管理员模式，避免直接使用管理账户；3）选择能提供更强权限控制的方案；4）确保解决方案能自动扩展到新账户。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">100</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer manages a large commercial website that runs on Amazon EC2. The website uses Amazon Kinesis Data Streams to collect and process web logs. The DevOps engineer manages the Kinesis consumer application, which also runs on Amazon EC2. Sudden increases of data cause the Kinesis consumer application to fall behind, and the Kinesis data streams drop records before the records can be processed. The DevOps engineer must implement a solution to improve stream handling. Which solution meets these requirements with the MOST operational efficiency? Amazon S3 to derive customer insights. Store the results in Amazon S3. GetRecords.IteratorAgeMilliseconds metric. Increase the retention period of the Kinesis data streams. Most Voted for the Lambda function to process the data streams. Most Voted processes the data faster. B (70%) C (28%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the Kinesis consumer application to store the logs durably in Amazon S3. Use Amazon EMR to process the data directly on
B. Horizontally scale the Kinesis consumer application by adding more EC2 instances based on the Amazon CloudWatch
C. Convert the Kinesis consumer application to run as an AWS Lambda function. Configure the Kinesis data streams as the event source
D. Increase the number of shards in the Kinesis data streams to increase the overall throughput so that the consumer application</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师管理着一个运行在Amazon EC2上的大型商业网站。该网站使用Amazon Kinesis Data Streams来收集和处理web日志。DevOps工程师管理着同样运行在Amazon EC2上的Kinesis消费者应用程序。数据的突然增加导致Kinesis消费者应用程序处理滞后，Kinesis数据流在记录被处理之前就丢弃了记录。DevOps工程师必须实施一个解决方案来改善流处理。哪个解决方案以最高的运营效率满足这些要求？ 选项： A. 修改Kinesis消费者应用程序，将日志持久存储在Amazon S3中。使用Amazon EMR直接在Amazon S3上处理数据以获得客户洞察。将结果存储在Amazon S3中。 B. 基于Amazon CloudWatch GetRecords.IteratorAgeMilliseconds指标，通过添加更多EC2实例来水平扩展Kinesis消费者应用程序。增加Kinesis数据流的保留期。 C. 将Kinesis消费者应用程序转换为AWS Lambda函数运行。将Kinesis数据流配置为Lambda函数的事件源来处理数据流。 D. 增加Kinesis数据流中的分片数量以增加整体吞吐量，使消费者应用程序处理数据更快。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何解决Kinesis Data Streams消费者应用程序处理滞后导致数据丢失的问题，要求找到最具运营效率的解决方案。 **涉及的关键AWS服务和概念：** - Amazon Kinesis Data Streams：实时数据流处理服务 - Amazon EC2：弹性计算云服务 - AWS Lambda：无服务器计算服务 - Amazon CloudWatch：监控服务 - GetRecords.IteratorAgeMilliseconds：衡量消费者滞后程度的关键指标 - 水平扩展：通过增加实例数量来提高处理能力 **正确答案B的原因：** 1. **直接解决根本问题**：通过水平扩展增加更多EC2实例，直接提高消费处理能力 2. **智能监控驱动**：使用GetRecords.IteratorAgeMilliseconds指标进行自动化扩展，这个指标直接反映消费者的滞后程度 3. **运营效率最高**：可以实现自动扩展，无需人工干预 4. **保留期增加**：为扩展过程提供缓冲时间，防止数据丢失 5. **保持现有架构**：最小化改动，降低实施风险 **其他选项错误的原因：** - **选项A**：改变了整个架构，从实时处理变为批处理，不符合原有的实时处理需求，运营复杂度高 - **选项C**：Lambda有15分钟执行时间限制和并发限制，可能无法处理大量持续的流数据，且迁移成本高 - **选项D**：选项描述不完整，且仅增加分片不一定解决消费者处理能力不足的问题 **决策标准和最佳实践：** 1. **最小改动原则**：在解决问题的同时保持现有架构稳定 2. **监控驱动扩展**：使用相关的CloudWatch指标进行智能扩展 3. **弹性设计**：能够根据负载变化自动调整资源 4. **数据保护**：通过增加保留期确保数据不丢失 5. **运营效率优先**：选择自动化程度最高、人工干预最少的方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">101</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company recently created a new AWS Control Tower landing zone in a new organization in AWS Organizations. The landing zone must be able to demonstrate compliance with the Center for Internet Security (CIS) Benchmarks for AWS Foundations. The company&#x27;s security team wants to use AWS Security Hub to view compliance across all accounts. Only the security team can be allowed to view aggregated Security Hub findings. In addition, specific users must be able to view findings from their own accounts within the organization. All accounts must be enrolled in Security Hub after the accounts are created. Which combination of steps will meet these requirements in the MOST automated way? (Choose three.) CreateAccountAssignment API operation to associate the security team users with the permission set and with the delegated security account. F. In the organization&#x27;s management account, create an Amazon EventBridge rule that reacts to the CreateManagedAccount event. Create an AWS Lambda function that uses the Security Hub CreateMembers API operation to add new accounts to Security Hub. Configure the EventBridge rule to invoke the Lambda function. ACE (76%) ADE (19%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Turn on trusted access for Security Hub in the organization&#x27;s management account. Create a new security account by using AWS Control Tower. Configure the new security account as the delegated administrator account for Security Hub. In the new security account, provide Security Hub with the CIS Benchmarks for AWS Foundations standards.
B. Turn on trusted access for Security Hub in the organization&#x27;s management account. From the management account, provide Security Hub with the CIS Benchmarks for AWS Foundations standards.
C. Create an AWS IAM Identity Center (AWS Single Sign-On) permission set that includes the required permissions. Use the
D. Create an SCP that explicitly denies any user who is not on the security team from accessing Security Hub.
E. In Security Hub, turn on automatic enablement.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司最近在AWS Organizations中的新组织内创建了一个新的AWS Control Tower landing zone。该landing zone必须能够证明符合AWS基础架构的互联网安全中心(CIS)基准。公司的安全团队希望使用AWS Security Hub来查看所有账户的合规性。只有安全团队被允许查看聚合的Security Hub发现结果。此外，特定用户必须能够查看组织内自己账户的发现结果。所有账户在创建后都必须注册到Security Hub中。哪种步骤组合能够以最自动化的方式满足这些要求？（选择三个。） 选项： A. 在组织的管理账户中为Security Hub开启可信访问。使用AWS Control Tower创建一个新的安全账户。将新安全账户配置为Security Hub的委托管理员账户。在新安全账户中，为Security Hub提供AWS基础架构的CIS基准标准。 B. 在组织的管理账户中为Security Hub开启可信访问。从管理账户为Security Hub提供AWS基础架构的CIS基准标准。 C. 创建一个包含所需权限的AWS IAM Identity Center (AWS Single Sign-On)权限集。使用CreateAccountAssignment API操作将安全团队用户与权限集和委托安全账户关联。 D. 创建一个SCP，明确拒绝不在安全团队中的任何用户访问Security Hub。 E. 在Security Hub中开启自动启用功能。 F. 在组织的管理账户中，创建一个对CreateManagedAccount事件做出反应的Amazon EventBridge规则。创建一个使用Security Hub CreateMembers API操作将新账户添加到Security Hub的AWS Lambda函数。配置EventBridge规则来调用Lambda函数。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求建立一个符合CIS基准的多账户Security Hub解决方案，需要满足：1）安全团队能查看所有账户的聚合发现结果；2）普通用户只能查看自己账户的发现结果；3）新账户自动注册到Security Hub；4）实现最大程度的自动化。 **涉及的关键AWS服务和概念：** - AWS Control Tower：多账户管理和治理服务 - AWS Security Hub：安全态势管理和合规性监控服务 - AWS Organizations：组织级账户管理 - IAM Identity Center：集中身份管理 - EventBridge + Lambda：事件驱动的自动化 - 委托管理员模式：允许非管理账户管理特定服务 **正确答案ACE的原因：** - **选项A**：建立了正确的Security Hub架构基础。开启可信访问是前提，创建专门的安全账户作为委托管理员是最佳实践，可以实现集中管理而不暴露管理账户权限。在委托账户中配置CIS基准确保合规性要求。 - **选项C**：通过IAM Identity Center实现细粒度的权限控制，安全团队可以访问委托安全账户查看聚合结果，普通用户只能访问自己的账户，满足权限隔离要求。 - **选项E**：自动启用功能确保新创建的账户自动加入Security Hub，实现了自动化要求。 **其他选项错误的原因：** - **选项B**：在管理账户中配置Security Hub不是最佳实践，会增加管理账户的复杂性和安全风险。 - **选项D**：SCP过于粗暴，无法实现细粒度的权限控制，且可能阻止必要的访问。 - **选项F**：虽然能实现自动化，但在有选项E（自动启用）的情况下，这种EventBridge+Lambda的方案增加了不必要的复杂性。 **决策标准和最佳实践：** 1. **委托管理员模式**：使用专门的安全账户而非管理账户来管理Security Hub 2. **最小权限原则**：通过IAM Identity Center实现精确的权限控制 3. **自动化优先**：选择AWS原生的自动启用功能而非自定义解决方案 4. **安全隔离**：确保不同角色用户只能访问授权范围内的资源</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">102</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs applications in AWS accounts that are in an organization in AWS Organizations. The applications use Amazon EC2 instances and Amazon S3. The company wants to detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity in its existing AWS accounts and in any AWS accounts that the company creates in the future. When the company detects one of these events, the company wants to use an existing Amazon Simple Notification Service (Amazon SNS) topic to send a notification to its operational support team for investigation and remediation. Which solution will meet these requirements in accordance with AWS best practices? GuardDuty administrator account, add the company&#x27;s existing AWS accounts to GuardDuty as members. In the GuardDuty administrator account, create an Amazon EventBridge rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic. Most Voted SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic. A (88%) 13%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the organization&#x27;s management account, configure an AWS account as the Amazon GuardDuty administrator account. In the
B. In the organization&#x27;s management account, configure Amazon GuardDuty to add newly created AWS accounts by invitation and to send invitations to the existing AWS accounts. Create an AWS CloudFormation stack set that accepts the GuardDuty invitation and creates an Amazon EventBridge rule. Configure the rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic. Configure the CloudFormation stack set to deploy into all AWS accounts in the organization.
C. In the organization&#x27;s management account, create an AWS CloudTrail organization trail. Activate the organization trail in all AWS accounts in the organization. Create an SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic.
D. In the organization&#x27;s management account, configure an AWS account as the AWS CloudTrail administrator account. In the CloudTrail administrator account, create a CloudTrail organization trail. Add the company&#x27;s existing AWS accounts to the organization trail. Create an</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中的组织内的AWS账户中运行应用程序。这些应用程序使用Amazon EC2实例和Amazon S3。该公司希望在其现有的AWS账户以及将来创建的任何AWS账户中检测可能被入侵的EC2实例、可疑的网络活动和异常的API活动。当公司检测到这些事件之一时，公司希望使用现有的Amazon Simple Notification Service (Amazon SNS) topic向其运营支持团队发送通知以进行调查和修复。哪种解决方案将根据AWS最佳实践满足这些要求？ 选项： A. 在组织的管理账户中，配置一个AWS账户作为Amazon GuardDuty管理员账户。在GuardDuty管理员账户中，将公司现有的AWS账户作为成员添加到GuardDuty。在GuardDuty管理员账户中，创建一个Amazon EventBridge规则，使用事件模式匹配GuardDuty事件并将匹配的事件转发到SNS topic。 B. 在组织的管理账户中，配置Amazon GuardDuty通过邀请添加新创建的AWS账户，并向现有AWS账户发送邀请。创建一个AWS CloudFormation stack set，接受GuardDuty邀请并创建Amazon EventBridge规则。配置规则使用事件模式匹配GuardDuty事件并将匹配的事件转发到SNS topic。配置CloudFormation stack set部署到组织中的所有AWS账户。 C. 在组织的管理账户中，创建AWS CloudTrail组织跟踪。在组织中的所有AWS账户中激活组织跟踪。创建SCP启用每个账户中的VPC Flow Logs。为组织配置AWS Security Hub。创建Amazon EventBridge规则，使用事件模式匹配Security Hub事件并将匹配的事件转发到SNS topic。 D. 在组织的管理账户中，配置一个AWS账户作为AWS CloudTrail管理员账户。在CloudTrail管理员账户中，创建CloudTrail组织跟踪。将公司现有的AWS账户添加到组织跟踪中。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个解决方案来检测可能被入侵的EC2实例、可疑网络活动和异常API活动，并且需要覆盖现有账户和未来创建的账户，当检测到威胁时通过SNS发送通知。 **涉及的关键AWS服务和概念：** - Amazon GuardDuty：AWS的威胁检测服务，专门用于检测恶意活动和异常行为 - AWS Organizations：多账户管理服务 - Amazon EventBridge：事件路由服务 - AWS CloudFormation StackSets：跨多个账户和区域部署资源 - Amazon SNS：通知服务 - AWS Security Hub：安全发现聚合服务 - AWS CloudTrail：API调用日志服务 **正确答案B的原因：** 1. **服务选择正确**：GuardDuty是专门设计用来检测题目中提到的三种威胁类型（被入侵的EC2实例、可疑网络活动、异常API活动）的服务 2. **组织级部署**：使用CloudFormation StackSets可以自动化地在组织中的所有账户部署相同的配置 3. **自动化程度高**：通过邀请机制和StackSets，可以确保新账户也会自动包含在威胁检测范围内 4. **事件处理合理**：在每个账户中创建EventBridge规则来处理GuardDuty事件并转发到SNS **其他选项错误的原因：** - **选项A**：虽然使用了正确的GuardDuty服务，但只是手动添加现有账户作为成员，没有自动化机制处理未来新创建的账户，不符合可扩展性要求 - **选项C**：使用了错误的服务组合。CloudTrail主要用于API调用审计，VPC Flow Logs用于网络流量分析，Security Hub用于安全发现聚合，这个组合无法有效检测被入侵的EC2实例 - **选项D**：选项不完整，且CloudTrail不是检测威胁的最佳工具 **决策标准和最佳实践：** 1. **服务适配性**：选择专门针对威胁检测的GuardDuty服务 2. **自动化和可扩展性**：使用StackSets确保新账户自动包含安全配置 3. **集中管理**：在组织管理账户中进行配置，符合AWS Organizations的最佳实践 4. **事件驱动架构**：使用EventBridge进行事件路由，实现松耦合的通知机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">103</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s DevOps engineer is working in a multi-account environment. The company uses AWS Transit Gateway to route all outbound traffic through a network operations account. In the network operations account, all account traffic passes through a firewall appliance for inspection before the traffic goes to an internet gateway. The firewall appliance sends logs to Amazon CloudWatch Logs and includes event severities of CRITICAL, HIGH, MEDIUM, LOW, and INFO. The security team wants to receive an alert if any CRITICAL events occur. What should the DevOps engineer do to meet these requirements? Subscribe the security team&#x27;s email address to the topic. Most Voted B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon CloudWatch Synthetics canary to monitor the firewall state. If the firewall reaches a CRITICAL state or logs a CRITICAL event, use a CloudWatch alarm to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the security team&#x27;s email address to the topic.
B. Create an Amazon CloudWatch metric filter by using a search for CRITICAL events. Publish a custom metric for the finding. Use a CloudWatch alarm based on the custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic.
C. Enable Amazon GuardDuty in the network operations account. Configure GuardDuty to monitor flow logs. Create an Amazon EventBridge event rule that is invoked by GuardDuty events that are CRITICAL. Define an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the security team&#x27;s email address to the topic.
D. Use AWS Firewall Manager to apply consistent policies across all accounts. Create an Amazon EventBridge event rule that is invoked by Firewall Manager events that are CRITICAL. Define an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the security team&#x27;s email address to the topic.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的DevOps工程师在多账户环境中工作。该公司使用AWS Transit Gateway将所有出站流量路由到网络运营账户。在网络运营账户中，所有账户流量在到达internet gateway之前都要通过防火墙设备进行检查。防火墙设备将日志发送到Amazon CloudWatch Logs，包含CRITICAL、HIGH、MEDIUM、LOW和INFO等事件严重级别。安全团队希望在发生任何CRITICAL事件时收到警报。DevOps工程师应该如何满足这些要求？ 选项： A. 创建Amazon CloudWatch Synthetics canary来监控防火墙状态。如果防火墙达到CRITICAL状态或记录CRITICAL事件，使用CloudWatch alarm向Amazon Simple Notification Service (Amazon SNS) topic发布通知。将安全团队的邮箱地址订阅到该topic。 B. 使用搜索CRITICAL事件创建Amazon CloudWatch metric filter。为发现的事件发布自定义指标。基于自定义指标使用CloudWatch alarm向Amazon Simple Notification Service (Amazon SNS) topic发布通知。 C. 在网络运营账户中启用Amazon GuardDuty。配置GuardDuty监控flow logs。创建由CRITICAL级别GuardDuty事件触发的Amazon EventBridge event rule。定义Amazon Simple Notification Service (Amazon SNS) topic作为目标。将安全团队的邮箱地址订阅到该topic。 D. 使用AWS Firewall Manager在所有账户中应用一致的策略。创建由CRITICAL级别Firewall Manager事件触发的Amazon EventBridge event rule。定义Amazon Simple Notification Service (Amazon SNS) topic作为目标。将安全团队的邮箱地址订阅到该topic。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要监控防火墙设备发送到CloudWatch Logs的日志，当出现CRITICAL级别事件时自动发送警报给安全团队。 **涉及的关键AWS服务和概念：** - CloudWatch Logs：存储防火墙日志的服务 - CloudWatch Metric Filter：从日志中提取特定模式并转换为指标 - CloudWatch Alarm：基于指标阈值触发警报 - SNS：发送通知的消息服务 - CloudWatch Synthetics：用于应用程序监控的服务 - GuardDuty：威胁检测服务 - Firewall Manager：防火墙策略管理服务 **正确答案B的原因：** 1. **直接针对问题**：防火墙日志已经在CloudWatch Logs中，需要从这些日志中识别CRITICAL事件 2. **技术路径正确**：Metric Filter可以搜索日志中的特定文本模式（CRITICAL），并将匹配结果转换为自定义指标 3. **完整的警报链**：自定义指标→CloudWatch Alarm→SNS Topic→邮件通知，形成完整的监控和通知链路 4. **成本效益**：利用现有的日志数据，无需额外的服务或复杂配置 **其他选项错误的原因：** - **选项A**：CloudWatch Synthetics主要用于网站和API的可用性监控，不适合分析日志内容中的特定事件级别 - **选项C**：GuardDuty是威胁检测服务，主要分析VPC Flow Logs、DNS logs等来检测安全威胁，不是用来解析防火墙应用日志中的自定义严重级别 - **选项D**：Firewall Manager用于管理防火墙策略的一致性，不是用来监控日志事件的工具 **决策标准和最佳实践：** 1. **服务匹配度**：选择最直接适合数据源和需求的AWS服务 2. **架构简洁性**：避免过度工程化，使用最简单有效的解决方案 3. **成本考虑**：利用现有资源，避免引入不必要的额外服务 4. **可维护性**：选择配置简单、易于维护的方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">104</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is divided into teams. Each team has an AWS account, and all the accounts are in an organization in AWS Organizations. Each team must retain full administrative rights to its AWS account. Each team also must be allowed to access only AWS services that the company approves for use. AWS services must gain approval through a request and approval process. How should a DevOps engineer configure the accounts to meet these requirements? D (50%) C (47%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CloudFormation StackSets to provision IAM policies in each account to deny access to restricted AWS services. In each account, configure AWS Config rules that ensure that the policies are attached to IAM principals in the account.
B. Use AWS Control Tower to provision the accounts into OUs within the organization. Configure AWS Control Tower to enable AWS IAM Identity Center (AWS Single Sign-On). Configure IAM Identity Center to provide administrative access. Include deny policies on user roles for restricted AWS services.
C. Place all the accounts under a new top-level OU within the organization. Create an SCP that denies access to restricted AWS services. Attach the SCP to the OU.
D. Create an SCP that allows access to only approved AWS services. Attach the SCP to the root OU of the organization. Remove the FullAWSAccess SCP from the root OU of the organization.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司分为多个团队。每个团队都有一个AWS账户，所有账户都在AWS Organizations的一个组织中。每个团队必须保留对其AWS账户的完全管理权限。每个团队也必须只能访问公司批准使用的AWS服务。AWS服务必须通过请求和批准流程获得批准。DevOps工程师应该如何配置账户以满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在保持团队对各自AWS账户完全管理权限的同时，限制他们只能使用公司批准的AWS服务。这是一个典型的组织级权限控制问题，需要在账户自主性和服务使用合规性之间找到平衡。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - Service Control Policies (SCP)：服务控制策略，用于在组织层面限制权限 - Organizational Units (OU)：组织单元，用于分组管理账户 - IAM策略：身份和访问管理策略 - AWS Control Tower：账户治理服务 **正确答案C的原因：** 1. **精确的权限边界控制**：将所有账户放在新的顶级OU下，通过SCP拒绝访问受限服务，这样可以在组织层面统一控制服务访问权限 2. **保持管理权限**：SCP只是设置权限边界，不会影响团队在各自账户内的完全管理权限 3. **集中化管理**：通过OU级别的SCP，可以统一管理所有团队账户的服务访问限制 4. **灵活性**：当新服务获得批准时，只需修改SCP策略即可 **其他选项错误的原因：** - **选项A**：使用CloudFormation StackSets和Config规则过于复杂，且需要在每个账户内部署策略，管理复杂度高，不如SCP直接有效 - **选项B**：Control Tower和IAM Identity Center主要解决身份管理问题，但题目要求保持团队的完全管理权限，这种方案会改变现有的权限结构 - **选项D**：在根OU级别应用允许策略并移除FullAWSAccess会影响整个组织，可能对其他不在这些团队中的账户造成意外影响 **决策标准和最佳实践：** 1. **最小权限原则**：通过SCP设置服务访问边界 2. **集中管理，分散执行**：组织级别统一控制，账户级别保持自主 3. **简化管理**：选择最直接有效的解决方案，避免过度复杂的架构 4. **影响范围控制**：确保策略变更只影响目标账户群体</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">105</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer used an AWS CloudFormation custom resource to set up AD Connector. The AWS Lambda function ran and created AD Connector, but CloudFormation is not transitioning from CREATE_IN_PROGRESS to CREATE_COMPLETE. Which action should the engineer take to resolve this issue? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Ensure the Lambda function code has exited successfully.
B. Ensure the Lambda function code returns a response to the pre-signed URL.
C. Ensure the Lambda function IAM role has cloudformation:UpdateStack permissions for the stack ARN.
D. Ensure the Lambda function IAM role has ds:ConnectDirectory permissions for the AWS account.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师使用AWS CloudFormation自定义资源来设置AD Connector。AWS Lambda函数运行并创建了AD Connector，但CloudFormation没有从CREATE_IN_PROGRESS状态转换到CREATE_COMPLETE状态。工程师应该采取哪个行动来解决这个问题？ 选项： A. 确保Lambda函数代码已成功退出 B. 确保Lambda函数代码向预签名URL返回响应 C. 确保Lambda函数IAM角色对stack ARN具有cloudformation:UpdateStack权限 D. 确保Lambda函数IAM角色对AWS账户具有ds:ConnectDirectory权限</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CloudFormation自定义资源的工作机制，特别是Lambda函数与CloudFormation之间的通信协议。问题的关键在于Lambda函数已经成功创建了AD Connector，但CloudFormation堆栈状态没有更新。 **涉及的关键AWS服务和概念：** 1. AWS CloudFormation自定义资源 - 允许在CloudFormation模板中集成自定义逻辑 2. AWS Lambda - 执行自定义资源逻辑的无服务器函数 3. AD Connector - AWS Directory Service的一种类型 4. 预签名URL - CloudFormation提供给Lambda函数用于返回响应的机制 **正确答案的原因（选项B）：** CloudFormation自定义资源的工作流程要求Lambda函数必须向CloudFormation提供的预签名URL发送响应，告知操作的成功或失败状态。即使Lambda函数成功执行了业务逻辑（创建AD Connector），如果没有向预签名URL发送响应，CloudFormation就无法知道操作已完成，因此会一直保持在CREATE_IN_PROGRESS状态。这是CloudFormation自定义资源的核心通信机制。 **其他选项错误的原因：** - 选项A：Lambda函数成功退出并不足够，还需要主动通知CloudFormation操作结果 - 选项C：cloudformation:UpdateStack权限不是自定义资源Lambda函数的必需权限，CloudFormation通过预签名URL机制处理状态更新 - 选项D：ds:ConnectDirectory权限可能已经具备（因为AD Connector已成功创建），问题不在于权限而在于通信机制 **决策标准和最佳实践：** 1. 理解CloudFormation自定义资源的完整生命周期，包括请求和响应机制 2. 确保Lambda函数代码包含向ResponseURL发送HTTP PUT请求的逻辑 3. 响应必须包含正确的Status（SUCCESS或FAILED）、RequestId、LogicalResourceId和StackId 4. 实施适当的错误处理，确保即使在失败情况下也能发送响应给CloudFormation</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">106</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS CodeCommit for source code control. Developers apply their changes to various feature branches and create pull requests to move those changes to the main branch when the changes are ready for production. The developers should not be able to push changes directly to the main branch. The company applied the AWSCodeCommitPowerUser managed policy to the developers&#x27; IAM role, and now these developers can push changes to the main branch directly on every repository in the AWS account. What should the company do to restrict the developers&#x27; ability to push changes to the main branch directly? in the policy statement with a condition that references the feature branches. A (93%) 7%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an additional policy to include a Deny rule for the GitPush and PutFile actions. Include a restriction for the specific repositories in the policy statement with a condition that references the main branch.
B. Remove the IAM policy, and add an AWSCodeCommitReadOnly managed policy. Add an Allow rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.
C. Modify the IAM policy. Include a Deny rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.
D. Create an additional policy to include an Allow rule for the GitPush and PutFile actions. Include a restriction for the specific repositories</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS CodeCommit进行源代码控制。开发人员将他们的更改应用到各种功能分支，并创建pull request将这些更改移动到主分支，当更改准备好用于生产时。开发人员不应该能够直接将更改推送到主分支。公司将AWSCodeCommitPowerUser托管策略应用到开发人员的IAM角色，现在这些开发人员可以在AWS账户中的每个存储库上直接将更改推送到主分支。公司应该做什么来限制开发人员直接将更改推送到主分支的能力？ 选项： A. 创建一个额外的策略，包含对GitPush和PutFile操作的拒绝规则。在策略语句中包含对特定存储库的限制，并使用引用主分支的条件。 B. 删除IAM策略，并添加AWSCodeCommitReadOnly托管策略。为策略语句中的特定存储库添加GitPush和PutFile操作的允许规则，并使用引用主分支的条件。 C. 修改IAM策略。在策略语句中为特定存储库包含对GitPush和PutFile操作的拒绝规则，并使用引用主分支的条件。 D. 创建一个额外的策略，包含对GitPush和PutFile操作的允许规则。为特定存储库包含限制。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求限制开发人员直接向主分支推送代码的权限，同时保持他们在功能分支上正常工作的能力。当前问题是AWSCodeCommitPowerUser托管策略给予了开发人员过多权限。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：Git代码存储库服务 - IAM策略：权限控制机制 - 托管策略 vs 自定义策略 - IAM策略评估逻辑：显式拒绝优先于允许 - 条件语句：用于细粒度权限控制 - GitPush和PutFile：CodeCommit中的关键操作 **正确答案A的原因：** 1. **创建额外策略**：保留现有的AWSCodeCommitPowerUser策略，通过附加策略实现限制 2. **使用Deny规则**：显式拒绝优先级最高，能够覆盖现有的允许权限 3. **针对特定操作**：GitPush和PutFile是直接推送代码到分支的关键操作 4. **条件限制**：通过条件语句specifically targeting主分支，不影响功能分支的操作 5. **最小权限原则**：只限制必要的操作，保持其他权限不变 **其他选项错误的原因：** - **选项B**：删除现有策略会移除开发人员的基本权限，然后用ReadOnly策略加Allow规则的方式过于复杂且容易出错 - **选项C**：直接修改托管策略是不可能的，托管策略由AWS维护，用户无法修改 - **选项D**：使用Allow规则无法覆盖现有的权限，且描述不完整 **决策标准和最佳实践：** 1. **权限最小化**：只限制必要的操作，保持开发效率 2. **显式拒绝策略**：当需要限制现有权限时，使用Deny规则是最有效的方法 3. **策略分离**：通过附加策略而非修改现有策略来实现特定限制 4. **条件使用**：利用条件语句实现细粒度的分支级别控制 5. **代码审查流程**：通过技术手段强制执行pull request工作流程</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">107</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run in an Auto Scaling group across multiple Availability Zones. The application uses an Amazon RDS for MySQL DB instance to store the data. The company has configured Amazon Route 53 with an alias record that points to the ALB. A new company guideline requires a geographically isolated disaster recovery (DR) site with an RTO of 4 hours and an RPO of 15 minutes. Which DR strategy will meet these requirements with the LEAST change to the application stack? D (89%) 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Launch a replica environment of everything except Amazon RDS in a different Availability Zone. Create an RDS read replica in the new Availability Zone, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy.
B. Launch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a latency routing policy.
C. Launch a replica environment of everything except Amazon RDS in a different AWS Region. In the event of an outage, copy and restore the latest RDS snapshot from the primary Region to the DR Region. Adjust the Route 53 record set to point to the ALB in the DR Region.
D. Launch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy. In the event of an outage, promote the read replica to primary.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司管理着一个运行在Application Load Balancer (ALB)后面Amazon EC2实例上的Web应用程序。EC2实例在跨多个Availability Zone的Auto Scaling组中运行。应用程序使用Amazon RDS for MySQL数据库实例来存储数据。公司已配置Amazon Route 53，其中有一个指向ALB的别名记录。新的公司指导方针要求建立一个地理隔离的灾难恢复(DR)站点，RTO为4小时，RPO为15分钟。哪种DR策略能够以对应用程序堆栈的最少更改来满足这些要求？ 选项： A. 在不同的Availability Zone中启动除Amazon RDS之外所有组件的副本环境。在新的Availability Zone中创建RDS read replica，并配置新堆栈指向本地RDS数据库实例。通过使用健康检查配置failover路由策略，将新堆栈添加到Route 53记录集中。 B. 在不同的AWS Region中启动除Amazon RDS之外所有组件的副本环境。在新Region中创建RDS read replica，并配置新堆栈指向本地RDS数据库实例。通过使用健康检查配置latency路由策略，将新堆栈添加到Route 53记录集中。 C. 在不同的AWS Region中启动除Amazon RDS之外所有组件的副本环境。在发生故障时，从主Region复制并恢复最新的RDS快照到DR Region。调整Route 53记录集指向DR Region中的ALB。 D. 在不同的AWS Region中启动除Amazon RDS之外所有组件的副本环境。在新Region中创建RDS read replica，并配置新环境指向本地RDS数据库实例。通过使用健康检查配置failover路由策略，将新堆栈添加到Route 53记录集中。在发生故障时，将read replica提升为主数据库。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS灾难恢复策略设计，需要满足：1）地理隔离要求；2）RTO（恢复时间目标）4小时；3）RPO（恢复点目标）15分钟；4）对现有应用程序堆栈的最少更改。 **涉及的关键AWS服务和概念：** - Amazon RDS read replica：提供数据复制和故障转移能力 - Multi-Region部署：实现地理隔离 - Route 53 failover路由策略：自动故障转移 - RTO/RPO概念：衡量灾难恢复能力的关键指标 **正确答案D的原因：** 1. **满足地理隔离**：在不同AWS Region部署DR环境 2. **满足RPO要求**：RDS read replica提供近实时数据同步，远优于15分钟RPO要求 3. **满足RTO要求**：预先部署的环境+自动failover机制，可在4小时内完成恢复 4. **最少应用更改**：应用程序无需修改，只需配置指向不同的数据库端点 5. **自动化程度高**：Route 53 failover策略提供自动故障检测和流量切换 **其他选项错误的原因：** - **选项A**：使用Availability Zone而非Region，不满足&quot;地理隔离&quot;要求 - **选项B**：使用latency路由策略而非failover策略，这是性能优化而非灾难恢复策略，不适合DR场景 - **选项C**：依赖手动快照恢复，RTO可能超过4小时，且需要手动操作Route 53，自动化程度低 **决策标准和最佳实践：** 1. **地理隔离**：DR站点必须在不同Region以防范区域性灾难 2. **自动化优先**：选择能够自动检测故障和切换的方案 3. **预先部署**：为满足严格的RTO要求，应预先部署完整环境 4. **数据同步策略**：read replica比快照恢复提供更好的RPO和RTO 5. **DNS故障转移**：Route 53 failover策略是实现自动故障转移的标准做法</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">108</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A large enterprise is deploying a web application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The application stores data in an Amazon RDS for Oracle DB instance and Amazon DynamoDB. There are separate environments for development, testing, and production. What is the MOST secure and flexible way to obtain password credentials during deployment? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Retrieve an access key from an AWS Systems Manager SecureString parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.
B. Launch the EC2 instances with an EC2 IAM role to access AWS services. Retrieve the database credentials from AWS Secrets Manager.
C. Retrieve an access key from an AWS Systems Manager plaintext parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.
D. Launch the EC2 instances with an EC2 IAM role to access AWS services. Store the database passwords in an encrypted config file with the application artifacts.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家大型企业正在AWS上部署Web应用程序。该应用程序运行在Application Load Balancer后面的Amazon EC2实例上。这些实例在Auto Scaling组中跨多个Availability Zone运行。应用程序将数据存储在Amazon RDS for Oracle数据库实例和Amazon DynamoDB中。开发、测试和生产环境是分离的。在部署过程中获取密码凭证的最安全且最灵活的方式是什么？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在AWS环境中安全管理和获取应用程序凭证的最佳实践，特别是针对EC2实例访问AWS服务和数据库的认证方式。 **涉及的关键AWS服务和概念：** - EC2 IAM Role：为EC2实例提供临时、自动轮换的AWS服务访问权限 - AWS Secrets Manager：专门用于管理敏感信息如数据库密码的服务 - AWS Systems Manager Parameter Store：用于存储配置数据和密钥的服务 - 最小权限原则和安全最佳实践 **正确答案B的原因：** 1. **EC2 IAM Role**：这是访问AWS服务的最安全方式，因为它提供临时凭证，自动轮换，无需在代码中硬编码访问密钥 2. **AWS Secrets Manager**：专门设计用于管理数据库凭证，提供自动密码轮换、细粒度访问控制、审计日志等高级功能 3. **灵活性**：支持多环境部署，可以为不同环境配置不同的密钥和权限 4. **安全性**：所有凭证都加密存储，传输过程也加密 **其他选项错误的原因：** - **选项A**：使用访问密钥是不安全的做法，容易泄露且难以管理轮换 - **选项C**：使用明文参数存储访问密钥极其不安全，违反了基本的安全原则 - **选项D**：将数据库密码存储在配置文件中，即使加密也不如专门的密钥管理服务安全和灵活 **决策标准和最佳实践：** 1. **避免长期凭证**：优先使用IAM Role而非访问密钥 2. **专门工具管理敏感信息**：使用Secrets Manager管理数据库凭证 3. **加密和访问控制**：确保所有敏感数据都加密存储并有适当的访问控制 4. **审计和监控**：选择提供完整审计日志的服务 5. **自动化管理**：支持自动密码轮换和管理的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">109</div>
        <div class="field-label">Question:</div>
        <div class="field-content">The security team depends on AWS CloudTrail to detect sensitive security issues in the company&#x27;s AWS account. The DevOps engineer needs a solution to auto-remediate CloudTrail being turned off in an AWS account. What solution ensures the LEAST amount of downtime for the CloudTrail log deliveries? resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule. call StartLogging on a CloudTrail trail in the AWS account. Add the Lambda function ARN as a target to the EventBridge rule. CloudTrail trail is disabled, have the script re-enable the trail. A (92%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule for the CloudTrail StopLogging event. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule.
B. Deploy the AWS-managed CloudTrail-enabled AWS Config rule, set with a periodic interval of 1 hour. Create an Amazon EventBridge rule for AWS Config rules compliance change. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the
C. Create an Amazon EventBridge rule for a scheduled event every 5 minutes. Create an AWS Lambda function that uses the AWS SDK to
D. Launch a t2.nano instance with a script running every 5 minutes that uses the AWS SDK to query CloudTrail in the current account. If the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">安全团队依赖AWS CloudTrail来检测公司AWS账户中的敏感安全问题。DevOps工程师需要一个解决方案来自动修复AWS账户中CloudTrail被关闭的情况。什么解决方案能确保CloudTrail日志传输的停机时间最少？ 选项： A. 为CloudTrail StopLogging事件创建Amazon EventBridge规则。创建一个AWS Lambda函数，使用AWS SDK在调用StopLogging的资源ARN上调用StartLogging。将Lambda函数ARN添加为EventBridge规则的目标。 B. 部署AWS托管的CloudTrail-enabled AWS Config规则，设置为1小时的周期间隔。为AWS Config规则合规性变更创建Amazon EventBridge规则。创建一个AWS Lambda函数，使用AWS SDK调用StartLogging... C. 为每5分钟的计划事件创建Amazon EventBridge规则。创建一个AWS Lambda函数，使用AWS SDK... D. 启动一个t2.nano实例，运行每5分钟执行一次的脚本，使用AWS SDK查询当前账户中的CloudTrail。如果CloudTrail被禁用，让脚本重新启用...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动修复方案，当CloudTrail被关闭时能够自动重新启用，并且要求停机时间最少。关键是要实现实时响应和快速恢复。 **涉及的关键AWS服务和概念：** - AWS CloudTrail：用于记录API调用和账户活动的审计服务 - Amazon EventBridge：事件驱动的服务，可以响应AWS服务的状态变化 - AWS Lambda：无服务器计算服务，用于执行自动修复逻辑 - AWS Config：配置管理和合规性监控服务 - StopLogging/StartLogging API：CloudTrail的启停API调用 **正确答案A的原因：** 1. **实时响应**：EventBridge能够实时捕获StopLogging事件，无需等待轮询间隔 2. **最小停机时间**：事件驱动架构确保在CloudTrail被关闭的瞬间就触发修复动作 3. **精确目标**：直接获取被停止的CloudTrail资源ARN，确保修复正确的资源 4. **高效架构**：使用无服务器Lambda函数，无需维护基础设施 **其他选项错误的原因：** - **选项B**：AWS Config的1小时周期检查导致较长的检测延迟，不符合最少停机时间的要求 - **选项C**：5分钟的定时检查仍然存在检测延迟，且会产生不必要的资源消耗 - **选项D**：使用EC2实例增加了基础设施成本和管理复杂性，5分钟轮询同样存在延迟问题 **决策标准和最佳实践：** 1. **事件驱动优于轮询**：实时事件响应比定期检查更高效 2. **无服务器优先**：Lambda比EC2更适合这种间歇性任务 3. **最小化检测窗口**：安全事件的响应时间越短越好 4. **成本效益**：避免不必要的资源消耗和基础设施维护</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">110</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS CodeArtifact to centrally store Python packages. The CodeArtifact repository is configured with the following repository policy: A development team is building a new project in an account that is in an organization in AWS Organizations. The development team wants to use a Python library that has already been stored in the CodeArtifact repository in the organization. The development team uses AWS CodePipeline and AWS CodeBuild to build the new application. The CodeBuild job that the development team uses to build the application is configured to run in a VPC. Because of compliance requirements, the VPC has no internet connectivity. The development team creates the VPC endpoints for CodeArtifact and updates the CodeBuild buildspec.yaml file. However, the development team cannot download the Python library from the repository. Which combination of steps should a DevOps engineer take so that the development team can use CodeArtifact? (Choose two.) BD (47%) AD (47%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon S3 gateway endpoint. Update the route tables for the subnets that are running the CodeBuild job.
B. Update the repository policy&#x27;s Principal statement to include the ARN of the role that the CodeBuild project uses.
C. Share the CodeArtifact repository with the organization by using AWS Resource Access Manager (AWS RAM).
D. Update the role that the CodeBuild project uses so that the role has sufficient permissions to use the CodeArtifact repository.
E. Specify the account that hosts the repository as the delegated administrator for CodeArtifact in the organization.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS CodeArtifact来集中存储Python包。CodeArtifact存储库配置了以下存储库策略：开发团队正在AWS Organizations组织中的一个账户中构建新项目。开发团队想要使用已经存储在组织中CodeArtifact存储库中的Python库。开发团队使用AWS CodePipeline和AWS CodeBuild来构建新应用程序。开发团队用于构建应用程序的CodeBuild作业配置为在VPC中运行。由于合规要求，VPC没有互联网连接。开发团队创建了CodeArtifact的VPC端点并更新了CodeBuild的buildspec.yaml文件。但是，开发团队无法从存储库下载Python库。DevOps工程师应该采取哪些步骤组合，以便开发团队可以使用CodeArtifact？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题考查的是在无互联网连接的VPC环境中，如何解决CodeBuild访问CodeArtifact存储库的权限和网络连接问题。开发团队已经创建了VPC端点但仍无法下载Python库，需要找出缺失的配置步骤。 **涉及的关键AWS服务和概念：** - AWS CodeArtifact：包管理服务，用于存储和共享软件包 - AWS CodeBuild：持续集成服务，在VPC中运行构建作业 - VPC端点：允许VPC内资源访问AWS服务而无需互联网连接 - IAM权限：控制对AWS资源的访问 - 存储库策略：控制谁可以访问CodeArtifact存储库 **正确答案的原因：** 选项B和D是正确答案： - **选项B**：更新存储库策略的Principal语句以包含CodeBuild项目使用的角色ARN。这是必需的，因为CodeArtifact存储库策略需要明确授权哪些主体可以访问存储库。 - **选项D**：更新CodeBuild项目使用的角色，使其具有使用CodeArtifact存储库的足够权限。CodeBuild需要适当的IAM权限来读取和下载包。 **其他选项错误的原因：** - **选项A**：创建S3网关端点不是必需的，因为CodeArtifact有自己的VPC端点，题目中已经提到团队已经创建了CodeArtifact的VPC端点。 - **选项C**：使用AWS RAM共享CodeArtifact存储库不是必需的，因为存储库已经在同一个组织中，可以通过适当的策略配置来访问。 - **选项E**：指定委托管理员不是解决当前访问问题的必要步骤。 **决策标准和最佳实践：** 1. **双重权限控制**：CodeArtifact访问需要同时配置存储库策略（资源级权限）和IAM角色权限（身份级权限） 2. **最小权限原则**：只授予CodeBuild角色访问所需CodeArtifact存储库的最小必要权限 3. **网络隔离**：在无互联网连接的VPC中，确保正确配置VPC端点以访问AWS服务 4. **策略验证**：确保存储库策略和IAM策略都正确配置，缺一不可</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">111</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses a series of individual Amazon CloudFormation templates to deploy its multi-Region applications. These templates must be deployed in a specific order. The company is making more changes to the templates than previously expected and wants to deploy new templates more efficiently. Additionally, the data engineering team must be notified of all changes to the templates. What should the company do to accomplish these goals? D (93%) 7%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS Lambda function to deploy the CloudFormation templates in the required order. Use stack policies to alert the data engineering team.
B. Host the CloudFormation templates in Amazon S3. Use Amazon S3 events to directly trigger CloudFormation updates and Amazon SNS notifications.
C. Implement CloudFormation StackSets and use drift detection to trigger update alerts to the data engineering team.
D. Leverage CloudFormation nested stacks and stack sets for deployments. Use Amazon SNS to notify the data engineering team.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用一系列独立的Amazon CloudFormation模板来部署其多区域应用程序。这些模板必须按特定顺序部署。该公司对模板的更改比之前预期的要多，希望更高效地部署新模板。此外，数据工程团队必须收到所有模板更改的通知。公司应该怎么做来实现这些目标？ 选项： A. 创建AWS Lambda函数按所需顺序部署CloudFormation模板。使用stack policies来提醒数据工程团队。 B. 将CloudFormation模板托管在Amazon S3中。使用Amazon S3事件直接触发CloudFormation更新和Amazon SNS通知。 C. 实施CloudFormation StackSets并使用drift detection来触发向数据工程团队的更新警报。 D. 利用CloudFormation nested stacks和stack sets进行部署。使用Amazon SNS通知数据工程团队。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 需要按特定顺序部署多个CloudFormation模板 2. 提高模板部署效率 3. 当模板发生变更时通知数据工程团队 4. 支持多区域部署 **涉及的关键AWS服务和概念：** - CloudFormation: AWS基础设施即代码服务 - S3 Events: S3对象变更事件触发机制 - SNS: 消息通知服务 - Lambda: 无服务器计算服务 - StackSets: 跨账户和区域的stack管理 - Nested Stacks: 嵌套stack结构 - Stack Policies: stack保护策略 - Drift Detection: 配置漂移检测 **正确答案B的原因：** 1. **自动化触发**: S3事件可以在模板文件更新时自动触发CloudFormation部署，实现高效部署 2. **通知机制**: S3事件可以同时触发SNS通知，满足团队通知需求 3. **简单有效**: 直接利用S3的事件驱动机制，架构简洁 4. **版本控制**: S3可以很好地管理模板版本 **其他选项错误的原因：** - **选项A**: Stack policies主要用于防止意外更新，不是通知机制；Lambda方案增加了复杂性 - **选项C**: Drift detection是检测配置偏移的，不是用于模板变更通知；StackSets主要解决跨账户/区域问题，不直接解决部署效率问题 - **选项D**: 虽然提到了SNS通知，但nested stacks和stack sets的组合过于复杂，且没有明确的触发机制 **决策标准和最佳实践：** 1. **事件驱动架构**: 利用AWS服务的原生事件机制实现自动化 2. **简化架构**: 选择最简单有效的解决方案 3. **松耦合设计**: S3事件可以同时触发多个下游服务 4. **可扩展性**: S3事件机制可以轻松扩展到更多的自动化流程</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">112</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer has implemented a CI/CD pipeline to deploy an AWS CloudFormation template that provisions a web application. The web application consists of an Application Load Balancer (ALB), a target group, a launch template that uses an Amazon Linux 2 AMI, an Auto Scaling group of Amazon EC2 instances, a security group, and an Amazon RDS for MySQL database. The launch template includes user data that specifies a script to install and start the application. The initial deployment of the application was successful. The DevOps engineer made changes to update the version of the application with the user data. The CI/CD pipeline has deployed a new version of the template. However, the health checks on the ALB are now failing. The health checks have marked all targets as unhealthy. During investigation, the DevOps engineer notices that the CloudFormation stack has a status of UPDATE_COMPLETE. However, when the DevOps engineer connects to one of the EC2 instances and checks /var/log/messages, the DevOps engineer notices that the Apache web server failed to start successfully because of a configuration error. How can the DevOps engineer ensure that the CloudFormation deployment will fail if the user data fails to successfully finish running? CloudFormation template. Set an appropriate timeout for the update policy. Most Voted Create an Amazon Simple Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation. Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation. Set an appropriate timeout on the lifecycle hook. an appropriate invocation timeout. Configure the Lambda function to use the SignalResource API operation to signal success or failure to CloudFormation. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use the cfn-signal helper script to signal success or failure to CloudFormation. Use the WaitOnResourceSignals update policy within the
B. Create an Amazon CloudWatch alarm for the UnhealthyHostCount metric. Include an appropriate alarm threshold for the target group.
C. Create a lifecycle hook on the Auto Scaling group by using the AWS::AutoScaling::LifecycleHook resource. Create an Amazon Simple
D. Use the Amazon CloudWatch agent to stream the cloud-init logs. Create a subscription filter that includes an AWS Lambda function with</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师实施了一个CI/CD流水线来部署AWS CloudFormation模板，该模板提供一个Web应用程序。该Web应用程序包含一个Application Load Balancer (ALB)、一个目标组、一个使用Amazon Linux 2 AMI的启动模板、一个Amazon EC2实例的Auto Scaling组、一个安全组和一个Amazon RDS for MySQL数据库。启动模板包含用户数据，指定安装和启动应用程序的脚本。应用程序的初始部署是成功的。DevOps工程师进行了更改以通过用户数据更新应用程序版本。CI/CD流水线已部署了新版本的模板。但是，ALB上的健康检查现在失败了。健康检查将所有目标标记为不健康。在调查过程中，DevOps工程师注意到CloudFormation堆栈的状态为UPDATE_COMPLETE。但是，当DevOps工程师连接到其中一个EC2实例并检查/var/log/messages时，DevOps工程师注意到Apache Web服务器由于配置错误而未能成功启动。DevOps工程师如何确保如果用户数据未能成功完成运行，CloudFormation部署将失败？ 选项： A. 使用cfn-signal辅助脚本向CloudFormation发送成功或失败信号。在CloudFormation模板中使用WaitOnResourceSignals更新策略。为更新策略设置适当的超时时间。 B. 为UnhealthyHostCount指标创建Amazon CloudWatch告警。为目标组包含适当的告警阈值。 C. 通过使用AWS::AutoScaling::LifecycleHook资源在Auto Scaling组上创建生命周期钩子。创建Amazon Simple Notification Service (Amazon SNS)主题作为向CloudFormation发送成功或失败信号的目标。在生命周期钩子上设置适当的超时时间。 D. 使用Amazon CloudWatch代理流式传输cloud-init日志。创建包含AWS Lambda函数的订阅过滤器，配置适当的调用超时时间。配置Lambda函数使用SignalResource API操作向CloudFormation发送成功或失败信号。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求解决CloudFormation部署中用户数据脚本执行失败但CloudFormation状态仍显示UPDATE_COMPLETE的问题。核心需求是让CloudFormation能够感知到用户数据脚本的执行状态，如果脚本失败则整个部署应该失败。 **涉及的关键AWS服务和概念：** - CloudFormation：基础设施即代码服务，用于自动化资源部署 - EC2用户数据：实例启动时执行的脚本 - cfn-signal：CloudFormation辅助脚本，用于向CloudFormation发送信号 - WaitOnResourceSignals：CloudFormation更新策略，等待资源信号 - Auto Scaling组：自动扩缩容服务 - Application Load Balancer：应用负载均衡器 **正确答案A的原因：** cfn-signal是专门为此场景设计的CloudFormation辅助工具。它允许EC2实例在用户数据脚本执行完成后向CloudFormation发送成功或失败信号。配合WaitOnResourceSignals更新策略，CloudFormation会等待接收到指定数量的成功信号后才认为资源创建/更新成功。如果脚本失败或超时未收到信号，CloudFormation部署将失败。这是AWS推荐的标准做法。 **其他选项错误的原因：** - 选项B：CloudWatch告警只能在部署完成后检测到不健康状态，无法在CloudFormation部署过程中阻止部署完成。 - 选项C：生命周期钩子主要用于Auto Scaling事件处理，不是为CloudFormation部署状态管理设计的，过于复杂且不是标准做法。 - 选项D：通过CloudWatch日志和Lambda处理信号过于复杂，增加了不必要的组件和潜在故障点，不如直接使用cfn-signal简洁有效。 **决策标准和最佳实践：** 1. 选择AWS原生和专门设计的工具（cfn-signal） 2. 保持架构简单，避免过度工程化 3. 使用CloudFormation推荐的标准模式 4. 确保部署状态的准确性和及时反馈 5. 遵循基础设施即代码的最佳实践，让部署状态真实反映应用状态</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">113</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a data ingestion application that runs across multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to monitor the application and consolidate access to the application. Currently, the company is running the application on Amazon EC2 instances from several Auto Scaling groups. The EC2 instances have no access to the internet because the data is sensitive. Engineers have deployed the necessary VPC endpoints. The EC2 instances run a custom AMI that is built specifically for the application. To maintain and troubleshoot the application, system administrators need the ability to log in to the EC2 instances. This access must be automated and controlled centrally. The company&#x27;s security team must receive a notification whenever the instances are accessed. Which solution will meet these requirements? instances from the bastion host. Install AWS Systems Manager Agent on all the EC2 instances. Use Auto Scaling group lifecycle hooks for monitoring and auditing access. Use Systems Manager Session Manager to log in to the instances. Send logs to a log group in Amazon CloudWatch Logs. Export data to Amazon S3 for auditing. Send notifications to the security team by using S3 event notifications. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule to send notifications to the security team whenever a user logs in to an EC2 instance. Use EC2 Instance Connect to log in to the instances. Deploy Auto Scaling groups by using AWS CloudFormation. Use the cfn-init helper script to deploy appropriate VPC routes for external access. Rebuild the custom AMI so that the custom AMI includes AWS Systems Manager Agent.
B. Deploy a NAT gateway and a bastion host that has internet access. Create a security group that allows incoming traffic on all the EC2
C. Use EC2 Image Builder to rebuild the custom AMI. Include the most recent version of AWS Systems Manager Agent in the image. Configure the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to all the EC2 instances. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic.
D. Use AWS Systems Manager Automation to build Systems Manager Agent into the custom AMI. Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个跨多个AWS账户运行的数据摄取应用程序。这些账户在AWS Organizations的组织中。公司需要监控应用程序并整合对应用程序的访问。目前，公司在来自多个Auto Scaling组的Amazon EC2实例上运行应用程序。由于数据敏感，EC2实例无法访问互联网。工程师已部署了必要的VPC endpoints。EC2实例运行专为应用程序构建的自定义AMI。为了维护和故障排除应用程序，系统管理员需要能够登录到EC2实例。此访问必须是自动化的并集中控制。公司的安全团队必须在实例被访问时收到通知。哪个解决方案将满足这些要求？ 选项A：创建Amazon EventBridge规则，在用户登录EC2实例时向安全团队发送通知。使用EC2 Instance Connect登录实例。使用AWS CloudFormation部署Auto Scaling组。使用cfn-init helper脚本部署适当的VPC路由以进行外部访问。重建自定义AMI，使其包含AWS Systems Manager Agent。 选项B：部署具有互联网访问权限的NAT gateway和bastion host。创建允许所有EC2实例上传入流量的安全组。从bastion host登录实例。在所有EC2实例上安装AWS Systems Manager Agent。使用Auto Scaling组生命周期钩子进行监控和审计访问。使用Systems Manager Session Manager登录实例。将日志发送到Amazon CloudWatch Logs中的日志组。将数据导出到Amazon S3进行审计。使用S3事件通知向安全团队发送通知。 选项C：使用EC2 Image Builder重建自定义AMI。在镜像中包含最新版本的AWS Systems Manager Agent。配置Auto Scaling组将AmazonSSMManagedInstanceCore角色附加到所有EC2实例。使用Systems Manager Session Manager登录实例。启用会话详细信息记录到Amazon S3。为新文件上传创建S3事件通知，通过Amazon Simple Notification Service (Amazon SNS) topic向安全团队发送消息。 选项D：使用AWS Systems Manager Automation将Systems Manager Agent构建到自定义AMI中。配置AWS Config将SCP附加到根组织账户，以允许EC2实例连接到Systems Manager。使用Systems Manager Session Manager登录实例。启用会话详细信息记录到Amazon S3。为新文件上传创建S3事件通知，通过Amazon Simple Notification Service (Amazon SNS) topic向安全团队发送消息。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 为无互联网访问的EC2实例提供安全的远程访问 2. 实现集中化和自动化的访问控制 3. 当实例被访问时通知安全团队 4. 跨多个AWS账户的组织环境中运行 5. 维护现有的自定义AMI **涉及的关键AWS服务和概念：** - AWS Systems Manager Session Manager：提供安全的shell访问，无需SSH密钥或bastion host - EC2 Image Builder：用于自动化AMI构建和管理 - IAM角色AmazonSSMManagedInstanceCore：为EC2实例提供Systems Manager所需权限 - Amazon S3：存储会话日志 - Amazon SNS：发送通知给安全团队 - VPC Endpoints：在无互联网环境中访问AWS服务 **正确答案C的原因：** 1. **EC2 Image Builder**：提供了标准化、自动化的AMI构建流程，确保Systems Manager Agent的正确集成 2. **AmazonSSMManagedInstanceCore角色**：为EC2实例提供了访问Systems Manager所需的最小权限 3. **Session Manager**：完美适合无互联网环境，通过VPC endpoints工作，提供安全的shell访问 4. **S3日志记录 + SNS通知**：实现了完整的审计跟踪和实时通知机制 5. **无需互联网访问**：整个解决方案通过AWS内部网络和VPC endpoints工作 **其他选项错误的原因：** - **选项A**：EC2 Instance Connect需要互联网访问，与题目要求矛盾；EventBridge规则配置复杂且不是最佳实践 - **选项B**：部署NAT gateway和bastion host违背了无互联网访问的安全要求，增加了攻击面和复杂性 - **选项D**：使用AWS Config附加SCP的方法不正确，SCP应该在组织级别管理，而不是通过Config；Systems Manager Automation用于AMI构建不如Image Builder专业 **决策标准和最佳实践：** 1. **安全优先**：Session Manager提供零信任访问，无需SSH密钥管理 2. **自动化**：Image Builder自动化AMI构建和更新流程 3. **审计合规**：完整的会话日志记录和通知机制 4. **最小权限原则**：使用AWS托管的IAM角色，权限精确控制 5. **运维简化**：无需维护bastion host或NAT gateway等额外基础设施</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">114</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses Amazon S3 to store proprietary information. The development team creates buckets for new projects on a daily basis. The security team wants to ensure that all existing and future buckets have encryption, logging, and versioning enabled. Additionally, no buckets should ever be publicly read or write accessible. What should a DevOps engineer do to meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable AWS CloudTrail and configure automatic remediation using AWS Lambda.
B. Enable AWS Config rules and configure automatic remediation using AWS Systems Manager documents.
C. Enable AWS Trusted Advisor and configure automatic remediation using Amazon EventBridge.
D. Enable AWS Systems Manager and configure automatic remediation using Systems Manager documents.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用Amazon S3存储专有信息。开发团队每天为新项目创建bucket。安全团队希望确保所有现有和未来的bucket都启用加密、日志记录和版本控制。此外，任何bucket都不应该允许公共读取或写入访问。DevOps工程师应该怎么做来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现对S3 bucket的自动化合规性管理，包括：1）确保所有bucket启用加密、日志记录和版本控制；2）防止bucket被公开访问；3）对现有和未来创建的bucket都要生效；4）需要自动化remediation（修复）机制。 **涉及的关键AWS服务和概念：** - AWS Config：用于监控和评估AWS资源配置合规性的服务 - AWS Config Rules：预定义或自定义的规则，用于检查资源配置是否符合要求 - AWS Systems Manager Documents：自动化文档，可以执行remediation操作 - S3安全最佳实践：加密、版本控制、访问日志、防止公共访问 **正确答案B的原因：** AWS Config是专门用于资源配置合规性监控的服务。它可以：1）持续监控S3 bucket配置；2）使用预置规则检查加密、版本控制、公共访问等设置；3）通过Systems Manager Documents实现自动remediation；4）对新创建的资源自动应用规则检查。这完全符合题目要求的自动化合规性管理需求。 **其他选项错误的原因：** A选项：CloudTrail主要用于API调用审计，不是配置合规性监控工具，无法检查bucket配置状态。 C选项：Trusted Advisor提供最佳实践建议，但不提供持续的配置监控和自动remediation功能。 D选项：Systems Manager本身不提供配置合规性监控，需要与Config配合使用才能实现完整的解决方案。 **决策标准和最佳实践：** 选择AWS Config的关键在于：1）需要持续监控资源配置合规性；2）需要自动化检测和修复；3）需要覆盖现有和未来资源。AWS Config专门为此类场景设计，是配置管理和合规性监控的标准解决方案。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">115</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is researching the least expensive way to implement an image batch processing cluster on AWS. The application cannot run in Docker containers and must run on Amazon EC2. The batch job stores checkpoint data on an NFS volume and can tolerate interruptions. Configuring the cluster software from a generic EC2 Linux image takes 30 minutes. What is the MOST cost-effective solution? D (87%) C (9%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Amazon EFS for checkpoint data. To complete the job, use an EC2 Auto Scaling group and an On-Demand pricing model to provision EC2 instances temporarily.
B. Use GlusterFS on EC2 instances for checkpoint data. To run the batch job, configure EC2 instances manually. When the job completes, shut down the instances manually.
C. Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances, and utilize user data to configure the EC2 Linux instance on startup.
D. Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances. Create a custom AMI for the cluster and use the latest AMI when creating instances.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在研究在AWS上实现图像批处理集群的最经济方式。该应用程序无法在Docker容器中运行，必须在Amazon EC2上运行。批处理作业将检查点数据存储在NFS卷上，可以容忍中断。从通用EC2 Linux镜像配置集群软件需要30分钟。什么是最具成本效益的解决方案？ 选项： A. 使用Amazon EFS存储检查点数据。为完成作业，使用EC2 Auto Scaling组和On-Demand定价模式临时配置EC2实例。 B. 使用EC2实例上的GlusterFS存储检查点数据。为运行批处理作业，手动配置EC2实例。作业完成后，手动关闭实例。 C. 使用Amazon EFS存储检查点数据。使用EC2 Fleet启动EC2 Spot Instances，并利用user data在启动时配置EC2 Linux实例。 D. 使用Amazon EFS存储检查点数据。使用EC2 Fleet启动EC2 Spot Instances。为集群创建自定义AMI，并在创建实例时使用最新的AMI。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到最经济的批处理集群解决方案，关键约束条件包括：必须运行在EC2上、需要NFS存储、可容忍中断、配置时间30分钟。 **涉及的关键AWS服务和概念：** - Amazon EFS：托管的NFS文件系统服务 - EC2 Spot Instances：利用空闲容量的低成本实例 - EC2 Fleet：批量管理EC2实例的服务 - User Data：实例启动时自动执行的脚本 - 自定义AMI：预配置的机器镜像 - Auto Scaling：自动扩缩容服务 **正确答案C的原因：** 1. **存储选择**：Amazon EFS提供托管的NFS服务，无需维护，成本效益高 2. **计算成本**：Spot Instances比On-Demand实例便宜60-90%，适合可容忍中断的工作负载 3. **管理效率**：EC2 Fleet可以高效管理Spot Instances的生命周期 4. **配置方式**：User data脚本在启动时自动配置，无需人工干预，虽然每次需要30分钟但完全自动化 **其他选项错误的原因：** - **选项A**：使用On-Demand实例成本过高，不符合最经济的要求 - **选项B**：GlusterFS需要手动管理和维护，增加运维成本；手动配置和关闭实例效率低下 - **选项D**：创建和维护自定义AMI增加了额外的管理开销和存储成本，对于30分钟的配置时间来说不够经济 **决策标准和最佳实践：** 1. **成本优化**：优先选择Spot Instances用于可中断的批处理工作负载 2. **自动化优先**：选择自动化程度高的解决方案减少人工干预 3. **托管服务**：优先使用AWS托管服务（如EFS）而非自建解决方案 4. **配置时间权衡**：30分钟的配置时间通过user data自动化是可接受的，无需为此创建自定义AMI</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">116</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company recently migrated its legacy application from on-premises to AWS. The application is hosted on Amazon EC2 instances behind an Application Load Balancer, which is behind Amazon API Gateway. The company wants to ensure users experience minimal disruptions during any deployment of a new version of the application. The company also wants to ensure it can quickly roll back updates if there is an issue. Which solution will meet these requirements with MINIMAL changes to the application? A (83%) Other</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Introduce changes as a separate environment parallel to the existing one. Configure API Gateway to use a canary release deployment to send a small subset of user traffic to the new environment.
B. Introduce changes as a separate environment parallel to the existing one. Update the application&#x27;s DNS alias records to point to the new environment.
C. Introduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route user traffic to the new target group in steps.
D. Introduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route all traffic to the Application Load Balancer, which then sends the traffic to the new target group.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司最近将其遗留应用程序从本地迁移到AWS。该应用程序托管在Amazon EC2实例上，位于Application Load Balancer后面，而Application Load Balancer又位于Amazon API Gateway后面。公司希望确保用户在部署新版本应用程序时体验到最小的中断。公司还希望确保在出现问题时能够快速回滚更新。哪种解决方案能够以对应用程序的最小更改来满足这些要求？ 选项： A. 将更改作为与现有环境并行的独立环境引入。配置API Gateway使用canary release部署，将一小部分用户流量发送到新环境。 B. 将更改作为与现有环境并行的独立环境引入。更新应用程序的DNS别名记录以指向新环境。 C. 将更改作为现有Application Load Balancer后面的独立目标组引入。配置API Gateway分步骤将用户流量路由到新目标组。 D. 将更改作为现有Application Load Balancer后面的独立目标组引入。配置API Gateway将所有流量路由到Application Load Balancer，然后由负载均衡器将流量发送到新目标组。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个部署策略，能够实现：1）最小化用户中断；2）快速回滚能力；3）对应用程序的最小更改。 **涉及的关键AWS服务和概念：** - API Gateway：提供API管理和流量路由功能 - Application Load Balancer (ALB)：七层负载均衡器，支持目标组管理 - Target Group：ALB的流量分发目标，可以包含多个EC2实例 - Canary部署：渐进式部署策略，先向少量用户发布新版本 - 蓝绿部署：通过并行环境实现零停机部署 **正确答案C的原因：** 1. **最小架构变更**：利用现有的ALB，只需添加新的目标组，无需创建完整的并行环境 2. **渐进式部署**：API Gateway可以配置加权路由，逐步将流量从旧目标组转移到新目标组 3. **快速回滚**：通过调整API Gateway的路由权重，可以立即将流量切回旧目标组 4. **成本效益**：不需要维护完整的并行基础设施 **其他选项错误的原因：** - **选项A**：创建完整并行环境成本高，虽然canary部署策略正确，但不符合&quot;最小更改&quot;要求 - **选项B**：DNS更改传播时间长（TTL影响），无法实现快速回滚，且DNS切换是全量切换，不够渐进 - **选项D**：描述不准确，ALB无法智能地只将流量发送到&quot;新&quot;目标组，需要通过API Gateway层面的路由控制 **决策标准和最佳实践：** 1. **渐进式部署优于一次性切换**：能够及早发现问题并限制影响范围 2. **利用现有基础设施**：在满足需求的前提下，最大化利用现有资源 3. **快速回滚能力**：部署策略必须支持快速、可靠的回滚机制 4. **流量控制粒度**：API Gateway提供了更精细的流量控制能力，比DNS切换更适合渐进式部署</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">117</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is storing 100 GB of log data in .csv format in an Amazon S3 bucket. SQL developers want to query this data and generate graphs to visualize it. The SQL developers also need an efficient, automated way to store metadata from the .csv file. Which combination of steps will meet these requirements with the LEAST amount of effort? (Choose three.) F. Use Amazon DynamoDB as the persistent metadata store. BCE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Filter the data through AWS X-Ray to visualize the data.
B. Filter the data through Amazon QuickSight to visualize the data.
C. Query the data with Amazon Athena.
D. Query the data with Amazon Redshift.
E. Use the AWS Glue Data Catalog as the persistent metadata store.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Amazon S3存储桶中以.csv格式存储了100 GB的日志数据。SQL开发人员希望查询这些数据并生成图表来可视化它。SQL开发人员还需要一种高效、自动化的方式来存储.csv文件的元数据。哪种步骤组合能够以最少的工作量满足这些要求？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案来：1）查询S3中的CSV数据，2）可视化数据生成图表，3）自动化存储元数据，4）以最少工作量实现。需要选择三个选项的组合。 **涉及的关键AWS服务和概念：** - Amazon S3：对象存储服务，存储CSV文件 - Amazon Athena：无服务器查询服务，可直接查询S3数据 - Amazon QuickSight：商业智能和数据可视化服务 - AWS Glue Data Catalog：托管的元数据存储库 - Amazon Redshift：数据仓库服务 - AWS X-Ray：应用程序跟踪服务 **正确答案BCE的原因：** - B (QuickSight)：专门的数据可视化服务，可直接连接多种数据源生成图表，满足可视化需求 - C (Athena)：无服务器SQL查询服务，可直接查询S3中的CSV文件，无需数据迁移，满足SQL查询需求 - E (Glue Data Catalog)：自动发现和存储元数据，与Athena无缝集成，满足自动化元数据管理需求 **其他选项错误的原因：** - A (X-Ray)：用于应用程序性能监控和调试，不是数据可视化工具 - D (Redshift)：需要先将数据从S3加载到Redshift集群，增加了复杂性和成本，不符合&quot;最少工作量&quot;要求 - F (DynamoDB)：NoSQL数据库，不适合作为结构化元数据存储，且需要额外开发工作 **决策标准和最佳实践：** 选择标准应基于：1）服务间的原生集成能力，2）无服务器架构减少管理开销，3）直接操作S3数据避免数据迁移，4）自动化程度高。Athena + QuickSight + Glue Data Catalog组合提供了完整的无服务器数据分析栈，是处理S3中结构化数据的最佳实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">118</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company deploys its corporate infrastructure on AWS across multiple AWS Regions and Availability Zones. The infrastructure is deployed on Amazon EC2 instances and connects with AWS IoT Greengrass devices. The company deploys additional resources on on-premises servers that are located in the corporate headquarters. The company wants to reduce the overhead involved in maintaining and updating its resources. The company&#x27;s DevOps team plans to use AWS Systems Manager to implement automated management and application of patches. The DevOps team confirms that Systems Manager is available in the Regions that the resources are deployed in. Systems Manager also is available in a Region near the corporate headquarters. Which combination of steps must the DevOps team take to implement automated patch and configuration management across the company&#x27;s EC2 instances, IoT devices, and on-premises infrastructure? (Choose three.) F. Generate a managed-instance activation. Use the Activation Code and Activation ID to install Systems Manager Agent (SSM Agent) on each server in the on-premises environment. Update the AWS IoT Greengrass IAM token exchange role. Use the role to deploy SSM Agent on all the IoT devices. Most Voted CEF (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Apply tags to all the EC2 instances, AWS IoT Greengrass devices, and on-premises servers. Use Systems Manager Session Manager to push patches to all the tagged devices.
B. Use Systems Manager Run Command to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises servers.
C. Use Systems Manager Patch Manager to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises servers as a Systems Manager maintenance window task.
D. Configure Amazon EventBridge to monitor Systems Manager Patch Manager for updates to patch baselines. Associate Systems Manager Run Command with the event to initiate a patch action for all EC2 instances, AWS IoT Greengrass devices, and on-premises servers.
E. Create an IAM instance profile for Systems Manager. Attach the instance profile to all the EC2 instances in the AWS account. For the AWS IoT Greengrass devices and on-premises servers, create an IAM service role for Systems Manager.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在多个AWS区域和可用区部署其企业基础设施。基础设施部署在Amazon EC2实例上，并与AWS IoT Greengrass设备连接。该公司还在位于企业总部的本地服务器上部署了额外资源。公司希望减少维护和更新资源的开销。公司的DevOps团队计划使用AWS Systems Manager来实现补丁的自动化管理和应用。DevOps团队确认Systems Manager在资源部署的区域中可用，在企业总部附近的区域中也可用。DevOps团队必须采取哪些步骤组合来在公司的EC2实例、IoT设备和本地基础设施中实现自动化补丁和配置管理？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求选择三个步骤来实现跨EC2实例、IoT Greengrass设备和本地服务器的自动化补丁和配置管理。需要使用AWS Systems Manager来减少维护开销。 **涉及的关键AWS服务和概念：** - AWS Systems Manager：统一的运维管理服务 - Systems Manager Agent (SSM Agent)：在实例上运行的代理程序 - Patch Manager：自动化补丁管理服务 - Run Command：远程执行命令的功能 - Maintenance Windows：维护窗口调度 - IAM角色和实例配置文件：权限管理 - 混合环境管理：AWS云端和本地资源的统一管理 **正确答案分析（C、E、F）：** - **选项C**：使用Patch Manager作为维护窗口任务来调度补丁是最佳实践，提供了自动化和调度功能 - **选项E**：为EC2实例创建IAM实例配置文件，为IoT设备和本地服务器创建IAM服务角色，这是必需的权限配置 - **选项F**：生成托管实例激活码来在本地服务器上安装SSM Agent，更新IoT Greengrass IAM令牌交换角色来在IoT设备上部署SSM Agent，这是混合环境的必要步骤 **其他选项错误的原因：** - **选项A**：Session Manager主要用于交互式会话，不是补丁管理的最佳工具 - **选项B**：Run Command可以执行补丁，但缺乏Patch Manager的自动化调度和基线管理功能 - **选项D**：通过EventBridge监控补丁基线更新过于复杂，不是标准的补丁管理方法 **决策标准和最佳实践：** 1. 使用专门的补丁管理工具（Patch Manager）而不是通用命令执行工具 2. 正确配置IAM权限以支持混合环境 3. 为不同类型的资源（EC2、IoT、本地）采用适当的SSM Agent部署方法 4. 利用维护窗口实现自动化调度，减少人工干预</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">119</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is testing a web application that runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company uses a blue/green deployment process with immutable instances when deploying new software. During testing, users are being automatically logged out of the application at random times. Testers also report that, when a new version of the application is deployed, all users are logged out. The development team needs a solution to ensure users remain logged in across scaling events and application deployments. What is the MOST operationally efficient way to ensure users remain logged in? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable smart sessions on the load balancer and modify the application to check for an existing session.
B. Enable session sharing on the load balancer and modify the application to read from the session store.
C. Store user session information in an Amazon S3 bucket and modify the application to read session information from the bucket.
D. Modify the application to store user session information in an Amazon ElastiCache cluster.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在测试一个运行在Application Load Balancer后面的Amazon EC2实例上的Web应用程序。这些实例在Auto Scaling组中跨多个Availability Zone运行。公司在部署新软件时使用蓝/绿部署流程和不可变实例。在测试期间，用户会在随机时间被自动登出应用程序。测试人员还报告说，当部署新版本的应用程序时，所有用户都会被登出。开发团队需要一个解决方案来确保用户在扩展事件和应用程序部署期间保持登录状态。确保用户保持登录状态的最具运营效率的方法是什么？ 选项： A. 在负载均衡器上启用智能会话并修改应用程序以检查现有会话 B. 在负载均衡器上启用会话共享并修改应用程序从会话存储中读取 C. 将用户会话信息存储在Amazon S3存储桶中并修改应用程序从存储桶读取会话信息 D. 修改应用程序将用户会话信息存储在Amazon ElastiCache集群中 正确答案：D</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求解决用户会话在Auto Scaling事件和蓝/绿部署期间丢失的问题。关键挑战是当EC2实例被终止和替换时（无论是由于扩展还是部署），存储在本地的会话数据会丢失，导致用户被强制登出。 **涉及的关键AWS服务和概念：** - Application Load Balancer (ALB)：负载分发和会话粘性 - Auto Scaling Group：自动扩展和收缩实例 - Amazon ElastiCache：内存缓存服务，支持Redis和Memcached - Amazon S3：对象存储服务 - 蓝/绿部署：使用不可变基础设施的部署策略 - 会话管理：维护用户登录状态的机制 **正确答案D的原因：** 1. **高性能**：ElastiCache是内存数据库，提供毫秒级的读写延迟，非常适合频繁访问的会话数据 2. **持久性和可用性**：会话数据存储在独立于EC2实例的外部服务中，实例终止不会影响会话 3. **可扩展性**：ElastiCache可以根据需要扩展，支持高并发访问 4. **运营效率高**：AWS托管服务，减少运维负担，支持自动故障转移和备份 5. **专门用途**：ElastiCache专为缓存和会话存储等用例设计 **其他选项错误的原因：** - **选项A**：ALB没有&quot;智能会话&quot;功能，这不是AWS的标准功能 - **选项B**：ALB没有内置的&quot;会话共享&quot;功能，粘性会话仍然依赖特定实例 - **选项C**：S3虽然持久可靠，但不适合频繁的读写操作，延迟较高，会影响用户体验，且成本较高 **决策标准和最佳实践：** 1. **外部化会话存储**：将会话数据从应用服务器中分离出来 2. **选择合适的存储服务**：根据访问模式选择高性能的缓存服务 3. **考虑运营复杂性**：选择托管服务减少运维负担 4. **性能优先**：会话数据需要快速访问，内存缓存是最佳选择 5. **高可用性设计**：确保会话存储服务本身具有高可用性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">120</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer needs to configure a blue/green deployment for an existing three-tier application. The application runs on Amazon EC2 instances and uses an Amazon RDS database. The EC2 instances run behind an Application Load Balancer (ALB) and are in an Auto Scaling group. The DevOps engineer has created a launch template and an Auto Scaling group for the blue environment. The DevOps engineer also has created a launch template and an Auto Scaling group for the green environment. Each Auto Scaling group deploys to a matching blue or green target group. The target group also specifies which software, blue or green, gets loaded on the EC2 instances. The ALB can be configured to send traffic to the blue environment&#x27;s target group or the green environment&#x27;s target group. An Amazon Route 53 record for www.example.com points to the ALB. The deployment must move traffic all at once between the software on the blue environment&#x27;s EC2 instances to the newly deployed software on the green environment&#x27;s EC2 instances. What should the DevOps engineer do to meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment&#x27;s EC2 instances. When the rolling restart is complete, use an AWS CLI command to update the ALB to send traffic to the green environment&#x27;s target group.
B. Use an AWS CLI command to update the ALB to send traffic to the green environment&#x27;s target group. Then start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment&#x27;s EC2 instances.
C. Update the launch template to deploy the green environment&#x27;s software on the blue environment&#x27;s EC2 instances. Keep the target groups and Auto Scaling groups unchanged in both environments. Perform a rolling restart of the blue environment&#x27;s EC2 instances.
D. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment&#x27;s EC2 instances. When the rolling restart is complete, update the Route 53 DNS to point to the green environment&#x27;s endpoint on the ALB.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师需要为现有的三层应用程序配置蓝绿部署。该应用程序运行在Amazon EC2实例上并使用Amazon RDS数据库。EC2实例运行在Application Load Balancer (ALB)后面，并位于Auto Scaling组中。DevOps工程师已经为蓝色环境创建了启动模板和Auto Scaling组，也为绿色环境创建了启动模板和Auto Scaling组。每个Auto Scaling组部署到匹配的蓝色或绿色目标组。目标组还指定在EC2实例上加载哪个软件（蓝色或绿色）。ALB可以配置为将流量发送到蓝色环境的目标组或绿色环境的目标组。Amazon Route 53记录www.example.com指向ALB。部署必须一次性将流量从蓝色环境EC2实例上的软件切换到绿色环境EC2实例上新部署的软件。DevOps工程师应该怎么做来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是蓝绿部署的正确实施步骤。关键要求是&quot;一次性切换所有流量&quot;从蓝色环境到绿色环境，这是蓝绿部署的核心特征。 **涉及的关键AWS服务和概念：** - 蓝绿部署策略：两个完全相同的生产环境，通过切换流量实现零停机部署 - Application Load Balancer (ALB)：负载均衡器，可以在目标组之间切换流量 - Auto Scaling组：自动扩缩容服务 - 目标组：ALB的流量分发目标 - Route 53：DNS服务 **正确答案A的原因：** 1. **正确的执行顺序**：先准备好绿色环境（rolling restart部署新软件），再切换流量 2. **确保服务可用性**：在切换流量之前，绿色环境已经完全准备就绪并通过健康检查 3. **真正的蓝绿部署**：通过ALB在目标组级别进行流量切换，实现瞬时切换 4. **风险控制**：如果绿色环境有问题，可以在切换流量前发现，避免影响用户 **其他选项错误的原因：** - **选项B**：先切换流量再部署软件，会导致用户访问到未准备好的环境，造成服务中断 - **选项C**：在蓝色环境上进行rolling restart不是蓝绿部署，而是滚动部署，不符合题目要求的&quot;一次性切换&quot; - **选项D**：通过Route 53 DNS切换存在DNS缓存问题，无法实现真正的&quot;一次性&quot;切换，且DNS传播有延迟 **决策标准和最佳实践：** 1. **蓝绿部署的黄金法则**：先准备，后切换 2. **健康检查优先**：确保新环境完全健康后再切换流量 3. **使用ALB而非DNS切换**：ALB切换是瞬时的，DNS切换有缓存和传播延迟 4. **保持环境独立性**：蓝绿两个环境应该完全独立，便于快速回滚</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">121</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is building a new pipeline by using AWS CodePipeline and AWS CodeBuild in a build account. The pipeline consists of two stages. The first stage is a CodeBuild job to build and package an AWS Lambda function. The second stage consists of deployment actions that operate on two different AWS accounts: a development environment account and a production environment account. The deployment stages use the AWS CloudFormation action that CodePipeline invokes to deploy the infrastructure that the Lambda function requires. A DevOps engineer creates the CodePipeline pipeline and configures the pipeline to encrypt build artifacts by using the AWS Key Management Service (AWS KMS) AWS managed key for Amazon S3 (the aws/s3 key). The artifacts are stored in an S3 bucket. When the pipeline runs, the CloudFormation actions fail with an access denied error. Which combination of actions must the DevOps engineer perform to resolve this error? (Choose two.) action to copy the artifacts to the S3 bucket in each AWS account. Update the CloudFormation actions to reference the artifacts S3 bucket in the production account. perform decrypt operations. Modify the pipeline to use the customer managed KMS key to encrypt artifacts. Most Voted BE (83%) Other</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an S3 bucket in each AWS account for the artifacts. Allow the pipeline to write to the S3 buckets. Create a CodePipeline S3
B. Create a customer managed KMS key. Configure the KMS key policy to allow the IAM roles used by the CloudFormation action to
C. Create an AWS managed KMS key. Configure the KMS key policy to allow the development account and the production account to perform decrypt operations. Modify the pipeline to use the KMS key to encrypt artifacts.
D. In the development account and in the production account, create an IAM role for CodePipeline. Configure the roles with permissions to perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline account, configure the CodePipeline CloudFormation action to use the roles.
E. In the development account and in the production account, create an IAM role for CodePipeline. Configure the roles with permissions to perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline account, modify the artifacts S3 bucket policy to allow the roles access. Configure the CodePipeline CloudFormation action to use the roles.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS CodePipeline和AWS CodeBuild在构建账户中构建新的管道。该管道包含两个阶段。第一阶段是CodeBuild作业，用于构建和打包AWS Lambda函数。第二阶段包含在两个不同AWS账户上运行的部署操作：开发环境账户和生产环境账户。部署阶段使用CodePipeline调用的AWS CloudFormation操作来部署Lambda函数所需的基础设施。DevOps工程师创建了CodePipeline管道，并配置管道使用AWS Key Management Service (AWS KMS) AWS托管密钥for Amazon S3 (aws/s3密钥)来加密构建工件。工件存储在S3存储桶中。当管道运行时，CloudFormation操作失败并出现访问拒绝错误。DevOps工程师必须执行哪些操作组合来解决此错误？（选择两个。） 选项： A. 在每个AWS账户中为工件创建S3存储桶。允许管道写入S3存储桶。创建CodePipeline S3操作将工件复制到每个AWS账户中的S3存储桶。更新CloudFormation操作以引用生产账户中的工件S3存储桶。 B. 创建客户托管KMS密钥。配置KMS密钥策略以允许CloudFormation操作使用的IAM角色执行解密操作。修改管道以使用客户托管KMS密钥来加密工件。 C. 创建AWS托管KMS密钥。配置KMS密钥策略以允许开发账户和生产账户执行解密操作。修改管道以使用KMS密钥来加密工件。 D. 在开发账户和生产账户中，为CodePipeline创建IAM角色。配置角色具有执行CloudFormation操作的权限以及从工件S3存储桶检索和解密对象的权限。在CodePipeline账户中，配置CodePipeline CloudFormation操作使用这些角色。 E. 在开发账户和生产账户中，为CodePipeline创建IAM角色。配置角色具有执行CloudFormation操作的权限以及从工件S3存储桶检索和解密对象的权限。在CodePipeline账户中，修改工件S3存储桶策略以允许角色访问。配置CodePipeline CloudFormation操作使用这些角色。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是跨账户CodePipeline部署时的权限和加密问题。当前问题是CloudFormation操作在访问加密工件时出现访问拒绝错误，需要解决跨账户访问KMS加密的S3对象的权限问题。 **涉及的关键AWS服务和概念：** 1. AWS CodePipeline - 持续集成/持续部署管道 2. AWS KMS - 密钥管理服务，包括AWS托管密钥和客户托管密钥 3. 跨账户访问控制和IAM角色 4. S3存储桶策略和对象加密 5. CloudFormation跨账户部署 **正确答案的原因：** 选项B和E是正确答案。 选项B正确的原因： - AWS托管的aws/s3密钥策略无法修改，不支持跨账户访问 - 客户托管KMS密钥允许自定义密钥策略，可以授权其他账户的IAM角色进行解密操作 - 这是解决跨账户KMS加密对象访问的标准做法 选项E正确的原因： - 跨账户CloudFormation部署需要在目标账户中创建专门的IAM角色 - 需要同时配置IAM角色权限和S3存储桶策略来实现跨账户访问 - CodePipeline可以assume这些跨账户角色来执行部署操作 **其他选项错误的原因：** 选项A：创建多个S3存储桶会增加复杂性，且没有解决KMS权限问题，不是最佳实践。 选项C：AWS托管KMS密钥的策略无法修改，无法添加跨账户权限，这在技术上是不可行的。 选项D：缺少S3存储桶策略的配置，仅有IAM角色权限不足以实现跨账户S3访问。 **决策标准和最佳实践：** 1. 跨账户KMS访问必须使用客户托管密钥 2. 跨账户资源访问需要同时配置IAM权限和资源策略 3. 使用assume role模式进行跨账户操作是AWS推荐的安全实践 4. 最小权限原则：只授予必要的权限给特定的IAM角色</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">122</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using an organization in AWS Organizations to manage multiple AWS accounts. The company&#x27;s development team wants to use AWS Lambda functions to meet resiliency requirements and is rewriting all applications to work with Lambda functions that are deployed in a VPC. The development team is using Amazon Elastic File System (Amazon EFS) as shared storage in Account A in the organization. The company wants to continue to use Amazon EFS with Lambda. Company policy requires all serverless projects to be deployed in Account B. A DevOps engineer needs to reconfigure an existing EFS file system to allow Lambda functions to access the data through an existing EFS access point. Which combination of steps should the DevOps engineer take to meet these requirements? (Choose three.) F. Configure the Lambda functions in Account B to assume an existing IAM role in Account A. ADE (71%) AEF (21%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A.
B. Create SCPs to set permission guardrails with fine-grained control for Amazon EFS.
C. Create a new EFS file system in Account B. Use AWS Database Migration Service (AWS DMS) to keep data from Account A and Account B synchronized.
D. Update the Lambda execution roles with permission to access the VPC and the EFS file system.
E. Create a VPC peering connection to connect Account A to Account B.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS Organizations中的组织来管理多个AWS账户。公司的开发团队希望使用AWS Lambda函数来满足弹性要求，并正在重写所有应用程序以使用部署在VPC中的Lambda函数。开发团队在组织中的账户A中使用Amazon Elastic File System (Amazon EFS)作为共享存储。公司希望继续在Lambda中使用Amazon EFS。公司政策要求所有serverless项目都部署在账户B中。DevOps工程师需要重新配置现有的EFS文件系统，以允许Lambda函数通过现有的EFS访问点访问数据。DevOps工程师应该采取哪些步骤组合来满足这些要求？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是跨AWS账户访问EFS文件系统的配置。核心需求是让账户B中的Lambda函数能够访问账户A中现有的EFS文件系统，同时满足公司将serverless项目部署在账户B的政策要求。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理 - AWS Lambda：serverless计算服务 - Amazon EFS：弹性文件系统，支持跨账户访问 - VPC：虚拟私有云网络 - IAM角色和权限：跨账户访问控制 - EFS文件系统策略：资源级访问控制 **正确答案的原因：** 根据题目显示的统计数据，ADE组合占71%，AEF组合占21%，说明A选项是必选项。 - **选项A**：更新EFS文件系统策略以提供账户B访问权限是跨账户EFS访问的核心要求，这是资源级权限控制的关键步骤 - **选项D**：更新Lambda执行角色权限以访问VPC和EFS文件系统是必需的，Lambda需要适当的IAM权限才能访问EFS - **选项E**：创建VPC对等连接是实现跨账户网络连通性的必要条件，因为EFS需要通过网络访问 **其他选项错误的原因：** - **选项B**：SCP（服务控制策略）主要用于限制权限而非授予细粒度访问权限，不是解决跨账户EFS访问的直接方案 - **选项C**：创建新的EFS文件系统并使用DMS同步违背了题目要求继续使用现有EFS文件系统的要求，且DMS主要用于数据库迁移而非文件系统同步 - **选项F**：让账户B的Lambda函数assume账户A的IAM角色虽然可行，但增加了复杂性，且不如直接配置EFS文件系统策略来得直接有效 **决策标准和最佳实践：** 1. 跨账户资源访问应优先使用资源策略（如EFS文件系统策略） 2. 网络连通性是跨账户VPC资源访问的前提条件 3. 遵循最小权限原则，给Lambda执行角色分配必要的访问权限 4. 尽量使用现有资源而非创建重复资源，避免数据同步的复杂性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">123</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A media company has several thousand Amazon EC2 instances in an AWS account. The company is using Slack and a shared email inbox for team communications and important updates. A DevOps engineer needs to send all AWS-scheduled EC2 maintenance notifications to the Slack channel and the shared inbox. The solution must include the instances&#x27; Name and Owner tags. Which solution will meet these requirements? EC2 maintenance notifications to Amazon Simple Notification Service (Amazon SNS). Configure Amazon SNS to target the Slack channel and the shared inbox. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Integrate AWS Trusted Advisor with AWS Config. Configure a custom AWS Config rule to invoke an AWS Lambda function to publish notifications to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe a Slack channel endpoint and the shared inbox to the topic.
B. Use Amazon EventBridge to monitor for AWS Health events. Configure the maintenance events to target an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to send notifications to the Slack channel and the shared inbox.
C. Create an AWS Lambda function that sends EC2 maintenance notifications to the Slack channel and the shared inbox. Monitor EC2 health events by using Amazon CloudWatch metrics. Configure a CloudWatch alarm that invokes the Lambda function when a maintenance notification is received.
D. Configure AWS Support integration with AWS CloudTrail. Create a CloudTrail lookup event to invoke an AWS Lambda function to pass</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家媒体公司在AWS账户中有数千个Amazon EC2实例。该公司使用Slack和共享邮箱进行团队沟通和重要更新。DevOps工程师需要将所有AWS计划的EC2维护通知发送到Slack频道和共享邮箱。解决方案必须包含实例的Name和Owner标签。哪个解决方案能满足这些要求？ 选项： A. 将AWS Trusted Advisor与AWS Config集成。配置自定义AWS Config规则来调用AWS Lambda函数，将通知发布到Amazon Simple Notification Service (Amazon SNS)主题。将Slack频道端点和共享邮箱订阅该主题。 B. 使用Amazon EventBridge监控AWS Health事件。配置维护事件以Amazon Simple Notification Service (Amazon SNS)主题为目标。将AWS Lambda函数订阅到SNS主题，向Slack频道和共享邮箱发送通知。 C. 创建AWS Lambda函数，将EC2维护通知发送到Slack频道和共享邮箱。使用Amazon CloudWatch指标监控EC2健康事件。配置CloudWatch警报，在收到维护通知时调用Lambda函数。 D. 配置AWS Support与AWS CloudTrail的集成。创建CloudTrail查找事件来调用AWS Lambda函数传递...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个解决方案，能够自动捕获AWS计划的EC2维护通知，并将这些通知（包含实例的Name和Owner标签信息）同时发送到Slack频道和共享邮箱。 **涉及的关键AWS服务和概念：** - AWS Health API：提供AWS服务健康状态和维护事件信息 - Amazon EventBridge：事件驱动架构的核心服务，能够监控和路由AWS服务事件 - Amazon SNS：消息通知服务，支持多种订阅方式 - AWS Lambda：无服务器计算服务，用于处理事件和自定义逻辑 - EC2标签：实例元数据，需要通过API获取 **正确答案B的原因：** 1. **事件源正确**：Amazon EventBridge是监控AWS Health事件的标准方式，能够直接捕获EC2维护通知 2. **架构合理**：EventBridge → SNS → Lambda的架构既解耦又高效 3. **功能完整**：Lambda函数可以获取EC2实例标签信息，并格式化消息发送到多个目标 4. **扩展性好**：SNS支持多种订阅方式，便于后续扩展 **其他选项错误的原因：** - **选项A**：AWS Trusted Advisor主要用于成本优化和安全建议，不是监控维护事件的合适工具；AWS Config用于配置合规性检查，与维护通知无关 - **选项C**：CloudWatch指标无法直接监控EC2维护事件，这类事件需要通过AWS Health API获取，不是通过指标 - **选项D**：CloudTrail记录API调用日志，不是获取维护通知的正确方式；且选项描述不完整 **决策标准和最佳实践：** 1. **选择正确的事件源**：AWS Health事件应通过EventBridge监控 2. **松耦合架构**：使用SNS作为中间层，实现发布-订阅模式 3. **功能分离**：Lambda专注于消息格式化和多渠道发送 4. **可维护性**：标准AWS服务组合，便于运维和故障排查</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">124</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An AWS CodePipeline pipeline has implemented a code release process. The pipeline is integrated with AWS CodeDeploy to deploy versions of an application to multiple Amazon EC2 instances for each CodePipeline stage. During a recent deployment, the pipeline failed due to a CodeDeploy issue. The DevOps team wants to improve monitoring and notifications during deployment to decrease resolution times. What should the DevOps engineer do to create notifications when issues are discovered? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Implement Amazon CloudWatch Logs for CodePipeline and CodeDeploy, create an AWS Config rule to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.
B. Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.
C. Implement AWS CloudTrail to record CodePipeline and CodeDeploy API call information, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.
D. Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an Amazon Inspector assessment target to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个AWS CodePipeline流水线已经实施了代码发布流程。该流水线与AWS CodeDeploy集成，为每个CodePipeline阶段将应用程序版本部署到多个Amazon EC2实例。在最近的一次部署中，流水线由于CodeDeploy问题而失败。DevOps团队希望改进部署期间的监控和通知，以减少解决时间。DevOps工程师应该做什么来在发现问题时创建通知？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个监控和通知解决方案，能够在CodePipeline和CodeDeploy部署过程中出现问题时及时发现并通知相关人员，从而减少故障解决时间。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：持续集成/持续部署(CI/CD)服务 - AWS CodeDeploy：自动化应用程序部署服务 - Amazon EventBridge：事件驱动架构的核心服务，用于应用程序集成 - AWS Lambda：无服务器计算服务，用于处理事件和业务逻辑 - Amazon SNS：消息通知服务 - CloudWatch Logs：日志监控服务 - AWS Config：配置管理和合规性监控服务 - AWS CloudTrail：API调用审计服务 - Amazon Inspector：安全评估服务 **正确答案B的原因：** 1. **EventBridge是最佳选择**：CodePipeline和CodeDeploy都原生支持向EventBridge发送状态变更事件，包括失败、成功等状态 2. **实时事件响应**：EventBridge能够实时捕获部署状态变更，无需轮询或延迟 3. **Lambda灵活处理**：Lambda函数可以灵活地处理事件数据，进行条件判断，决定是否需要发送通知 4. **架构简洁高效**：EventBridge → Lambda → SNS 形成了一个高效的事件驱动通知链路 **其他选项错误的原因：** - **选项A错误**：AWS Config主要用于资源配置合规性检查，不是为了监控部署状态而设计的，且CloudWatch Logs需要主动查询，不是事件驱动的 - **选项C错误**：CloudTrail记录API调用信息，主要用于审计目的，不是实时监控部署状态的最佳选择，且会产生大量不必要的日志数据 - **选项D错误**：Amazon Inspector是安全漏洞评估工具，用于评估应用程序安全性，与部署状态监控无关 **决策标准和最佳实践：** 1. **选择事件驱动架构**：对于状态变更监控，事件驱动比轮询更高效 2. **使用原生集成**：选择与目标服务原生集成度最高的监控方案 3. **实时性要求**：部署监控需要实时响应，EventBridge提供近实时的事件传递 4. **成本效益**：避免使用过度复杂或不相关的服务，保持架构简洁 5. **可扩展性**：EventBridge + Lambda的组合易于扩展和维护</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">125</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A global company manages multiple AWS accounts by using AWS Control Tower. The company hosts internal applications and public applications. Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations. One of the AWS Control Tower member accounts serves as a centralized DevOps account with CI/CD pipelines that application teams use to deploy applications to their respective target AWS accounts. An IAM role for deployment exists in the centralized DevOps account. An application team is attempting to deploy its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in an application AWS account. An IAM role for deployment exists in the application AWS account. The deployment is through an AWS CodeBuild project that is set up in the centralized DevOps account. The CodeBuild project uses an IAM service role for CodeBuild. The deployment is failing with an Unauthorized error during attempts to connect to the cross-account EKS cluster from CodeBuild. Which solution will resolve this error? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure the application account&#x27;s deployment IAM role to have a trust relationship with the centralized DevOps account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account&#x27;s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.
B. Configure the centralized DevOps account&#x27;s deployment IAM role to have a trust relationship with the application account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the centralized DevOps account&#x27;s deployment IAM role to allow the required access to CodeBuild.
C. Configure the centralized DevOps account&#x27;s deployment IAM role to have a trust relationship with the application account. Configure the trust relationship to allow the sts:AssumeRoleWithSAML action. Configure the centralized DevOps account&#x27;s deployment IAM role to allow the required access to CodeBuild.
D. Configure the application account&#x27;s deployment IAM role to have a trust relationship with the AWS Control Tower management account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account&#x27;s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家全球公司使用AWS Control Tower管理多个AWS账户。该公司托管内部应用程序和公共应用程序。公司中的每个应用程序团队都有自己的AWS账户用于应用程序托管。这些账户在AWS Organizations中的一个组织中进行整合。其中一个AWS Control Tower成员账户作为集中式DevOps账户，具有CI/CD管道，应用程序团队使用这些管道将应用程序部署到各自的目标AWS账户。集中式DevOps账户中存在用于部署的IAM角色。应用程序账户中也存在用于部署的IAM角色。一个应用程序团队正在尝试将其应用程序部署到应用程序AWS账户中的Amazon Elastic Kubernetes Service (Amazon EKS)集群。部署是通过在集中式DevOps账户中设置的AWS CodeBuild项目进行的。CodeBuild项目使用CodeBuild的IAM服务角色。在尝试从CodeBuild连接到跨账户EKS集群时，部署失败并出现未授权错误。哪个解决方案可以解决此错误？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求解决跨账户EKS集群访问的权限问题。具体场景是从集中式DevOps账户的CodeBuild项目访问应用程序账户中的EKS集群时出现未授权错误。 **涉及的关键AWS服务和概念：** - AWS Control Tower：多账户管理服务 - AWS Organizations：账户组织管理 - Amazon EKS：托管Kubernetes服务 - AWS CodeBuild：持续集成服务 - IAM跨账户角色假设：sts:AssumeRole - EKS aws-auth ConfigMap：EKS集群的RBAC配置 **正确答案B的原因：** 选项B是错误的标准答案。实际上正确的应该是选项A。让我分析为什么： 1. **跨账户访问的正确流程**：CodeBuild在DevOps账户中运行，需要访问应用程序账户的EKS集群，应该是DevOps账户假设应用程序账户的角色 2. **信任关系方向**：应用程序账户的部署角色应该信任DevOps账户，允许DevOps账户假设该角色 3. **EKS访问权限**：需要在应用程序账户的部署角色上配置EKS访问权限，并在aws-auth ConfigMap中映射该角色 **其他选项错误的原因：** - 选项B：信任关系方向错误，应该是应用程序账户信任DevOps账户，而不是相反 - 选项C：使用了错误的STS操作（AssumeRoleWithSAML用于SAML联合身份验证） - 选项D：不应该涉及Control Tower管理账户，这会增加不必要的复杂性 **决策标准和最佳实践：** 1. **最小权限原则**：只授予必要的跨账户访问权限 2. **信任关系方向**：资源所在账户的角色应该信任需要访问的账户 3. **EKS RBAC配置**：必须在aws-auth ConfigMap中正确映射IAM角色到Kubernetes权限 4. **跨账户部署模式**：集中式DevOps账户模式是企业级多账户架构的最佳实践 注：题目标注的正确答案B实际上是错误的，正确答案应该是A。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">126</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A highly regulated company has a policy that DevOps engineers should not log in to their Amazon EC2 instances except in emergencies. If a DevOps engineer does log in, the security team must be notified within 15 minutes of the occurrence. Which solution will meet these requirements? B (94%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Install the Amazon Inspector agent on each EC2 instance. Subscribe to Amazon EventBridge notifications. Invoke an AWS Lambda function to check if a message is about user logins. If it is, send a notification to the security team using Amazon SNS.
B. Install the Amazon CloudWatch agent on each EC2 instance. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user logins. If a login is found, send a notification to the security team using Amazon SNS.
C. Set up AWS CloudTrail with Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Attach AWS Lambda to Kinesis to parse and determine if a log contains a user login. If it does, send a notification to the security team using Amazon SNS.
D. Set up a script on each Amazon EC2 instance to push all logs to Amazon S3. Set up an S3 event to invoke an AWS Lambda function, which invokes an Amazon Athena query to run. The Athena query checks for logins and sends the output to the security team using Amazon SNS.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家受到严格监管的公司有一项政策，DevOps工程师不应该登录到他们的Amazon EC2实例，除非在紧急情况下。如果DevOps工程师确实登录了，安全团队必须在事件发生后15分钟内收到通知。哪个解决方案能满足这些要求？ 选项： A. 在每个EC2实例上安装Amazon Inspector代理。订阅Amazon EventBridge通知。调用AWS Lambda函数检查消息是否关于用户登录。如果是，使用Amazon SNS向安全团队发送通知。 B. 在每个EC2实例上安装Amazon CloudWatch代理。配置代理将所有日志推送到Amazon CloudWatch Logs，并设置CloudWatch指标过滤器搜索用户登录。如果发现登录，使用Amazon SNS向安全团队发送通知。 C. 设置AWS CloudTrail与Amazon CloudWatch Logs。将CloudWatch Logs订阅到Amazon Kinesis。将AWS Lambda附加到Kinesis来解析和确定日志是否包含用户登录。如果包含，使用Amazon SNS向安全团队发送通知。 D. 在每个Amazon EC2实例上设置脚本将所有日志推送到Amazon S3。设置S3事件调用AWS Lambda函数，该函数调用Amazon Athena查询运行。Athena查询检查登录并使用Amazon SNS将输出发送给安全团队。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个监控系统，能够检测DevOps工程师对EC2实例的登录行为，并在15分钟内通知安全团队。关键需求包括：实时监控、快速检测登录事件、及时通知机制。 **涉及的关键AWS服务和概念：** - Amazon CloudWatch：监控和日志管理服务 - CloudWatch Logs：集中化日志存储和分析 - CloudWatch Metric Filter：实时日志模式匹配和告警 - Amazon SNS：消息通知服务 - AWS CloudTrail：API调用审计（主要用于AWS服务调用） - Amazon Inspector：安全漏洞评估工具 - 实时日志监控和告警机制 **正确答案B的原因：** 1. **直接监控登录日志**：CloudWatch代理可以收集系统日志（如/var/log/auth.log），直接捕获SSH登录事件 2. **实时处理能力**：CloudWatch Metric Filter可以实时扫描日志流，一旦匹配到登录模式立即触发告警 3. **满足时间要求**：整个流程从日志生成到SNS通知可以在几分钟内完成，远少于15分钟要求 4. **架构简洁高效**：直接的日志→过滤→告警链路，减少了中间环节和延迟 **其他选项错误的原因：** - **选项A**：Amazon Inspector是漏洞扫描工具，不是用来监控用户登录行为的，且EventBridge在此场景下不是最直接的解决方案 - **选项C**：CloudTrail主要记录AWS API调用，不会记录EC2实例内部的SSH登录事件；通过Kinesis增加了不必要的复杂性和延迟 - **选项D**：基于S3和Athena的方案是批处理架构，无法满足15分钟内通知的实时性要求，Athena适合历史数据分析而非实时监控 **决策标准和最佳实践：** 1. **实时性优先**：选择能够实时处理日志的服务组合 2. **监控目标匹配**：区分AWS API监控（CloudTrail）和系统级监控（CloudWatch） 3. **架构简化原则**：避免不必要的中间件，减少延迟和故障点 4. **成本效益考虑**：CloudWatch Logs + Metric Filter是成本效益最优的实时日志监控方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">127</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company updated the AWS CloudFormation template for a critical business application. The stack update process failed due to an error in the updated template, and AWS CloudFormation automatically began the stack rollback process. Later, a DevOps engineer discovered that the application was still unavailable and that the stack was in the UPDATE_ROLLBACK_FAILED state. Which combination of actions should the DevOps engineer perform so that the stack rollback can complete successfully? (Choose two.) CD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Attach the AWSCloudFormationFullAccess IAM policy to the AWS CloudFormation role.
B. Automatically recover the stack resources by using AWS CloudFormation drift detection.
C. Issue a ContinueUpdateRollback command from the AWS CloudFormation console or the AWS CLI.
D. Manually adjust the resources to match the expectations of the stack.
E. Update the existing AWS CloudFormation stack by using the original template.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司更新了关键业务应用程序的AWS CloudFormation模板。由于更新模板中的错误，堆栈更新过程失败了，AWS CloudFormation自动开始了堆栈回滚过程。后来，一名DevOps工程师发现应用程序仍然不可用，堆栈处于UPDATE_ROLLBACK_FAILED状态。DevOps工程师应该执行哪些操作组合，以便堆栈回滚能够成功完成？（选择两个。） 选项： A. 将AWSCloudFormationFullAccess IAM策略附加到AWS CloudFormation角色。 B. 使用AWS CloudFormation漂移检测自动恢复堆栈资源。 C. 从AWS CloudFormation控制台或AWS CLI发出ContinueUpdateRollback命令。 D. 手动调整资源以匹配堆栈的期望。 E. 使用原始模板更新现有的AWS CloudFormation堆栈。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是当CloudFormation堆栈处于UPDATE_ROLLBACK_FAILED状态时的恢复策略。堆栈更新失败后自动回滚也失败了，需要找到正确的方法来完成回滚过程。 **涉及的关键AWS服务和概念：** - AWS CloudFormation堆栈状态管理 - UPDATE_ROLLBACK_FAILED状态的含义和处理 - ContinueUpdateRollback操作 - CloudFormation资源状态同步 - 堆栈回滚机制 **正确答案的原因：** 选项C（ContinueUpdateRollback命令）是正确的，因为： - 当堆栈处于UPDATE_ROLLBACK_FAILED状态时，这是AWS官方推荐的标准解决方案 - 该命令专门设计用于继续失败的回滚操作 - 可以跳过无法回滚的资源，继续回滚其他资源 选项D（手动调整资源）也是正确的，因为： - UPDATE_ROLLBACK_FAILED通常是因为某些资源无法自动回滚到原始状态 - 需要手动修复这些资源，使其状态与堆栈模板期望的状态一致 - 这是ContinueUpdateRollback成功执行的前提条件 **其他选项错误的原因：** A. 添加IAM权限 - 如果是权限问题，堆栈不会进入UPDATE_ROLLBACK_FAILED状态，而是会显示权限相关错误 B. 漂移检测 - 这只是检测工具，不能修复UPDATE_ROLLBACK_FAILED状态 E. 使用原始模板更新 - 在UPDATE_ROLLBACK_FAILED状态下无法直接进行新的更新操作 **决策标准和最佳实践：** 1. 首先识别导致回滚失败的具体资源和原因 2. 手动修复问题资源，确保其状态符合回滚目标 3. 使用ContinueUpdateRollback命令继续回滚过程 4. 建议在生产环境中使用变更集预览更新，避免此类问题 5. 实施适当的测试和验证流程，减少模板错误的可能性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">128</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A development team manually builds an artifact locally and then places it in an Amazon S3 bucket. The application has a local cache that must be cleared when a deployment occurs. The team runs a command to do this, downloads the artifact from Amazon S3, and unzips the artifact to complete the deployment. A DevOps team wants to migrate to a CI/CD process and build in checks to stop and roll back the deployment when a failure occurs. This requires the team to track the progression of the deployment. Which combination of actions will accomplish this? (Choose three.) F. Use AWS Systems Manager to fetch the artifact from Amazon S3 and deploy it to all the instances. BDE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Allow developers to check the code into a code repository. Using Amazon EventBridge, on every pull into the main branch, invoke an AWS Lambda function to build the artifact and store it in Amazon S3.
B. Create a custom script to clear the cache. Specify the script in the BeforeInstall lifecycle hook in the AppSpec file.
C. Create user data for each Amazon EC2 instance that contains the clear cache script. Once deployed, test the application. If it is not successful, deploy it again.
D. Set up AWS CodePipeline to deploy the application. Allow developers to check the code into a code repository as a source for the pipeline.
E. Use AWS CodeBuild to build the artifact and place it in Amazon S3. Use AWS CodeDeploy to deploy the artifact to Amazon EC2 instances.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个开发团队手动在本地构建工件，然后将其放置在Amazon S3存储桶中。应用程序有一个本地缓存，在部署发生时必须清除。团队运行命令来执行此操作，从Amazon S3下载工件，并解压工件以完成部署。DevOps团队希望迁移到CI/CD流程，并内置检查以在发生故障时停止和回滚部署。这需要团队跟踪部署的进度。哪种操作组合将实现这一目标？（选择三个。） 选项： A. 允许开发人员将代码检入代码仓库。使用Amazon EventBridge，在每次拉取到主分支时，调用AWS Lambda函数来构建工件并将其存储在Amazon S3中。 B. 创建自定义脚本来清除缓存。在AppSpec文件的BeforeInstall生命周期钩子中指定该脚本。 C. 为每个Amazon EC2实例创建包含清除缓存脚本的用户数据。部署后，测试应用程序。如果不成功，再次部署。 D. 设置AWS CodePipeline来部署应用程序。允许开发人员将代码检入代码仓库作为管道的源。 E. 使用AWS CodeBuild构建工件并将其放置在Amazon S3中。使用AWS CodeDeploy将工件部署到Amazon EC2实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求将手动部署流程迁移到自动化CI/CD流程，需要满足以下关键需求： 1. 自动化构建和部署流程 2. 在部署时清除本地缓存 3. 具备失败检查、停止和回滚能力 4. 能够跟踪部署进度 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务，提供端到端的自动化部署流程 - AWS CodeBuild：托管构建服务，用于编译源代码和创建工件 - AWS CodeDeploy：自动化部署服务，支持生命周期钩子和回滚功能 - AppSpec文件：CodeDeploy的配置文件，定义部署步骤和生命周期钩子 - 生命周期钩子：允许在部署过程的特定阶段执行自定义脚本 **正确答案BDE的原因：** - **选项B**：使用AppSpec文件的BeforeInstall钩子清除缓存是CodeDeploy的标准做法，确保在安装新版本前清理环境 - **选项D**：CodePipeline提供完整的CI/CD管道，支持自动化流程、进度跟踪和失败处理 - **选项E**：CodeBuild+CodeDeploy的组合是AWS推荐的构建和部署解决方案，CodeDeploy天然支持回滚和部署跟踪 **其他选项错误的原因：** - **选项A**：使用EventBridge+Lambda的方案过于复杂，缺乏专业CI/CD工具的部署跟踪和回滚能力 - **选项C**：用户数据方案无法提供可靠的失败检测和回滚机制，且&quot;失败后重新部署&quot;不是回滚策略 **决策标准和最佳实践：** 1. 选择AWS原生CI/CD服务（CodePipeline、CodeBuild、CodeDeploy）获得最佳集成 2. 利用CodeDeploy的生命周期钩子处理部署前后的自定义操作 3. 使用AppSpec文件标准化部署配置和脚本管理 4. 确保解决方案具备内置的监控、跟踪和回滚能力 5. 避免过度工程化，选择简单可靠的AWS托管服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">129</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is working on a project that is hosted on Amazon Linux and has failed a security review. The DevOps manager has been asked to review the company buildspec.yaml file for an AWS CodeBuild project and provide recommendations. The buildspec.yaml file is configured as follows: What changes should be recommended to comply with AWS security best practices? (Choose three.) F. Scramble the environment variables using XOR followed by Base64, add a section to install, and then run XOR and Base64 to the build phase. BCE (76%) ABC (24%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a post-build command to remove the temporary files from the container before termination to ensure they cannot be seen by other CodeBuild users.
B. Update the CodeBuild project role with the necessary permissions and then remove the AWS credentials from the environment variable.
C. Store the DB_PASSWORD as a SecureString value in AWS Systems Manager Parameter Store and then remove the DB_PASSWORD from the environment variables.
D. Move the environment variables to the &#x27;db-deploy-bucket&#x27; Amazon S3 bucket, add a prebuild stage to download, then export the variables.
E. Use AWS Systems Manager run command versus scp and ssh commands directly to the instance.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师正在处理一个托管在Amazon Linux上的项目，该项目未通过安全审查。DevOps经理被要求审查公司的buildspec.yaml文件（用于AWS CodeBuild项目）并提供建议。buildspec.yaml文件配置如下：应该推荐哪些更改来符合AWS安全最佳实践？（选择三个。） 选项： A. 添加post-build命令在容器终止前删除临时文件，确保其他CodeBuild用户无法看到它们。 B. 更新CodeBuild项目角色的必要权限，然后从环境变量中删除AWS凭证。 C. 将DB_PASSWORD作为SecureString值存储在AWS Systems Manager Parameter Store中，然后从环境变量中删除DB_PASSWORD。 D. 将环境变量移动到&#x27;db-deploy-bucket&#x27; Amazon S3存储桶，添加prebuild阶段来下载，然后导出变量。 E. 使用AWS Systems Manager run command而不是直接使用scp和ssh命令到实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodeBuild安全最佳实践，特别是如何安全地处理敏感信息（如凭证和密码）以及构建环境的安全配置。题目要求选择三个符合AWS安全最佳实践的建议。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：持续集成服务 - IAM角色和权限管理 - AWS Systems Manager Parameter Store：安全参数存储 - 环境变量安全处理 - 容器安全和临时文件管理 **正确答案分析：** 根据题目显示，正确答案是B，但从安全最佳实践角度，A、B、C都应该是正确的： - **选项A正确**：删除临时文件防止敏感信息泄露给其他用户，这是容器安全的基本实践 - **选项B正确**：使用IAM角色而不是硬编码AWS凭证是AWS安全的核心原则 - **选项C正确**：使用Parameter Store存储敏感信息如数据库密码是标准的安全实践 **其他选项错误的原因：** - **选项D错误**：将敏感的环境变量存储在S3存储桶中并不能提高安全性，反而可能增加暴露风险 - **选项E错误**：虽然Systems Manager Session Manager比直接SSH更安全，但这不是buildspec.yaml文件的直接安全问题 - **选项F错误**：使用XOR和Base64进行&quot;加密&quot;是不安全的，这只是编码而非真正的加密 **决策标准和最佳实践：** 1. **最小权限原则**：使用IAM角色而非硬编码凭证 2. **敏感信息保护**：使用专门的安全服务存储密码和密钥 3. **数据清理**：构建完成后清理敏感的临时文件 4. **避免明文存储**：不在配置文件中直接暴露敏感信息 5. **使用AWS托管服务**：优先使用AWS提供的安全服务而非自制解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">130</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a legacy application. A DevOps engineer needs to automate the process of building the deployable artifact for the legacy application. The solution must store the deployable artifact in an existing Amazon S3 bucket for future deployments to reference. Which solution will meet these requirements in the MOST operationally efficient way? Amazon Elastic Container Registry (Amazon ECR) repository. Use the custom Docker image inside the EKS cluster to build the deployable artifact and to save the artifact to the S3 bucket. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new Amazon Elastic Container Registry (Amazon ECR) repository. Configure a new AWS CodeBuild project to use the custom Docker image to build the deployable artifact and to save the artifact to the S3 bucket.
B. Launch a new Amazon EC2 instance. Install all the dependencies for the legacy application on the EC2 instance. Use the EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.
C. Create a custom EC2 Image Builder image. Install all the dependencies for the legacy application on the image. Launch a new Amazon EC2 instance from the image. Use the new EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.
D. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with an AWS Fargate profile that runs in multiple Availability Zones. Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个遗留应用程序。DevOps工程师需要自动化构建该遗留应用程序可部署工件的过程。解决方案必须将可部署工件存储在现有的Amazon S3存储桶中，以供未来部署时引用。哪个解决方案能以最具运营效率的方式满足这些要求？ 选项： A. 创建一个包含遗留应用程序所有依赖项的自定义Docker镜像。将自定义Docker镜像存储在新的Amazon Elastic Container Registry (Amazon ECR)存储库中。配置新的AWS CodeBuild项目，使用自定义Docker镜像构建可部署工件并将工件保存到S3存储桶。 B. 启动新的Amazon EC2实例。在EC2实例上安装遗留应用程序的所有依赖项。使用EC2实例构建可部署工件并将工件保存到S3存储桶。 C. 创建自定义EC2 Image Builder镜像。在镜像上安装遗留应用程序的所有依赖项。从该镜像启动新的Amazon EC2实例。使用新的EC2实例构建可部署工件并将工件保存到S3存储桶。 D. 创建在多个可用区运行的带有AWS Fargate配置文件的Amazon Elastic Kubernetes Service (Amazon EKS)集群。创建包含遗留应用程序所有依赖项的自定义Docker镜像。将自定义Docker镜像存储在新的Amazon Elastic Container Registry (Amazon ECR)存储库中。使用EKS集群内的自定义Docker镜像构建可部署工件并将工件保存到S3存储桶。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为遗留应用程序建立自动化构建流程，需要满足：1）自动化构建可部署工件；2）将工件存储到现有S3存储桶；3）以最具运营效率的方式实现。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：托管的构建服务，专门用于编译源代码、运行测试和生成可部署的软件包 - Amazon ECR：托管的Docker容器注册表服务 - Amazon S3：对象存储服务，用于存储构建工件 - Amazon EC2：虚拟服务器实例 - Amazon EKS：托管的Kubernetes服务 - EC2 Image Builder：用于创建和维护服务器镜像的服务 **正确答案A的原因：** 1. **专门化服务**：CodeBuild是专为CI/CD构建任务设计的托管服务，无需管理底层基础设施 2. **运营效率最高**：完全托管，自动扩缩容，按使用付费，无需维护服务器 3. **集成度好**：原生支持与S3集成，可直接将构建工件上传到指定存储桶 4. **容器化优势**：使用Docker镜像确保构建环境的一致性和可重复性 5. **成本效益**：只在构建时付费，无持续运行成本 **其他选项错误的原因：** - **选项B**：使用EC2实例需要手动管理服务器、安装依赖、维护运行时环境，运营开销大，不够自动化 - **选项C**：虽然使用了Image Builder提高了一致性，但仍需管理EC2实例，运营效率不如托管服务 - **选项D**：EKS集群过于复杂，适合容器化应用运行而非简单的构建任务，存在过度工程化问题，运营成本和复杂度都很高 **决策标准和最佳实践：** 1. **选择专门化服务**：优先选择为特定用途设计的AWS托管服务 2. **最小化运营开销**：避免管理不必要的基础设施 3. **自动化优先**：选择支持完全自动化的解决方案 4. **成本效益**：考虑按需付费vs持续运行成本 5. **避免过度工程化**：不要为简单需求选择过于复杂的架构</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">131</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company builds a container image in an AWS CodeBuild project by running Docker commands. After the container image is built, the CodeBuild project uploads the container image to an Amazon S3 bucket. The CodeBuild project has an IAM service role that has permissions to access the S3 bucket. A DevOps engineer needs to replace the S3 bucket with an Amazon Elastic Container Registry (Amazon ECR) repository to store the container images. The DevOps engineer creates an ECR private image repository in the same AWS Region of the CodeBuild project. The DevOps engineer adjusts the IAM service role with the permissions that are necessary to work with the new ECR repository. The DevOps engineer also places new repository information into the docker build command and the docker push command that are used in the buildspec.yml file. When the CodeBuild project runs a build job, the job fails when the job tries to access the ECR repository. Which solution will resolve the issue of failed access to the ECR repository? authentication token. Update the docker login command to use the authentication token to access the ECR repository. Most Voted CodeBuild project&#x27;s IAM service role. Update the buildspec.yml file to use the new environment variable to log in with the docker login command to access the ECR repository. allows the IAM service role to have access. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the buildspec.yml file to log in to the ECR repository by using the aws ecr get-login-password AWS CLI command to obtain an
B. Add an environment variable of type SECRETS_MANAGER to the CodeBuild project. In the environment variable, include the ARN of the
C. Update the ECR repository to be a public image repository. Add an ECR repository policy that allows the IAM service role to have access.
D. Update the buildspec.yml file to use the AWS CLI to assume the IAM service role for ECR operations. Add an ECR repository policy that</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS CodeBuild项目中通过运行Docker命令构建容器镜像。容器镜像构建完成后，CodeBuild项目将容器镜像上传到Amazon S3存储桶。CodeBuild项目有一个IAM服务角色，该角色有访问S3存储桶的权限。DevOps工程师需要用Amazon Elastic Container Registry (Amazon ECR)仓库替换S3存储桶来存储容器镜像。DevOps工程师在CodeBuild项目的同一AWS区域创建了一个ECR私有镜像仓库。DevOps工程师调整了IAM服务角色，添加了使用新ECR仓库所需的权限。DevOps工程师还在buildspec.yml文件中使用的docker build命令和docker push命令中放入了新的仓库信息。当CodeBuild项目运行构建作业时，作业在尝试访问ECR仓库时失败。哪个解决方案能解决访问ECR仓库失败的问题？ 选项： A. 更新buildspec.yml文件，使用aws ecr get-login-password AWS CLI命令获取认证令牌来登录ECR仓库。更新docker login命令以使用认证令牌访问ECR仓库。 B. 向CodeBuild项目添加SECRETS_MANAGER类型的环境变量。在环境变量中包含CodeBuild项目IAM服务角色的ARN。更新buildspec.yml文件以使用新的环境变量通过docker login命令登录访问ECR仓库。 C. 将ECR仓库更新为公共镜像仓库。添加允许IAM服务角色访问的ECR仓库策略。 D. 更新buildspec.yml文件以使用AWS CLI为ECR操作承担IAM服务角色。添加允许IAM服务角色访问的ECR仓库策略。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何解决CodeBuild项目访问ECR私有仓库时的认证失败问题。问题的关键在于理解ECR私有仓库需要特殊的认证机制，不同于S3的访问方式。 **涉及的关键AWS服务和概念：** 1. **AWS CodeBuild** - CI/CD构建服务 2. **Amazon ECR** - 容器镜像注册表服务 3. **IAM服务角色** - 为AWS服务提供权限的身份 4. **Docker认证** - 容器镜像推送拉取的认证机制 5. **buildspec.yml** - CodeBuild的构建规范文件 **正确答案A的原因：** - ECR私有仓库需要特殊的认证令牌才能进行docker操作 - `aws ecr get-login-password`命令是AWS官方推荐的获取ECR认证令牌的标准方法 - 该命令会利用CodeBuild项目的IAM服务角色自动获取临时认证令牌 - 通过管道传递给`docker login`命令可以实现无缝认证 - 这是最安全、最简单的解决方案，符合AWS最佳实践 **其他选项错误的原因：** - **选项B错误**：SECRETS_MANAGER环境变量用于存储静态密钥，但ECR认证需要动态令牌，且IAM角色ARN不是认证凭据 - **选项C错误**：将私有仓库改为公共仓库违背了安全原则，且题目明确要求使用私有仓库 - **选项D错误**：CodeBuild已经在IAM服务角色下运行，无需再次assume角色，这会增加不必要的复杂性 **决策标准和最佳实践：** 1. **安全性优先** - 使用临时令牌而非静态凭据 2. **简化配置** - 利用现有IAM角色，避免额外的权限管理 3. **遵循AWS标准** - 使用官方推荐的ECR认证方法 4. **最小权限原则** - 不改变仓库的私有性质 5. **操作简便性** - 通过buildspec.yml中的简单命令实现认证</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">132</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manually provisions IAM access for its employees. The company wants to replace the manual process with an automated process. The company has an existing Active Directory system configured with an external SAML 2.0 identity provider (IdP). The company wants employees to use their existing corporate credentials to access AWS. The groups from the existing Active Directory system must be available for permission management in AWS Identity and Access Management (IAM). A DevOps engineer has completed the initial configuration of AWS IAM Identity Center (AWS Single Sign-On) in the company&#x27;s AWS account. What should the DevOps engineer do next to meet the requirements? protocol. A (90%) 10%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure an external IdP as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol.
B. Configure AWS Directory Service as an identity source. Configure automatic provisioning of users and groups by using the SAML
C. Configure an AD Connector as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol.
D. Configure an external IdP as an identity source. Configure automatic provisioning of users and groups by using the SAML protocol.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司手动为其员工配置IAM访问权限。该公司希望用自动化流程替换手动流程。该公司有一个现有的Active Directory系统，配置了外部SAML 2.0身份提供商(IdP)。公司希望员工使用其现有的企业凭证来访问AWS。现有Active Directory系统中的组必须在AWS Identity and Access Management (IAM)中可用于权限管理。DevOps工程师已经在公司的AWS账户中完成了AWS IAM Identity Center (AWS Single Sign-On)的初始配置。DevOps工程师接下来应该做什么来满足要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 实现从手动IAM配置到自动化的转换 - 集成现有的Active Directory和外部SAML 2.0 IdP - 员工使用现有企业凭证访问AWS - Active Directory中的组要在AWS IAM中可用于权限管理 - 基于已配置的AWS IAM Identity Center进行后续配置 **涉及的关键AWS服务和概念：** - AWS IAM Identity Center (原AWS SSO)：统一身份管理服务 - External IdP：外部身份提供商，支持SAML 2.0 - SCIM协议：用于身份信息跨域管理的标准协议，支持自动用户和组同步 - SAML协议：主要用于身份验证，不是用于自动provisioning - AD Connector：用于连接本地Active Directory的代理服务 **正确答案A的原因：** 1. 配置外部IdP作为身份源符合题目中已有&quot;外部SAML 2.0 IdP&quot;的要求 2. SCIM协议是专门用于自动化用户和组provisioning的标准协议 3. 通过SCIM可以自动同步Active Directory中的用户和组到AWS IAM Identity Center 4. 这种配置能够实现完全自动化的身份管理，满足替换手动流程的需求 **其他选项错误的原因：** - 选项B：AWS Directory Service不是外部IdP，且SAML协议不用于自动provisioning - 选项C：AD Connector是AWS的服务，不是外部IdP，与题目要求的&quot;外部SAML 2.0 IdP&quot;不符 - 选项D：SAML协议主要用于身份验证和授权，不是用于自动provisioning用户和组的协议 **决策标准和最佳实践：** 1. 身份源选择：题目明确提到有外部SAML 2.0 IdP，应该利用现有基础设施 2. 自动化协议选择：SCIM是行业标准的身份provisioning协议，支持用户和组的自动同步 3. 集成策略：利用现有的外部IdP可以减少架构复杂性和维护成本 4. 权限管理：通过SCIM同步的组可以直接在AWS IAM中用于权限分配</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">133</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using AWS to run digital workloads. Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations. The company wants to enforce security standards across the entire organization. To avoid noncompliance because of security misconfiguration, the company has enforced the use of AWS CloudFormation. A production support team can modify resources in the production environment by using the AWS Management Console to troubleshoot and resolve application-related issues. A DevOps engineer must implement a solution to identify in near real time any AWS service misconfiguration that results in noncompliance. The solution must automatically remediate the issue within 15 minutes of identification. The solution also must track noncompliant resources and events in a centralized dashboard with accurate timestamps. Which solution will meet these requirements with the LEAST development overhead? AWS Step Functions to track query results on Athena for drift detection and to invoke an AWS Lambda function for remediation. For tracking, set up an Amazon QuickSight dashboard that uses Athena as the data source. C (85%) A (15%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use CloudFormation drift detection to identify noncompliant resources. Use drift detection events from CloudFormation to invoke an AWS Lambda function for remediation. Configure the Lambda function to publish logs to an Amazon CloudWatch Logs log group. Configure an Amazon CloudWatch dashboard to use the log group for tracking.
B. Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon Athena to identify noncompliant resources. Use
C. Turn on the configuration recorder in AWS Config in all the AWS accounts to identify noncompliant resources. Enable AWS Security Hub with the --no-enable-default-standards option in all the AWS accounts. Set up AWS Config managed rules and custom rules. Set up automatic remediation by using AWS Config conformance packs. For tracking, set up a dashboard on Security Hub in a designated Security Hub administrator account.
D. Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon CloudWatch Logs to identify noncompliant resources. Use CloudWatch Logs filters for drift detection. Use Amazon EventBridge to invoke the Lambda function for remediation. Stream filtered CloudWatch logs to Amazon OpenSearch Service. Set up a dashboard on OpenSearch Service for tracking.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS运行数字化工作负载。公司中的每个应用团队都有自己的AWS账户来托管应用程序。这些账户在AWS Organizations中的一个组织里进行了整合。公司希望在整个组织中强制执行安全标准。为了避免因安全配置错误而导致的不合规，公司已强制使用AWS CloudFormation。生产支持团队可以通过AWS Management Console修改生产环境中的资源，以排查和解决应用程序相关问题。DevOps工程师必须实施一个解决方案，以近实时识别任何导致不合规的AWS服务配置错误。该解决方案必须在识别后15分钟内自动修复问题。解决方案还必须在集中式仪表板中跟踪不合规资源和事件，并提供准确的时间戳。哪个解决方案能以最少的开发开销满足这些要求？ 选项： A. 使用CloudFormation漂移检测来识别不合规资源。使用CloudFormation的漂移检测事件调用AWS Lambda函数进行修复。配置Lambda函数将日志发布到Amazon CloudWatch Logs日志组。配置Amazon CloudWatch仪表板使用该日志组进行跟踪。 B. 在AWS账户中开启AWS CloudTrail。使用Amazon Athena分析CloudTrail日志以识别不合规资源。使用AWS Step Functions跟踪Athena上的查询结果进行漂移检测并调用AWS Lambda函数进行修复。对于跟踪，设置使用Athena作为数据源的Amazon QuickSight仪表板。 C. 在所有AWS账户中开启AWS Config的配置记录器以识别不合规资源。在所有AWS账户中使用--no-enable-default-standards选项启用AWS Security Hub。设置AWS Config托管规则和自定义规则。使用AWS Config conformance packs设置自动修复。对于跟踪，在指定的Security Hub管理员账户中设置Security Hub仪表板。 D. 在AWS账户中开启AWS CloudTrail。使用Amazon CloudWatch Logs分析CloudTrail日志以识别不合规资源。使用CloudWatch Logs过滤器进行漂移检测。使用Amazon EventBridge调用Lambda函数进行修复。将过滤的CloudWatch日志流式传输到Amazon OpenSearch Service。在OpenSearch Service上设置跟踪仪表板。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个解决方案来：1）近实时检测AWS服务配置的不合规情况；2）在15分钟内自动修复；3）在集中式仪表板中跟踪不合规资源和事件；4）最少的开发开销。关键约束是多账户环境（AWS Organizations）且生产团队可能通过控制台手动修改资源。 **涉及的关键AWS服务和概念：** - AWS Config：配置合规性监控和自动修复的核心服务 - AWS Security Hub：集中式安全管理和合规性仪表板 - AWS Config conformance packs：预定义的合规规则集合，支持自动修复 - CloudFormation drift detection：检测资源与模板的偏差 - CloudTrail：API调用日志记录 - 多账户管理和集中式监控 **正确答案C的原因：** 1. **AWS Config是专门的合规性监控服务**：天然支持近实时配置变更检测和合规性评估 2. **自动修复能力**：Config conformance packs提供开箱即用的自动修复功能，无需自定义开发 3. **集中式管理**：Security Hub作为管理员账户可以聚合所有账户的合规性数据 4. **最少开发开销**：使用托管服务和预定义规则，无需编写复杂的检测逻辑 5. **完整的跟踪能力**：Security Hub提供统一的仪表板和准确的时间戳记录 **其他选项错误的原因：** - **选项A**：CloudFormation漂移检测只能检测与模板的偏差，无法检测所有类型的安全配置错误，且需要手动触发检测 - **选项B**：基于CloudTrail日志的分析是被动的，需要大量自定义开发来解析日志和定义合规规则，开发开销大 - **选项D**：同样依赖CloudTrail日志分析，需要复杂的日志过滤和自定义检测逻辑，不是专门的合规性解决方案 **决策标准和最佳实践：** 1. **选择专用服务**：对于合规性监控，AWS Config是最佳选择 2. **最小化自定义开发**：优先使用托管服务和预定义规则 3. **集中式管理**：在多账户环境中，使用Security Hub进行统一管理 4. **自动化优先**：使用conformance packs实现自动修复，减少人工干预</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">134</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Organizations to manage its AWS accounts. The organization root has an OU that is named Environments. The Environments OU has two child OUs that are named Development and Production, respectively. The Environments OU and the child OUs have the default FullAWSAccess policy in place. A DevOps engineer plans to remove the FullAWSAccess policy from the Development OU and replace the policy with a policy that allows all actions on Amazon EC2 resources. What will be the outcome of this policy replacement? B (77%) A (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. All users in the Development OU will be allowed all API actions on all resources.
B. All users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied.
C. All users in the Development OU will be denied all API actions on all resources.
D. All users in the Development OU will be denied all API actions on EC2 resources. All other API actions will be allowed.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations来管理其AWS账户。组织根有一个名为Environments的OU。Environments OU有两个子OU，分别名为Development和Production。Environments OU和子OU都配置了默认的FullAWSAccess策略。一名DevOps工程师计划从Development OU中移除FullAWSAccess策略，并用一个允许对Amazon EC2资源执行所有操作的策略来替换。这种策略替换的结果是什么？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查AWS Organizations中Service Control Policies (SCP)的工作机制，特别是当移除宽泛权限策略并替换为更具体的限制性策略时会产生什么影响。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. Organizational Units (OU) - 组织单元，用于分组管理账户 3. Service Control Policies (SCP) - 服务控制策略，用于设置权限边界 4. FullAWSAccess策略 - AWS提供的默认策略，允许所有AWS服务的所有操作 5. SCP的&quot;拒绝优先&quot;和&quot;白名单&quot;工作原理 **正确答案B的原因：** 1. SCP采用&quot;白名单&quot;模式工作 - 只有明确允许的操作才能执行 2. 移除FullAWSAccess策略后，原本的&quot;允许所有操作&quot;权限被撤销 3. 新策略只允许EC2相关的所有操作，因此用户只能对EC2资源执行操作 4. 对于其他AWS服务（如S3、RDS等），由于新策略中没有明确允许，这些操作将被拒绝 5. SCP是权限边界，即使用户的IAM策略允许某些操作，如果SCP不允许，操作仍会被拒绝 **其他选项错误的原因：** - 选项A错误：新策略不再是FullAWSAccess，不会允许所有资源的所有操作 - 选项C错误：新策略明确允许EC2操作，不会拒绝所有操作 - 选项D错误：逻辑完全颠倒，实际上EC2操作被允许，其他操作被拒绝 **决策标准和最佳实践：** 1. 理解SCP的&quot;最小权限原则&quot; - 只授予必要的权限 2. SCP策略应该采用渐进式收紧，避免一次性过度限制影响业务 3. 在生产环境实施前，应在测试环境验证策略效果 4. 建议为不同环境（开发、测试、生产）设置不同级别的权限控制 5. 定期审查和更新SCP策略，确保符合安全要求和业务需求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">135</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is examining its disaster recovery capability and wants the ability to switch over its daily operations to a secondary AWS Region. The company uses AWS CodeCommit as a source control tool in the primary Region. A DevOps engineer must provide the capability for the company to develop code in the secondary Region. If the company needs to use the secondary Region, developers can add an additional remote URL to their local Git configuration. Which solution will meet these requirements? primary Region&#x27;s CodeCommit repository to the secondary Region&#x27;s CodeCommit repository. Create an AWS Lambda function that invokes the CodeBuild project. Create an Amazon EventBridge rule that reacts to merge events in the primary Region&#x27;s CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function. Most Voted Region&#x27;s CodeCommit repository and copy the result to the S3 bucket. Create an AWS Lambda function that initiates the Fargate task. Create an Amazon EventBridge rule that reacts to merge events in the CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function. A (86%) 14%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CodeCommit repository in the secondary Region. Create an AWS CodeBuild project to perform a Git mirror operation of the
B. Create an Amazon S3 bucket in the secondary Region. Create an AWS Fargate task to perform a Git mirror operation of the primary
C. Create an AWS CodeArtifact repository in the secondary Region. Create an AWS CodePipeline pipeline that uses the primary Region&#x27;s CodeCommit repository for the source action. Create a cross-Region stage in the pipeline that packages the CodeCommit repository contents and stores the contents in the CodeArtifact repository when a pull request is merged into the CodeCommit repository.
D. Create an AWS Cloud9 environment and a CodeCommit repository in the secondary Region. Configure the primary Region&#x27;s CodeCommit repository as a remote repository in the AWS Cloud9 environment. Connect the secondary Region&#x27;s CodeCommit repository to the AWS Cloud9 environment.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在检查其灾难恢复能力，希望能够将其日常运营切换到辅助AWS区域。该公司在主区域使用AWS CodeCommit作为源代码控制工具。DevOps工程师必须为公司提供在辅助区域开发代码的能力。如果公司需要使用辅助区域，开发人员可以在其本地Git配置中添加额外的远程URL。哪种解决方案能满足这些要求？ 选项： A. 在辅助区域创建一个CodeCommit存储库。创建一个AWS CodeBuild项目来执行主区域CodeCommit存储库到辅助区域CodeCommit存储库的Git镜像操作。创建一个AWS Lambda函数来调用CodeBuild项目。创建一个Amazon EventBridge规则来响应主区域CodeCommit存储库中的合并事件。配置EventBridge规则来调用Lambda函数。 B. 在辅助区域创建一个Amazon S3存储桶。创建一个AWS Fargate任务来执行主区域CodeCommit存储库的Git镜像操作并将结果复制到S3存储桶。创建一个AWS Lambda函数来启动Fargate任务。创建一个Amazon EventBridge规则来响应CodeCommit存储库中的合并事件。配置EventBridge规则来调用Lambda函数。 C. 在辅助区域创建一个AWS CodeArtifact存储库。创建一个AWS CodePipeline管道，使用主区域的CodeCommit存储库作为源操作。在管道中创建一个跨区域阶段，当拉取请求合并到CodeCommit存储库时，打包CodeCommit存储库内容并将内容存储在CodeArtifact存储库中。 D. 在辅助区域创建一个AWS Cloud9环境和一个CodeCommit存储库。将主区域的CodeCommit存储库配置为AWS Cloud9环境中的远程存储库。将辅助区域的CodeCommit存储库连接到AWS Cloud9环境。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为灾难恢复场景设计一个解决方案，使公司能够在辅助区域进行代码开发，并且开发人员可以通过添加远程URL的方式访问辅助区域的代码库。关键需求是实现主区域到辅助区域的代码同步，并保持Git工作流的兼容性。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：Git代码存储库服务 - Amazon S3：对象存储服务，可以存储Git存储库 - AWS Fargate：无服务器容器计算服务 - AWS Lambda：无服务器计算服务 - Amazon EventBridge：事件驱动架构服务 - 跨区域灾难恢复架构 - Git镜像操作和远程存储库概念 **正确答案B的原因：** 1. **满足Git远程URL需求**：S3可以作为Git的远程存储库，开发人员可以直接添加S3作为远程URL 2. **灾难恢复适用性**：S3具有高可用性和跨区域复制能力，适合灾难恢复场景 3. **自动化同步**：通过EventBridge监听合并事件，Lambda触发Fargate任务执行Git镜像，实现自动同步 4. **成本效益**：S3存储成本较低，Fargate按需计费，适合灾难恢复的间歇性使用模式 **其他选项错误的原因：** - **选项A**：虽然技术可行，但CodeCommit不能直接作为Git远程URL添加到本地配置中，需要特殊的凭证配置 - **选项C**：CodeArtifact主要用于软件包管理，不是Git存储库服务，不符合源代码控制的需求 - **选项D**：Cloud9是IDE环境，不能解决代码同步问题，且没有自动化机制 **决策标准和最佳实践：** 1. **兼容性**：解决方案必须与现有Git工作流兼容 2. **自动化**：灾难恢复方案应该自动同步，减少人工干预 3. **成本优化**：选择适合灾难恢复场景的成本效益最佳服务 4. **可靠性**：确保辅助区域的代码库始终保持最新状态 5. **简单性**：避免过度复杂的架构，便于维护和故障排除</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">136</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps team is merging code revisions for an application that uses an Amazon RDS Multi-AZ DB cluster for its production database. The DevOps team uses continuous integration to periodically verify that the application works. The DevOps team needs to test the changes before the changes are deployed to the production database. Which solution will meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use a buildspec file in AWS CodeBuild to restore the DB cluster from a snapshot of the production database, run integration tests, and drop the restored database after verification.
B. Deploy the application to production. Configure an audit log of data control language (DCL) operations to capture database activities to perform if verification fails.
C. Create a snapshot of the DB cluster before deploying the application. Use the Update requires: Replacement property on the DB instance in AWS CloudFormation to deploy the application and apply the changes.
D. Ensure that the DB cluster is a Multi-AZ deployment. Deploy the application with the updates. Fail over to the standby instance if verification fails.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps团队正在为一个使用Amazon RDS Multi-AZ DB集群作为生产数据库的应用程序合并代码修订。DevOps团队使用持续集成来定期验证应用程序是否正常工作。DevOps团队需要在将更改部署到生产数据库之前测试这些更改。哪个解决方案能满足这些要求？ 选项： A. 在AWS CodeBuild中使用buildspec文件从生产数据库的快照恢复DB集群，运行集成测试，并在验证后删除恢复的数据库。 B. 将应用程序部署到生产环境。配置数据控制语言(DCL)操作的审计日志来捕获数据库活动，以便在验证失败时执行回滚。 C. 在部署应用程序之前创建DB集群的快照。在AWS CloudFormation中使用DB实例的Update requires: Replacement属性来部署应用程序并应用更改。 D. 确保DB集群是Multi-AZ部署。部署带有更新的应用程序。如果验证失败则故障转移到备用实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在生产环境中安全测试数据库更改的最佳实践。关键需求是在不影响生产数据库的情况下测试代码更改。 **涉及的关键AWS服务和概念：** - Amazon RDS Multi-AZ DB集群：提供高可用性和自动故障转移 - AWS CodeBuild：持续集成服务 - RDS快照：数据库备份和恢复机制 - Multi-AZ部署：跨可用区的数据库复制 **正确答案的原因：** 选项A是正确答案，因为： 1. 使用生产数据库快照创建测试环境是标准的最佳实践 2. 在隔离的环境中进行测试，不会影响生产数据 3. 测试完成后删除临时数据库，成本效益高 4. 通过CodeBuild的buildspec文件实现自动化，符合CI/CD流程 5. 真正实现了&quot;测试后再部署到生产&quot;的要求 **其他选项错误的原因：** - 选项B：直接在生产环境部署然后依赖审计日志回滚，风险极高，不符合测试要求 - 选项C：CloudFormation的&quot;Update requires: Replacement&quot;会直接影响生产数据库，没有提供测试机制 - 选项D：Multi-AZ故障转移是高可用性机制，不是测试机制，且仍然是在生产环境中操作 **决策标准和最佳实践：** 1. 永远不要直接在生产环境测试未验证的更改 2. 使用数据库快照创建测试环境是行业标准做法 3. 自动化测试流程应该包括环境创建、测试执行和清理 4. 测试环境应该尽可能接近生产环境（使用生产快照） 5. 成本控制：测试完成后及时清理临时资源</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">137</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages a multi-tenant environment in its VPC and has configured Amazon GuardDuty for the corresponding AWS account. The company sends all GuardDuty findings to AWS Security Hub. Traffic from suspicious sources is generating a large number of findings. A DevOps engineer needs to implement a solution to automatically deny traffic across the entire VPC when GuardDuty discovers a new suspicious source. Which solution will meet these requirements? C (91%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a GuardDuty threat list. Configure GuardDuty to reference the list. Create an AWS Lambda function that will update the threat list. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.
B. Configure an AWS WAF web ACL that includes a custom rule group. Create an AWS Lambda function that will create a block rule in the custom rule group. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.
C. Configure a firewall in AWS Network Firewall. Create an AWS Lambda function that will create a Drop action rule in the firewall policy. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.
D. Create an AWS Lambda function that will create a GuardDuty suppression rule. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在其VPC中管理多租户环境，并为相应的AWS账户配置了Amazon GuardDuty。该公司将所有GuardDuty发现结果发送到AWS Security Hub。来自可疑来源的流量产生了大量发现结果。DevOps工程师需要实施一个解决方案，当GuardDuty发现新的可疑来源时，自动拒绝整个VPC中的流量。哪个解决方案能满足这些要求？ 选项： A. 创建GuardDuty威胁列表。配置GuardDuty引用该列表。创建AWS Lambda函数来更新威胁列表。配置Lambda函数响应来自GuardDuty的新Security Hub发现结果。 B. 配置包含自定义规则组的AWS WAF web ACL。创建AWS Lambda函数在自定义规则组中创建阻止规则。配置Lambda函数响应来自GuardDuty的新Security Hub发现结果。 C. 在AWS Network Firewall中配置防火墙。创建AWS Lambda函数在防火墙策略中创建Drop动作规则。配置Lambda函数响应来自GuardDuty的新Security Hub发现结果。 D. 创建AWS Lambda函数来创建GuardDuty抑制规则。配置Lambda函数响应来自GuardDuty的新Security Hub发现结果。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要实现自动化解决方案，当GuardDuty检测到可疑来源时，能够自动阻止这些来源的流量访问整个VPC。关键要求是&quot;自动拒绝流量&quot;和&quot;整个VPC范围&quot;。 **涉及的关键AWS服务和概念：** - Amazon GuardDuty：威胁检测服务，监控恶意活动 - AWS Security Hub：安全发现结果的集中管理 - AWS WAF：Web应用防火墙，可以阻止恶意IP - AWS Network Firewall：网络层防火墙 - AWS Lambda：无服务器计算，用于自动化响应 **正确答案B的原因：** AWS WAF是最适合这个场景的解决方案，因为： 1. WAF可以通过IP阻止规则有效拦截来自可疑来源的流量 2. 自定义规则组允许动态添加新的阻止规则 3. Lambda函数可以自动解析Security Hub的GuardDuty发现结果，提取可疑IP并创建相应的阻止规则 4. WAF能够在应用层提供实时的流量阻止能力 5. 实现了完整的自动化流程：检测→触发→阻止 **其他选项错误的原因：** - 选项A：GuardDuty威胁列表主要用于检测增强，而不是主动阻止流量。它只是提高检测准确性，不能实际拒绝网络流量。 - 选项C：AWS Network Firewall虽然可以阻止流量，但它主要用于网络边界保护，对于基于GuardDuty发现结果的动态响应来说过于复杂，且成本较高。 - 选项D：GuardDuty抑制规则只是隐藏特定类型的发现结果，完全不涉及流量阻止，与需求不符。 **决策标准和最佳实践：** 1. 选择能够实际阻止网络流量的服务（排除纯检测服务） 2. 考虑自动化集成的便利性和响应速度 3. 评估解决方案的成本效益比 4. 确保解决方案能够处理动态威胁情报 5. 优先选择专门设计用于Web流量保护的服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">138</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Secrets Manager to store a set of sensitive API keys that an AWS Lambda function uses. When the Lambda function is invoked, the Lambda function retrieves the API keys and makes an API call to an external service. The Secrets Manager secret is encrypted with the default AWS Key Management Service (AWS KMS) key. A DevOps engineer needs to update the infrastructure to ensure that only the Lambda function&#x27;s execution role can access the values in Secrets Manager. The solution must apply the principle of least privilege. Which combination of steps will meet these requirements? (Choose two.) BD (94%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the default KMS key for Secrets Manager to allow only the Lambda function&#x27;s execution role to decrypt.
B. Create a KMS customer managed key that trusts Secrets Manager and allows the Lambda function&#x27;s execution role to decrypt. Update Secrets Manager to use the new customer managed key.
C. Create a KMS customer managed key that trusts Secrets Manager and allows the account&#x27;s root principal to decrypt. Update Secrets Manager to use the new customer managed key
D. Ensure that the Lambda function&#x27;s execution role has the KMS permissions scoped on the resource level. Configure the permissions so that the KMS key can encrypt the Secrets Manager secret
E. Remove all KMS permissions from the Lambda function&#x27;s execution role</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Secrets Manager存储一组敏感的API密钥，供AWS Lambda函数使用。当Lambda函数被调用时，Lambda函数检索API密钥并向外部服务发起API调用。Secrets Manager密钥使用默认的AWS Key Management Service (AWS KMS)密钥进行加密。DevOps工程师需要更新基础设施，以确保只有Lambda函数的执行角色能够访问Secrets Manager中的值。解决方案必须应用最小权限原则。哪种步骤组合将满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 确保只有特定Lambda函数的执行角色能访问Secrets Manager中的敏感数据 - 应用最小权限原则 - 需要选择两个正确的步骤 **涉及的关键AWS服务和概念：** - AWS Secrets Manager：存储和管理敏感信息的服务 - AWS KMS：密钥管理服务，用于加密和解密数据 - Lambda执行角色：Lambda函数运行时使用的IAM角色 - 客户管理密钥 vs 默认密钥：不同类型的KMS密钥及其权限控制能力 **正确答案分析（应该是B和D的组合）：** 选项B正确的原因： - 创建客户管理的KMS密钥可以提供精细的权限控制 - 密钥策略需要信任Secrets Manager服务 - 只允许Lambda执行角色进行解密操作，符合最小权限原则 选项D正确的原因： - Lambda执行角色需要在资源级别具有KMS权限 - 需要配置适当的KMS权限以支持Secrets Manager的加密操作 - 确保权限范围限定在特定资源上 **其他选项错误的原因：** 选项A错误：无法修改默认KMS密钥的权限策略，默认密钥由AWS管理 选项C错误：允许账户根主体解密违反了最小权限原则，权限范围过大 选项E错误：完全移除KMS权限会导致Lambda函数无法访问加密的密钥 **决策标准和最佳实践：** - 使用客户管理的KMS密钥以获得完全的权限控制 - 在KMS密钥策略中明确指定可以使用密钥的主体 - 确保IAM角色具有必要的KMS权限且范围限定在特定资源 - 遵循最小权限原则，只授予完成任务所需的最小权限</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">139</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s DevOps engineer is creating an AWS Lambda function to process notifications from an Amazon Simple Notification Service (Amazon SNS) topic. The Lambda function will process the notification messages and will write the contents of the notification messages to an Amazon RDS Multi-AZ DB instance. During testing, a database administrator accidentally shut down the DB instance. While the database was down the company lost several of the SNS notification messages that were delivered during that time. The DevOps engineer needs to prevent the loss of notification messages in the future. Which solutions will meet this requirement? (Choose two.) CD (95%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Replace the RDS Multi-AZ DB instance with an Amazon DynamoDB table.
B. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination of the Lambda function.
C. Configure an Amazon Simple Queue Service (Amazon SQS) dead-letter queue for the SNS topic.
D. Subscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Configure the Lambda function to process messages from the SQS queue.
E. Replace the SNS topic with an Amazon EventBridge event bus. Configure an EventBridge rule on the new event bus to invoke the Lambda function for each event.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的DevOps工程师正在创建一个AWS Lambda函数来处理来自Amazon Simple Notification Service (Amazon SNS) topic的通知。Lambda函数将处理通知消息并将通知消息的内容写入Amazon RDS Multi-AZ DB实例。在测试期间，数据库管理员意外关闭了DB实例。当数据库宕机时，公司丢失了在此期间传递的几条SNS通知消息。DevOps工程师需要防止将来丢失通知消息。哪些解决方案能满足这个要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求解决消息丢失的问题。当RDS数据库不可用时，Lambda函数无法成功处理SNS消息，导致消息丢失。需要找到能够确保消息持久化和重试机制的解决方案。 **涉及的关键AWS服务和概念：** - Amazon SNS：发布/订阅消息服务，但消息传递后如果处理失败可能会丢失 - AWS Lambda：无服务器计算服务，处理SNS消息 - Amazon RDS：关系数据库服务，可能出现不可用情况 - Amazon SQS：消息队列服务，提供消息持久化和重试机制 - Amazon DynamoDB：NoSQL数据库，高可用性 - Amazon EventBridge：事件总线服务 **正确答案分析：** 根据题目显示正确答案是A，但这个答案存在问题。实际上最佳答案应该是C和D： - **选项C（SQS dead-letter queue）**：为SNS topic配置死信队列可以捕获处理失败的消息，防止消息丢失 - **选项D（SQS队列订阅SNS）**：让SQS队列订阅SNS topic，然后Lambda从SQS处理消息，利用SQS的持久化和重试机制确保消息不丢失 **其他选项分析：** - **选项A**：虽然DynamoDB比RDS更高可用，但这只是改变存储方式，没有解决消息传递层面的丢失问题 - **选项B**：将SQS配置为Lambda的目标是错误的配置方式 - **选项E**：EventBridge虽然可行，但没有从根本上解决数据库不可用时的消息丢失问题 **决策标准和最佳实践：** 1. 使用SQS作为缓冲层来解耦SNS和Lambda 2. 配置适当的重试策略和死信队列 3. 确保消息的持久化存储 4. 实现优雅的错误处理机制 题目给出的正确答案A可能有误，实际最佳实践应该是通过SQS队列来确保消息的可靠传递。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">140</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that runs on Amazon EC2 instances. The company uses an AWS CodePipeline pipeline to deploy the application into multiple AWS Regions. The pipeline is configured with a stage for each Region. Each stage contains an AWS CloudFormation action for each Region. When the pipeline deploys the application to a Region, the company wants to confirm that the application is in a healthy state before the pipeline moves on to the next Region. Amazon Route 53 record sets are configured for the application in each Region. A DevOps engineer creates a Route 53 health check that is based on an Amazon CloudWatch alarm for each Region where the application is deployed. What should the DevOps engineer do next to meet the requirements? A (93%) 7%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS Step Functions workflow to check the state of the CloudWatch alarm. Configure the Step Functions workflow to exit with an error if the alarm is in the ALARM state. Create a new stage in the pipeline between each Region deployment stage. In each new stage, include an action to invoke the Step Functions workflow.
B. Configure an AWS CodeDeploy application to deploy a CloudFormation template with automatic rollback. Configure the CloudWatch alarm as the instance health check for the CodeDeploy application. Remove the CloudFormation actions from the pipeline. Create a CodeDeploy action in the pipeline stage for each Region.
C. Create a new pipeline stage for each Region where the application is deployed. Configure a CloudWatch alarm action for the new stage to check the state of the CloudWatch alarm and to exit with an error if the alarm is in the ALARM state.
D. Configure the CloudWatch agent on the EC2 instances to report the application status to the Route 53 health check. Create a new pipeline stage for each Region where the application is deployed. Configure a CloudWatch alarm action to exit with an error if the CloudWatch alarm is in the ALARM state.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个运行在Amazon EC2实例上的应用程序。该公司使用AWS CodePipeline管道将应用程序部署到多个AWS区域。管道配置了每个区域对应的阶段。每个阶段包含针对每个区域的AWS CloudFormation操作。当管道将应用程序部署到一个区域时，公司希望在管道继续到下一个区域之前确认应用程序处于健康状态。Amazon Route 53记录集已为每个区域的应用程序配置。DevOps工程师为部署应用程序的每个区域创建了基于Amazon CloudWatch告警的Route 53健康检查。DevOps工程师接下来应该做什么来满足要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在CodePipeline的多区域部署过程中，实现区域间的健康状态检查机制，确保应用程序在一个区域部署成功并处于健康状态后，才能继续部署到下一个区域。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务 - Amazon EC2：计算实例 - AWS CloudFormation：基础设施即代码 - Amazon Route 53：DNS和健康检查服务 - Amazon CloudWatch：监控和告警服务 - CloudWatch Agent：实例级别的监控代理 **正确答案D的原因：** 1. **直接的健康状态报告**：通过在EC2实例上配置CloudWatch agent，可以直接从应用程序层面报告状态到Route 53健康检查，这是最准确的健康状态判断方式 2. **集成的监控链路**：建立了从应用程序→CloudWatch agent→Route 53健康检查→CloudWatch告警的完整监控链路 3. **管道集成**：在管道中为每个区域创建新的阶段，通过CloudWatch告警操作来检查状态，实现了自动化的健康检查流程 4. **实时性**：CloudWatch agent能够实时监控应用程序状态，提供及时的健康状态反馈 **其他选项错误的原因：** - **选项A**：使用Step Functions增加了不必要的复杂性，而且没有直接利用已有的Route 53健康检查机制 - **选项B**：完全改变了部署架构，用CodeDeploy替换CloudFormation，这不符合现有的基础设施部署需求，过度工程化 - **选项C**：没有说明如何获取准确的应用程序健康状态数据，缺少从应用程序到监控系统的数据链路 **决策标准和最佳实践：** 1. **最小化架构变更**：在现有架构基础上进行增强，而不是重新设计 2. **利用现有资源**：充分利用已配置的Route 53健康检查和CloudWatch告警 3. **监控数据的准确性**：通过CloudWatch agent从应用程序层面获取最准确的健康状态 4. **自动化集成**：将健康检查无缝集成到CodePipeline流程中，实现自动化的部署门控机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">141</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company plans to use Amazon CloudWatch to monitor its Amazon EC2 instances. The company needs to stop EC2 instances when the average of the NetworkPacketsIn metric is less than 5 for at least 3 hours in a 12-hour time window. The company must evaluate the metric every hour. The EC2 instances must continue to run if there is missing data for the NetworkPacketsIn metric during the evaluation period. A DevOps engineer creates a CloudWatch alarm for the NetworkPacketsIn metric. The DevOps engineer configures a threshold value of 5 and an evaluation period of 1 hour. Which set of additional actions should the DevOps engineer take to meet these requirements? Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure the Datapoints to Alarm value to be 3 out of 12. Configure the alarm to treat missing data as breaching the threshold. Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state.
B. Configure the Datapoints to Alarm value to be 3 out of 12. Configure the alarm to treat missing data as not breaching the threshold. Add an EC2 action to stop the instance when the alarm enters the ALARM state.
C. Configure the Datapoints to Alarm value to be 9 out of 12. Configure the alarm to treat missing data as breaching the threshold. Add an EC2 action to stop the instance when the alarm enters the ALARM state.
D. Configure the Datapoints to Alarm value to be 9 out of 12. Configure the alarm to treat missing data as not breaching the threshold.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司计划使用Amazon CloudWatch来监控其Amazon EC2实例。公司需要在12小时时间窗口内，当NetworkPacketsIn指标的平均值连续至少3小时小于5时停止EC2实例。公司必须每小时评估一次该指标。如果在评估期间NetworkPacketsIn指标数据缺失，EC2实例必须继续运行。一位DevOps工程师为NetworkPacketsIn指标创建了CloudWatch告警，配置了阈值为5，评估周期为1小时。DevOps工程师还应该采取哪些额外操作来满足这些要求？ 选项： A. 配置Datapoints to Alarm值为3 out of 12。配置告警将缺失数据视为违反阈值。添加AWS Systems Manager操作在告警进入ALARM状态时停止实例。 B. 配置Datapoints to Alarm值为3 out of 12。配置告警将缺失数据视为不违反阈值。添加EC2操作在告警进入ALARM状态时停止实例。 C. 配置Datapoints to Alarm值为9 out of 12。配置告警将缺失数据视为违反阈值。添加EC2操作在告警进入ALARM状态时停止实例。 D. 配置Datapoints to Alarm值为9 out of 12。配置告警将缺失数据视为不违反阈值。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 在12小时窗口内每小时评估NetworkPacketsIn指标 2. 当平均值连续至少3小时小于5时停止EC2实例 3. 缺失数据时实例必须继续运行（不触发告警） 4. 需要配置自动停止实例的操作 **涉及的关键AWS服务和概念：** - CloudWatch告警的Datapoints to Alarm配置 - CloudWatch告警的缺失数据处理策略 - EC2自动操作vs Systems Manager操作 - 告警阈值和评估逻辑 **正确答案C的原因：** 1. **Datapoints配置正确**：9 out of 12意味着在12个数据点中需要9个违反阈值才触发告警。由于要求&quot;至少3小时小于5&quot;，实际上是要求大部分时间（9小时）都正常，只有少数时间异常才停止 2. **缺失数据处理正确**：配置为&quot;breaching threshold&quot;意味着缺失数据被视为违反阈值，但由于需要9个违反点才触发，偶尔的缺失不会立即触发告警 3. **操作类型正确**：EC2 action可以直接停止实例，比Systems Manager更直接有效 **其他选项错误的原因：** - **选项A错误**：使用Systems Manager而非EC2直接操作，且3 out of 12的配置过于敏感 - **选项B错误**：3 out of 12配置错误，会导致告警过于敏感，不符合&quot;至少3小时&quot;的要求 - **选项D错误**：缺少停止实例的操作配置 **决策标准和最佳实践：** 1. CloudWatch告警应该根据业务需求合理配置敏感度 2. 对于自动化运维操作，应优先使用原生服务功能 3. 缺失数据的处理策略应该与业务连续性要求保持一致 4. 告警配置应该避免误报，特别是涉及自动停止资源的场景</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">142</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages 500 AWS accounts that are in an organization in AWS Organizations. The company discovers many unattached Amazon Elastic Block Store (Amazon EBS) volumes in all the accounts. The company wants to automatically tag the unattached EBS volumes for investigation. A DevOps engineer needs to deploy an AWS Lambda function to all the AWS accounts. The Lambda function must run every 30 minutes to tag all the EBS volumes that have been unattached for a period of 7 days or more. Which solution will meet these requirements in the MOST operationally efficient manner? Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Create a custom script in the organization&#x27;s management account that assumes the role and deploys the CloudFormation template to the member accounts. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization. Create an Amazon EventBridge event bus in the delegated administrator account to invoke the Lambda function in each member account every 30 minutes.
B. Create a cross-account IAM role in the organization&#x27;s member accounts. Attach the AWSLambda_FullAccess policy and the AWSCloudFormationFullAccess policy to the role. Create an AWS CloudFormation template that contains the Lambda function and an
C. Configure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization
D. Create a cross-account IAM role in the organization&#x27;s member accounts. Attach the AmazonS3FullAccess policy and the AWSCodeDeployDeployerAccess policy to the role. Use AWS CodeDeploy to assume the role to deploy the Lambda function from the organization&#x27;s management account. Configure an Amazon EventBridge scheduled rule in the member accounts to invoke the Lambda function every 30 minutes.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司管理着500个AWS账户，这些账户在AWS Organizations中的一个组织里。公司发现所有账户中有许多未附加的Amazon Elastic Block Store (Amazon EBS)卷。公司希望自动为这些未附加的EBS卷打标签以便调查。DevOps工程师需要将AWS Lambda函数部署到所有AWS账户中。Lambda函数必须每30分钟运行一次，为所有已经未附加7天或更长时间的EBS卷打标签。哪个解决方案能以最具操作效率的方式满足这些要求？ 选项： A. 为组织配置委托管理员账户。创建包含Lambda函数的AWS CloudFormation模板。使用CloudFormation StackSets从委托管理员账户将CloudFormation模板部署到组织中的所有成员账户。在委托管理员账户中创建Amazon EventBridge事件总线，每30分钟调用每个成员账户中的Lambda函数。 B. 在组织的成员账户中创建跨账户IAM角色。将AWSLambda_FullAccess策略和AWSCloudFormationFullAccess策略附加到该角色。创建包含Lambda函数和Amazon EventBridge计划规则的AWS CloudFormation模板，每30分钟调用Lambda函数。在组织的管理账户中创建自定义脚本，该脚本承担角色并将CloudFormation模板部署到成员账户。 C. 为组织配置委托管理员账户。创建包含Lambda函数和Amazon EventBridge计划规则的AWS CloudFormation模板，每30分钟调用Lambda函数。使用CloudFormation StackSets从委托管理员账户将CloudFormation模板部署到组织中的所有成员账户。 D. 在组织的成员账户中创建跨账户IAM角色。将AmazonS3FullAccess策略和AWSCodeDeployDeployerAccess策略附加到该角色。使用AWS CodeDeploy承担角色从组织的管理账户部署Lambda函数。在成员账户中配置Amazon EventBridge计划规则，每30分钟调用Lambda函数。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在500个AWS账户中部署Lambda函数来自动标记未附加7天以上的EBS卷，需要每30分钟执行一次，并且要求最具操作效率的解决方案。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - CloudFormation StackSets：跨多个账户和区域部署CloudFormation模板的服务 - 委托管理员账户：Organizations中可以代表组织执行管理任务的账户 - Lambda函数：无服务器计算服务 - EventBridge：事件驱动的服务，可以创建计划规则 - 跨账户IAM角色：允许跨账户访问的权限机制 **正确答案A的原因：** 1. **使用委托管理员账户**：这是AWS Organizations的最佳实践，避免直接使用管理账户进行操作部署 2. **CloudFormation StackSets**：专门设计用于跨多个账户部署资源，是最适合大规模部署的工具 3. **集中化事件总线**：在委托管理员账户中创建EventBridge事件总线，可以统一管理和调度所有成员账户中的Lambda函数 4. **操作效率最高**：一次性部署，集中管理，易于维护和监控 **其他选项错误的原因：** - **选项B**：使用自定义脚本而不是StackSets，操作复杂度高，不够标准化，维护困难 - **选项C**：虽然使用了StackSets和委托管理员账户，但每个账户都有独立的EventBridge规则，缺乏集中管理，不如选项A高效 - **选项D**：使用CodeDeploy部署Lambda函数不是标准做法，且权限配置不当（S3和CodeDeploy权限与Lambda部署需求不匹配） **决策标准和最佳实践：** 1. **大规模部署优先选择StackSets**：对于跨多个账户的资源部署，StackSets是AWS推荐的标准解决方案 2. **使用委托管理员账户**：避免管理账户承担过多操作任务，提高安全性 3. **集中化管理**：统一的事件调度比分散的调度更易于管理和监控 4. **标准化工具优于自定义脚本**：使用AWS原生服务比自定义解决方案更可靠和可维护</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">143</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s production environment uses an AWS CodeDeploy blue/green deployment to deploy an application. The deployment includes Amazon EC2 Auto Scaling groups that launch instances that run Amazon Linux 2. A working appspec.yml file exists in the code repository and contains the following text: A DevOps engineer needs to ensure that a script downloads and installs a license file onto the instances before the replacement instances start to handle request traffic. The DevOps engineer adds a hooks section to the appspec.yml file. Which hook should the DevOps engineer use to run the script that downloads and installs the license file? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. AfterBlockTraffic
B. BeforeBlockTraffic
C. BeforeInstall
D. DownloadBundle</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的生产环境使用AWS CodeDeploy蓝绿部署来部署应用程序。该部署包括启动运行Amazon Linux 2实例的Amazon EC2 Auto Scaling组。代码仓库中存在一个可用的appspec.yml文件并包含以下文本：一名DevOps工程师需要确保在替换实例开始处理请求流量之前，有一个脚本下载并安装许可证文件到实例上。DevOps工程师向appspec.yml文件添加了一个hooks部分。DevOps工程师应该使用哪个hook来运行下载和安装许可证文件的脚本？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodeDeploy蓝绿部署中hooks的执行顺序和用途。需要在新实例开始接收流量之前下载并安装许可证文件。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：自动化部署服务 - 蓝绿部署：零停机部署策略 - appspec.yml：部署配置文件 - Deployment hooks：部署生命周期中的钩子函数 - EC2 Auto Scaling：自动扩展服务 **正确答案的原因：** 虽然题目显示答案是D (DownloadBundle)，但这实际上是错误的。正确答案应该是C (BeforeInstall)。原因如下： - BeforeInstall hook在应用程序文件安装之前执行 - 这是设置环境、下载依赖文件（如许可证）的理想时机 - 确保在应用程序启动前所有必需的文件都已就位 **其他选项错误的原因：** - A. AfterBlockTraffic：在阻止流量后执行，时机太晚 - B. BeforeBlockTraffic：在阻止流量前执行，但这时新实例可能已经在处理流量 - D. DownloadBundle：这不是一个有效的CodeDeploy hook名称 **决策标准和最佳实践：** - 理解CodeDeploy hooks的执行顺序：DownloadBundle → BeforeInstall → Install → AfterInstall → ApplicationStart → ApplicationStop → BeforeBlockTraffic → BlockTraffic → AfterBlockTraffic - 环境准备工作应在BeforeInstall阶段完成 - 许可证文件属于应用程序运行的前置条件，应尽早安装</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">144</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that includes AWS Lambda functions. The Lambda functions run Python code that is stored in an AWS CodeCommit repository. The company has recently experienced failures in the production environment because of an error in the Python code. An engineer has written unit tests for the Lambda functions to help avoid releasing any future defects into the production environment. The company&#x27;s DevOps team needs to implement a solution to integrate the unit tests into an existing AWS CodePipeline pipeline. The solution must produce reports about the unit tests for the company to view. Which solution will meet these requirements? unit tests with an output of HTML in the phases section. In the reports section, upload the test reports to the S3 bucket. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a buildspec.yml file in the CodeCommit repository. In the buildspec.yml file, define the actions to run a CodeGuru review.
B. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a CodeBuild report group. Create a buildspec.yml file in the CodeCommit repository. In the buildspec.yml file, define the actions to run the unit tests with an output of JUNITXML in the build phase section. Configure the test reports to be uploaded to the new CodeBuild report group.
C. Create a new AWS CodeArtifact repository. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create an appspec.yml file in the original CodeCommit repository. In the appspec.yml file, define the actions to run the unit tests with an output of CUCUMBERJSON in the build phase section. Configure the test reports to be sent to the new CodeArtifact repository.
D. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a new Amazon S3 bucket. Create a buildspec.yml file in the CodeCommit repository. In the buildspec.yml file, define the actions to run the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个包含AWS Lambda函数的应用程序。Lambda函数运行存储在AWS CodeCommit存储库中的Python代码。该公司最近在生产环境中遇到了因Python代码错误导致的故障。一名工程师为Lambda函数编写了单元测试，以帮助避免将来的缺陷发布到生产环境中。公司的DevOps团队需要实施一个解决方案，将单元测试集成到现有的AWS CodePipeline管道中。该解决方案必须生成关于单元测试的报告供公司查看。哪个解决方案能满足这些要求？ 选项： A. 将CodeCommit存储库与Amazon CodeGuru Reviewer关联。创建一个新的AWS CodeBuild项目。在CodePipeline管道中，配置一个使用新CodeBuild项目的测试阶段。在CodeCommit存储库中创建buildspec.yml文件。在buildspec.yml文件中，定义运行CodeGuru审查的操作。 B. 创建一个新的AWS CodeBuild项目。在CodePipeline管道中，配置一个使用新CodeBuild项目的测试阶段。创建一个CodeBuild报告组。在CodeCommit存储库中创建buildspec.yml文件。在buildspec.yml文件中，定义在构建阶段运行单元测试并输出JUNITXML格式的操作。配置测试报告上传到新的CodeBuild报告组。 C. 创建一个新的AWS CodeArtifact存储库。创建一个新的AWS CodeBuild项目。在CodePipeline管道中，配置一个使用新CodeBuild项目的测试阶段。在原始CodeCommit存储库中创建appspec.yml文件。在appspec.yml文件中，定义在构建阶段运行单元测试并输出CUCUMBERJSON格式的操作。配置测试报告发送到新的CodeArtifact存储库。 D. 创建一个新的AWS CodeBuild项目。在CodePipeline管道中，配置一个使用新CodeBuild项目的测试阶段。创建一个新的Amazon S3存储桶。在CodeCommit存储库中创建buildspec.yml文件。在buildspec.yml文件中，定义运行...（选项D似乎不完整）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在现有的CodePipeline中集成单元测试，并且能够生成和查看测试报告。关键需求包括：1）集成单元测试到CI/CD管道；2）生成可查看的测试报告；3）使用现有的CodeCommit存储库。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务 - AWS CodeBuild：构建和测试服务 - AWS CodeCommit：Git代码存储库 - buildspec.yml：CodeBuild的构建规范文件 - CodeBuild报告组：用于收集和展示测试报告 - 测试报告格式：JUNITXML、CUCUMBERJSON等 **正确答案的原因：** 虽然题目显示正确答案是C，但从技术角度分析，选项B实际上是最合适的解决方案： - 使用CodeBuild项目来运行单元测试是标准做法 - buildspec.yml是CodeBuild的正确配置文件（而非appspec.yml） - JUNITXML是Python单元测试的标准输出格式 - CodeBuild报告组是AWS原生的测试报告展示解决方案 - 整个流程符合AWS最佳实践 **其他选项错误的原因：** - 选项A：CodeGuru Reviewer主要用于代码质量审查，不是专门用于运行单元测试和生成测试报告 - 选项C：使用了错误的配置文件（appspec.yml用于CodeDeploy而非CodeBuild），CUCUMBERJSON格式不适合Python单元测试，CodeArtifact是包管理服务而非报告存储 - 选项D：内容不完整，无法完整评估 **决策标准和最佳实践：** 选择测试集成方案时应考虑：1）使用正确的配置文件格式；2）选择适合的测试报告格式；3）使用AWS原生服务来管理报告；4）确保解决方案的可维护性和可扩展性。CodeBuild + 报告组的组合是AWS推荐的标准做法。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">145</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages multiple AWS accounts in AWS Organizations. The company&#x27;s security policy states that AWS account root user credentials for member accounts must not be used. The company monitors access to the root user credentials. A recent alert shows that the root user in a member account launched an Amazon EC2 instance. A DevOps engineer must create an SCP at the organization&#x27;s root level that will prevent the root user in member accounts from making any AWS service API calls. Which SCP will meet these requirements? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content"></div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中管理多个AWS账户。公司的安全策略规定不得使用成员账户的AWS账户root用户凭证。公司监控对root用户凭证的访问。最近的一个警报显示成员账户中的root用户启动了一个Amazon EC2实例。DevOps工程师必须在组织的根级别创建一个SCP，以防止成员账户中的root用户进行任何AWS服务API调用。哪个SCP能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要创建一个Service Control Policy (SCP)来阻止AWS Organizations成员账户中的root用户执行任何AWS服务API调用，同时该SCP需要部署在组织的根级别。 **涉及的关键AWS服务和概念：** - AWS Organizations：用于集中管理多个AWS账户的服务 - Service Control Policy (SCP)：组织级别的权限边界策略，用于限制账户中用户和角色的最大权限 - Root用户：每个AWS账户的最高权限用户 - Principal概念：在IAM策略中用于指定被授权或拒绝的身份 **正确答案的原因：** 选项B应该包含以下关键要素： - 使用&quot;Deny&quot;效果来明确拒绝root用户的操作 - 在Principal字段中正确指定root用户（通常使用&quot;aws:PrincipalType&quot;: &quot;Root&quot;条件或类似的root用户标识） - 对所有AWS服务API调用进行限制（Action: &quot;*&quot;） - 策略结构符合SCP的JSON格式要求 **其他选项错误的原因：** - 可能使用了错误的Principal指定方式 - 可能使用了&quot;Allow&quot;而非&quot;Deny&quot;效果 - 可能没有正确覆盖所有AWS API调用 - 可能在条件语句中存在语法错误或逻辑错误 **决策标准和最佳实践：** 1. SCP采用默认拒绝模式，需要明确的Deny语句来阻止特定操作 2. 正确识别root用户的方法是关键，通常通过aws:PrincipalType条件键 3. SCP应该精确定位目标主体，避免影响其他合法用户和角色 4. 在生产环境部署前应该在测试环境验证SCP的效果 5. 考虑紧急情况下的访问恢复机制，确保不会完全锁定账户管理能力</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">146</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS and has a VPC that contains critical compute infrastructure with predictable traffic patterns. The company has configured VPC flow logs that are published to a log group in Amazon CloudWatch Logs. The company&#x27;s DevOps team needs to configure a monitoring solution for the VPC flow logs to identify anomalies in network traffic to the VPC over time. If the monitoring solution detects an anomaly, the company needs the ability to initiate a response to the anomaly. How should the DevOps team configure the monitoring solution to meet these requirements? B (60%) A (40%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon Kinesis data stream. Subscribe the log group to the data stream. Configure Amazon Kinesis Data Analytics to detect log anomalies in the data stream. Create an AWS Lambda function to use as the output of the data stream. Configure the Lambda function to write to the default Amazon EventBridge event bus in the event of an anomaly finding.
B. Create an Amazon Kinesis Data Firehose delivery stream that delivers events to an Amazon S3 bucket. Subscribe the log group to the delivery stream. Configure Amazon Lookout for Metrics to monitor the data in the S3 bucket for anomalies. Create an AWS Lambda function to run in response to Lookout for Metrics anomaly findings. Configure the Lambda function to publish to the default Amazon EventBridge event bus.
C. Create an AWS Lambda function to detect anomalies. Configure the Lambda function to publish an event to the default Amazon EventBridge event bus if the Lambda function detects an anomaly. Subscribe the Lambda function to the log group.
D. Create an Amazon Kinesis data stream. Subscribe the log group to the data stream. Create an AWS Lambda function to detect log anomalies. Configure the Lambda function to write to the default Amazon EventBridge event bus if the Lambda function detects an anomaly. Set the Lambda function as the processor for the data stream.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS，拥有一个包含关键计算基础设施且流量模式可预测的VPC。该公司已配置VPC flow logs并将其发布到Amazon CloudWatch Logs中的日志组。公司的DevOps团队需要为VPC flow logs配置监控解决方案，以识别VPC网络流量中的异常情况。如果监控解决方案检测到异常，公司需要能够对异常启动响应。DevOps团队应该如何配置监控解决方案来满足这些要求？ 选项： A. 创建Amazon Kinesis data stream。将日志组订阅到数据流。配置Amazon Kinesis Data Analytics来检测数据流中的日志异常。创建AWS Lambda函数作为数据流的输出。配置Lambda函数在发现异常时写入默认的Amazon EventBridge事件总线。 B. 创建Amazon Kinesis Data Firehose delivery stream将事件传送到Amazon S3存储桶。将日志组订阅到delivery stream。配置Amazon Lookout for Metrics监控S3存储桶中的数据异常。创建AWS Lambda函数响应Lookout for Metrics异常发现。配置Lambda函数发布到默认的Amazon EventBridge事件总线。 C. 创建AWS Lambda函数检测异常。配置Lambda函数在检测到异常时发布事件到默认的Amazon EventBridge事件总线。将Lambda函数订阅到日志组。 D. 创建Amazon Kinesis data stream。将日志组订阅到数据流。创建AWS Lambda函数检测日志异常。配置Lambda函数在检测到异常时写入默认的Amazon EventBridge事件总线。将Lambda函数设置为数据流的处理器。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要为VPC flow logs建立一个监控解决方案，能够检测网络流量异常并在发现异常时启动响应机制。关键要求包括：实时流处理、异常检测、自动化响应能力。 **涉及的关键AWS服务和概念：** - VPC Flow Logs：网络流量日志记录 - Amazon Kinesis Data Streams：实时数据流处理 - Amazon Kinesis Data Analytics：流数据分析和异常检测 - Amazon Kinesis Data Firehose：数据传输服务 - Amazon Lookout for Metrics：机器学习异常检测服务 - AWS Lambda：无服务器计算 - Amazon EventBridge：事件驱动架构 **正确答案A的原因：** 1. **架构完整性**：Kinesis Data Streams提供实时流处理能力，适合处理持续的VPC flow logs 2. **专业异常检测**：Kinesis Data Analytics内置异常检测算法，专门设计用于流数据分析，无需自定义开发 3. **实时响应**：整个流程支持近实时处理，从日志产生到异常检测到响应触发 4. **可扩展性**：Kinesis服务具有良好的自动扩展能力，适合处理可预测的流量模式 **其他选项错误的原因：** - **选项B**：虽然技术上可行，但Firehose主要用于批量数据传输到S3，增加了延迟；Lookout for Metrics更适合业务指标监控而非网络日志异常检测 - **选项C**：Lambda直接处理CloudWatch Logs存在扩展性限制，且需要自行实现异常检测算法，复杂度高 - **选项D**：虽然使用了Kinesis Data Streams，但异常检测逻辑需要在Lambda中自行实现，不如使用专门的Kinesis Data Analytics服务 **决策标准和最佳实践：** 1. **选择专门服务**：优先使用AWS专门的异常检测服务而非自行开发 2. **实时性考虑**：网络安全监控需要尽可能实时的响应能力 3. **架构简洁性**：避免不必要的中间存储步骤（如S3）来减少延迟 4. **可维护性**：使用托管服务减少运维复杂度</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">147</div>
        <div class="field-label">Question:</div>
        <div class="field-content">AnyCompany is using AWS Organizations to create and manage multiple AWS accounts. AnyCompany recently acquired a smaller company, Example Corp. During the acquisition process, Example Corp&#x27;s single AWS account joined AnyCompany&#x27;s management account through an Organizations invitation. AnyCompany moved the new member account under an OU that is dedicated to Example Corp. AnyCompany&#x27;s DevOps engineer has an IAM user that assumes a role that is named OrganizationAccountAccessRole to access member accounts. This role is configured with a full access policy. When the DevOps engineer tries to use the AWS Management Console to assume the role in Example Corp&#x27;s new member account, the DevOps engineer receives the following error message: &quot;Invalid information in one or more fields. Check your information or contact your administrator.&quot; Which solution will give the DevOps engineer access to the new member account? C (89%) D (6%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the management account, grant the DevOps engineer&#x27;s IAM user permission to assume the OrganizationAccountAccessRole IAM role in the new member account.
B. In the management account, create a new SCP. In the SCP, grant the DevOps engineer&#x27;s IAM user full access to all resources in the new member account. Attach the SCP to the OU that contains the new member account.
C. In the new member account, create a new IAM role that is named OrganizationAccountAccessRole. Attach the AdministratorAccess AWS managed policy to the role. In the role&#x27;s trust policy, grant the management account permission to assume the role.
D. In the new member account, edit the trust policy for the OrganizationAccountAccessRole IAM role. Grant the management account permission to assume the role.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">AnyCompany正在使用AWS Organizations来创建和管理多个AWS账户。AnyCompany最近收购了一家较小的公司Example Corp。在收购过程中，Example Corp的单个AWS账户通过Organizations邀请加入了AnyCompany的管理账户。AnyCompany将新成员账户移动到专门为Example Corp设立的OU下。AnyCompany的DevOps工程师有一个IAM用户，该用户承担名为OrganizationAccountAccessRole的角色来访问成员账户。此角色配置了完全访问策略。当DevOps工程师尝试使用AWS Management Console在Example Corp的新成员账户中承担该角色时，DevOps工程师收到以下错误消息：&quot;一个或多个字段中的信息无效。请检查您的信息或联系您的管理员。&quot;哪个解决方案将为DevOps工程师提供对新成员账户的访问权限？ 选项： A. 在管理账户中，授予DevOps工程师的IAM用户在新成员账户中承担OrganizationAccountAccessRole IAM角色的权限。 B. 在管理账户中，创建一个新的SCP。在SCP中，授予DevOps工程师的IAM用户对新成员账户中所有资源的完全访问权限。将SCP附加到包含新成员账户的OU。 C. 在新成员账户中，创建一个名为OrganizationAccountAccessRole的新IAM角色。将AdministratorAccess AWS托管策略附加到该角色。在角色的信任策略中，授予管理账户承担该角色的权限。 D. 在新成员账户中，编辑OrganizationAccountAccessRole IAM角色的信任策略。授予管理账户承担该角色的权限。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题考查的是AWS Organizations中跨账户角色访问的配置问题。DevOps工程师无法承担新加入成员账户中的OrganizationAccountAccessRole角色，需要找到正确的解决方案来修复访问权限。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. IAM角色和跨账户访问 - 身份和访问管理 3. 信任策略(Trust Policy) - 定义谁可以承担角色 4. SCP(Service Control Policies) - 组织级别的权限边界 5. 账户邀请和加入流程 **正确答案D的原因：** 1. 当外部账户通过邀请加入Organizations时，该账户中现有的OrganizationAccountAccessRole角色的信任策略可能没有正确配置，不允许管理账户承担该角色 2. 错误消息&quot;Invalid information in one or more fields&quot;通常表示角色存在但信任关系配置不正确 3. 编辑现有角色的信任策略是最直接和高效的解决方案，只需要添加管理账户的权限即可 4. 这种方法保持了现有的角色结构，避免了重复创建 **其他选项错误的原因：** - **选项A错误：** 问题不在于管理账户中DevOps用户的权限，而在于目标账户中角色的信任策略配置 - **选项B错误：** SCP是用于设置权限边界的，不能直接授予访问权限，且SCP不能解决角色信任关系的问题 - **选项C错误：** 虽然技术上可行，但创建新角色是不必要的，因为角色已经存在，只需要修复信任策略即可 **决策标准和最佳实践：** 1. 优先修复现有配置而不是重新创建资源 2. 理解跨账户访问需要双向权限：源账户的权限和目标账户角色的信任策略 3. 在Organizations环境中，通过邀请加入的账户需要手动配置信任关系 4. 信任策略应该明确指定管理账户ID或使用适当的条件来限制访问 5. 遵循最小权限原则，确保信任策略只授予必要的访问权限</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">148</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is designing an application that integrates with a legacy REST API. The application has an AWS Lambda function that reads records from an Amazon Kinesis data stream. The Lambda function sends the records to the legacy REST API. Approximately 10% of the records that the Lambda function sends from the Kinesis data stream have data errors and must be processed manually. The Lambda function event source configuration has an Amazon Simple Queue Service (Amazon SQS) dead-letter queue as an on-failure destination. The DevOps engineer has configured the Lambda function to process records in batches and has implemented retries in case of failure. During testing, the DevOps engineer notices that the dead-letter queue contains many records that have no data errors and that already have been processed by the legacy REST API. The DevOps engineer needs to configure the Lambda function&#x27;s event source options to reduce the number of errorless records that are sent to the dead-letter queue. Which solution will meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Increase the retry attempts.
B. Configure the setting to split the batch when an error occurs.
C. Increase the concurrent batches per shard.
D. Decrease the maximum age of record.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在设计一个与遗留REST API集成的应用程序。该应用程序有一个AWS Lambda函数，从Amazon Kinesis数据流中读取记录。Lambda函数将这些记录发送到遗留REST API。Lambda函数从Kinesis数据流发送的记录中，大约10%存在数据错误，必须手动处理。Lambda函数事件源配置有一个Amazon Simple Queue Service (Amazon SQS)死信队列作为失败时的目标。DevOps工程师已配置Lambda函数批量处理记录，并在失败时实现重试。在测试过程中，DevOps工程师注意到死信队列包含许多没有数据错误且已被遗留REST API成功处理的记录。DevOps工程师需要配置Lambda函数的事件源选项，以减少发送到死信队列的无错误记录数量。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题的关键在于解决批处理中的&quot;连坐&quot;问题。当Lambda函数批量处理Kinesis记录时，如果批次中有一条记录失败，整个批次都会被重试，最终可能导致整个批次（包括正常记录）都被发送到死信队列。 **涉及的关键AWS服务和概念：** - Amazon Kinesis Data Streams：流数据处理服务 - AWS Lambda：无服务器计算服务，作为Kinesis的消费者 - Lambda事件源映射：连接Kinesis和Lambda的配置 - Amazon SQS死信队列：存储处理失败记录的队列 - 批处理机制：Lambda可以批量处理多条Kinesis记录 - BisectBatchOnFunctionError设置：出错时拆分批次的功能 **正确答案B的原因：** 配置&quot;split the batch when an error occurs&quot;（BisectBatchOnFunctionError=true）是最佳解决方案。当批次中有记录失败时，Lambda会自动将批次一分为二，分别重试两个较小的批次。这样可以逐步隔离有问题的记录，避免正常记录因为同批次中的错误记录而被误送到死信队列。这种二分法最终能够精确定位到具体的错误记录。 **其他选项错误的原因：** - A选项（增加重试次数）：只会延长处理时间，不能解决正常记录被连累的根本问题 - C选项（增加每个分片的并发批次）：提高并发性能，但不解决批次内记录连坐的问题 - D选项（减少记录最大存活时间）：会导致记录更快过期，可能丢失有效数据 **决策标准和最佳实践：** 在设计流数据处理架构时，应该考虑错误隔离机制，避免&quot;一个坏苹果影响整筐苹果&quot;的情况。BisectBatchOnFunctionError是AWS专门为此场景设计的功能，能够在保持批处理效率的同时，精确处理错误记录，这是处理部分记录失败场景的最佳实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">149</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has microservices running in AWS Lambda that read data from Amazon DynamoDB. The Lambda code is manually deployed by developers after successful testing. The company now needs the tests and deployments to be automated and run in the cloud. Additionally, traffic to the new versions of each microservice should be incrementally shifted over time after deployment. What solution meets all the requirements, ensuring the MOST developer velocity? event trigger that runs a Lambda function that deploys the new version. Use an interval in the Lambda function to deploy the code over time at the required percentage. C (94%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CodePipeline configuration and set up a post-commit hook to trigger the pipeline after tests have passed. Use AWS CodeDeploy and create a Canary deployment configuration that specifies the percentage of traffic and interval.
B. Create an AWS CodeBuild configuration that triggers when the test code is pushed. Use AWS CloudFormation to trigger an AWS CodePipeline configuration that deploys the new Lambda versions and specifies the traffic shift percentage and interval.
C. Create an AWS CodePipeline configuration and set up the source code step to trigger when code is pushed. Set up the build step to use AWS CodeBuild to run the tests. Set up an AWS CodeDeploy configuration to deploy, then select the CodeDeployDefault.LambdaLinear10PercentEvery3Minutes option.
D. Use the AWS CLI to set up a post-commit hook that uploads the code to an Amazon S3 bucket after tests have passed. Set up an S3</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有运行在AWS Lambda上的微服务，这些服务从Amazon DynamoDB读取数据。Lambda代码目前由开发人员在成功测试后手动部署。现在公司需要将测试和部署自动化并在云中运行。此外，在部署后，流量应该随着时间逐步增量地转移到每个微服务的新版本。什么解决方案能满足所有要求，确保最高的开发速度？ 选项： A. 创建AWS CodePipeline配置，设置post-commit hook在测试通过后触发pipeline。使用AWS CodeDeploy并创建Canary部署配置来指定流量百分比和间隔。 B. 创建AWS CodeBuild配置，在测试代码推送时触发。使用AWS CloudFormation触发AWS CodePipeline配置来部署新的Lambda版本并指定流量转移百分比和间隔。 C. 创建AWS CodePipeline配置，设置源代码步骤在代码推送时触发。设置构建步骤使用AWS CodeBuild运行测试。设置AWS CodeDeploy配置进行部署，然后选择CodeDeployDefault.LambdaLinear10PercentEvery3Minutes选项。 D. 使用AWS CLI设置post-commit hook，在测试通过后将代码上传到Amazon S3存储桶。设置S3事件触发器运行Lambda函数来部署新版本。在Lambda函数中使用间隔来按要求的百分比逐步部署代码。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 自动化测试和部署流程 2. 在云中运行CI/CD 3. 支持增量流量转移（渐进式部署） 4. 确保最高的开发速度 **涉及的关键AWS服务和概念：** - AWS CodePipeline：完整的CI/CD管道服务 - AWS CodeBuild：托管的构建服务 - AWS CodeDeploy：自动化部署服务，支持Lambda的渐进式部署 - Lambda别名和版本管理 - 流量转移策略（Canary部署、Linear部署） **正确答案A的原因：** 1. **完整的自动化流程**：CodePipeline提供端到端的CI/CD管道 2. **合适的触发机制**：post-commit hook确保代码提交后自动触发，但只在测试通过后执行 3. **专业的部署服务**：CodeDeploy专门设计用于处理应用程序部署，包括Lambda函数 4. **灵活的流量管理**：Canary部署配置允许自定义流量百分比和时间间隔 5. **开发速度最优**：使用AWS托管服务，减少自定义代码和维护工作 **其他选项错误的原因：** - **选项B**：架构过于复杂，使用CloudFormation触发CodePipeline是不必要的间接方式，降低了开发速度 - **选项C**：虽然技术上可行，但在测试失败时仍会触发部署流程，不符合&quot;测试通过后部署&quot;的要求 - **选项D**：使用自定义Lambda函数实现部署逻辑，需要大量自定义代码，维护复杂度高，不利于开发速度 **决策标准和最佳实践：** 1. **优先使用托管服务**：减少自定义代码和运维负担 2. **确保测试门控**：只有测试通过才能进行部署 3. **选择合适的部署策略**：Canary部署比Linear部署更灵活 4. **考虑开发体验**：简单、可靠的工具链有助于提高开发速度 5. **遵循AWS Well-Architected原则**：使用专门的服务处理特定任务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">150</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is building a web and mobile application that uses a serverless architecture powered by AWS Lambda and Amazon API Gateway. The company wants to fully automate the backend Lambda deployment based on code that is pushed to the appropriate environment branch in an AWS CodeCommit repository. The deployment must have the following: • Separate environment pipelines for testing and production • Automatic deployment that occurs for test environments only Which steps should be taken to meet these requirements? C (100%) Get IT Certification Unlock free, top-quality video courses on ExamTopics with a simple registration. Enhance your learning with our expertly curated content and educational resources designed for ExamTopics!</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a new AWS CodePipeline service. Create a CodeCommit repository for each environment. Set up CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.
B. Create two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create a CodeCommit repository for each environment. Set up each CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.
C. Create two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Set up each CodePipeline to retrieve the source code from the appropriate branch in the repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.
D. Create an AWS CodeBuild configuration for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Push the Lambda function code to an Amazon S3 bucket. Set up the deployment step to deploy the Lambda functions from the S3 bucket.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在构建一个Web和移动应用程序，该应用程序使用由AWS Lambda和Amazon API Gateway驱动的无服务器架构。公司希望基于推送到AWS CodeCommit存储库中相应环境分支的代码，完全自动化后端Lambda部署。部署必须满足以下要求：• 为测试和生产环境提供独立的环境管道 • 仅对测试环境进行自动部署。应该采取哪些步骤来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个CI/CD解决方案，需要满足：1）为测试和生产环境创建独立的部署管道；2）测试环境自动部署，生产环境需要手动批准；3）基于不同分支的代码推送触发部署；4）部署Lambda函数。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：持续集成/持续部署服务，用于自动化代码构建、测试和部署流程 - AWS CodeCommit：托管的Git版本控制服务 - AWS Lambda：无服务器计算服务 - AWS CloudFormation：基础设施即代码服务，用于自动化资源部署 - 分支策略：使用不同分支管理不同环境的代码 **正确答案C的原因：** 选项C完美满足所有要求：1）创建两个独立的CodePipeline配置分别处理测试和生产环境；2）生产管道配置手动批准步骤，确保生产部署的可控性；3）使用单一CodeCommit存储库的不同分支管理代码，这是标准的Git工作流实践；4）每个管道从对应分支获取源代码；5）使用CloudFormation部署Lambda函数，提供基础设施即代码的优势。 **其他选项错误的原因：** - 选项A：缺少生产环境的手动批准步骤，且为每个环境创建独立存储库不是最佳实践 - 选项B：为每个环境创建独立的CodeCommit存储库增加了管理复杂性，违背了使用分支管理不同环境的Git最佳实践 - 选项D：使用CodeBuild而非CodePipeline作为主要编排工具不合适，且手动推送到S3的方式不够自动化 **决策标准和最佳实践：** 1）环境隔离：生产和测试环境应有独立的部署管道；2）部署控制：生产环境需要手动批准机制；3）代码管理：使用分支策略而非多存储库管理不同环境；4）自动化程度：测试环境全自动，生产环境半自动；5）基础设施即代码：使用CloudFormation确保部署的一致性和可重复性。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">151</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer wants to find a solution to migrate an application from on premises to AWS. The application is running on Linux and needs to run on specific versions of Apache Tomcat, HAProxy, and Varnish Cache to function properly. The application&#x27;s operating system-level parameters require tuning. The solution must include a way to automate the deployment of new application versions. The infrastructure should be scalable and faulty servers should be replaced automatically. Which solution should the DevOps engineer use? uses CodeCommit as a source and Elastic Beanstalk as a deployment provider. Create an AWS CodeDeploy deployment group associated with an Amazon EC2 Auto Scaling group. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and CodeDeploy as a deployment provider. Most Voted D (89%) 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Upload the application as a Docker image that contains all the necessary software to Amazon ECR. Create an Amazon ECS cluster using an AWS Fargate launch type and an Auto Scaling group. Create an AWS CodePipeline pipeline that uses Amazon ECR as a source and Amazon ECS as a deployment provider.
B. Upload the application code to an AWS CodeCommit repository with a saved configuration file to configure and install the software. Create an AWS Elastic Beanstalk web server tier and a load balanced-type environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.
C. Upload the application code to an AWS CodeCommit repository with a set of .ebextensions files to configure and install the software. Create an AWS Elastic Beanstalk worker tier environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that
D. Upload the application code to an AWS CodeCommit repository with an appspec.yml file to configure and install the necessary software.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师想要找到一个解决方案，将应用程序从本地迁移到AWS。该应用程序运行在Linux上，需要在特定版本的Apache Tomcat、HAProxy和Varnish Cache上运行才能正常工作。应用程序的操作系统级参数需要调优。解决方案必须包括自动化部署新应用程序版本的方法。基础设施应该是可扩展的，故障服务器应该自动替换。DevOps工程师应该使用哪种解决方案？ 选项： A. 将应用程序作为包含所有必要软件的Docker镜像上传到Amazon ECR。使用AWS Fargate启动类型和Auto Scaling组创建Amazon ECS集群。创建一个AWS CodePipeline流水线，使用Amazon ECR作为源，Amazon ECS作为部署提供者。 B. 将应用程序代码上传到AWS CodeCommit存储库，带有保存的配置文件来配置和安装软件。创建一个AWS Elastic Beanstalk Web服务器层和使用Tomcat解决方案堆栈的负载均衡类型环境。创建一个AWS CodePipeline流水线，使用CodeCommit作为源，Elastic Beanstalk作为部署提供者。 C. 将应用程序代码上传到AWS CodeCommit存储库，带有一组.ebextensions文件来配置和安装软件。创建一个使用Tomcat解决方案堆栈的AWS Elastic Beanstalk工作层环境。创建一个AWS CodePipeline流水线... D. 将应用程序代码上传到AWS CodeCommit存储库，带有appspec.yml文件来配置和安装必要的软件。创建一个与Amazon EC2 Auto Scaling组关联的AWS CodeDeploy部署组。创建一个AWS CodePipeline流水线，使用CodeCommit作为源，CodeDeploy作为部署提供者。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为一个Linux应用程序设计迁移方案，该应用需要特定版本的Apache Tomcat、HAProxy和Varnish Cache，需要操作系统级参数调优，同时要求自动化部署、可扩展性和故障自动恢复。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：源代码管理服务 - AWS CodeDeploy：应用程序部署服务 - AWS CodePipeline：持续集成/持续部署(CI/CD)流水线 - Amazon EC2 Auto Scaling：自动扩缩容 - AWS Elastic Beanstalk：平台即服务(PaaS) - Amazon ECS/Fargate：容器化服务 - appspec.yml：CodeDeploy的部署规范文件 **正确答案D的原因：** 1. **灵活性最高**：CodeDeploy允许在EC2实例上进行精确的软件安装和配置，可以安装特定版本的Apache Tomcat、HAProxy和Varnish Cache 2. **操作系统级控制**：EC2实例提供完整的操作系统访问权限，可以进行系统级参数调优 3. **自动扩展和故障恢复**：Auto Scaling组确保基础设施的可扩展性和故障实例的自动替换 4. **自动化部署**：CodePipeline + CodeDeploy提供完整的CI/CD解决方案 5. **appspec.yml**：提供了详细的部署步骤控制，可以精确管理软件安装和配置过程 **其他选项错误的原因：** - **选项A（ECS/Fargate）**：虽然容器化是好的实践，但Fargate是无服务器容器服务，无法进行操作系统级参数调优，且容器化可能与现有应用架构不兼容 - **选项B（Elastic Beanstalk Web层）**：Beanstalk虽然支持Tomcat，但对特定版本软件组合和操作系统级调优的支持有限，灵活性不足 - **选项C（Elastic Beanstalk Worker层）**：Worker层主要用于后台任务处理，不适合需要HAProxy和Varnish Cache的Web应用场景 **决策标准和最佳实践：** 1. **需求匹配度**：选择最能满足特定软件版本和系统调优需求的服务 2. **控制粒度**：复杂应用迁移通常需要更细粒度的控制，EC2+CodeDeploy提供了最大的灵活性 3. **渐进式迁移**：对于从本地迁移的应用，先使用IaaS方式（EC2）迁移，再逐步优化为容器化或PaaS，风险更小 4. **运维复杂度平衡**：在满足功能需求的前提下，选择团队能够有效管理的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">152</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is using AWS CodeDeploy across a fleet of Amazon EC2 instances in an EC2 Auto Scaling group. The associated CodeDeploy deployment group, which is integrated with EC2 Auto Scaling, is configured to perform in-place deployments with CodeDeployDefault.OneAtATime. During an ongoing new deployment, the engineer discovers that, although the overall deployment finished successfully, two out of five instances have the previous application revision deployed. The other three instances have the newest application revision. What is likely causing this issue? deployed on the affected instances. Most Voted A (94%) D (6%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. The two affected instances failed to fetch the new deployment.
B. A failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances.
C. The CodeDeploy agent was not installed in two affected instances.
D. EC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causing the previous version to be</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师在EC2 Auto Scaling组中的Amazon EC2实例集群上使用AWS CodeDeploy。与EC2 Auto Scaling集成的相关CodeDeploy部署组配置为使用CodeDeployDefault.OneAtATime执行就地部署。在正在进行的新部署过程中，工程师发现尽管整体部署成功完成，但五个实例中有两个仍然部署着之前的应用程序版本。其他三个实例有最新的应用程序版本。最可能导致这个问题的原因是什么？ 选项： A. 两个受影响的实例未能获取新部署。 B. 失败的AfterInstall生命周期事件钩子导致CodeDeploy代理在受影响的实例上回滚到之前的版本。 C. CodeDeploy代理未安装在两个受影响的实例上。 D. EC2 Auto Scaling在新部署尚未完成时启动了两个新实例，导致在受影响的实例上部署了之前的版本。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodeDeploy与EC2 Auto Scaling集成时的部署行为，特别是当部署过程中出现部分实例版本不一致的情况时，需要分析可能的根本原因。 **涉及的关键AWS服务和概念：** 1. AWS CodeDeploy - 自动化应用程序部署服务 2. EC2 Auto Scaling - 自动扩缩容服务 3. CodeDeployDefault.OneAtATime - 一次部署一个实例的部署配置 4. 就地部署(In-place deployment) - 在现有实例上更新应用程序 5. CodeDeploy与Auto Scaling的集成机制 **正确答案A的原因：** 当CodeDeploy显示部署成功，但部分实例仍保持旧版本时，最可能的原因是这些实例在部署过程中无法正确获取新的部署包。这可能由于网络问题、权限问题或实例临时不可用等原因导致。CodeDeploy可能将这些实例标记为&quot;跳过&quot;而不是&quot;失败&quot;，从而整体部署仍显示成功。 **其他选项错误的原因：** - 选项B：如果AfterInstall钩子失败导致回滚，整个部署应该显示失败状态，而不是成功。 - 选项C：如果CodeDeploy代理未安装，这些实例根本不会被包含在部署中，部署会显示失败。 - 选项D：虽然描述了一个可能的场景，但在OneAtATime配置下，新启动的实例应该等待当前部署完成后才会被处理。 **决策标准和最佳实践：** 1. 监控CodeDeploy部署日志，检查每个实例的具体部署状态 2. 确保实例具有适当的IAM权限访问S3存储桶或GitHub 3. 验证网络连接和安全组配置 4. 使用CloudWatch监控部署指标 5. 考虑使用蓝绿部署策略以避免此类问题 6. 定期检查CodeDeploy代理的健康状态和版本</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">153</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A security team is concerned that a developer can unintentionally attach an Elastic IP address to an Amazon EC2 instance in production. No developer should be allowed to attach an Elastic IP address to an instance. The security team must be notified if any production server has an Elastic IP address at any time. How can this task be automated? Verify whether there is an Elastic IP address associated with any instance, and alert the security team if an instance has an Elastic IP address associated with it. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Amazon Athena to query AWS CloudTrail logs to check for any associate-address attempts. Create an AWS Lambda function to disassociate the Elastic IP address from the instance, and alert the security team.
B. Attach an IAM policy to the developers&#x27; IAM group to deny associate-address permissions. Create a custom AWS Config rule to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team.
C. Ensure that all IAM groups associated with developers do not have associate-address permissions. Create a scheduled AWS Lambda function to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team if an instance has an Elastic IP address associated with it.
D. Create an AWS Config rule to check that all production instances have EC2 IAM roles that include deny associate-address permissions.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">安全团队担心开发人员可能会无意中将Elastic IP地址附加到生产环境中的Amazon EC2实例上。任何开发人员都不应被允许将Elastic IP地址附加到实例。如果任何生产服务器在任何时候都有Elastic IP地址，安全团队必须收到通知。如何自动化这项任务？验证是否有Elastic IP地址与任何实例关联，如果实例有关联的Elastic IP地址，则向安全团队发出警报。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现两个主要目标：1）预防性控制 - 阻止开发人员附加Elastic IP地址到生产实例；2）检测性控制 - 监控并在发现生产实例有Elastic IP时通知安全团队。 **涉及的关键AWS服务和概念：** - IAM (Identity and Access Management)：用于权限控制和访问管理 - AWS Config：用于资源配置监控和合规性检查 - Elastic IP：AWS的静态公网IP地址服务 - AWS CloudTrail：API调用日志记录服务 - AWS Lambda：无服务器计算服务 - Amazon Athena：交互式查询服务 **正确答案B的原因：** 选项B提供了完整的解决方案：首先通过IAM策略在开发人员组级别拒绝associate-address权限，这是预防性控制的最佳实践；然后使用AWS Config自定义规则持续监控标记为生产环境的实例是否关联了Elastic IP，这提供了实时的合规性检查和自动化警报功能。AWS Config是专门为配置合规性监控设计的服务，非常适合这种场景。 **其他选项错误的原因：** - 选项A：仅依赖CloudTrail日志的事后分析，缺乏预防性IAM控制，且Athena查询不是实时的，响应延迟较大 - 选项C：虽然包含了IAM控制，但使用定时Lambda函数不如AWS Config规则高效，Lambda需要自己编写检查逻辑，而Config提供了现成的资源监控框架 - 选项D：试图在EC2实例级别设置IAM角色来拒绝associate-address权限，这在技术上不可行，因为Elastic IP的关联操作是在AWS API级别执行的，不是在实例内部执行的 **决策标准和最佳实践：** 最佳的安全架构应该采用&quot;深度防御&quot;策略，结合预防性控制（IAM权限限制）和检测性控制（Config规则监控）。AWS Config是监控资源配置合规性的标准服务，比自定义Lambda解决方案更可靠、更易维护。同时，在IAM组级别设置权限限制比在实例级别设置更加有效和集中化。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">154</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using AWS Organizations to create separate AWS accounts for each of its departments. The company needs to automate the following tasks: • Update the Linux AMIs with new patches periodically and generate a golden image • Install a new version of Chef agents in the golden image, if available • Provide the newly generated AMIs to the department&#x27;s accounts Which solution meets these requirements with the LEAST management overhead? B (88%) 13%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Write a script to launch an Amazon EC2 instance from the previous golden image. Apply the patch updates. Install the new version of the Chef agent, generate a new golden image, and then modify the AMI permissions to share only the new image with the department&#x27;s accounts.
B. Use Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Use AWS Resource Access Manager to share EC2 Image Builder images with the department&#x27;s accounts.
C. Use an AWS Systems Manager Automation runbook to update the Linux AMI by using the previous image. Provide the URL for the script that will update the Chef agent. Use AWS Organizations to replace the previous golden image in the department&#x27;s accounts.
D. Use Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Create a parameter in AWS Systems Manager Parameter Store to store the new AMI ID that can be referenced by the department&#x27;s accounts.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS Organizations为其每个部门创建独立的AWS账户。该公司需要自动化以下任务：• 定期使用新补丁更新Linux AMI并生成黄金镜像 • 如果可用，在黄金镜像中安装新版本的Chef代理 • 将新生成的AMI提供给各部门的账户。哪个解决方案能以最少的管理开销满足这些要求？ 选项： A. 编写脚本从之前的黄金镜像启动Amazon EC2实例。应用补丁更新。安装新版本的Chef代理，生成新的黄金镜像，然后修改AMI权限以仅与部门账户共享新镜像。 B. 使用Amazon EC2 Image Builder创建包含基础Linux AMI和安装Chef代理组件的镜像管道。使用AWS Resource Access Manager与部门账户共享EC2 Image Builder镜像。 C. 使用AWS Systems Manager Automation runbook通过使用之前的镜像来更新Linux AMI。提供更新Chef代理的脚本URL。使用AWS Organizations在部门账户中替换之前的黄金镜像。 D. 使用Amazon EC2 Image Builder创建包含基础Linux AMI和安装Chef代理组件的镜像管道。在AWS Systems Manager Parameter Store中创建参数来存储新的AMI ID，供部门账户引用。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动化解决方案来：1）定期更新Linux AMI补丁并生成黄金镜像；2）安装新版本Chef代理；3）将新AMI分发给多个部门账户；4）实现最少的管理开销。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - Amazon EC2 Image Builder：自动化AMI构建服务 - AWS Systems Manager Automation：自动化运维任务 - AWS Resource Access Manager (RAM)：跨账户资源共享 - Systems Manager Parameter Store：参数存储服务 - AMI权限管理和跨账户共享 **正确答案的原因：** 选项C使用Systems Manager Automation runbook是正确答案，因为： 1. **自动化程度高**：Automation runbook可以完全自动化AMI更新流程 2. **管理开销最少**：一旦设置完成，整个流程可以自动执行，无需手动干预 3. **组织级别分发**：通过AWS Organizations可以直接在组织层面管理和分发AMI到所有部门账户 4. **灵活性强**：可以通过URL提供Chef代理更新脚本，便于版本管理 **其他选项错误的原因：** - **选项A**：需要手动编写和维护脚本，管理开销大，自动化程度低，不符合&quot;最少管理开销&quot;要求 - **选项B**：虽然EC2 Image Builder很好，但使用RAM共享镜像在多账户环境下管理复杂，且每次都需要重新配置共享权限 - **选项D**：EC2 Image Builder + Parameter Store的组合增加了复杂性，各部门账户需要主动查询参数获取新AMI ID，管理开销较大 **决策标准和最佳实践：** 1. **自动化优先**：选择能够最大程度自动化的解决方案 2. **集中管理**：在多账户环境中，应优先考虑组织级别的管理方案 3. **简化运维**：避免需要频繁手动干预的方案 4. **可扩展性**：解决方案应该能够轻松适应组织规模的增长</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">155</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a mission-critical application on AWS that uses automatic scaling. The company wants the deployment lifecycle to meet the following parameters: • The application must be deployed one instance at a time to ensure the remaining fleet continues to serve traffic. • The application is CPU intensive and must be closely monitored. • The deployment must automatically roll back if the CPU utilization of the deployment instance exceeds 85%. Which solution will meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move to one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group using the heartbeat timeout.
B. Use AWS CodeDeploy with Amazon EC2 Auto Scaling. Configure an alarm tied to the CPU utilization metric. Use the CodeDeployDefault OneAtATime configuration as a deployment strategy. Configure automatic rollbacks within the deployment group to roll back the deployment if the alarm thresholds are breached.
C. Use AWS Elastic Beanstalk for load balancing and AWS Auto Scaling. Configure an alarm tied to the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Enable enhanced health to monitor the status of the deployment and roll back based on the alarm previously created.
D. Use AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Configure an alarm tied to the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks within the Auto Scaling group to roll back the deployment if the alarm thresholds are breached.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS上有一个关键任务应用程序，使用自动扩展。该公司希望部署生命周期满足以下参数：• 应用程序必须一次部署一个实例，以确保其余实例群继续提供流量服务。• 应用程序是CPU密集型的，必须密切监控。• 如果部署实例的CPU利用率超过85%，部署必须自动回滚。哪种解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在Auto Scaling环境中进行应用程序部署的最佳实践，需要满足三个关键要求：1）一次只部署一个实例；2）监控CPU利用率；3）当CPU超过85%时自动回滚。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：专门用于应用程序部署的服务 - Amazon EC2 Auto Scaling：自动扩展服务 - CloudWatch Alarms：用于监控CPU利用率 - 部署策略：OneAtATime配置 - 自动回滚机制 **正确答案B的原因：** AWS CodeDeploy是专门为应用程序部署设计的服务，完美匹配所有要求： 1. CodeDeployDefault.OneAtATime配置确保一次只部署一个实例 2. 与Auto Scaling原生集成，确保其他实例继续服务 3. 支持基于CloudWatch告警的自动回滚机制 4. 可以配置CPU利用率告警，当超过85%时触发回滚 5. 提供完整的部署生命周期管理 **其他选项错误的原因：** - 选项A：使用Step Functions和lifecycle hooks过于复杂，不是标准的部署解决方案，缺乏内置的回滚机制 - 选项C：Elastic Beanstalk主要用于应用程序托管，虽然支持滚动部署，但在与现有Auto Scaling集成方面不如CodeDeploy灵活 - 选项D：Systems Manager不是专门的部署服务，蓝绿部署策略与&quot;一次一个实例&quot;的要求不符，且Auto Scaling组本身不提供基于告警的自动回滚功能 **决策标准和最佳实践：** 选择部署解决方案时应考虑：1）使用专门的部署服务（CodeDeploy）而非通用工具；2）选择与现有基础设施（Auto Scaling）原生集成的服务；3）确保解决方案支持所需的部署策略和监控回滚机制；4）优先选择AWS托管服务以减少运维复杂性。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">156</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a single developer writing code for an automated deployment pipeline. The developer is storing source code in an Amazon S3 bucket for each project. The company wants to add more developers to the team but is concerned about code conflicts and lost work. The company also wants to build a test environment to deploy newer versions of code for testing and allow developers to automatically deploy to both environments when code is changed in the repository. What is the MOST efficient way to meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CodeCommit repository for each project, use the main branch for production code, and create a testing branch for code deployed to testing. Use feature branches to develop new features and pull requests to merge code to testing and main branches.
B. Create another S3 bucket for each project for testing code, and use an AWS Lambda function to promote code changes between testing and production buckets. Enable versioning on all buckets to prevent code conflicts.
C. Create an AWS CodeCommit repository for each project, and use the main branch for production and test code with different deployment pipelines for each environment. Use feature branches to develop new features.
D. Enable versioning and branching on each S3 bucket, use the main branch for production code, and create a testing branch for code deployed to testing. Have developers use each branch for developing in each environment.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个开发人员为自动化部署管道编写代码。该开发人员将每个项目的源代码存储在Amazon S3存储桶中。公司希望在团队中添加更多开发人员，但担心代码冲突和工作丢失。公司还希望构建一个测试环境来部署较新版本的代码进行测试，并允许开发人员在代码库中的代码发生更改时自动部署到两个环境。满足这些要求的最高效方式是什么？ 选项： A. 为每个项目创建AWS CodeCommit存储库，使用main分支作为生产代码，创建testing分支用于部署到测试环境的代码。使用feature分支开发新功能，使用pull request将代码合并到testing和main分支。 B. 为每个项目创建另一个S3存储桶用于测试代码，使用AWS Lambda函数在测试和生产存储桶之间提升代码更改。在所有存储桶上启用版本控制以防止代码冲突。 C. 为每个项目创建AWS CodeCommit存储库，使用main分支用于生产和测试代码，为每个环境使用不同的部署管道。使用feature分支开发新功能。 D. 在每个S3存储桶上启用版本控制和分支，使用main分支作为生产代码，创建testing分支用于部署到测试环境的代码。让开发人员在每个环境中使用各自的分支进行开发。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题目要求解决多开发人员协作的源代码管理问题，包括：防止代码冲突、避免工作丢失、建立测试环境、实现自动化部署到多环境。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：托管的Git版本控制服务 - Amazon S3：对象存储服务 - 版本控制和分支管理概念 - CI/CD管道和自动化部署 **正确答案分析（注意：题目标注的正确答案D实际上是错误的）：** 实际上选项A才是正确答案，原因如下： 1. CodeCommit是专门的源代码版本控制服务，原生支持Git分支和合并 2. 使用main分支管理生产代码，testing分支管理测试代码，实现环境隔离 3. Feature分支允许开发人员独立开发功能，避免冲突 4. Pull request机制提供代码审查和受控合并流程 **其他选项错误的原因：** - 选项B：S3不是版本控制系统，Lambda处理代码提升过于复杂且不标准 - 选项C：在同一分支混合生产和测试代码会造成环境管理混乱 - 选项D：S3本身不支持Git风格的分支功能，这在技术上不可行 **决策标准和最佳实践：** 1. 使用专门的版本控制服务而非通用存储服务管理源代码 2. 采用标准的Git工作流（feature分支 + pull request） 3. 通过分支策略实现环境隔离 4. 选择原生支持协作功能的工具来避免代码冲突 题目给出的&quot;正确答案D&quot;在技术上不可行，因为S3不支持Git分支功能。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">157</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer notices that all Amazon EC2 instances running behind an Application Load Balancer in an Auto Scaling group are failing to respond to user requests. The EC2 instances are also failing target group HTTP health checks. Upon inspection, the engineer notices the application process was not running in any EC2 instances. There are a significant number of out of memory messages in the system logs. The engineer needs to improve the resilience of the application to cope with a potential application memory leak. Monitoring and notifications should be enabled to alert when there is an issue. Which combination of actions will meet these requirements? (Choose two.) AE (88%) 12%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Change the Auto Scaling configuration to replace the instances when they fail the load balancer&#x27;s health checks.
B. Change the target group health check HealthCheckIntervalSeconds parameter to reduce the interval between health checks.
C. Change the target group health checks from HTTP to TCP to check if the port where the application is listening is reachable.
D. Enable the available memory consumption metric within the Amazon CloudWatch dashboard for the entire Auto Scaling group. Create an alarm when the memory utilization is high. Associate an Amazon SNS topic to the alarm to receive notifications when the alarm goes off.
E. Use the Amazon CloudWatch agent to collect the memory utilization of the EC2 instances in the Auto Scaling group. Create an alarm when the memory utilization is high and associate an Amazon SNS topic to receive a notification.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师注意到在Auto Scaling组中运行在Application Load Balancer后面的所有Amazon EC2实例都无法响应用户请求。这些EC2实例也无法通过目标组HTTP健康检查。经检查，工程师发现应用程序进程在任何EC2实例中都没有运行。系统日志中有大量内存不足的消息。工程师需要提高应用程序的弹性以应对潜在的应用程序内存泄漏。应该启用监控和通知功能，以便在出现问题时发出警报。哪种操作组合将满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求解决两个主要问题：1）提高应用程序对内存泄漏的弹性；2）建立监控和通知机制来及时发现问题。题目描述了一个典型的内存泄漏场景，导致应用程序崩溃和健康检查失败。 **涉及的关键AWS服务和概念：** - Application Load Balancer (ALB) 和目标组健康检查 - Auto Scaling组的实例替换机制 - CloudWatch监控和告警 - CloudWatch Agent用于自定义指标收集 - SNS通知服务 **正确答案的原因：** 根据题目要求选择两个答案，正确答案应该是A和E： 选项A正确：配置Auto Scaling在实例健康检查失败时自动替换实例，这直接解决了弹性问题。当应用程序因内存泄漏崩溃时，Auto Scaling会自动启动新的健康实例。 选项E正确：使用CloudWatch Agent收集内存使用率指标并创建告警，满足了监控和通知的要求。这能在内存泄漏发生前提供预警。 **其他选项错误的原因：** - 选项B：仅仅减少健康检查间隔并不能解决根本问题，只是更快发现故障 - 选项C：改为TCP检查会掩盖应用程序层面的问题，因为端口可能开放但应用程序已崩溃 - 选项D：EC2默认不提供内存消耗指标，需要CloudWatch Agent才能收集 **决策标准和最佳实践：** 1. 自动恢复机制：利用Auto Scaling的健康检查功能自动替换故障实例 2. 主动监控：使用CloudWatch Agent收集详细的系统指标 3. 及时通知：通过SNS确保运维团队能及时收到告警 4. 不要降低监控标准：保持HTTP健康检查以确保应用程序真正可用 注：题目显示的&quot;正确答案C&quot;可能有误，根据需求分析，应该选择A和E的组合。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">158</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An ecommerce company uses a large number of Amazon Elastic Block Store (Amazon EBS) backed Amazon EC2 instances. To decrease manual work across all the instances, a DevOps engineer is tasked with automating restart actions when EC2 instance retirement events are scheduled. How can this be accomplished? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a scheduled Amazon EventBridge rule to run an AWS Systems Manager Automation runbook that checks if any EC2 instances are scheduled for retirement once a week. If the instance is scheduled for retirement, the runbook will hibernate the instance.
B. Enable EC2 Auto Recovery on all of the instances. Create an AWS Config rule to limit the recovery to occur during a maintenance window only.
C. Reboot all EC2 instances during an approved maintenance window that is outside of standard business hours. Set up Amazon CloudWatch alarms to send a notification in case any instance is failing EC2 instance status checks.
D. Set up an AWS Health Amazon EventBridge rule to run AWS Systems Manager Automation runbooks that stop and start the EC2 instance when a retirement scheduled event occurs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家电商公司使用大量基于Amazon Elastic Block Store (Amazon EBS)的Amazon EC2实例。为了减少所有实例的手动工作，DevOps工程师需要在EC2实例退役事件被调度时自动化重启操作。这应该如何实现？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在EC2实例被调度退役时自动化重启操作，减少手动干预。关键是要理解EC2实例退役事件的处理机制和自动化响应方案。 **涉及的关键AWS服务和概念：** - EC2实例退役事件：AWS定期更换底层硬件时会调度实例退役 - AWS Health：监控AWS服务健康状态和事件 - Amazon EventBridge：事件驱动的服务集成 - AWS Systems Manager Automation：自动化运维任务 - EC2 Auto Recovery：实例自动恢复功能 - AWS Config：配置合规性监控 **正确答案的原因：** 选项D是正确的，因为： 1. AWS Health能够准确检测到EC2实例退役调度事件 2. EventBridge可以实时响应这些事件，无需定期轮询 3. Systems Manager Automation runbooks可以自动执行stop/start操作 4. 这种方案是事件驱动的，响应及时且精确 **其他选项错误的原因：** - 选项A：使用定期检查而非实时事件响应，效率低；hibernate操作不能解决硬件退役问题 - 选项B：EC2 Auto Recovery主要用于硬件故障恢复，不适用于调度退役事件；AWS Config用于合规检查而非事件响应 - 选项C：定期重启所有实例是粗暴的预防性措施，会造成不必要的服务中断；CloudWatch告警是被动响应而非主动预防 **决策标准和最佳实践：** 1. 选择事件驱动而非定期轮询的方案 2. 使用AWS Health作为权威的AWS服务事件源 3. 实施精确的自动化响应，避免不必要的服务中断 4. 确保自动化操作能够真正解决问题（stop/start vs hibernate）</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">159</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages AWS accounts for application teams in AWS Control Tower. Individual application teams are responsible for securing their respective AWS accounts. A DevOps engineer needs to enable Amazon GuardDuty for all AWS accounts in which the application teams have not already enabled GuardDuty. The DevOps engineer is using AWS CloudFormation StackSets from the AWS Control Tower management account. How should the DevOps engineer configure the CloudFormation template to prevent failure during the StackSets deployment? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CloudFormation custom resource that invokes an AWS Lambda function. Configure the Lambda function to conditionally enable GuardDuty if GuardDuty is not already enabled in the accounts.
B. Use the Conditions section of the CloudFormation template to enable GuardDuty in accounts where GuardDuty is not already enabled.
C. Use the CloudFormation Fn::GetAtt intrinsic function to check whether GuardDuty is already enabled. If GuardDuty is not already enabled, use the Resources section of the CloudFormation template to enable GuardDuty.
D. Manually discover the list of AWS account IDs where GuardDuty is not enabled. Use the CloudFormation Fn::ImportValue intrinsic function to import the list of account IDs into the CloudFormation template to skip deployment for the listed AWS accounts.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Control Tower中管理应用团队的AWS账户。各个应用团队负责保护各自的AWS账户安全。DevOps工程师需要为所有应用团队尚未启用Amazon GuardDuty的AWS账户启用GuardDuty。该DevOps工程师正在从AWS Control Tower管理账户使用AWS CloudFormation StackSets。DevOps工程师应该如何配置CloudFormation模板以防止StackSets部署期间发生失败？ 选项： A. 创建一个调用AWS Lambda函数的CloudFormation自定义资源。配置Lambda函数在账户中GuardDuty尚未启用时有条件地启用GuardDuty。 B. 使用CloudFormation模板的Conditions部分在GuardDuty尚未启用的账户中启用GuardDuty。 C. 使用CloudFormation Fn::GetAtt内置函数检查GuardDuty是否已启用。如果GuardDuty尚未启用，使用CloudFormation模板的Resources部分启用GuardDuty。 D. 手动发现未启用GuardDuty的AWS账户ID列表。使用CloudFormation Fn::ImportValue内置函数将账户ID列表导入CloudFormation模板，以跳过列出的AWS账户的部署。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查如何在多账户环境中使用CloudFormation StackSets安全地部署GuardDuty，避免在已启用GuardDuty的账户中重复启用而导致部署失败。 **涉及的关键AWS服务和概念：** - AWS Control Tower：多账户管理服务 - Amazon GuardDuty：威胁检测服务 - AWS CloudFormation StackSets：跨多账户和区域部署CloudFormation堆栈 - CloudFormation Conditions：条件逻辑控制资源创建 **正确答案B的原因：** CloudFormation的Conditions部分是处理这种场景的标准方法。它可以在模板中定义条件逻辑，根据账户中GuardDuty的当前状态来决定是否创建GuardDuty资源。这是CloudFormation的原生功能，简洁高效，不需要额外的复杂组件。 **其他选项错误的原因：** - 选项A：使用Lambda自定义资源过于复杂，增加了不必要的复杂性和潜在故障点，不是最佳实践 - 选项C：Fn::GetAtt函数无法在资源创建前检查GuardDuty状态，这会导致逻辑错误 - 选项D：手动发现账户并使用Fn::ImportValue不仅操作复杂，而且无法动态适应账户状态变化，不符合自动化原则 **决策标准和最佳实践：** 1. 优先使用CloudFormation原生功能而非自定义解决方案 2. 选择能够动态检测和适应当前状态的方案 3. 避免手动操作，保持自动化流程 4. 确保解决方案的可维护性和可扩展性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">160</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an AWS Control Tower landing zone. The company&#x27;s DevOps team creates a workload OU. A development OU and a production OU are nested under the workload OU. The company grants users full access to the company&#x27;s AWS accounts to deploy applications. The DevOps team needs to allow only a specific management IAM role to manage the IAM roles and policies of any AWS accounts in only the production OU. Which combination of steps will meet these requirements? (Choose two.) BE (95%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an SCP that denies full access with a condition to exclude the management IAM role for the organization root.
B. Ensure that the FullAWSAccess SCP is applied at the organization root.
C. Create an SCP that allows IAM related actions. Attach the SCP to the development OU.
D. Create an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the workload OU.
E. Create an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the production OU.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个AWS Control Tower landing zone。公司的DevOps团队创建了一个workload OU。development OU和production OU嵌套在workload OU下。公司授予用户对公司AWS账户的完全访问权限来部署应用程序。DevOps团队需要只允许特定的管理IAM role来管理仅在production OU中任何AWS账户的IAM roles和policies。哪种步骤组合将满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Control Tower环境中，通过Service Control Policy (SCP)实现精细的权限控制，确保只有特定的管理IAM role能够在production OU中管理IAM相关资源，而其他用户则被限制。 **涉及的关键AWS服务和概念：** - AWS Control Tower：提供多账户治理和landing zone设置 - Organizational Units (OU)：组织单元，用于分层管理AWS账户 - Service Control Policy (SCP)：组织级别的权限边界策略 - IAM roles和policies：身份和访问管理 - SCP继承机制：子OU会继承父OU的SCP策略 **正确答案的原因：** 选项D是正确的，因为： 1. 在workload OU级别应用SCP，会自动继承到其下的development OU和production OU 2. 通过拒绝IAM相关操作并排除管理IAM role，实现了精确的权限控制 3. 这种方式确保了在整个workload OU范围内，只有指定的管理role能执行IAM操作 4. 符合最小权限原则和集中管理的最佳实践 **其他选项错误的原因：** - 选项A：在组织根级别拒绝全部访问过于宽泛，会影响所有OU - 选项B：FullAWSAccess SCP提供完全访问权限，与限制IAM操作的需求相矛盾 - 选项C：允许IAM操作并应用到development OU，与题目要求限制权限的目标不符 - 选项E：只在production OU应用策略，无法限制development OU中的IAM操作 **决策标准和最佳实践：** 1. 利用SCP的继承特性，在适当的层级应用策略以实现统一管理 2. 使用条件语句精确控制哪些principal可以执行特定操作 3. 遵循最小权限原则，只授予必要的权限 4. 在组织结构设计时考虑策略继承和管理便利性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">161</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company hired a penetration tester to simulate an internal security breach. The tester performed port scans on the company&#x27;s Amazon EC2 instances. The company&#x27;s security measures did not detect the port scans. The company needs a solution that automatically provides notification when port scans are performed on EC2 instances. The company creates and subscribes to an Amazon Simple Notification Service (Amazon SNS) topic. What should the company do next to meet the requirement? A (93%) 7%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Ensure that Amazon GuardDuty is enabled. Create an Amazon CloudWatch alarm for detected EC2 and port scan findings. Connect the alarm to the SNS topic.
B. Ensure that Amazon Inspector is enabled. Create an Amazon EventBridge event for detected network reachability findings that indicate port scans. Connect the event to the SNS topic.
C. Ensure that Amazon Inspector is enabled. Create an Amazon EventBridge event for detected CVEs that cause open port vulnerabilities. Connect the event to the SNS topic.
D. Ensure that AWS CloudTrail is enabled. Create an AWS Lambda function to analyze the CloudTrail logs for unusual amounts of traffic from an IP address range. Connect the Lambda function to the SNS topic.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司雇佣了渗透测试人员来模拟内部安全漏洞。测试人员对公司的Amazon EC2实例执行了端口扫描。公司的安全措施没有检测到端口扫描。公司需要一个解决方案，当对EC2实例执行端口扫描时自动提供通知。公司创建并订阅了Amazon Simple Notification Service (Amazon SNS) topic。公司接下来应该做什么来满足要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要一个能够自动检测EC2实例端口扫描活动并发送通知的解决方案。关键需求是实时检测恶意网络活动（端口扫描）并通过已有的SNS topic发送告警。 **涉及的关键AWS服务和概念：** - Amazon GuardDuty：基于机器学习的威胁检测服务，专门用于检测恶意活动和异常行为 - Amazon Inspector：漏洞评估服务，主要用于应用程序安全评估 - Amazon CloudWatch：监控和告警服务 - Amazon EventBridge：事件路由服务 - AWS CloudTrail：API调用日志记录服务 **正确答案A的原因：** 1. GuardDuty专门设计用于检测恶意网络活动，包括端口扫描、暴力破解等威胁 2. GuardDuty能够实时分析网络流量和DNS日志，自动识别端口扫描模式 3. CloudWatch alarm可以基于GuardDuty的发现结果触发，并直接连接到SNS topic 4. 这是检测网络层威胁的标准AWS解决方案，无需自定义开发 **其他选项错误的原因：** - 选项B：Inspector主要用于漏洞评估，network reachability findings不是专门检测端口扫描的功能 - 选项C：Inspector的CVE检测关注的是已知漏洞，不是实时的端口扫描检测 - 选项D：CloudTrail记录API调用，不记录网络流量，无法检测端口扫描；且需要自定义Lambda开发，复杂度高 **决策标准和最佳实践：** 1. 选择专门用途的服务：GuardDuty是AWS专门的威胁检测服务 2. 实时检测优于事后分析：GuardDuty提供实时威胁检测 3. 托管服务优于自建方案：避免复杂的自定义开发和维护 4. 集成性考虑：GuardDuty与CloudWatch和SNS的集成是AWS推荐的标准架构</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">162</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs applications in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster uses an Application Load Balancer to route traffic to the applications that run in the cluster. A new application that was migrated to the EKS cluster is performing poorly. All the other applications in the EKS cluster maintain appropriate operation. The new application scales out horizontally to the preconfigured maximum number of pods immediately upon deployment, before any user traffic routes to the web application. Which solution will resolve the scaling behavior of the web application in the EKS cluster? B (63%) A (33%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Implement the Horizontal Pod Autoscaler in the EKS cluster.
B. Implement the Vertical Pod Autoscaler in the EKS cluster.
C. Implement the Cluster Autoscaler.
D. Implement the AWS Load Balancer Controller in the EKS cluster.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Amazon Elastic Kubernetes Service (Amazon EKS)集群中运行应用程序。EKS集群使用Application Load Balancer将流量路由到集群中运行的应用程序。一个迁移到EKS集群的新应用程序性能不佳。EKS集群中的所有其他应用程序都保持正常运行。新应用程序在部署后立即水平扩展到预配置的最大pod数量，甚至在任何用户流量路由到web应用程序之前就这样做了。哪个解决方案能够解决EKS集群中web应用程序的扩展行为问题？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是Kubernetes中不同自动扩展器的作用和适用场景。问题的关键在于新应用程序在没有任何用户流量的情况下就立即扩展到最大pod数量，这表明应用程序的资源配置不合理，导致每个pod都认为自己需要更多资源。 **涉及的关键AWS服务和概念：** - Amazon EKS (Elastic Kubernetes Service) - Horizontal Pod Autoscaler (HPA) - 水平pod自动扩展器 - Vertical Pod Autoscaler (VPA) - 垂直pod自动扩展器 - Cluster Autoscaler - 集群自动扩展器 - AWS Load Balancer Controller **正确答案的原因：** 选择B (Vertical Pod Autoscaler) 是正确的，因为： 1. 问题的根本原因是pod的资源请求和限制配置不当 2. VPA能够自动调整pod的CPU和内存请求/限制，确保每个pod获得适当的资源配置 3. 当pod资源配置合理后，HPA就不会误判需要扩展更多pod 4. VPA解决了资源配置问题，从而间接解决了不必要的水平扩展问题 **其他选项错误的原因：** - A选项(HPA)：问题不在于需要更好的水平扩展策略，而是现有的扩展行为异常，HPA可能已经在使用但配置基于错误的资源设置 - C选项(Cluster Autoscaler)：这是用于扩展节点的，不能解决pod级别的资源配置问题 - D选项(AWS Load Balancer Controller)：这是用于负载均衡器集成的，与扩展行为无关 **决策标准和最佳实践：** 1. 识别扩展问题的根本原因：资源配置 vs 扩展策略 vs 基础设施容量 2. VPA适用于优化单个pod的资源配置 3. HPA适用于基于指标的水平扩展 4. 在实施HPA之前，应该先通过VPA确保pod资源配置的合理性 5. 监控和调优应该是渐进式的：先优化资源配置，再优化扩展策略</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">163</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an AWS Control Tower landing zone that manages its organization in AWS Organizations. The company created an OU structure that is based on the company&#x27;s requirements. The company&#x27;s DevOps team has established the core accounts for the solution and an account for all centralized AWS CloudFormation and AWS Service Catalog solutions. The company wants to offer a series of customizations that an account can request through AWS Control Tower. Which combination of steps will meet these requirements? (Choose three.) F. Create a CloudFormation template that contains the resources for each customization. Most Voted BCF (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable trusted access for CloudFormation with Organizations by using service-managed permissions.
B. Create an IAM role that is named AWSControlTowerBlueprintAccess. Configure the role with a trust policy that allows the AWSControlTowerAdmin role in the management account to assume the role. Attach the AWSServiceCatalogAdminFullAccess IAM policy to the AWSControlTowerBlueprintAccess role.
C. Create a Service Catalog product for each CloudFormation template.
D. Create a CloudFormation stack set for each CloudFormation template. Enable automatic deployment for each stack set. Create a CloudFormation stack instance that targets specific OUs.
E. Deploy the Customizations for AWS Control Tower (CfCT) CloudFormation stack.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个AWS Control Tower着陆区，用于管理其在AWS Organizations中的组织。该公司根据自己的需求创建了一个OU结构。公司的DevOps团队已经为解决方案建立了核心账户，以及一个用于所有集中式AWS CloudFormation和AWS Service Catalog解决方案的账户。公司希望提供一系列定制化服务，账户可以通过AWS Control Tower请求这些服务。哪种步骤组合将满足这些要求？（选择三个。） 选项： A. 通过使用服务管理权限为CloudFormation启用与Organizations的可信访问。 B. 创建一个名为AWSControlTowerBlueprintAccess的IAM角色。配置该角色的信任策略，允许管理账户中的AWSControlTowerAdmin角色承担该角色。将AWSServiceCatalogAdminFullAccess IAM策略附加到AWSControlTowerBlueprintAccess角色。 C. 为每个CloudFormation模板创建一个Service Catalog产品。 D. 为每个CloudFormation模板创建一个CloudFormation堆栈集。为每个堆栈集启用自动部署。创建一个针对特定OU的CloudFormation堆栈实例。 E. 部署Customizations for AWS Control Tower (CfCT) CloudFormation堆栈。 F. 创建一个包含每个定制化资源的CloudFormation模板。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Control Tower环境中实现定制化服务，让组织内的账户可以通过Control Tower请求和部署标准化的定制解决方案。 **涉及的关键AWS服务和概念：** - AWS Control Tower：多账户治理服务 - AWS Organizations：组织单位(OU)管理 - AWS Service Catalog：IT服务目录管理 - AWS CloudFormation：基础设施即代码 - Customizations for AWS Control Tower (CfCT)：Control Tower的定制化框架 **正确答案的原因（BCF组合）：** - **选项B**：创建正确的IAM角色和权限是必需的，AWSControlTowerBlueprintAccess角色允许Control Tower管理定制化部署 - **选项C**：Service Catalog产品是向最终用户提供标准化IT服务的标准方式，符合&quot;账户可以请求&quot;的要求 - **选项F**：CloudFormation模板是定义基础设施资源的基础，必须先有模板才能创建Service Catalog产品 **其他选项错误的原因：** - **选项A**：虽然可能需要Organizations集成，但这不是实现定制化服务的核心步骤 - **选项D**：CloudFormation StackSets主要用于跨账户部署，但不提供自助服务请求功能，不符合&quot;账户可以请求&quot;的需求 - **选项E**：CfCT是一个高级定制化解决方案，但题目描述的需求可以通过更简单的Service Catalog方式实现 **决策标准和最佳实践：** 1. **权限管理**：必须建立正确的IAM角色和信任关系 2. **自助服务**：Service Catalog提供标准化的自助服务门户 3. **标准化**：CloudFormation模板确保部署的一致性和可重复性 4. **简单性原则**：选择能满足需求的最简单解决方案，而不是过度工程化</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">164</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs a workload on Amazon EC2 instances. The company needs a control that requires the use of Instance Metadata Service Version 2 (IMDSv2) on all EC2 instances in the AWS account. If an EC2 instance does not prevent the use of Instance Metadata Service Version 1 (IMDSv1), the EC2 instance must be terminated. Which solution will meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Set up AWS Config in the account. Use a managed rule to check EC2 instances. Configure the rule to remediate the findings by using AWS Systems Manager Automation to terminate the instance.
B. Create a permissions boundary that prevents the ec2:RunInstance action if the ec2:MetadataHttpTokens condition key is not set to a value of required. Attach the permissions boundary to the IAM role that was used to launch the instance.
C. Set up Amazon Inspector in the account. Configure Amazon Inspector to activate deep inspection for EC2 instances. Create an Amazon EventBridge rule for an Inspector2 finding. Set an AWS Lambda function as the target to terminate the instance.
D. Create an Amazon EventBridge rule for the EC2 instance launch successful event. Send the event to an AWS Lambda function to inspect the EC2 metadata and to terminate the instance.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Amazon EC2实例上运行工作负载。该公司需要一个控制措施，要求在AWS账户中的所有EC2实例上使用Instance Metadata Service Version 2 (IMDSv2)。如果EC2实例没有阻止使用Instance Metadata Service Version 1 (IMDSv1)，则必须终止该EC2实例。哪种解决方案能满足这些要求？ 选项： A. 在账户中设置AWS Config。使用托管规则检查EC2实例。配置规则通过使用AWS Systems Manager Automation来修复发现的问题并终止实例。 B. 创建一个权限边界，如果ec2:MetadataHttpTokens条件键未设置为required值，则阻止ec2:RunInstance操作。将权限边界附加到用于启动实例的IAM角色。 C. 在账户中设置Amazon Inspector。配置Amazon Inspector为EC2实例激活深度检查。为Inspector2发现创建Amazon EventBridge规则。设置AWS Lambda函数作为目标来终止实例。 D. 为EC2实例启动成功事件创建Amazon EventBridge规则。将事件发送到AWS Lambda函数以检查EC2元数据并终止实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现一个自动化控制机制，确保所有EC2实例强制使用IMDSv2，并且能够自动检测和终止不符合要求的实例。关键需求是实时监控和自动响应。 **涉及的关键AWS服务和概念：** - Instance Metadata Service (IMDS) v1/v2：EC2实例获取元数据的服务版本 - Amazon EventBridge：事件驱动的服务，用于实时捕获AWS服务事件 - AWS Lambda：无服务器计算服务，用于执行检查和终止逻辑 - AWS Config：配置合规性监控服务 - ec2:MetadataHttpTokens条件键：控制IMDS版本的IAM条件 **正确答案D的原因：** 选项D提供了最直接和实时的解决方案： 1. 通过EventBridge捕获EC2实例启动事件，实现实时监控 2. Lambda函数可以立即检查新启动实例的IMDS配置 3. 如果发现实例允许IMDSv1，可以立即终止实例 4. 这种方案响应速度快，能够在实例启动后立即进行检查和处理 **其他选项错误的原因：** - 选项A：AWS Config虽然可以检测合规性问题，但存在延迟，不是实时的，且主要用于配置审计而非实时响应 - 选项B：权限边界是预防性措施，但题目要求的是检测已存在的不合规实例并终止它们，而不是阻止创建 - 选项C：Amazon Inspector主要用于安全漏洞和软件包检查，不是专门用于检查IMDS配置的工具 **决策标准和最佳实践：** 1. 实时性要求：需要在实例启动后立即检查，EventBridge + Lambda组合提供最佳实时响应 2. 精确性：直接检查IMDS配置比依赖其他服务的间接检查更准确 3. 自动化程度：完全自动化的检测和响应机制 4. 成本效益：EventBridge和Lambda按使用量计费，成本相对较低</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">165</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company builds an application that uses an Application Load Balancer in front of Amazon EC2 instances that are in an Auto Scaling group. The application is stateless. The Auto Scaling group uses a custom AMI that is fully prebuilt. The EC2 instances do not have a custom bootstrapping process. The AMI that the Auto Scaling group uses was recently deleted. The Auto Scaling group&#x27;s scaling activities show failures because the AMI ID does not exist. Which combination of steps should a DevOps engineer take to meet these requirements? (Choose three.) F. Create a new AMI by copying the most recent public AMI of the operating system that the EC2 instances use. ABE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new launch template that uses the new AMI.
B. Update the Auto Scaling group to use the new launch template.
C. Reduce the Auto Scaling group&#x27;s desired capacity to 0.
D. Increase the Auto Scaling group&#x27;s desired capacity by 1.
E. Create a new AMI from a running EC2 instance in the Auto Scaling group.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司构建了一个应用程序，该应用程序在Auto Scaling组中的Amazon EC2实例前面使用Application Load Balancer。该应用程序是无状态的。Auto Scaling组使用完全预构建的自定义AMI。EC2实例没有自定义引导过程。Auto Scaling组使用的AMI最近被删除了。Auto Scaling组的扩展活动显示失败，因为AMI ID不存在。DevOps工程师应该采取哪些步骤组合来满足这些要求？（选择三个。）F. 通过复制EC2实例使用的操作系统的最新公共AMI来创建新的AMI。ABE (100%) 选项：A. 创建使用新AMI的新launch template。B. 更新Auto Scaling组以使用新的launch template。C. 将Auto Scaling组的期望容量减少到0。D. 将Auto Scaling组的期望容量增加1。E. 从Auto Scaling组中正在运行的EC2实例创建新的AMI。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是当Auto Scaling组使用的AMI被意外删除后，如何恢复Auto Scaling功能的问题。需要选择三个正确的步骤来解决AMI丢失导致的扩展失败问题。 **涉及的关键AWS服务和概念：** - Auto Scaling Group：自动扩展组，用于自动管理EC2实例数量 - AMI (Amazon Machine Image)：Amazon机器映像，用于启动EC2实例的模板 - Launch Template：启动模板，定义Auto Scaling组启动实例时使用的配置 - Application Load Balancer：应用程序负载均衡器 - 无状态应用程序：不保存用户会话状态的应用程序 **正确答案分析（A、B、E）：** - **选项A正确**：创建使用新AMI的新launch template是必需的，因为原AMI已删除，需要新的启动配置 - **选项B正确**：必须更新Auto Scaling组配置，让它使用新的launch template，否则仍会尝试使用已删除的AMI - **选项E正确**：从当前运行的实例创建新AMI是最佳选择，因为这些实例包含了应用程序的完整配置，比使用基础公共AMI更合适 **其他选项错误的原因：** - **选项C错误**：将期望容量减少到0会导致服务中断，这不是解决AMI问题的必要步骤 - **选项D错误**：在修复AMI问题之前增加容量只会导致更多的启动失败 - **选项F错误**：使用公共AMI意味着需要重新配置应用程序环境，而从现有实例创建AMI更简单有效 **决策标准和最佳实践：** 1. **最小化服务中断**：应该在不影响现有服务的情况下解决问题 2. **保持配置一致性**：从现有实例创建AMI确保新实例与当前运行实例配置相同 3. **遵循AWS最佳实践**：使用launch template而不是launch configuration是当前推荐的做法 4. **渐进式更新**：先准备好新的AMI和launch template，再更新Auto Scaling组配置</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">166</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company deploys a web application on Amazon EC2 instances that are behind an Application Load Balancer (ALB). The company stores the application code in an AWS CodeCommit repository. When code is merged to the main branch, an AWS Lambda function invokes an AWS CodeBuild project. The CodeBuild project packages the code, stores the packaged code in AWS CodeArtifact, and invokes AWS Systems Manager Run Command to deploy the packaged code to the EC2 instances. Previous deployments have resulted in defects, EC2 instances that are not running the latest version of the packaged code, and inconsistencies between instances. Which combination of actions should a DevOps engineer take to implement a more reliable deployment solution? (Choose two.) for the deployment group. Most Voted BC (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a pipeline in AWS CodePipeline that uses the CodeCommit repository as a source provider. Configure pipeline stages that run the CodeBuild project in parallel to build and test the application. In the pipeline, pass the CodeBuild project output artifact to an AWS CodeDeploy action.
B. Create a pipeline in AWS CodePipeline that uses the CodeCommit repository as a source provider. Create separate pipeline stages that run a CodeBuild project to build and then test the application. In the pipeline, pass the CodeBuild project output artifact to an AWS CodeDeploy action.
C. Create an AWS CodeDeploy application and a deployment group to deploy the packaged code to the EC2 instances. Configure the ALB
D. Create individual Lambda functions that use AWS CodeDeploy instead of Systems Manager to run build, test, and deploy actions.
E. Create an Amazon S3 bucket. Modify the CodeBuild project to store the packages in the S3 bucket instead of in CodeArtifact. Use deploy actions in CodeDeploy to deploy the artifact to the EC2 instances.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Application Load Balancer (ALB)后面的Amazon EC2实例上部署Web应用程序。该公司将应用程序代码存储在AWS CodeCommit存储库中。当代码合并到主分支时，AWS Lambda函数调用AWS CodeBuild项目。CodeBuild项目打包代码，将打包的代码存储在AWS CodeArtifact中，并调用AWS Systems Manager Run Command将打包的代码部署到EC2实例。之前的部署导致了缺陷、EC2实例未运行最新版本的打包代码，以及实例之间的不一致性。DevOps工程师应该采取哪些组合操作来实现更可靠的部署解决方案？（选择两个）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求改进现有的部署流程，解决当前部署中存在的缺陷、版本不一致和实例间差异等问题。需要选择两个最佳的改进措施来提高部署的可靠性。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务，提供自动化的构建、测试和部署流程 - AWS CodeCommit：Git代码存储库服务 - AWS CodeBuild：构建和测试服务 - AWS CodeDeploy：应用程序部署服务，专门用于EC2实例的可靠部署 - AWS Systems Manager Run Command：在EC2实例上执行命令的服务 - Application Load Balancer (ALB)：应用程序负载均衡器 **正确答案的原因：** 选项A正确，因为： 1. 使用CodePipeline创建完整的CI/CD管道，提供更好的可视化和流程控制 2. 并行运行构建和测试阶段可以提高效率 3. 使用CodeDeploy替代Systems Manager Run Command，提供更可靠的部署机制 4. CodeDeploy专门设计用于应用程序部署，具有滚动部署、健康检查和回滚功能 **其他选项错误的原因：** - 选项B：虽然使用了CodePipeline和CodeDeploy，但串行执行构建和测试会降低效率，不如并行执行 - 选项C：描述不完整，只提到创建CodeDeploy应用程序和部署组，但没有说明如何集成到完整的CI/CD流程中 - 选项D：使用多个Lambda函数会增加复杂性和维护成本，不如使用专门的CI/CD服务 - 选项E：虽然S3存储有其优势，但这不是解决当前部署可靠性问题的关键因素 **决策标准和最佳实践：** 1. 使用专门的CI/CD工具（CodePipeline）而不是自定义的Lambda函数组合 2. 选择专业的部署服务（CodeDeploy）而不是通用的命令执行服务（Systems Manager） 3. 实现自动化的端到端流程，减少人工干预和错误 4. 利用并行处理提高部署效率 5. 确保部署过程具有一致性和可重复性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">167</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an organization in AWS Organizations to manage its AWS accounts. The company&#x27;s automation account contains a CI/CD pipeline that creates and configures new AWS accounts. The company has a group of internal service teams that provide services to accounts in the organization. The service teams operate out of a set of services accounts. The service teams want to receive an AWS CloudTrail event in their services accounts when the CreateAccount API call creates a new account. How should the company share this CloudTrail event with the service accounts? automation account. Create an EventBridge rule in the services account that directly listens to CloudTrail events from the automation account. A (83%) B (17%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule in the automation account to send account creation events to the default event bus in the services accounts. Update the default event bus in the services accounts to allow events from the automation account.
B. Create a custom Amazon EventBridge event bus in the services accounts. Update the custom event bus to allow events from the
C. Create a custom Amazon EventBridge event bus in the automation account and the services accounts. Create an EventBridge rule and policy that connects the custom event buses that are in the automation account and the services accounts.
D. Create a custom Amazon EventBridge event bus in the automation account. Create an EventBridge rule and policy that connects the custom event bus to the default event buses in the services accounts.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations中的组织来管理其AWS账户。该公司的自动化账户包含一个CI/CD管道，用于创建和配置新的AWS账户。该公司有一组内部服务团队，为组织中的账户提供服务。服务团队在一组服务账户中运营。当CreateAccount API调用创建新账户时，服务团队希望在其服务账户中接收AWS CloudTrail事件。公司应该如何与服务账户共享此CloudTrail事件？ 选项： A. 在自动化账户中创建Amazon EventBridge规则，将账户创建事件发送到服务账户中的默认事件总线。更新服务账户中的默认事件总线以允许来自自动化账户的事件。 B. 在服务账户中创建自定义Amazon EventBridge事件总线。更新自定义事件总线以允许来自自动化账户的事件。在服务账户中创建EventBridge规则，直接监听来自自动化账户的CloudTrail事件。 C. 在自动化账户和服务账户中创建自定义Amazon EventBridge事件总线。创建EventBridge规则和策略，连接自动化账户和服务账户中的自定义事件总线。 D. 在自动化账户中创建自定义Amazon EventBridge事件总线。创建EventBridge规则和策略，将自定义事件总线连接到服务账户中的默认事件总线。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是跨AWS账户的事件共享机制。需要将自动化账户中的CreateAccount API调用产生的CloudTrail事件分发到多个服务账户中，实现跨账户的事件通知。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. AWS CloudTrail - API调用审计和日志记录服务 3. Amazon EventBridge - 事件路由和处理服务 4. 跨账户事件共享机制 5. EventBridge事件总线（默认总线vs自定义总线） 6. EventBridge规则和资源策略 **正确答案D的原因：** 1. **集中化管理**：在源头（自动化账户）创建自定义事件总线，便于统一管理和控制事件分发 2. **简化目标配置**：使用服务账户的默认事件总线作为目标，无需在每个服务账户中创建额外的自定义总线 3. **最佳实践架构**：遵循&quot;在源头集中处理，在目标简化接收&quot;的设计原则 4. **扩展性好**：当需要添加新的服务账户时，只需更新自动化账户中的规则和策略 **其他选项错误的原因：** - **选项A**：直接使用默认事件总线进行跨账户发送存在安全和管理上的限制，不是推荐的做法 - **选项B**：描述不完整，且让每个服务账户都创建自定义总线增加了管理复杂性 - **选项C**：要求在所有账户（自动化账户+多个服务账户）都创建自定义总线，过度复杂化了架构，增加了维护成本 **决策标准和最佳实践：** 1. **简化原则**：在满足功能需求的前提下，选择最简单的架构方案 2. **集中管理**：事件源头负责分发逻辑，目标端负责接收处理 3. **可扩展性**：方案应该便于后续添加新的服务账户 4. **安全性**：通过自定义事件总线和适当的策略确保事件传输的安全性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">168</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is building a solution that uses Amazon Simple Queue Service (Amazon SQS) standard queues. The solution also includes an AWS Lambda function and an Amazon DynamoDB table. The Lambda function pulls content from an SQS queue event source and writes the content to the DynamoDB table. The solution must maximize the scalability of Lambda and must prevent successfully processed SQS messages from being processed multiple times. Which solution will meet these requirements? C (92%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Decrease the batch window to 1 second when configuring the Lambda function&#x27;s event source mapping.
B. Decrease the batch size to 1 when configuring the Lambda function&#x27;s event source mapping.
C. Include the ReportBatchItemFailures value in the FunctionResponseTypes list in the Lambda function&#x27;s event source mapping.
D. Set the queue visibility timeout on the Lambda function&#x27;s event source mapping to account for invocation throttling of the Lambda function.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师正在构建一个使用Amazon Simple Queue Service (Amazon SQS)标准队列的解决方案。该解决方案还包括一个AWS Lambda函数和一个Amazon DynamoDB表。Lambda函数从SQS队列事件源拉取内容并将内容写入DynamoDB表。该解决方案必须最大化Lambda的可扩展性，并且必须防止成功处理的SQS消息被多次处理。哪个解决方案能满足这些要求？ 选项： A. 在配置Lambda函数的事件源映射时，将批处理窗口减少到1秒 B. 在配置Lambda函数的事件源映射时，将批处理大小减少到1 C. 在Lambda函数的事件源映射的FunctionResponseTypes列表中包含ReportBatchItemFailures值 D. 在Lambda函数的事件源映射上设置队列可见性超时以考虑Lambda函数的调用限制</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案，既能最大化Lambda的可扩展性，又能防止成功处理的SQS消息被重复处理。这是一个关于Lambda与SQS集成的优化问题。 **涉及的关键AWS服务和概念：** - Amazon SQS标准队列：消息队列服务，支持at-least-once交付 - AWS Lambda事件源映射：Lambda与SQS的集成机制 - 批处理大小(batch size)：Lambda一次处理的消息数量 - 批处理窗口(batch window)：收集消息的时间窗口 - 可见性超时(visibility timeout)：消息被接收后对其他消费者不可见的时间 - ReportBatchItemFailures：部分批处理失败报告机制 **正确答案B的原因：** 将批处理大小设置为1意味着每次Lambda调用只处理一条消息。这样做的好处是： 1. 最大化可扩展性：每条消息都会触发独立的Lambda调用，充分利用Lambda的并发能力 2. 防止重复处理：如果Lambda函数成功处理了消息，该消息会被自动从队列中删除；如果失败，只有这一条消息会重新变为可见，不会影响其他消息 3. 简化错误处理：避免了批处理中部分成功、部分失败的复杂情况 **其他选项错误的原因：** A. 减少批处理窗口到1秒：这只是减少了等待时间，但不能解决重复处理问题，也不能最大化可扩展性 C. ReportBatchItemFailures：这是用于处理批处理中部分失败的情况，但题目要求最大化可扩展性，使用批处理本身就限制了可扩展性 D. 设置可见性超时：这只是调整消息重新可见的时间，不能从根本上解决重复处理问题，也不能提高可扩展性 **决策标准和最佳实践：** 1. 可扩展性优先：Lambda的最大优势是其并发能力，应该充分利用 2. 消息处理的原子性：每条消息独立处理可以避免复杂的错误处理逻辑 3. 简单性原则：批处理大小为1是最简单直接的解决方案 4. 成本考虑：虽然会增加Lambda调用次数，但换来了更好的可扩展性和可靠性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">169</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a new AWS account that teams will use to deploy various applications. The teams will create many Amazon S3 buckets for application-specific purposes and to store AWS CloudTrail logs. The company has enabled Amazon Macie for the account. A DevOps engineer needs to optimize the Macie costs for the account without compromising the account&#x27;s functionality. Which solutions will meet these requirements? (Choose two.) AD (83%) Other</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Exclude S3 buckets that contain CloudTrail logs from automated discovery.
B. Exclude S3 buckets that have public read access from automated discovery.
C. Configure scheduled daily discovery jobs for all S3 buckets in the account.
D. Configure discovery jobs to include S3 objects based on the last modified criterion.
E. Configure discovery jobs to include S3 objects that are tagged as production only.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个新的AWS账户，团队将使用该账户部署各种应用程序。团队将为特定应用程序目的创建许多Amazon S3存储桶，并存储AWS CloudTrail日志。公司已为该账户启用了Amazon Macie。DevOps工程师需要在不影响账户功能的情况下优化Macie成本。哪些解决方案将满足这些要求？（选择两个） 选项：A. 从自动发现中排除包含CloudTrail日志的S3存储桶。B. 从自动发现中排除具有公共读取访问权限的S3存储桶。C. 为账户中的所有S3存储桶配置计划的每日发现作业。D. 配置发现作业以基于最后修改标准包含S3对象。E. 配置发现作业仅包含标记为生产环境的S3对象。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在保持账户功能完整的前提下优化Amazon Macie的成本。需要选择两个正确的解决方案来实现成本优化。 **涉及的关键AWS服务和概念：** - Amazon Macie：AWS的数据安全和隐私服务，用于发现、分类和保护敏感数据 - Amazon S3：对象存储服务 - AWS CloudTrail：API调用日志记录服务 - Macie发现作业：扫描S3存储桶中敏感数据的自动化任务 - 成本优化策略 **正确答案分析：** 题目显示正确答案是C，但根据Macie成本优化的最佳实践，实际上应该选择A和D或A和E： A选项正确：CloudTrail日志通常不包含敏感的个人数据，扫描这些日志对数据保护意义不大，排除它们可以显著减少Macie的扫描成本。 D选项正确：通过最后修改时间筛选，可以避免重复扫描未更改的旧数据，只关注新增或修改的内容，有效控制成本。 **其他选项分析：** B选项错误：公共读取访问的存储桶可能包含敏感数据，排除它们会增加数据泄露风险。 C选项问题：每日扫描所有存储桶会产生大量成本，不符合成本优化要求。 E选项部分正确：只扫描生产环境数据可以减少成本，但可能遗漏其他环境中的敏感数据。 **决策标准和最佳实践：** 1. 识别不需要扫描的数据类型（如系统日志） 2. 使用时间和标签过滤器减少扫描范围 3. 避免重复扫描未更改的数据 4. 平衡成本控制与数据保护需求 5. 定期审查和调整Macie配置以优化成本效益</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">170</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an organization in AWS Organizations to manage its AWS accounts. The company recently acquired another company that has standalone AWS accounts. The acquiring company&#x27;s DevOps team needs to consolidate the administration of the AWS accounts for both companies and retain full administrative control of the accounts. The DevOps team also needs to collect and group findings across all the accounts to implement and maintain a security posture. Which combination of steps should the DevOps team take to meet these requirements? (Choose two.) BC (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Invite the acquired company&#x27;s AWS accounts to join the organization. Create an SCP that has full administrative privileges. Attach the SCP to the management account.
B. Invite the acquired company&#x27;s AWS accounts to join the organization. Create the OrganizationAccountAccessRole IAM role in the invited accounts. Grant permission to the management account to assume the role.
C. Use AWS Security Hub to collect and group findings across all accounts. Use Security Hub to automatically detect new accounts as the accounts are added to the organization.
D. Use AWS Firewall Manager to collect and group findings across all accounts. Enable all features for the organization. Designate an account in the organization as the delegated administrator account for Firewall Manager.
E. Use Amazon Inspector to collect and group findings across all accounts. Designate an account in the organization as the delegated administrator account for Amazon Inspector.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations中的组织来管理其AWS账户。该公司最近收购了另一家拥有独立AWS账户的公司。收购方公司的DevOps团队需要整合两家公司AWS账户的管理，并保留对账户的完全管理控制权。DevOps团队还需要收集和分组所有账户的发现结果，以实施和维护安全态势。DevOps团队应该采取哪些步骤组合来满足这些要求？（选择两个。） 选项： A. 邀请被收购公司的AWS账户加入组织。创建具有完全管理权限的SCP。将SCP附加到管理账户。 B. 邀请被收购公司的AWS账户加入组织。在受邀账户中创建OrganizationAccountAccessRole IAM角色。授予管理账户权限来承担该角色。 C. 使用AWS Security Hub收集和分组所有账户的发现结果。使用Security Hub在账户添加到组织时自动检测新账户。 D. 使用AWS Firewall Manager收集和分组所有账户的发现结果。为组织启用所有功能。指定组织中的一个账户作为Firewall Manager的委托管理员账户。 E. 使用Amazon Inspector收集和分组所有账户的发现结果。指定组织中的一个账户作为Amazon Inspector的委托管理员账户。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求解决两个主要需求：1）整合和管理多个AWS账户，保持完全管理控制权；2）跨所有账户收集和分组安全发现结果以维护安全态势。 **涉及的关键AWS服务和概念：** - AWS Organizations：用于集中管理多个AWS账户 - OrganizationAccountAccessRole：组织中用于跨账户访问的标准IAM角色 - AWS Security Hub：安全态势管理和发现结果聚合服务 - SCP (Service Control Policy)：组织级别的权限边界策略 - AWS Firewall Manager和Amazon Inspector：其他安全服务 **正确答案的原因：** 选项B和C是正确答案。 - 选项B：邀请账户加入组织并创建OrganizationAccountAccessRole是AWS Organizations的标准最佳实践，这个角色允许管理账户对成员账户进行完全管理访问，满足&quot;保留完全管理控制权&quot;的要求。 - 选项C：AWS Security Hub专门设计用于跨多个账户和服务收集、聚合和分组安全发现结果，完美满足&quot;收集和分组发现结果以维护安全态势&quot;的需求，且能自动检测组织中的新账户。 **其他选项错误的原因：** - 选项A：将具有完全管理权限的SCP附加到管理账户是错误的，SCP不应该附加到管理账户，且SCP是用来限制权限而不是授予权限的。 - 选项D：AWS Firewall Manager主要用于防火墙规则管理，不是用于收集和分组安全发现结果的主要工具。 - 选项E：Amazon Inspector主要专注于EC2实例和容器镜像的漏洞评估，范围相对狭窄，不如Security Hub全面。 **决策标准和最佳实践：** 1. 账户整合应使用AWS Organizations的标准流程和角色 2. 安全态势管理应选择专门的聚合服务（Security Hub） 3. 避免在管理账户上应用SCP 4. 选择覆盖范围最广、最适合需求的安全服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">171</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application and a CI/CD pipeline. The CI/CD pipeline consists of an AWS CodePipeline pipeline and an AWS CodeBuild project. The CodeBuild project runs tests against the application as part of the build process and outputs a test report. The company must keep the test reports for 90 days. Which solution will meet these requirements? B (63%) D (37%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a new stage in the CodePipeline pipeline after the stage that contains the CodeBuild project. Create an Amazon S3 bucket to store the reports. Configure an S3 deploy action type in the new CodePipeline stage with the appropriate path and format for the reports.
B. Add a report group in the CodeBuild project buildspec file with the appropriate path and format for the reports. Create an Amazon S3 bucket to store the reports. Configure an Amazon EventBridge rule that invokes an AWS Lambda function to copy the reports to the S3 bucket when a build is completed. Create an S3 Lifecycle rule to expire the objects after 90 days.
C. Add a new stage in the CodePipeline pipeline. Configure a test action type with the appropriate path and format for the reports. Configure the report expiration time to be 90 days in the CodeBuild project buildspec file.
D. Add a report group in the CodeBuild project buildspec file with the appropriate path and format for the reports. Create an Amazon S3 bucket to store the reports. Configure the report group as an artifact in the CodeBuild project buildspec file. Configure the S3 bucket as the artifact destination. Set the object expiration to 90 days.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个应用程序和一个CI/CD流水线。该CI/CD流水线由AWS CodePipeline流水线和AWS CodeBuild项目组成。CodeBuild项目在构建过程中对应用程序运行测试，并输出测试报告。公司必须将测试报告保存90天。哪种解决方案能满足这些要求？ 选项： A. 在包含CodeBuild项目的阶段之后，在CodePipeline流水线中添加一个新阶段。创建一个Amazon S3存储桶来存储报告。在新的CodePipeline阶段中配置S3部署操作类型，使用适当的路径和格式来处理报告。 B. 在CodeBuild项目的buildspec文件中添加一个报告组，使用适当的路径和格式来处理报告。创建一个Amazon S3存储桶来存储报告。配置一个Amazon EventBridge规则，当构建完成时调用AWS Lambda函数将报告复制到S3存储桶。创建S3生命周期规则，在90天后使对象过期。 C. 在CodePipeline流水线中添加一个新阶段。配置测试操作类型，使用适当的路径和格式来处理报告。在CodeBuild项目的buildspec文件中配置报告过期时间为90天。 D. 在CodeBuild项目的buildspec文件中添加一个报告组，使用适当的路径和格式来处理报告。创建一个Amazon S3存储桶来存储报告。在CodeBuild项目的buildspec文件中将报告组配置为构件。将S3存储桶配置为构件目标。设置对象过期时间为90天。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个解决方案来保存CodeBuild项目生成的测试报告90天。关键需求包括：1）从CodeBuild项目中收集测试报告；2）将报告存储在持久化存储中；3）确保报告在90天后自动删除。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：构建服务，可以生成测试报告 - CodeBuild报告组（Report Groups）：用于收集和管理测试报告的功能 - Amazon S3：对象存储服务，用于长期存储报告 - S3生命周期策略：自动管理对象的生命周期 - Amazon EventBridge：事件驱动服务 - AWS Lambda：无服务器计算服务 - CodePipeline构件（Artifacts）：流水线阶段间传递的数据 **正确答案B的原因：** 选项B提供了完整且正确的解决方案：1）使用CodeBuild报告组正确收集测试报告；2）通过EventBridge和Lambda实现事件驱动的报告传输到S3；3）使用S3生命周期规则实现90天自动过期。这种方案架构清晰，各组件职责明确，符合AWS最佳实践。 **其他选项错误的原因：** 选项A错误：CodePipeline的S3部署操作主要用于部署构件，不是专门为处理测试报告设计的，且没有涉及报告组的配置。选项C错误：CodePipeline没有专门的&quot;测试操作类型&quot;，且buildspec文件中无法直接配置报告过期时间。选项D错误：虽然提到了报告组，但将报告组配置为构件的概念是错误的，报告组和构件是不同的概念，不能混用。 **决策标准和最佳实践：** 选择方案时应考虑：1）使用AWS服务的原生功能（如CodeBuild报告组）；2）采用事件驱动架构提高系统解耦性；3）利用S3生命周期策略实现自动化管理；4）确保方案的可扩展性和维护性。选项B完美体现了这些最佳实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">172</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an Amazon API Gateway regional REST API to host its application API. The REST API has a custom domain. The REST API&#x27;s default endpoint is deactivated. The company&#x27;s internal teams consume the API. The company wants to use mutual TLS between the API and the internal teams as an additional layer of authentication. Which combination of steps will meet these requirements? (Choose two.) key that is stored in the S3 bucket as the trust store. private CA certificate that is stored in the S3 bucket as the trust store. Most Voted AE (72%) AC (20%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Certificate Manager (ACM) to create a private certificate authority (CA). Provision a client certificate that is signed by the private CA.
B. Provision a client certificate that is signed by a public certificate authority (CA). Import the certificate into AWS Certificate Manager (ACM).
C. Upload the provisioned client certificate to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the client certificate that is stored in the S3 bucket as the trust store.
D. Upload the provisioned client certificate private key to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the private
E. Upload the root private certificate authority (CA) certificate to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用Amazon API Gateway区域REST API来托管其应用程序API。该REST API有一个自定义域名。REST API的默认端点已被停用。公司的内部团队使用该API。公司希望在API和内部团队之间使用双向TLS作为额外的身份验证层。哪种步骤组合将满足这些要求？（选择两个） 选项： A. 使用AWS Certificate Manager (ACM)创建私有证书颁发机构(CA)。提供由私有CA签名的客户端证书。 B. 提供由公共证书颁发机构(CA)签名的客户端证书。将证书导入AWS Certificate Manager (ACM)。 C. 将提供的客户端证书上传到Amazon S3存储桶。配置API Gateway双向TLS使用存储在S3存储桶中的客户端证书作为信任存储。 D. 将提供的客户端证书私钥上传到Amazon S3存储桶。配置API Gateway双向TLS使用私钥... E. 将根私有证书颁发机构(CA)证书上传到Amazon S3存储桶。配置API Gateway双向TLS使用...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为API Gateway REST API实现双向TLS (mTLS)认证，用于公司内部团队访问API时提供额外的安全认证层。 **涉及的关键AWS服务和概念：** - API Gateway双向TLS配置 - AWS Certificate Manager (ACM)私有CA - 客户端证书管理 - S3作为信任存储的配置 - 证书链和信任关系 **正确答案的原因：** 选项A正确，因为： 1. 使用ACM私有CA为内部使用场景提供了完全的控制权 2. 私有CA可以签发客户端证书，确保只有授权的内部团队能够访问 3. ACM私有CA与API Gateway mTLS原生集成，简化管理流程 4. 符合内部团队使用的安全最佳实践 还需要选择一个配置信任存储的选项，应该是选项E（上传根CA证书到S3作为信任存储）。 **其他选项错误的原因：** - 选项B：公共CA签名的证书不适合内部认证场景，无法提供足够的访问控制 - 选项C：客户端证书本身不应作为信任存储，信任存储应该是CA证书 - 选项D：私钥不应上传到S3，存在安全风险，且私钥不是信任存储的组成部分 **决策标准和最佳实践：** 1. 内部应用应使用私有CA而非公共CA 2. 双向TLS需要正确配置客户端证书和服务器端信任存储 3. 信任存储应包含CA证书而非客户端证书 4. 私钥应安全保管，不应上传到云存储服务 5. 利用AWS托管服务(ACM)简化证书生命周期管理</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">173</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Directory Service for Microsoft Active Directory as its identity provider (IdP). The company requires all infrastructure to be defined and deployed by AWS CloudFormation. A DevOps engineer needs to create a fleet of Windows-based Amazon EC2 instances to host an application. The DevOps engineer has created a CloudFormation template that contains an EC2 launch template, IAM role, EC2 security group, and EC2 Auto Scaling group. The DevOps engineer must implement a solution that joins all EC2 instances to the domain of the AWS Managed Microsoft AD directory. Which solution will meet these requirements with the MOST operational efficiency? domain by using the parameters for the existing directory. Update the launch template to include the SSMAssociation property to use the new SSM document. Attach the AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess AWS managed policies to the IAM role that the EC2 instances use. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the CloudFormation template, create an AWS::SSM::Document resource that joins the EC2 instance to the AWS Managed Microsoft AD
B. In the CloudFormation template, update the launch template to include specific tags that propagate on launch. Create an AWS::SSM::Association resource to associate the AWS-JoinDirectoryServiceDomain Automation runbook with the EC2 instances that have the specified tags. Define the required parameters to join the AWS Managed Microsoft AD directory. Attach the AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess AWS managed policies to the IAM role that the EC2 instances use.
C. Store the existing AWS Managed Microsoft AD domain connection details in AWS Secrets Manager. In the CloudFormation template, create an AWS::SSM::Association resource to associate the AWS-CreateManagedWindowsInstanceWithApproval Automation runbook with the EC2 Auto Scaling group. Pass the ARNs for the parameters from Secrets Manager to join the domain. Attach the AmazonSSMDirectoryServiceAccess and SecretsManagerReadWrite AWS managed policies to the IAM role that the EC2 instances use.
D. Store the existing AWS Managed Microsoft AD domain administrator credentials in AWS Secrets Manager. In the CloudFormation template, update the EC2 launch template to include user data. Configure the user data to pull the administrator credentials from Secrets Manager and to join the AWS Managed Microsoft AD domain. Attach the AmazonSSMManagedInstanceCore and SecretsManagerReadWrite AWS managed policies to the IAM role that the EC2 instances use.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Directory Service for Microsoft Active Directory作为其身份提供商(IdP)。公司要求所有基础设施都通过AWS CloudFormation定义和部署。DevOps工程师需要创建一组基于Windows的Amazon EC2实例来托管应用程序。DevOps工程师已经创建了一个CloudFormation模板，包含EC2启动模板、IAM角色、EC2安全组和EC2 Auto Scaling组。DevOps工程师必须实现一个解决方案，将所有EC2实例加入到AWS Managed Microsoft AD目录的域中。哪个解决方案能够以最高的运营效率满足这些要求？ 选项： A. 在CloudFormation模板中，创建一个AWS::SSM::Document资源，将EC2实例加入到AWS Managed Microsoft AD域中，使用现有目录的参数。更新启动模板以包含SSMAssociation属性来使用新的SSM文档。将AmazonSSMManagedInstanceCore和AmazonSSMDirectoryServiceAccess AWS托管策略附加到EC2实例使用的IAM角色。 B. 在CloudFormation模板中，更新启动模板以包含在启动时传播的特定标签。创建AWS::SSM::Association资源，将AWS-JoinDirectoryServiceDomain自动化运行手册与具有指定标签的EC2实例关联。定义加入AWS Managed Microsoft AD目录所需的参数。将AmazonSSMManagedInstanceCore和AmazonSSMDirectoryServiceAccess AWS托管策略附加到EC2实例使用的IAM角色。 C. 将现有的AWS Managed Microsoft AD域连接详细信息存储在AWS Secrets Manager中。在CloudFormation模板中，创建AWS::SSM::Association资源，将AWS-CreateManagedWindowsInstanceWithApproval自动化运行手册与EC2 Auto Scaling组关联。传递来自Secrets Manager的参数ARN以加入域。将AmazonSSMDirectoryServiceAccess和SecretsManagerReadWrite AWS托管策略附加到EC2实例使用的IAM角色。 D. 将现有的AWS Managed Microsoft AD域管理员凭据存储在AWS Secrets Manager中。在CloudFormation模板中，更新EC2启动模板以包含用户数据。配置用户数据从Secrets Manager拉取管理员凭据并加入AWS Managed Microsoft AD域。将AmazonSSMManagedInstanceCore和SecretsManagerReadWrite AWS托管策略附加到EC2实例使用的IAM角色。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在CloudFormation环境中，以最高运营效率的方式将Auto Scaling组中的Windows EC2实例自动加入到AWS Managed Microsoft AD域中。 **涉及的关键AWS服务和概念：** - AWS Directory Service for Microsoft Active Directory：托管的Active Directory服务 - AWS Systems Manager (SSM)：用于自动化实例管理和域加入操作 - AWS Secrets Manager：安全存储敏感信息如域凭据 - CloudFormation：基础设施即代码服务 - EC2 Auto Scaling：自动扩展EC2实例 **正确答案C的原因：** 1. **使用预构建的自动化运行手册**：AWS-CreateManagedWindowsInstanceWithApproval是AWS提供的标准自动化运行手册，专门用于创建和配置Windows实例并加入域 2. **安全的凭据管理**：使用Secrets Manager存储域连接详细信息，避免在模板中硬编码敏感信息 3. **与Auto Scaling集成**：直接与Auto Scaling组关联，确保所有新启动的实例都自动加入域 4. **运营效率最高**：利用AWS原生的自动化功能，减少自定义脚本和手动配置 **其他选项错误的原因：** - **选项A**：需要创建自定义SSM文档，增加了复杂性和维护负担，运营效率较低 - **选项B**：基于标签的关联方式虽然可行，但AWS-JoinDirectoryServiceDomain主要用于已存在的实例，不如选项C针对Auto Scaling场景优化 - **选项D**：使用用户数据脚本的方式过于原始，缺乏错误处理和重试机制，且难以管理和调试 **决策标准和最佳实践：** 1. **优先使用AWS托管服务**：选择AWS预构建的自动化运行手册而非自定义解决方案 2. **安全性考虑**：使用Secrets Manager管理敏感凭据，避免在代码中暴露 3. **自动化程度**：选择能够与Auto Scaling无缝集成的解决方案 4. **可维护性**：减少自定义代码，提高解决方案的可靠性和可维护性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">174</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Organizations to manage its AWS accounts. The company has a root OU that has a child OU. The root OU has an SCP that allows all actions on all resources. The child OU has an SCP that allows all actions for Amazon DynamoDB and AWS Lambda, and denies all other actions. The company has an AWS account that is named vendor-data in the child OU. A DevOps engineer has an IAM user that is attached to the AdministratorAccess IAM policy in the vendor-data account. The DevOps engineer attempts to launch an Amazon EC2 instance in the vendor-data account but receives an access denied error. Which change should the DevOps engineer make to launch the EC2 instance in the vendor-data account? C (70%) B (30%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Attach the AmazonEC2FullAccess IAM policy to the IAM user.
B. Create a new SCP that allows all actions for Amazon EC2. Attach the SCP to the vendor-data account.
C. Update the SCP in the child OU to allow all actions for Amazon EC2.
D. Create a new SCP that allows all actions for Amazon EC2. Attach the SCP to the root OU.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations来管理其AWS账户。该公司有一个根OU，根OU下有一个子OU。根OU有一个SCP，允许对所有资源执行所有操作。子OU有一个SCP，允许对Amazon DynamoDB和AWS Lambda执行所有操作，但拒绝所有其他操作。该公司在子OU中有一个名为vendor-data的AWS账户。一名DevOps工程师有一个IAM用户，该用户在vendor-data账户中附加了AdministratorAccess IAM策略。DevOps工程师尝试在vendor-data账户中启动Amazon EC2实例，但收到访问拒绝错误。DevOps工程师应该做出什么更改才能在vendor-data账户中启动EC2实例？ 选项：A. 将AmazonEC2FullAccess IAM策略附加到IAM用户。B. 创建一个新的SCP，允许对Amazon EC2执行所有操作。将SCP附加到vendor-data账户。C. 更新子OU中的SCP以允许对Amazon EC2执行所有操作。D. 创建一个新的SCP，允许对Amazon EC2执行所有操作。将SCP附加到根OU。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS Organizations中SCP（Service Control Policy）与IAM策略之间的关系和权限控制机制。需要理解为什么拥有AdministratorAccess权限的用户仍然无法启动EC2实例。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. SCP (Service Control Policy) - 服务控制策略，用于在组织级别限制权限 3. OU (Organizational Unit) - 组织单元 4. IAM策略 - 身份和访问管理策略 5. 权限边界和继承机制 **正确答案的原因：** 答案A是错误的。这里存在一个关键的理解误区： - SCP作为&quot;护栏&quot;(guardrails)，定义了账户中用户和角色的最大权限边界 - 子OU的SCP只允许DynamoDB和Lambda操作，拒绝所有其他操作（包括EC2） - 无论IAM用户有什么权限（即使是AdministratorAccess），都不能超越SCP的限制 - 添加AmazonEC2FullAccess策略无法解决SCP层面的限制 **实际正确答案应该是C：** 更新子OU中的SCP以允许Amazon EC2操作，因为： - SCP的限制优先于IAM策略 - 必须在SCP层面先允许EC2操作 - 子OU的SCP直接影响vendor-data账户 **其他选项错误的原因：** - 选项B：不能直接将SCP附加到单个账户，SCP只能附加到OU或根组织 - 选项D：在根OU添加新SCP不会覆盖子OU的限制性SCP **决策标准和最佳实践：** 1. 理解SCP的&quot;拒绝优先&quot;原则 2. SCP与IAM策略的交集决定最终权限 3. 权限问题要从组织层级（SCP）到账户层级（IAM）逐层排查 4. SCP应该用于设置安全边界，而不是授予权限</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">175</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s security policies require the use of security hardened AMIs in production environments. A DevOps engineer has used EC2 Image Builder to create a pipeline that builds the AMIs on a recurring schedule. The DevOps engineer needs to update the launch templates of the company&#x27;s Auto Scaling groups. The Auto Scaling groups must use the newest AMIs during the launch of Amazon EC2 instances. Which solution will meet these requirements with the MOST operational efficiency? D (69%) C (16%) B (16%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure an Amazon EventBridge rule to receive new AMI events from Image Builder. Target an AWS Systems Manager Run Command document that updates the launch templates of the Auto Scaling groups with the newest AMI ID.
B. Configure an Amazon EventBridge rule to receive new AMI events from Image Builder. Target an AWS Lambda function that updates the launch templates of the Auto Scaling groups with the newest AMI ID.
C. Configure the launch template to use a value from AWS Systems Manager Parameter Store for the AMI ID. Configure the Image Builder pipeline to update the Parameter Store value with the newest AMI ID.
D. Configure the Image Builder distribution settings to update the launch templates with the newest AMI ID. Configure the Auto Scaling groups to use the newest version of the launch template.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的安全策略要求在生产环境中使用经过安全加固的AMI。DevOps工程师已经使用EC2 Image Builder创建了一个管道，按照定期计划构建AMI。DevOps工程师需要更新公司Auto Scaling groups的launch templates。Auto Scaling groups在启动Amazon EC2实例时必须使用最新的AMI。哪种解决方案能够以最高的运营效率满足这些要求？ 选项： A. 配置Amazon EventBridge规则来接收来自Image Builder的新AMI事件。目标设为AWS Systems Manager Run Command文档，用最新的AMI ID更新Auto Scaling groups的launch templates。 B. 配置Amazon EventBridge规则来接收来自Image Builder的新AMI事件。目标设为AWS Lambda函数，用最新的AMI ID更新Auto Scaling groups的launch templates。 C. 配置launch template使用AWS Systems Manager Parameter Store中的值作为AMI ID。配置Image Builder管道用最新的AMI ID更新Parameter Store值。 D. 配置Image Builder分发设置来用最新的AMI ID更新launch templates。配置Auto Scaling groups使用launch template的最新版本。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个最具运营效率的解决方案，确保Auto Scaling groups能够自动使用EC2 Image Builder定期构建的最新安全加固AMI。关键在于实现自动化更新机制，减少手动干预。 **涉及的关键AWS服务和概念：** - EC2 Image Builder：用于自动化构建和维护AMI的服务 - Auto Scaling Groups：自动扩缩容服务 - Launch Templates：启动模板，定义EC2实例启动配置 - AWS Systems Manager Parameter Store：参数存储服务 - Amazon EventBridge：事件驱动服务 - AWS Lambda：无服务器计算服务 **正确答案C的原因：** 1. **解耦设计**：通过Parameter Store作为中间层，将AMI ID与launch template解耦，launch template引用参数而非硬编码AMI ID 2. **自动化程度高**：Image Builder可以直接更新Parameter Store参数，无需额外的事件处理逻辑 3. **运营效率最高**：一旦配置完成，整个流程完全自动化，无需人工干预 4. **简化架构**：减少了组件数量，降低了故障点和维护复杂度 5. **实时生效**：Auto Scaling启动新实例时会实时读取Parameter Store中的最新值 **其他选项错误的原因：** - **选项A**：使用Systems Manager Run Command增加了复杂性，需要编写和维护文档，运营效率较低 - **选项B**：需要开发和维护Lambda函数，增加了代码管理负担，虽然可行但效率不如C - **选项D**：Image Builder的分发设置无法直接更新launch templates，这个功能在AWS中不存在 **决策标准和最佳实践：** 1. **最小化组件复杂度**：选择需要最少组件和配置的方案 2. **自动化优先**：避免需要额外编程或脚本维护的解决方案 3. **解耦架构**：使用参数化配置而非硬编码值 4. **AWS原生集成**：优先选择AWS服务间原生支持的集成方式 5. **运营效率**：考虑长期维护成本和故障排除的便利性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">176</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has configured an Amazon S3 event source on an AWS Lambda function. The company needs the Lambda function to run when a new object is created or an existing object is modified in a particular S3 bucket. The Lambda function will use the S3 bucket name and the S3 object key of the incoming event to read the contents of the created or modified S3 object. The Lambda function will parse the contents and save the parsed contents to an Amazon DynamoDB table. The Lambda function&#x27;s execution role has permissions to read from the S3 bucket and to write to the DynamoDB table. During testing, a DevOps engineer discovers that the Lambda function does not run when objects are added to the S3 bucket or when existing objects are modified. Which solution will resolve this problem? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Increase the memory of the Lambda function to give the function the ability to process large files from the S3 bucket.
B. Create a resource policy on the Lambda function to grant Amazon S3 the permission to invoke the Lambda function for the S3 bucket.
C. Configure an Amazon Simple Queue Service (Amazon SQS) queue as an OnFailure destination for the Lambda function.
D. Provision space in the /tmp folder of the Lambda function to give the function the ability to process large files from the S3 bucket.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Lambda函数上配置了Amazon S3事件源。公司需要Lambda函数在特定S3存储桶中创建新对象或修改现有对象时运行。Lambda函数将使用传入事件的S3存储桶名称和S3对象键来读取已创建或修改的S3对象的内容。Lambda函数将解析内容并将解析后的内容保存到Amazon DynamoDB表中。Lambda函数的执行角色具有从S3存储桶读取和写入DynamoDB表的权限。在测试期间，DevOps工程师发现当对象添加到S3存储桶或修改现有对象时，Lambda函数不会运行。哪个解决方案能解决这个问题？ 选项：A. 增加Lambda函数的内存，使函数能够处理S3存储桶中的大文件。B. 在Lambda函数上创建资源策略，授予Amazon S3调用该S3存储桶Lambda函数的权限。C. 配置Amazon Simple Queue Service (Amazon SQS)队列作为Lambda函数的OnFailure目标。D. 在Lambda函数的/tmp文件夹中预置空间，使函数能够处理S3存储桶中的大文件。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题描述了Lambda函数配置了S3事件源，但在S3对象创建或修改时函数没有被触发执行。需要找到解决Lambda函数无法被S3事件触发的根本原因。 **涉及的关键AWS服务和概念：** - S3事件通知和Lambda触发器 - Lambda函数权限和资源策略 - S3服务调用Lambda的权限模型 - Lambda执行角色vs资源策略的区别 **正确答案的原因：** 选项B是正确答案。当S3事件源触发Lambda函数时，需要两层权限： 1. Lambda执行角色权限（已配置）- 允许Lambda访问其他AWS服务 2. Lambda资源策略权限（缺失）- 允许S3服务调用Lambda函数 题目中提到执行角色已有读S3和写DynamoDB的权限，但缺少允许S3服务调用Lambda的资源策略，这正是函数无法被触发的根本原因。 **其他选项错误的原因：** - 选项A（增加内存）：问题不在于处理能力，而是函数根本没有被触发 - 选项C（SQS OnFailure目标）：这是处理失败后的操作，与触发问题无关 - 选项D（/tmp空间）：同样是处理能力问题，但核心是触发权限缺失 **决策标准和最佳实践：** - 区分Lambda执行角色权限和资源策略权限的不同用途 - S3事件触发Lambda需要明确的资源策略授权 - 排查Lambda问题时要先确认是否能被正常触发，再考虑执行过程中的问题 - 遵循最小权限原则，精确配置所需的触发权限</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">177</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has deployed a critical application in two AWS Regions. The application uses an Application Load Balancer (ALB) in both Regions. The company has Amazon Route 53 alias DNS records for both ALBs. The company uses Amazon Route 53 Application Recovery Controller to ensure that the application can fail over between the two Regions. The Route 53 ARC configuration includes a routing control for both Regions. The company uses Route 53 ARC to perform quarterly disaster recovery (DR) tests. During the most recent DR test, a DevOps engineer accidentally turned off both routing controls. The company needs to ensure that at least one routing control is turned on at all times. Which solution will meet these requirements? the ATLEAST type with a threshold of 1. Most Voted OR type with a threshold of 1. ARNs of the two routing controls as the target resource. Create a new readiness check for the resource set. resource type. Add the domain names of the two Route 53 alias DNS records as the target resource. Create a new readiness check for the resource set. A (93%) 7%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In Route 53 ARC, create a new assertion safety rule. Apply the assertion safety rule to the two routing controls. Configure the rule with
B. In Route 53 ARC, create a new gating safety rule. Apply the assertion safety rule to the two routing controls. Configure the rule with the
C. In Route 53 ARC, create a new resource set. Configure the resource set with an AWS::Route53::HealthCheck resource type. Specify the
D. In Route 53 ARC, create a new resource set. Configure the resource set with an AWS::Route53RecoveryReadiness::DNSTargetResource</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在两个AWS Region中部署了一个关键应用程序。该应用程序在两个Region中都使用Application Load Balancer (ALB)。公司为两个ALB都配置了Amazon Route 53别名DNS记录。公司使用Amazon Route 53 Application Recovery Controller来确保应用程序可以在两个Region之间进行故障转移。Route 53 ARC配置包括两个Region的routing control。公司使用Route 53 ARC执行季度灾难恢复(DR)测试。在最近的DR测试中，一名DevOps工程师意外地关闭了两个routing control。公司需要确保至少有一个routing control始终处于开启状态。哪个解决方案能满足这些要求？ 选项： A. 在Route 53 ARC中，创建一个新的assertion safety rule。将assertion safety rule应用到两个routing control上。配置规则使用ATLEAST类型，阈值为1。 B. 在Route 53 ARC中，创建一个新的gating safety rule。将assertion safety rule应用到两个routing control上。配置规则使用OR类型，阈值为1。 C. 在Route 53 ARC中，创建一个新的resource set。使用AWS::Route53::HealthCheck资源类型配置resource set。将两个routing control的ARN作为目标资源。为resource set创建新的readiness check。 D. 在Route 53 ARC中，创建一个新的resource set。使用AWS::Route53RecoveryReadiness::DNSTargetResource资源类型配置resource set。将两个Route 53别名DNS记录的域名作为目标资源。为resource set创建新的readiness check。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题要求防止意外关闭所有routing control的情况，确保至少有一个routing control始终保持开启状态，以避免应用程序完全不可用。 **涉及的关键AWS服务和概念：** 1. Route 53 Application Recovery Controller (ARC) - AWS的应用程序恢复控制服务 2. Routing Control - 控制流量路由的开关机制 3. Safety Rules - 防护规则，包括assertion safety rule和gating safety rule 4. Resource Set和Readiness Check - 用于监控资源就绪状态 5. Health Check - Route 53的健康检查功能 **正确答案的原因：** 选项A是正确答案。Assertion safety rule专门用于防止意外的配置更改，ATLEAST类型确保指定数量的routing control保持开启状态。设置阈值为1意味着至少要有1个routing control处于开启状态，这正好满足需求。这种配置会阻止用户同时关闭所有routing control。 **其他选项错误的原因：** - 选项B：Gating safety rule主要用于控制故障转移操作的顺序和条件，而不是防止意外关闭所有控制。另外，OR类型在这个上下文中不是正确的配置。 - 选项C和D：Resource set和readiness check主要用于监控资源的就绪状态和健康状况，而不是防止routing control被意外关闭。这些是监控工具，不能阻止操作执行。 **决策标准和最佳实践：** 1. 使用safety rules作为防护机制，防止人为错误导致的服务中断 2. Assertion safety rule适用于防止危险的配置更改 3. 设置合理的阈值确保服务的最低可用性要求 4. 区分监控工具(readiness check)和防护机制(safety rules)的不同用途 5. 在多Region部署中，始终保持至少一个Region可用是关键的最佳实践</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">178</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A healthcare services company is concerned about the growing costs of software licensing for an application for monitoring patient wellness. The company wants to create an audit process to ensure that the application is running exclusively on Amazon EC2 Dedicated Hosts. A DevOps engineer must create a workflow to audit the application to ensure compliance. What steps should the engineer take to meet this requirement with the LEAST administrative overhead? obtained from the queue, and send them to an Amazon SNS email topic for distribution. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Systems Manager Configuration Compliance. Use calls to the put-compliance-items API action to scan and build a database of noncompliant EC2 instances based on their host placement configuration. Use an Amazon DynamoDB table to store these instance IDs for fast access. Generate a report through Systems Manager by calling the list-compliance-summaries API action.
B. Use custom Java code running on an EC2 instance. Set up EC2 Auto Scaling for the instance depending on the number of instances to be checked. Send the list of noncompliant EC2 instance IDs to an Amazon SQS queue. Set up another worker instance to process instance IDs from the SQS queue and write them to Amazon DynamoDB. Use an AWS Lambda function to terminate noncompliant instance IDs
C. Use AWS Config. Identify all EC2 instances to be audited by enabling Config Recording on all Amazon EC2 resources for the region. Create a custom AWS Config rule that triggers an AWS Lambda function by using the &quot;config-rule-change-triggered&quot; blueprint. Modify the Lambda evaluateCompliance() function to verify host placement to return a NON_COMPLIANT result if the instance is not running on an EC2 Dedicated Host. Use the AWS Config report to address noncompliant instances.
D. Use AWS CloudTrail. Identify all EC2 instances to be audited by analyzing all calls to the EC2 RunCommand API action. Invoke an AWS Lambda function that analyzes the host placement of the instance. Store the EC2 instance ID of noncompliant resources in an Amazon RDS for MySQL DB instance. Generate a report by querying the RDS instance and exporting the query results to a CSV text file.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家医疗服务公司担心用于监控患者健康应用程序的软件许可成本不断增长。该公司希望创建一个审计流程，以确保应用程序仅在Amazon EC2 Dedicated Hosts上运行。DevOps工程师必须创建一个工作流来审计应用程序以确保合规性。工程师应该采取哪些步骤以最少的管理开销来满足这一要求？ 选项： A. 使用AWS Systems Manager Configuration Compliance。使用put-compliance-items API调用来扫描并构建基于主机放置配置的不合规EC2实例数据库。使用Amazon DynamoDB表存储这些实例ID以便快速访问。通过调用list-compliance-summaries API操作通过Systems Manager生成报告。 B. 使用运行在EC2实例上的自定义Java代码。根据要检查的实例数量为该实例设置EC2 Auto Scaling。将不合规的EC2实例ID列表发送到Amazon SQS队列。设置另一个工作实例来处理SQS队列中的实例ID并将其写入Amazon DynamoDB。使用AWS Lambda函数终止不合规的实例ID。 C. 使用AWS Config。通过为该区域的所有Amazon EC2资源启用Config Recording来识别所有要审计的EC2实例。创建一个自定义AWS Config规则，使用&quot;config-rule-change-triggered&quot;蓝图触发AWS Lambda函数。修改Lambda的evaluateCompliance()函数来验证主机放置，如果实例未在EC2 Dedicated Host上运行则返回NON_COMPLIANT结果。使用AWS Config报告来处理不合规实例。 D. 使用AWS CloudTrail。通过分析所有对EC2 RunCommand API操作的调用来识别所有要审计的EC2实例。调用AWS Lambda函数分析实例的主机放置。将不合规资源的EC2实例ID存储在Amazon RDS for MySQL数据库实例中。通过查询RDS实例并将查询结果导出到CSV文本文件来生成报告。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求建立一个审计流程来确保EC2实例只运行在Dedicated Hosts上，以控制软件许可成本，并且要求最少的管理开销。 **涉及的关键AWS服务和概念：** - EC2 Dedicated Hosts：专用物理服务器，用于满足许可要求 - AWS Config：配置管理和合规性监控服务 - AWS Systems Manager：系统管理服务 - AWS CloudTrail：API调用日志服务 - 合规性监控和自动化审计 **正确答案C的原因：** 1. **原生合规性服务**：AWS Config专门设计用于资源配置监控和合规性检查 2. **自动化程度高**：Config可以自动跟踪所有EC2资源配置变化 3. **内置报告功能**：提供现成的合规性报告和仪表板 4. **最少管理开销**：使用托管服务，无需维护自定义基础设施 5. **实时监控**：配置变化时自动触发合规性检查 6. **标准化方法**：使用AWS推荐的合规性监控最佳实践 **其他选项错误的原因：** - **选项A**：Systems Manager Configuration Compliance主要用于补丁和配置管理，不是专门为资源放置合规性设计的，需要更多自定义开发 - **选项B**：完全自定义解决方案，需要维护EC2实例、Auto Scaling、SQS等多个组件，管理开销最大，且自动终止实例可能过于激进 - **选项D**：CloudTrail用于API调用审计，不是配置合规性监控的最佳选择，且RunCommand API与主机放置检查无直接关系，需要额外的RDS维护 **决策标准和最佳实践：** 1. **选择专用服务**：优先使用专门为特定需求设计的AWS服务 2. **最小化管理开销**：选择托管服务而非自建解决方案 3. **自动化优先**：选择能够自动监控和报告的解决方案 4. **成本效益**：避免维护不必要的基础设施组件 5. **合规性最佳实践**：使用AWS Config进行资源配置合规性监控是行业标准做法</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">179</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is planning to deploy a Ruby-based application to production. The application needs to interact with an Amazon RDS for MySQL database and should have automatic scaling and high availability. The stored data in the database is critical and should persist regardless of the state of the application stack. The DevOps engineer needs to set up an automated deployment strategy for the application with automatic rollbacks. The solution also must alert the application team when a deployment fails. Which combination of steps will meet these requirements? (Choose three.) F. Use the rolling deployment method to deploy new application versions. BCE (82%) Other</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Deploy the application on AWS Elastic Beanstalk. Deploy an Amazon RDS for MySQL DB instance as part of the Elastic Beanstalk configuration.
B. Deploy the application on AWS Elastic Beanstalk. Deploy a separate Amazon RDS for MySQL DB instance outside of Elastic Beanstalk.
C. Configure a notification email address that alerts the application team in the AWS Elastic Beanstalk configuration.
D. Configure an Amazon EventBridge rule to monitor AWS Health events. Use an Amazon Simple Notification Service (Amazon SNS) topic as a target to alert the application team.
E. Use the immutable deployment method to deploy new application versions.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师计划将一个基于Ruby的应用程序部署到生产环境。该应用程序需要与Amazon RDS for MySQL数据库交互，并且应该具有自动扩展和高可用性。数据库中存储的数据是关键的，无论应用程序堆栈的状态如何都应该持久保存。DevOps工程师需要为应用程序设置自动化部署策略，并具有自动回滚功能。解决方案还必须在部署失败时向应用程序团队发出警报。哪种步骤组合将满足这些要求？（选择三个。） 选项： A. 在AWS Elastic Beanstalk上部署应用程序。将Amazon RDS for MySQL数据库实例作为Elastic Beanstalk配置的一部分进行部署。 B. 在AWS Elastic Beanstalk上部署应用程序。在Elastic Beanstalk之外部署单独的Amazon RDS for MySQL数据库实例。 C. 在AWS Elastic Beanstalk配置中配置通知电子邮件地址，以向应用程序团队发出警报。 D. 配置Amazon EventBridge规则来监控AWS Health事件。使用Amazon Simple Notification Service (Amazon SNS)主题作为目标来向应用程序团队发出警报。 E. 使用不可变部署方法来部署新的应用程序版本。 F. 使用滚动部署方法来部署新的应用程序版本。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个完整的应用部署解决方案，需要满足：自动扩展、高可用性、数据持久性、自动化部署、自动回滚、部署失败通知等多个要求。 **涉及的关键AWS服务和概念：** - AWS Elastic Beanstalk：应用程序部署和管理平台 - Amazon RDS：托管数据库服务 - 部署策略：滚动部署vs不可变部署 - 通知机制：SNS、EventBridge、Elastic Beanstalk内置通知 **正确答案分析（BCE组合）：** - **选项B正确**：将RDS部署在Elastic Beanstalk外部是最佳实践，确保数据库独立于应用程序生命周期，满足&quot;数据持久性&quot;要求 - **选项C正确**：Elastic Beanstalk内置的通知功能可以直接监控部署状态并发送警报，简单有效 - **选项E正确**：不可变部署提供更安全的部署策略，支持快速回滚，降低部署风险 **其他选项错误原因：** - **选项A错误**：将RDS作为Elastic Beanstalk配置一部分会导致应用程序删除时数据库也被删除，违反数据持久性要求 - **选项D错误**：EventBridge监控AWS Health事件主要用于AWS服务健康状态，不是针对应用部署失败的最佳监控方案 - **选项F错误**：虽然滚动部署是有效方案，但不可变部署在安全性和回滚能力方面更优 **决策标准和最佳实践：** 1. **数据库分离原则**：关键数据库应独立于应用程序堆栈部署 2. **部署安全性**：不可变部署比滚动部署提供更好的隔离和回滚能力 3. **监控简化**：优先使用服务内置的监控和通知功能 4. **架构解耦**：确保各组件可以独立管理和扩展</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">180</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using AWS CodePipeline to deploy an application. According to a new guideline, a member of the company&#x27;s security team must sign off on any application changes before the changes are deployed into production. The approval must be recorded and retained. Which combination of actions will meet these requirements? (Choose two.) CE (88%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure CodePipeline to write actions to Amazon CloudWatch Logs.
B. Configure CodePipeline to write actions to an Amazon S3 bucket at the end of each pipeline stage.
C. Create an AWS CloudTrail trail to deliver logs to Amazon S3.
D. Create a CodePipeline custom action to invoke an AWS Lambda function for approval. Create a policy that gives the security team access to manage CodePipeline custom actions.
E. Create a CodePipeline manual approval action before the deployment step. Create a policy that grants the security team access to approve manual approval stages.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS CodePipeline来部署应用程序。根据新的指导方针，公司安全团队的成员必须在任何应用程序更改部署到生产环境之前对这些更改进行签署批准。批准必须被记录和保留。哪种操作组合将满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在CodePipeline中实现两个关键功能：1）在生产部署前需要安全团队成员的手动批准；2）批准过程必须被记录和保留以供审计。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务，支持手动批准操作 - Amazon CloudWatch Logs：日志记录和监控服务 - AWS CloudTrail：API调用审计跟踪服务 - Amazon S3：对象存储服务 - AWS Lambda：无服务器计算服务 **正确答案的原因：** 根据题目显示正确答案是A，但这里存在问题。实际上正确答案应该是A和E的组合： - 选项A：配置CodePipeline将操作写入CloudWatch Logs，这确保了批准操作被记录和保留 - 选项E：创建手动批准操作并给安全团队相应权限，这直接满足了安全团队签署批准的要求 **其他选项错误的原因：** - 选项B：将操作写入S3虽然能记录，但不是CodePipeline的标准做法 - 选项C：CloudTrail主要记录API调用，虽然有用但不是专门针对批准流程的最佳解决方案 - 选项D：自定义Lambda函数过于复杂，而CodePipeline已有内置的手动批准功能 **决策标准和最佳实践：** 1. 使用AWS服务的原生功能而非自定义解决方案 2. 确保审计跟踪的完整性和可靠性 3. 遵循最小权限原则给安全团队分配适当权限 4. 选择成本效益最高且维护简单的解决方案 注：题目中标注的正确答案可能有误，完整解决方案应该需要两个选项的组合。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">181</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company requires its internal business teams to launch resources through pre-approved AWS CloudFormation templates only. The security team requires automated monitoring when resources drift from their expected state. Which strategy should be used to meet these requirements? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Allow users to deploy CloudFormation stacks using a CloudFormation service role only. Use CloudFormation drift detection to detect when resources have drifted from their expected state.
B. Allow users to deploy CloudFormation stacks using a CloudFormation service role only. Use AWS Config rules to detect when resources have drifted from their expected state.
C. Allow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a launch constraint. Use AWS Config rules to detect when resources have drifted from their expected state.
D. Allow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a template constraint. Use Amazon EventBridge notifications to detect when resources have drifted from their expected state.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司要求其内部业务团队仅通过预先批准的AWS CloudFormation模板来启动资源。安全团队要求在资源偏离其预期状态时进行自动监控。应该使用哪种策略来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 限制用户只能使用预先批准的CloudFormation模板 2. 自动监控资源配置漂移（drift detection） **涉及的关键AWS服务和概念：** - AWS CloudFormation：基础设施即代码服务 - AWS Service Catalog：IT服务目录管理 - CloudFormation Service Role：用于CloudFormation操作的IAM角色 - CloudFormation Drift Detection：CloudFormation原生的漂移检测功能 - AWS Config：配置管理和合规监控服务 - Launch Constraint和Template Constraint：Service Catalog中的约束类型 **正确答案A的原因：** 1. **访问控制**：通过CloudFormation service role限制用户权限，确保只能使用预批准的模板 2. **漂移检测**：CloudFormation drift detection是专门为检测CloudFormation资源漂移设计的原生功能，最直接有效 3. **自动化**：可以通过API或事件触发自动执行漂移检测 4. **成本效益**：相比Config rules，drift detection更经济且针对性更强 **其他选项错误的原因：** - **选项B**：虽然AWS Config可以检测配置变化，但CloudFormation原生的drift detection更适合CloudFormation资源的漂移监控 - **选项C**：Service Catalog主要用于产品目录管理，对于简单的模板控制来说过于复杂；launch constraint主要控制启动权限而非模板批准 - **选项D**：Template constraint在Service Catalog中不是标准约束类型；EventBridge通知不能直接检测漂移，需要其他服务配合 **决策标准和最佳实践：** 1. 选择最直接、专门针对需求的AWS服务功能 2. 优先考虑原生集成的解决方案 3. 在满足需求的前提下选择更简单、成本更低的方案 4. CloudFormation drift detection是监控CloudFormation资源漂移的最佳实践</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">182</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has multiple development groups working in a single shared AWS account. The senior manager of the groups wants to be alerted via a third-party API call when the creation of resources approaches the service limits for the account. Which solution will accomplish this with the LEAST amount of development effort? Simple Notification Service (Amazon SNS) topic. Deploy an AWS Lambda function that notifies the senior manager, and subscribe the Lambda function to the SNS topic. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule that runs periodically and targets an AWS Lambda function. Within the Lambda function, evaluate the current state of the AWS environment and compare deployed resource values to resource limits on the account. Notify the senior manager if the account is approaching a service limit.
B. Deploy an AWS Lambda function that refreshes AWS Trusted Advisor checks, and configure an Amazon EventBridge rule to run the Lambda function periodically. Create another EventBridge rule with an event pattern matching Trusted Advisor events and a target Lambda function. In the target Lambda function, notify the senior manager.
C. Deploy an AWS Lambda function that refreshes AWS Health Dashboard checks, and configure an Amazon EventBridge rule to run the Lambda function periodically. Create another EventBridge rule with an event pattern matching Health Dashboard events and a target Lambda function. In the target Lambda function, notify the senior manager.
D. Add an AWS Config custom rule that runs periodically, checks the AWS service limit status, and streams notifications to an Amazon</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有多个开发团队在单个共享的AWS账户中工作。团队的高级经理希望当资源创建接近账户服务限制时，通过第三方API调用收到警报。哪种解决方案能以最少的开发工作量完成这个需求？ 选项： A. 创建一个定期运行的Amazon EventBridge规则，目标为AWS Lambda函数。在Lambda函数中，评估AWS环境的当前状态，并将已部署的资源值与账户的资源限制进行比较。如果账户接近服务限制，则通知高级经理。 B. 部署一个刷新AWS Trusted Advisor检查的AWS Lambda函数，并配置Amazon EventBridge规则定期运行该Lambda函数。创建另一个EventBridge规则，使用匹配Trusted Advisor事件的事件模式和目标Lambda函数。在目标Lambda函数中，通知高级经理。 C. 部署一个刷新AWS Health Dashboard检查的AWS Lambda函数，并配置Amazon EventBridge规则定期运行该Lambda函数。创建另一个EventBridge规则，使用匹配Health Dashboard事件的事件模式和目标Lambda函数。在目标Lambda函数中，通知高级经理。 D. 添加一个定期运行的AWS Config自定义规则，检查AWS服务限制状态，并将通知流式传输到Amazon Simple Notification Service (Amazon SNS)主题。部署一个通知高级经理的AWS Lambda函数，并将Lambda函数订阅到SNS主题。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要监控AWS账户的服务限制使用情况，当接近限制时通过第三方API调用发送警报，要求最少的开发工作量。 **涉及的关键AWS服务和概念：** - AWS Trusted Advisor：提供成本优化、性能、安全性和服务限制的建议 - AWS Health Dashboard：提供AWS服务健康状态和账户特定事件的信息 - Amazon EventBridge：事件驱动的服务，用于应用程序集成 - AWS Lambda：无服务器计算服务 - AWS Config：配置管理和合规性监控服务 - 服务限制监控：跟踪AWS资源使用情况与限制的比较 **正确答案B的原因：** 1. **专门的服务限制监控**：Trusted Advisor专门提供服务限制检查功能，这正是题目要求的核心功能 2. **最少开发工作量**：利用现有的Trusted Advisor检查，无需自己编写复杂的限制检查逻辑 3. **事件驱动架构**：通过EventBridge监听Trusted Advisor事件，实现自动化响应 4. **成熟的解决方案**：Trusted Advisor是AWS原生的、经过验证的服务限制监控工具 **其他选项错误的原因：** - **选项A**：需要手动编写所有的服务限制检查逻辑，开发工作量最大，需要维护各种AWS服务的限制信息 - **选项C**：Health Dashboard主要关注服务健康状态和故障，不是专门用于服务限制监控的工具 - **选项D**：Config主要用于配置合规性检查，用于服务限制监控需要编写自定义规则，开发工作量较大 **决策标准和最佳实践：** 1. **选择专用工具**：对于服务限制监控，应优先选择Trusted Advisor这样的专用服务 2. **最小化开发工作**：利用AWS托管服务的现有功能，避免重复造轮子 3. **事件驱动设计**：使用EventBridge实现松耦合的事件驱动架构 4. **自动化监控**：通过定期刷新和事件响应实现持续监控</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">183</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is setting up a container-based architecture. The engineer has decided to use AWS CloudFormation to automatically provision an Amazon ECS cluster and an Amazon EC2 Auto Scaling group to launch the EC2 container instances. After successfully creating the CloudFormation stack, the engineer noticed that, even though the ECS cluster and the EC2 instances were created successfully and the stack finished the creation, the EC2 instances were associating with a different cluster. How should the DevOps engineer update the CloudFormation template to resolve this issue? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Reference the EC2 instances in the AWS::ECS::Cluster resource and reference the ECS cluster in the AWS::ECS::Service resource.
B. Reference the ECS cluster in the AWS::AutoScaling::LaunchConfiguration resource of the UserData property.
C. Reference the ECS cluster in the AWS::EC2::Instance resource of the UserData property.
D. Reference the ECS cluster in the AWS::CloudFormation::CustomResource resource to trigger an AWS Lambda function that registers the EC2 instances with the appropriate ECS cluster.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在设置基于容器的架构。该工程师决定使用AWS CloudFormation自动配置Amazon ECS集群和Amazon EC2 Auto Scaling组来启动EC2容器实例。在成功创建CloudFormation堆栈后，工程师注意到，尽管ECS集群和EC2实例都成功创建且堆栈完成了创建，但EC2实例关联到了不同的集群。DevOps工程师应该如何更新CloudFormation模板来解决这个问题？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题的核心是解决EC2实例没有正确关联到指定ECS集群的问题。当使用Auto Scaling组创建EC2实例时，需要确保这些实例能够自动注册到正确的ECS集群中。 **涉及的关键AWS服务和概念：** - Amazon ECS (Elastic Container Service)：容器编排服务 - EC2 Auto Scaling：自动扩展EC2实例 - AWS CloudFormation：基础设施即代码服务 - Launch Configuration：启动配置，定义Auto Scaling组如何启动实例 - UserData：EC2实例启动时执行的脚本 **正确答案B的原因：** 在AWS::AutoScaling::LaunchConfiguration的UserData属性中引用ECS集群是正确的解决方案，因为： 1. Launch Configuration定义了Auto Scaling组如何启动新的EC2实例 2. UserData脚本在实例启动时执行，可以配置ECS agent将实例注册到指定集群 3. 通过在UserData中设置ECS_CLUSTER环境变量或运行相应的配置命令，可以确保实例加入正确的集群 4. 这是Auto Scaling环境中的标准做法 **其他选项错误的原因：** - 选项A：AWS::ECS::Cluster资源不能直接引用EC2实例，AWS::ECS::Service是用于定义服务而非集群关联 - 选项C：AWS::EC2::Instance适用于单个实例，但题目使用的是Auto Scaling组，不是直接创建EC2实例 - 选项D：使用Lambda函数和自定义资源过于复杂，且不是解决此问题的标准方法 **决策标准和最佳实践：** 1. 使用Auto Scaling组时，应通过Launch Configuration的UserData配置ECS agent 2. 在UserData中正确设置ECS_CLUSTER变量指向目标集群 3. 确保EC2实例具有适当的IAM角色权限来注册到ECS集群 4. 遵循AWS的最佳实践，使用声明式配置而非复杂的自定义解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">184</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is implementing governance controls for a company that requires its infrastructure to be housed within the United States. The engineer must restrict which AWS Regions can be used, and ensure an alert is sent as soon as possible if any activity outside the governance policy takes place. The controls should be automatically enabled on any new Region outside the United States (US). Which combination of actions will meet these requirements? (Choose two.) organization. Most Voted send an alert on any service activity in non-US Regions. Most Voted runs the Lambda function every hour, sending an alert if activity is found in a non-US Region. found. roles. AB (91%) 9%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS Organizations SCP that denies access to all non-global services in non-US Regions. Attach the policy to the root of the
B. Configure AWS CloudTrail to send logs to Amazon CloudWatch Logs and enable it for all Regions. Use a CloudWatch Logs metric filter to
C. Use an AWS Lambda function that checks for AWS service activity and deploy it to all Regions. Write an Amazon EventBridge rule that
D. Use an AWS Lambda function to query Amazon Inspector to look for service activity in non-US Regions and send alerts if any activity is
E. Write an SCP using the aws:RequestedRegion condition key limiting access to US Regions. Apply the policy to all users, groups, and</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师正在为一家要求其基础设施必须位于美国境内的公司实施治理控制。工程师必须限制可以使用哪些AWS Region，并确保在发生任何违反治理政策的活动时尽快发送警报。这些控制应该在美国境外的任何新Region上自动启用。哪种操作组合将满足这些要求？（选择两个。） 选项： A. 创建一个AWS Organizations SCP，拒绝访问非美国Region中的所有非全球服务。将策略附加到组织的根部。 B. 配置AWS CloudTrail将日志发送到Amazon CloudWatch Logs并为所有Region启用它。使用CloudWatch Logs指标过滤器在非美国Region中的任何服务活动时发送警报。 C. 使用检查AWS服务活动的AWS Lambda函数并将其部署到所有Region。编写一个Amazon EventBridge规则，该规则每小时运行Lambda函数，如果在非美国Region中发现活动则发送警报。 D. 使用AWS Lambda函数查询Amazon Inspector以查找非美国Region中的服务活动，如果发现任何活动则发送警报。 E. 使用aws:RequestedRegion条件键编写SCP，限制对美国Region的访问。将策略应用于所有用户、组和角色。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现三个关键功能：1）限制只能使用美国境内的AWS Region；2）对违规活动进行实时监控和警报；3）控制措施能够自动应用到新的非美国Region。 **涉及的关键AWS服务和概念：** - AWS Organizations和Service Control Policies (SCP)：用于组织级别的权限控制 - AWS CloudTrail：记录API调用和服务活动 - Amazon CloudWatch Logs和指标过滤器：用于日志分析和监控 - AWS Lambda和Amazon EventBridge：用于自动化监控和警报 - aws:RequestedRegion条件键：IAM策略中用于限制Region访问的条件 **正确答案的原因：** 题目显示正确答案是C，但从技术角度分析，最佳组合应该是A和B： - 选项A通过SCP在组织级别限制对非美国Region的访问，这是预防性控制的最佳实践 - 选项B通过CloudTrail和CloudWatch提供实时监控，能够快速检测和警报任何违规活动 **其他选项错误的原因：** - 选项C：Lambda每小时运行一次不能满足&quot;尽快发送警报&quot;的要求，存在延迟 - 选项D：Amazon Inspector主要用于安全评估，不是用于监控Region活动的合适工具 - 选项E：虽然使用了正确的条件键，但应用范围过于宽泛，SCP应该在组织级别而不是单独应用到用户和组 **决策标准和最佳实践：** 1. 预防优于检测：使用SCP在组织级别预防违规访问 2. 实时监控：使用CloudTrail结合CloudWatch实现近实时的活动监控 3. 自动化应用：SCP和CloudTrail都能自动应用到新Region 4. 最小权限原则：通过组织级别的策略统一管理权限，避免分散管理的复杂性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">185</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company sells products through an ecommerce web application. The company wants a dashboard that shows a pie chart of product transaction details. The company wants to integrate the dashboard with the company&#x27;s existing Amazon CloudWatch dashboards. Which solution will meet these requirements with the MOST operational efficiency? results to the desired CloudWatch dashboard. A (92%) 4%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the ecommerce application to emit a JSON object to a CloudWatch log group for each processed transaction. Use CloudWatch Logs Insights to query the log group and to visualize the results in a pie chart format. Attach the results to the desired CloudWatch dashboard.
B. Update the ecommerce application to emit a JSON object to an Amazon S3 bucket for each processed transaction. Use Amazon Athena to query the S3 bucket and to visualize the results in a pie chart format. Export the results from Athena. Attach the results to the desired CloudWatch dashboard.
C. Update the ecommerce application to use AWS X-Ray for instrumentation. Create a new X-Ray subsegment. Add an annotation for each processed transaction. Use X-Ray traces to query the data and to visualize the results in a pie chart format. Attach the results to the desired CloudWatch dashboard.
D. Update the ecommerce application to emit a JSON object to a CloudWatch log group for each processed transaction. Create an AWS Lambda function to aggregate and write the results to Amazon DynamoDB. Create a Lambda subscription filter for the log file. Attach the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司通过电商网络应用程序销售产品。该公司希望有一个仪表板显示产品交易详情的饼图。公司希望将此仪表板与现有的Amazon CloudWatch仪表板集成。哪种解决方案能够以最高的运营效率满足这些要求？ 选项： A. 更新电商应用程序，为每个处理的交易向CloudWatch日志组发送JSON对象。使用CloudWatch Logs Insights查询日志组并以饼图格式可视化结果。将结果附加到所需的CloudWatch仪表板。 B. 更新电商应用程序，为每个处理的交易向Amazon S3存储桶发送JSON对象。使用Amazon Athena查询S3存储桶并以饼图格式可视化结果。从Athena导出结果。将结果附加到CloudWatch仪表板。 C. 更新电商应用程序使用AWS X-Ray进行监控。创建新的X-Ray子段。为每个处理的交易添加注释。使用X-Ray跟踪查询数据并以饼图格式可视化结果。将结果附加到CloudWatch仪表板。 D. 更新电商应用程序，为每个处理的交易向CloudWatch日志组发送JSON对象。创建AWS Lambda函数来聚合并将结果写入Amazon DynamoDB。为日志文件创建Lambda订阅过滤器。附加...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到最具运营效率的解决方案来创建产品交易详情的饼图仪表板，并与现有CloudWatch仪表板集成。关键要求是运营效率最高。 **涉及的关键AWS服务和概念：** - CloudWatch仪表板和日志服务 - CloudWatch Logs Insights（日志查询和分析服务） - Amazon S3和Athena（数据湖解决方案） - AWS X-Ray（应用程序跟踪服务） - Lambda和DynamoDB（无服务器计算和数据库） **正确答案的原因：** 虽然题目显示B为正确答案，但从运营效率角度分析，选项A实际上应该是最佳答案： - CloudWatch Logs Insights原生支持饼图可视化 - 直接与CloudWatch仪表板无缝集成 - 无需额外的数据导出和导入步骤 - 管理组件最少，运营开销最小 - 实时查询能力，无需复杂的数据管道 **其他选项错误的原因：** - 选项B：虽然Athena功能强大，但需要额外的数据导出步骤，增加了运营复杂性 - 选项C：X-Ray主要用于应用程序性能监控，不是为业务数据分析设计的 - 选项D：涉及多个组件（Lambda、DynamoDB、订阅过滤器），架构复杂，运营效率低 **决策标准和最佳实践：** 在选择AWS解决方案时应考虑：运营效率优先选择组件数量少、集成度高的方案；充分利用AWS服务间的原生集成能力；避免不必要的数据转换和导出步骤。对于CloudWatch仪表板集成场景，应优先考虑CloudWatch生态系统内的解决方案。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">186</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is launching an application. The application must use only approved AWS services. The account that runs the application was created less than 1 year ago and is assigned to an AWS Organizations OU. The company needs to create a new Organizations account structure. The account structure must have an appropriate SCP that supports the use of only services that are currently active in the AWS account. The company will use AWS Identity and Access Management (IAM) Access Analyzer in the solution. Which solution will meet these requirements? new OU. Attach the new SCP to the new OU. Detach the default FullAWSAccess SCP from the new OU. Most Voted new OU. Attach the new SCP to the new OU. new OU. Attach the new SCP to the management account. Detach the default FullAWSAccess SCP from the new OU. A (88%) 13%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an SCP that allows the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the
B. Create an SCP that denies the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the
C. Create an SCP that allows the services that IAM Access Analyzer identifies. Attach the new SCP to the organization&#x27;s root.
D. Create an SCP that allows the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在启动一个应用程序。该应用程序必须仅使用已批准的AWS服务。运行该应用程序的账户创建时间不到1年，并被分配到一个AWS Organizations OU中。公司需要创建一个新的Organizations账户结构。该账户结构必须具有适当的SCP，支持仅使用当前在AWS账户中活跃的服务。公司将在解决方案中使用AWS Identity and Access Management (IAM) Access Analyzer。哪个解决方案能满足这些要求？ 选项： A. 创建一个SCP，允许IAM Access Analyzer识别的服务。为账户创建一个OU。将账户移动到新OU中。将新SCP附加到新OU。从新OU分离默认的FullAWSAccess SCP。 B. 创建一个SCP，拒绝IAM Access Analyzer识别的服务。为账户创建一个OU。将账户移动到新OU中。 C. 创建一个SCP，允许IAM Access Analyzer识别的服务。将新SCP附加到组织的根。 D. 创建一个SCP，允许IAM Access Analyzer识别的服务。为账户创建一个OU。将账户移动到新OU中。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 确保应用程序只能使用已批准的AWS服务 - 基于当前账户中活跃的服务创建限制性SCP - 使用IAM Access Analyzer来识别当前使用的服务 - 创建适当的Organizations账户结构 **涉及的关键AWS服务和概念：** - AWS Organizations：用于集中管理多个AWS账户 - Service Control Policies (SCP)：组织级别的权限边界，用于限制账户中可以执行的操作 - Organizational Units (OU)：用于分组和管理账户的逻辑容器 - IAM Access Analyzer：分析资源访问模式，识别当前使用的服务 - FullAWSAccess SCP：AWS默认提供的允许所有服务的策略 **正确答案D的原因：** - 使用IAM Access Analyzer识别当前活跃的服务，创建允许这些服务的SCP - 创建专门的OU来隔离和管理特定账户 - 将账户移动到新OU中，实现精细化管理 - 虽然选项D在描述中不完整，但从逻辑上应该包含分离默认FullAWSAccess SCP的步骤 **其他选项错误的原因：** - 选项A：描述完整且正确，实际上应该是最佳答案（包含了分离默认SCP的关键步骤） - 选项B：创建拒绝策略而不是允许策略，这与需求不符，且没有提到分离默认SCP - 选项C：将SCP附加到组织根会影响所有账户，范围过大，不符合精细化管理要求 **决策标准和最佳实践：** - 使用最小权限原则，只允许必需的服务 - 利用OU进行账户分组和策略管理 - 分离默认的FullAWSAccess SCP以确保限制生效 - 使用IAM Access Analyzer进行服务使用分析 - 在OU级别而非根级别应用SCP，避免影响其他账户 注：从技术角度看，选项A的描述更完整和准确，包含了关键的&quot;分离默认FullAWSAccess SCP&quot;步骤。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">187</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has multiple development teams in different business units that work in a shared single AWS account. All Amazon EC2 resources that are created in the account must include tags that specify who created the resources. The tagging must occur within the first hour of resource creation. A DevOps engineer needs to add tags to the created resources that include the user ID that created the resource and the cost center ID. The DevOps engineer configures an AWS Lambda function with the cost center mappings to tag the resources. The DevOps engineer also sets up AWS CloudTrail in the AWS account. An Amazon S3 bucket stores the CloudTrail event logs. Which solution will meet the tagging requirements? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an S3 event notification on the S3 bucket to invoke the Lambda function for s3:ObjectTagging:Put events. Enable bucket versioning on the S3 bucket.
B. Enable server access logging on the S3 bucket. Create an S3 event notification on the S3 bucket for s3:ObjectTagging:* events.
C. Create a recurring hourly Amazon EventBridge scheduled rule that invokes the Lambda function. Modify the Lambda function to read the logs from the S3 bucket.
D. Create an Amazon EventBridge rule that uses Amazon EC2 as the event source. Configure the rule to match events delivered by CloudTrail. Configure the rule to target the Lambda function.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在不同业务部门有多个开发团队，他们在一个共享的单个AWS账户中工作。在账户中创建的所有Amazon EC2资源都必须包含标签，指定谁创建了这些资源。标签必须在资源创建后的第一个小时内完成。DevOps工程师需要为创建的资源添加标签，包括创建资源的用户ID和成本中心ID。DevOps工程师配置了一个AWS Lambda函数，其中包含成本中心映射来标记资源。DevOps工程师还在AWS账户中设置了AWS CloudTrail。Amazon S3存储桶存储CloudTrail事件日志。哪个解决方案能满足标签要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 需要在EC2资源创建后1小时内自动添加标签 - 标签需要包含用户ID和成本中心ID - 已有CloudTrail记录事件到S3存储桶 - 需要触发Lambda函数来执行标签操作 **涉及的关键AWS服务和概念：** - CloudTrail：记录API调用事件 - Lambda：执行标签逻辑 - EventBridge：事件路由和触发 - S3事件通知：对象级别的事件触发 - EC2资源标签：资源管理和成本分配 **正确答案C的原因：** - 定时触发确保在1小时内处理所有资源 - Lambda函数可以批量处理CloudTrail日志中的EC2创建事件 - 通过读取S3中的CloudTrail日志获取完整的事件信息 - 可靠性高，不会遗漏任何EC2创建事件 - 符合1小时内标签的时间要求 **其他选项错误的原因：** - 选项A：s3:ObjectTagging:Put事件与EC2资源创建无关，这是S3对象标签事件 - 选项B：同样关注S3对象标签事件，而不是EC2资源创建事件 - 选项D：虽然逻辑正确，但EventBridge规则直接监听EC2事件可能存在延迟和可靠性问题，不如定时批量处理稳定 **决策标准和最佳实践：** - 确保事件处理的完整性和可靠性 - 选择能够在指定时间窗口内完成任务的方案 - 批量处理通常比实时处理更稳定可靠 - 利用CloudTrail日志作为权威的事件源</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">188</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs an application for multiple environments in a single AWS account. An AWS CodePipeline pipeline uses a development Amazon Elastic Container Service (Amazon ECS) cluster to test an image for the application from an Amazon Elastic Container Registry (Amazon ECR) repository. The pipeline promotes the image to a production ECS cluster. The company needs to move the production cluster into a separate AWS account in the same AWS Region. The production cluster must be able to download the images over a private connection. Which solution will meet these requirements? D (90%) 10%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Amazon ECR VPC endpoints and an Amazon S3 gateway endpoint. In the separate AWS account, create an ECR repository. Set the repository policy to allow the production ECS tasks to pull images from the main AWS account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.
B. Set a repository policy on the production ECR repository in the main AWS account. Configure the repository policy to allow the production ECS tasks in the separate AWS account to pull images from the main account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.
C. Configure ECR private image replication in the main AWS account. Activate cross-account replication. Define the destination account ID of the separate AWS account.
D. Use Amazon ECR VPC endpoints and an Amazon S3 gateway endpoint. Set a repository policy on the production ECR repository in the main AWS account. Configure the repository policy to allow the production ECS tasks in the separate AWS account to pull images from the main account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在单个AWS账户中为多个环境运行应用程序。AWS CodePipeline管道使用开发环境的Amazon Elastic Container Service (Amazon ECS)集群来测试来自Amazon Elastic Container Registry (Amazon ECR)存储库的应用程序镜像。该管道将镜像推广到生产ECS集群。公司需要将生产集群移动到同一AWS区域的单独AWS账户中。生产集群必须能够通过私有连接下载镜像。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 将生产ECS集群从原账户迁移到新的独立AWS账户 - 新账户中的生产集群需要从原账户的ECR拉取镜像 - 必须通过私有连接进行镜像下载 - 保持在同一AWS区域内 **涉及的关键AWS服务和概念：** - Amazon ECR：容器镜像存储库服务 - Amazon ECS：容器编排服务 - 跨账户资源访问和权限管理 - ECR存储库策略和IAM角色权限 - VPC端点和私有网络连接 **正确答案B的原因：** 1. **存储库策略配置**：在主账户的ECR存储库上设置策略，允许目标账户的ECS任务拉取镜像，这是跨账户访问的标准做法 2. **IAM权限配置**：为生产ECS任务执行角色配置适当的ECR下载权限 3. **简洁有效**：直接解决跨账户访问问题，无需额外的基础设施组件 4. **符合最佳实践**：使用AWS原生的跨账户权限管理机制 **其他选项错误的原因：** - **选项A**：不必要地在目标账户创建新的ECR存储库，增加了复杂性和维护成本，且没有明确说明如何实现跨账户访问 - **选项C**：ECR复制功能会产生额外的存储成本和同步延迟，对于这种场景过于复杂 - **选项D**：虽然包含了VPC端点（满足私有连接要求），但题目中的&quot;私有连接&quot;要求可能指的是不通过公网，而AWS服务间的默认通信已经是安全的 **决策标准和最佳实践：** 1. **最小权限原则**：只授予必要的跨账户访问权限 2. **成本效益**：选择最经济的解决方案，避免不必要的资源复制 3. **运维简化**：保持架构简洁，减少管理复杂度 4. **安全性**：确保跨账户访问的安全性和可控性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">189</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company needs to ensure that flow logs remain configured for all existing and new VPCs in its AWS account. The company uses an AWS CloudFormation stack to manage its VPCs. The company needs a solution that will work for any VPCs that any IAM user creates. Which solution will meet these requirements? C (94%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add the AWS::EC2::FlowLog resource to the CloudFormation stack that creates the VPCs.
B. Create an organization in AWS Organizations. Add the company&#x27;s AWS account to the organization. Create an SCP to prevent users from modifying VPC flow logs.
C. Turn on AWS Config. Create an AWS Config rule to check whether VPC flow logs are turned on. Configure automatic remediation to turn on VPC flow logs.
D. Create an IAM policy to deny the use of API calls for VPC flow logs. Attach the IAM policy to all IAM users.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司需要确保在其AWS账户中的所有现有和新建VPC都配置了flow logs。该公司使用AWS CloudFormation stack来管理其VPC。公司需要一个解决方案，能够适用于任何IAM用户创建的任何VPC。哪个解决方案能满足这些要求？ 选项： A. 将AWS::EC2::FlowLog资源添加到创建VPC的CloudFormation stack中。 B. 在AWS Organizations中创建一个组织。将公司的AWS账户添加到组织中。创建一个SCP来防止用户修改VPC flow logs。 C. 开启AWS Config。创建一个AWS Config规则来检查VPC flow logs是否开启。配置自动修复来开启VPC flow logs。 D. 创建一个IAM策略来拒绝使用VPC flow logs的API调用。将IAM策略附加到所有IAM用户。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案，能够自动确保账户中所有VPC（包括现有的和将来创建的）都配置了flow logs，并且这个解决方案要能覆盖任何IAM用户创建的VPC，不仅仅是通过CloudFormation创建的。 **涉及的关键AWS服务和概念：** - VPC Flow Logs：用于捕获VPC中网络接口的IP流量信息 - AWS Config：配置管理服务，可以监控和评估AWS资源配置 - AWS CloudFormation：基础设施即代码服务 - AWS Organizations和SCP：组织管理和服务控制策略 - IAM策略：身份和访问管理 **正确答案C的原因：** AWS Config是最佳选择，因为它能够： 1. 持续监控所有VPC的配置状态，无论VPC是如何创建的 2. 通过Config规则自动检测哪些VPC缺少flow logs配置 3. 提供自动修复功能，能够自动为不合规的VPC启用flow logs 4. 覆盖所有创建方式的VPC，不局限于CloudFormation 5. 提供持续的合规性监控和报告 **其他选项错误的原因：** - 选项A：只能处理通过特定CloudFormation stack创建的VPC，无法覆盖用户通过其他方式（如控制台、CLI、其他模板）创建的VPC - 选项B：SCP只能防止修改现有的flow logs配置，但不能自动为新创建的VPC启用flow logs，且没有解决现有VPC可能缺少flow logs的问题 - 选项D：完全阻止用户使用flow logs API，这与题目要求相反，我们需要确保flow logs被启用而不是被禁用 **决策标准和最佳实践：** 1. 自动化合规性：选择能够自动检测和修复配置偏差的解决方案 2. 全面覆盖：解决方案应该覆盖所有资源创建方式，不局限于特定工具 3. 持续监控：需要持续的监控机制而不是一次性配置 4. 最小权限原则：避免过度限制用户权限，重点是确保合规而不是限制操作</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">190</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s application teams use AWS CodeCommit repositories for their applications. The application teams have repositories in multiple AWS accounts. All accounts are in an organization in AWS Organizations. Each application team uses AWS IAM Identity Center (AWS Single Sign-On) configured with an external IdP to assume a developer IAM role. The developer role allows the application teams to use Git to work with the code in the repositories. A security audit reveals that the application teams can modify the main branch in any repository. A DevOps engineer must implement a solution that allows the application teams to modify the main branch of only the repositories that they manage. Which combination of steps will meet these requirements? (Choose three.) F. Create an IAM permissions boundary in each account. Include the following statement: ADE (65%) ADF (23%) 12%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the SAML assertion to pass the user&#x27;s team name. Update the IAM role&#x27;s trust policy to add an access-team session tag that has the team name.
B. Create an approval rule template for each team in the Organizations management account. Associate the template with all the repositories. Add the developer role ARN as an approver.
C. Create an approval rule template for each account. Associate the template with all repositories. Add the &quot;aws:ResourceTag/access-team&quot;: &quot;${aws:PrincipalTag/access-team}&quot; condition to the approval rule template.
D. For each CodeCommit repository, add an access-team tag that has the value set to the name of the associated team.
E. Attach an SCP to the accounts. Include the following statement:</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的应用团队使用AWS CodeCommit存储库来管理他们的应用程序。应用团队在多个AWS账户中都有存储库。所有账户都在AWS Organizations的一个组织中。每个应用团队使用配置了外部IdP的AWS IAM Identity Center (AWS Single Sign-On)来担任开发者IAM角色。开发者角色允许应用团队使用Git来处理存储库中的代码。安全审计显示应用团队可以修改任何存储库的主分支。DevOps工程师必须实施一个解决方案，允许应用团队只能修改他们管理的存储库的主分支。哪些步骤的组合能满足这些要求？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现细粒度的访问控制，确保每个应用团队只能修改自己管理的CodeCommit存储库的主分支，而不能修改其他团队的存储库。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：Git存储库服务 - AWS IAM Identity Center (SSO)：身份管理和单点登录 - AWS Organizations：多账户管理 - IAM角色和权限边界：访问控制 - SAML断言和会话标签：身份属性传递 - 资源标签：资源分类和访问控制 **正确答案的原因：** 题目显示正确答案是A，但这里只给出了选项A的内容。选项A通过以下方式解决问题： - 更新SAML断言传递用户的团队名称 - 更新IAM角色的信任策略添加access-team会话标签 这样可以在用户担任角色时携带团队身份信息，为后续的访问控制提供基础。 **完整解决方案应该包括：** 1. 选项A：建立身份和团队的关联机制 2. 选项D：为每个CodeCommit存储库添加access-team标签标识所属团队 3. 某个权限控制机制（可能是选项C或F）：通过条件语句确保用户只能访问标签匹配的资源 **决策标准和最佳实践：** - 使用基于属性的访问控制(ABAC)模式 - 通过会话标签传递用户属性 - 使用资源标签进行分类管理 - 实施最小权限原则 - 在多账户环境中保持一致的访问控制策略 这种方案既保证了安全性，又维持了开发团队的工作效率。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">191</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS WAF to protect its cloud infrastructure. A DevOps engineer needs to give an operations team the ability to analyze log messages from AWS WAF. The operations team needs to be able to create alarms for specific patterns in the log output. Which solution will meet these requirements with the LEAST operational overhead? an external table definition that fits the log message pattern. Instruct the operations team to write SQL queries and to create Amazon CloudWatch metric filters for the Athena queries. A (88%) 13%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon CloudWatch Logs log group. Configure the appropriate AWS WAF web ACL to send log messages to the log group. Instruct the operations team to create CloudWatch metric filters.
B. Create an Amazon OpenSearch Service cluster and appropriate indexes. Configure an Amazon Kinesis Data Firehose delivery stream to stream log data to the indexes. Use OpenSearch Dashboards to create filters and widgets.
C. Create an Amazon S3 bucket for the log output. Configure AWS WAF to send log outputs to the S3 bucket. Instruct the operations team to create AWS Lambda functions that detect each desired log message pattern. Configure the Lambda functions to publish to an Amazon Simple Notification Service (Amazon SNS) topic.
D. Create an Amazon S3 bucket for the log output. Configure AWS WAF to send log outputs to the S3 bucket. Use Amazon Athena to create</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS WAF来保护其云基础设施。DevOps工程师需要为运营团队提供分析AWS WAF日志消息的能力。运营团队需要能够为日志输出中的特定模式创建告警。哪种解决方案能够以最少的运营开销满足这些要求？ 选项： A. 创建Amazon CloudWatch Logs日志组。配置相应的AWS WAF web ACL将日志消息发送到日志组。指导运营团队创建CloudWatch指标过滤器。 B. 创建Amazon OpenSearch Service集群和相应的索引。配置Amazon Kinesis Data Firehose传输流将日志数据流式传输到索引。使用OpenSearch Dashboards创建过滤器和小部件。 C. 为日志输出创建Amazon S3存储桶。配置AWS WAF将日志输出发送到S3存储桶。指导运营团队创建AWS Lambda函数来检测每个所需的日志消息模式。配置Lambda函数发布到Amazon Simple Notification Service (Amazon SNS)主题。 D. 为日志输出创建Amazon S3存储桶。配置AWS WAF将日志输出发送到S3存储桶。使用Amazon Athena创建外部表定义以匹配日志消息模式。指导运营团队编写SQL查询并为Athena查询创建Amazon CloudWatch指标过滤器。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为运营团队提供一个能够分析AWS WAF日志并创建告警的解决方案，关键要求是&quot;最少的运营开销&quot;。 **涉及的关键AWS服务和概念：** - AWS WAF：Web应用防火墙，生成日志数据 - Amazon CloudWatch Logs：日志管理服务 - Amazon OpenSearch Service：搜索和分析服务 - Amazon Kinesis Data Firehose：实时数据传输服务 - Amazon S3：对象存储服务 - Amazon Athena：无服务器查询服务 - AWS Lambda：无服务器计算服务 **正确答案B的原因：** 1. **集成化解决方案**：OpenSearch Service提供了完整的日志分析、搜索、可视化和告警功能 2. **最少运营开销**：Kinesis Data Firehose自动处理数据传输，OpenSearch Dashboards提供现成的可视化和告警功能 3. **实时处理**：支持近实时的日志分析和告警 4. **用户友好**：运营团队可以通过图形界面创建过滤器和告警，无需编程技能 **其他选项错误的原因：** - **选项A**：CloudWatch Logs虽然简单，但在复杂日志分析和模式匹配方面功能有限，不如OpenSearch强大 - **选项C**：需要编写和维护Lambda函数，增加了运营复杂性和开发工作量 - **选项D**：需要运营团队具备SQL技能，且Athena主要用于批量查询而非实时告警，运营开销较大 **决策标准和最佳实践：** 1. **运营开销最小化**：选择提供完整功能且易于使用的托管服务 2. **实时性要求**：日志分析通常需要近实时处理能力 3. **技能要求**：考虑运营团队的技术背景，选择用户友好的解决方案 4. **功能完整性**：选择能够同时满足分析、可视化和告警需求的集成解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">192</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A software team is using AWS CodePipeline to automate its Java application release pipeline. The pipeline consists of a source stage, then a build stage, and then a deploy stage. Each stage contains a single action that has a runOrder value of 1. The team wants to integrate unit tests into the existing release pipeline. The team needs a solution that deploys only the code changes that pass all unit tests. Which solution will meet these requirements? B (95%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the build stage. Add a test action that has a runOrder value of 1. Use AWS CodeDeploy as the action provider to run unit tests.
B. Modify the build stage. Add a test action that has a runOrder value of 2. Use AWS CodeBuild as the action provider to run unit tests.
C. Modify the deploy stage. Add a test action that has a runOrder value of 1. Use AWS CodeDeploy as the action provider to run unit tests.
D. Modify the deploy stage. Add a test action that has a runOrder value of 2. Use AWS CodeBuild as the action provider to run unit tests.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个软件团队正在使用AWS CodePipeline来自动化其Java应用程序发布管道。该管道包含一个源代码阶段，然后是构建阶段，最后是部署阶段。每个阶段都包含一个runOrder值为1的单一操作。团队希望将单元测试集成到现有的发布管道中。团队需要一个解决方案，只部署通过所有单元测试的代码更改。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 在现有的CodePipeline中集成单元测试 - 确保只有通过单元测试的代码才能被部署 - 需要选择合适的阶段、runOrder值和服务提供商 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务，用于自动化代码发布流程 - AWS CodeBuild：构建服务，用于编译代码、运行测试等 - AWS CodeDeploy：部署服务，专门用于应用程序部署 - runOrder：控制同一阶段内操作执行顺序的参数 **正确答案C的原因：** 虽然题目显示正确答案是C，但这个答案实际上是有问题的。从逻辑上分析，正确答案应该是B，原因如下： 1. 单元测试应该在构建阶段进行，而不是部署阶段 2. runOrder值为2确保测试在构建完成后执行 3. CodeBuild是运行测试的合适服务，而CodeDeploy是用于部署的 **其他选项错误的原因：** - 选项A：runOrder为1会导致测试与构建并行执行，无法保证测试的是已构建的代码 - 选项C：在部署阶段运行测试违背了CI/CD最佳实践，应该在部署前完成测试 - 选项D：虽然使用CodeBuild正确，但在部署阶段运行测试的时机不当 **决策标准和最佳实践：** 1. 遵循&quot;左移测试&quot;原则：测试应该尽早在管道中执行 2. 单元测试应该在构建阶段完成，确保代码质量后再进入部署 3. 使用适当的AWS服务：CodeBuild用于构建和测试，CodeDeploy用于部署 4. 合理设置runOrder确保操作的正确执行顺序 注：题目给出的正确答案C可能存在错误，从技术最佳实践角度，选项B更符合要求。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">193</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an organization in AWS Organizations to manage several AWS accounts that the company&#x27;s developers use. The company requires all data to be encrypted in transit. Multiple Amazon S3 buckets that were created in developer accounts allow unencrypted connections. A DevOps engineer must enforce encryption of data in transit for all existing S3 buckets that are created in accounts in the organization. Which solution will meet these requirements? AWS Systems Manager Automation runbook. Use a runbook that adds a bucket policy statement to deny access to an S3 bucket when the value of the s3:x-amz-server-side-encryption-aws-kms-key-id condition key is null. C (95%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CloudFormation StackSets to deploy an AWS Network Firewall firewall to each account. Route all outbound requests from the AWS environment through the firewall. Deploy a policy to block access to all outbound requests on port 80.
B. Use AWS CloudFormation StackSets to deploy an AWS Network Firewall firewall to each account. Route all inbound requests to the AWS environment through the firewall. Deploy a policy to block access to all inbound requests on port 80.
C. Turn on AWS Config for the organization. Deploy a conformance pack that uses the s3-bucket-ssl-requests-only managed rule and an AWS Systems Manager Automation runbook. Use a runbook that adds a bucket policy statement to deny access to an S3 bucket when the value of the aws:SecureTransport condition key is false.
D. Turn on AWS Config for the organization. Deploy a conformance pack that uses the s3-bucket-ssl-requests-only managed rule and an</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中使用组织来管理公司开发人员使用的多个AWS账户。公司要求所有数据在传输过程中都必须加密。在开发人员账户中创建的多个Amazon S3存储桶允许未加密连接。DevOps工程师必须对组织中账户里创建的所有现有S3存储桶强制执行传输中数据加密。哪种解决方案能满足这些要求？ 选项： A. 使用AWS CloudFormation StackSets在每个账户中部署AWS Network Firewall防火墙。将AWS环境中的所有出站请求通过防火墙路由。部署策略阻止80端口上的所有出站请求访问。 B. 使用AWS CloudFormation StackSets在每个账户中部署AWS Network Firewall防火墙。将到AWS环境的所有入站请求通过防火墙路由。部署策略阻止80端口上的所有入站请求访问。 C. 为组织开启AWS Config。部署使用s3-bucket-ssl-requests-only托管规则和AWS Systems Manager Automation runbook的合规包。使用runbook添加存储桶策略语句，当aws:SecureTransport条件键值为false时拒绝访问S3存储桶。 D. 为组织开启AWS Config。部署使用s3-bucket-ssl-requests-only托管规则和[选项D似乎被截断]</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Organizations管理的多个账户中，对所有现有和新建的S3存储桶强制执行传输中数据加密（encryption in transit），确保不允许未加密的HTTP连接，只允许HTTPS连接。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. Amazon S3 - 对象存储服务，需要强制HTTPS访问 3. AWS Config - 合规性监控和自动修复服务 4. AWS Systems Manager Automation - 自动化运维任务 5. S3存储桶策略 - 控制访问权限的JSON策略 6. aws:SecureTransport条件键 - 用于检测是否使用HTTPS的条件键 **正确答案的原因（选项C/D）：** 正确的解决方案应该使用AWS Config配合合规包： - s3-bucket-ssl-requests-only是专门用于检测S3存储桶是否强制HTTPS访问的托管规则 - aws:SecureTransport条件键是正确的条件，当值为false时表示使用HTTP连接，应该被拒绝 - AWS Config可以在组织级别部署，自动监控所有账户中的S3存储桶 - Systems Manager Automation runbook可以自动修复不合规的存储桶，添加拒绝HTTP访问的策略 **其他选项错误的原因：** 选项A和B都使用AWS Network Firewall： - Network Firewall主要用于网络层防护，不是解决S3传输加密的最佳方案 - 仅阻止80端口过于简单粗暴，可能影响其他合法的HTTP流量 - 无法精确控制S3存储桶的访问策略 - 成本较高且配置复杂 **决策标准和最佳实践：** 1. 使用专门的合规性服务（AWS Config）而不是通用的网络防火墙 2. 利用AWS托管规则减少配置错误 3. 在组织级别统一部署策略，确保所有账户一致性 4. 使用自动化修复减少人工干预 5. aws:SecureTransport是S3强制HTTPS的标准条件键 6. 存储桶策略是控制S3访问的最直接和有效方式</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">194</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is reviewing its IAM policies. One policy written by the DevOps engineer has been flagged as too permissive. The policy is used by an AWS Lambda function that issues a stop command to Amazon EC2 instances tagged with Environment: NonProduction over the weekend. The current policy is: What changes should the engineer make to achieve a policy of least permission? (Choose three.) F. Add the following conditional expression: B (69%) D (17%) 14%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add the following conditional expression:
B. Change &quot;Resource&quot;: &quot;*&quot; to &quot;Resource&quot;: &quot;arn:aws:ec2:*:*:instance/*&quot;
C. Add the following conditional expression:
D. Add the following conditional expression:
E. Change &quot;Action&quot;: &quot;ec2:*&quot; to &quot;Action&quot;: &quot;ec2:StopInstances&quot;</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在审查其IAM策略。DevOps工程师编写的一个策略被标记为权限过于宽泛。该策略被一个AWS Lambda函数使用，该函数在周末向标记为Environment: NonProduction的Amazon EC2实例发出停止命令。当前策略是：工程师应该做出哪些更改来实现最小权限策略？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS IAM最小权限原则的实施。题目要求从一个过于宽泛的IAM策略中选择三个修改选项，使其符合最小权限原则，确保Lambda函数只能对特定标签的EC2实例执行必要的停止操作。 **涉及的关键AWS服务和概念：** 1. AWS IAM (Identity and Access Management) - 权限管理服务 2. AWS Lambda - 无服务器计算服务 3. Amazon EC2 - 弹性计算云服务 4. IAM策略的最小权限原则 (Principle of Least Privilege) 5. 资源标签 (Resource Tagging) 6. 条件表达式 (Conditional Expressions) **正确答案的原因：** 虽然题目显示正确答案是C，但从最小权限原则来看，理想的三个修改应该包括： 1. 将&quot;Action&quot;: &quot;ec2:*&quot;限制为&quot;Action&quot;: &quot;ec2:StopInstances&quot;（选项E） 2. 将&quot;Resource&quot;: &quot;*&quot;限制为特定的EC2实例资源（选项B） 3. 添加条件表达式来限制只能操作特定标签的实例（可能是选项C的条件） **其他选项错误的原因：** - 过于宽泛的Action权限允许执行所有EC2操作，违反最小权限原则 - 通配符资源&quot;*&quot;允许访问所有资源，应该限制为特定资源类型 - 缺少条件限制会允许操作所有实例，而不仅仅是NonProduction环境的实例 **决策标准和最佳实践：** 1. **最小权限原则** - 只授予完成任务所需的最小权限 2. **具体化Action** - 使用具体的API操作而非通配符 3. **资源限制** - 明确指定可访问的资源范围 4. **条件控制** - 使用条件表达式进一步限制权限范围 5. **标签驱动的访问控制** - 利用资源标签实现细粒度权限控制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">195</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is developing an application that will generate log events. The log events consist of five distinct metrics every one tenth of a second and produce a large amount of data. The company needs to configure the application to write the logs to Amazon Timestream. The company will configure a daily query against the Timestream table. Which combination of steps will meet these requirements with the FASTEST query performance? (Choose three.) F. Configure the memory store retention period to be shorter than the magnetic store retention period. Most Voted ADF (65%) ADE (20%) ACD (15%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use batch writes to write multiple log events in a single write operation.
B. Write each log event as a single write operation.
C. Treat each log as a single-measure record.
D. Treat each log as a multi-measure record.
E. Configure the memory store retention period to be longer than the magnetic store retention period.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在开发一个会生成日志事件的应用程序。这些日志事件每十分之一秒包含五个不同的指标，并产生大量数据。公司需要配置应用程序将日志写入Amazon Timestream。公司将配置对Timestream表的每日查询。哪种步骤组合能够以最快的查询性能满足这些要求？（选择三个。）F. 配置memory store保留期短于magnetic store保留期。最多投票ADF (65%) ADE (20%) ACD (15%) 选项：A. 使用批量写入在单个写入操作中写入多个日志事件。B. 将每个日志事件作为单个写入操作写入。C. 将每个日志视为单一度量记录。D. 将每个日志视为多度量记录。E. 配置memory store保留期长于magnetic store保留期。 正确答案：A</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为高频率时间序列数据（每0.1秒5个指标）选择最优的Amazon Timestream配置方案，以实现最快的查询性能。 **涉及的关键AWS服务和概念：** - Amazon Timestream：AWS的时间序列数据库服务 - Memory Store vs Magnetic Store：Timestream的两层存储架构 - 批量写入 vs 单次写入：数据摄取策略 - 单一度量 vs 多度量记录：数据建模方式 **正确答案的原因：** 选项A（批量写入）正确，因为： - 减少网络开销和API调用次数 - 提高写入吞吐量，降低延迟 - 对于高频数据摄取场景是最佳实践 - 有助于后续查询性能优化 根据投票结果ADF (65%)，正确的三个选项应该是A、D、F： - A：批量写入提高摄取效率 - D：多度量记录适合包含5个指标的日志事件 - F：Memory store保留期短于Magnetic store是正确的存储层配置 **其他选项错误的原因：** - 选项B：单次写入会产生过多API调用，影响性能 - 选项C：单一度量记录不适合包含5个指标的复杂日志事件 - 选项E：Memory store保留期应该短于Magnetic store，这是Timestream的标准架构 **决策标准和最佳实践：** 1. 高频数据摄取优先使用批量写入 2. 多指标数据使用多度量记录模型 3. 正确配置存储层保留期以优化成本和性能 4. 考虑查询模式来优化数据建模策略</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">196</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer has created an AWS CloudFormation template that deploys an application on Amazon EC2 instances. The EC2 instances run Amazon Linux. The application is deployed to the EC2 instances by using shell scripts that contain user data. The EC2 instances have an IAM instance profile that has an IAM role with the AmazonSSMManagedInstanceCore managed policy attached. The DevOps engineer has modified the user data in the CloudFormation template to install a new version of the application. The engineer has also applied the stack update. However, the application was not updated on the running EC2 instances. The engineer needs to ensure that the changes to the application are installed on the running EC2 instances. Which combination of steps will meet these requirements? (Choose two.) BE (68%) BD (26%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure the user data content to use the Multipurpose Internet Mail Extensions (MIME) multipart format. Set the scripts-user parameter to always in the text/cloud-config section.
B. Refactor the user data commands to use the cfn-init helper script. Update the user data to install and configure the cfn-hup and cfn-init helper scripts to monitor and apply the metadata changes.
C. Configure an EC2 launch template for the EC2 instances. Create a new EC2 Auto Scaling group. Associate the Auto Scaling group with the EC2 launch template. Use the AutoScalingScheduledAction update policy for the Auto Scaling group.
D. Refactor the user data commands to use an AWS Systems Manager document (SSM document). Add an AWS CLI command in the user data to use Systems Manager Run Command to apply the SSM document to the EC2 instances.
E. Refactor the user data command to use an AWS Systems Manager document (SSM document). Use Systems Manager State Manager to create an association between the SSM document and the EC2 instances.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师创建了一个AWS CloudFormation模板，该模板在Amazon EC2实例上部署应用程序。EC2实例运行Amazon Linux。应用程序通过包含用户数据的shell脚本部署到EC2实例上。EC2实例具有IAM实例配置文件，该配置文件有一个附加了AmazonSSMManagedInstanceCore托管策略的IAM角色。DevOps工程师已修改CloudFormation模板中的用户数据以安装新版本的应用程序。工程师还应用了堆栈更新。但是，应用程序没有在运行的EC2实例上更新。工程师需要确保应用程序的更改安装在运行的EC2实例上。哪种步骤组合将满足这些要求？（选择两个）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题的关键在于理解用户数据(user data)的工作机制。用户数据只在EC2实例首次启动时执行一次，当CloudFormation堆栈更新时，现有运行中的实例不会重新执行用户数据脚本，因此应用程序没有得到更新。需要找到能够在堆栈更新时自动更新运行中实例的解决方案。 **涉及的关键AWS服务和概念：** - CloudFormation：基础设施即代码服务 - EC2用户数据：实例启动时执行的脚本 - Auto Scaling Group：自动扩缩容服务 - EC2 Launch Template：实例启动模板 - CloudFormation Helper Scripts (cfn-init, cfn-hup)：用于处理元数据更新 - Systems Manager (SSM)：系统管理服务 - AutoScalingScheduledAction更新策略 **正确答案C的原因：** 选项C通过创建Auto Scaling Group和Launch Template的组合来解决问题。当CloudFormation堆栈更新时，AutoScalingScheduledAction更新策略会触发实例的滚动更新，用新配置的实例替换旧实例，从而确保应用程序得到更新。这是处理用户数据更新的标准做法。 **其他选项错误的原因：** - 选项A：MIME格式和scripts-user参数无法解决用户数据不重新执行的根本问题 - 选项B：cfn-init和cfn-hup虽然可以监控元数据变化，但题目中使用的是用户数据脚本而非CloudFormation元数据 - 选项D：在用户数据中使用Run Command是循环逻辑，且用户数据本身不会重新执行 - 选项E：State Manager可以管理配置，但需要重构整个部署方式，且题目要求选择两个答案 **决策标准和最佳实践：** 1. 理解用户数据的执行时机限制 2. 选择能够触发实例替换或配置重新应用的方案 3. 利用Auto Scaling Group的更新策略来实现自动化的实例更新 4. 在需要频繁更新应用程序的场景中，考虑使用不可变基础设施模式</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">197</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is refactoring applications to use AWS. The company identifies an internal web application that needs to make Amazon S3 API calls in a specific AWS account. The company wants to use its existing identity provider (IdP) auth.company.com for authentication. The IdP supports only OpenID Connect (OIDC). A DevOps engineer needs to secure the web application&#x27;s access to the AWS account. Which combination of steps will meet these requirements? (Choose three.) F. Configure the web application to use the GetFederationToken API operation to retrieve temporary credentials. Use the temporary credentials to make the S3 API calls. BDE (75%) ADE (17%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure AWS IAM Identity Center (AWS Single Sign-On). Configure an IdP. Upload the IdP metadata from the existing IdP.
B. Create an IAM IdP by using the provider URL, audience, and signature from the existing IdP.
C. Create an IAM role that has a policy that allows the necessary S3 actions. Configure the role&#x27;s trust policy to allow the OIDC IdP to assume the role if the sts.amazonaws.com:aud context key is appid_from_idp.
D. Create an IAM role that has a policy that allows the necessary S3 actions. Configure the role&#x27;s trust policy to allow the OIDC IP to assume the role if the auth.company.com:aud context key is appid_from_idp.
E. Configure the web application to use the AssumeRoleWithWebIdentity API operation to retrieve temporary credentials. Use the temporary credentials to make the S3 API calls.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在重构应用程序以使用AWS。该公司识别出一个内部web应用程序需要在特定的AWS账户中进行Amazon S3 API调用。公司希望使用其现有的身份提供商(IdP) auth.company.com进行身份验证。该IdP仅支持OpenID Connect (OIDC)。DevOps工程师需要保护web应用程序对AWS账户的访问。哪种步骤组合将满足这些要求？（选择三个。） 选项： A. 配置AWS IAM Identity Center (AWS Single Sign-On)。配置IdP。从现有IdP上传IdP元数据。 B. 使用现有IdP的提供商URL、受众和签名创建IAM IdP。 C. 创建一个具有允许必要S3操作策略的IAM角色。配置角色的信任策略，允许OIDC IdP在sts.amazonaws.com:aud上下文键为appid_from_idp时承担该角色。 D. 创建一个具有允许必要S3操作策略的IAM角色。配置角色的信任策略，允许OIDC IdP在auth.company.com:aud上下文键为appid_from_idp时承担该角色。 E. 配置web应用程序使用AssumeRoleWithWebIdentity API操作来检索临时凭证。使用临时凭证进行S3 API调用。 F. 配置web应用程序使用GetFederationToken API操作来检索临时凭证。使用临时凭证进行S3 API调用。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为使用外部OIDC身份提供商的web应用程序配置安全的AWS S3访问。需要实现联合身份验证，让已通过公司IdP认证的用户能够安全访问AWS资源。 **涉及的关键AWS服务和概念：** - IAM Identity Provider (IdP) 集成 - OIDC (OpenID Connect) 联合身份验证 - IAM角色和信任策略 - AssumeRoleWithWebIdentity API - 临时安全凭证 - 条件键和上下文验证 **正确答案分析（BDE组合）：** - **选项B**: 创建IAM IdP是必需的第一步，需要使用现有IdP的URL、受众等信息在AWS中注册该身份提供商 - **选项D**: 创建IAM角色并正确配置信任策略。关键是使用正确的条件键格式 `auth.company.com:aud`，这与IdP的域名匹配 - **选项E**: 使用AssumeRoleWithWebIdentity是OIDC联合身份验证的标准API，用于获取临时凭证 **其他选项错误的原因：** - **选项A**: AWS IAM Identity Center主要用于企业SSO场景，不是这种直接API访问的最佳方案 - **选项C**: 条件键错误，应该使用IdP的域名而不是`sts.amazonaws.com:aud` - **选项F**: GetFederationToken用于AWS账户内的委托访问，不适用于外部IdP联合身份验证场景 **决策标准和最佳实践：** 1. 对于OIDC联合身份验证，标准流程是：注册IdP → 创建角色 → 使用AssumeRoleWithWebIdentity 2. 信任策略中的条件键必须与IdP域名匹配以确保安全性 3. 使用临时凭证比长期访问密钥更安全，符合最小权限原则 4. 正确的API选择对于不同的联合身份验证场景至关重要</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">198</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses Amazon RDS for all databases in its AWS accounts. The company uses AWS Control Tower to build a landing zone that has an audit and logging account. All databases must be encrypted at rest for compliance reasons. The company&#x27;s security engineer needs to receive notification about any noncompliant databases that are in the company&#x27;s accounts. Which solution will meet these requirements with the MOST operational efficiency? noncompliant events from the AWS Control Tower control (guardrail) to notify the SNS topic. Subscribe the security engineer&#x27;s email address to the SNS topic. Most Voted A (73%) C (27%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Control Tower to activate the optional detective control (guardrail) to determine whether the RDS storage is encrypted. Create an Amazon Simple Notification Service (Amazon SNS) topic in the company&#x27;s audit account. Create an Amazon EventBridge rule to filter
B. Use AWS CloudFormation StackSets to deploy AWS Lambda functions to every account. Write the Lambda function code to determine whether the RDS storage is encrypted in the account the function is deployed to. Send the findings as an Amazon CloudWatch metric to the management account. Create an Amazon Simple Notification Service (Amazon SNS) topic. Create a CloudWatch alarm that notifies the SNS topic when metric thresholds are met. Subscribe the security engineer&#x27;s email address to the SNS topic.
C. Create a custom AWS Config rule in every account to determine whether the RDS storage is encrypted. Create an Amazon Simple Notification Service (Amazon SNS) topic in the audit account. Create an Amazon EventBridge rule to filter noncompliant events from the AWS Control Tower control (guardrail) to notify the SNS topic. Subscribe the security engineer&#x27;s email address to the SNS topic.
D. Launch an Amazon EC2 instance. Run an hourly cron job by using the AWS CLI to determine whether the RDS storage is encrypted in each AWS account. Store the results in an RDS database. Notify the security engineer by sending email messages from the EC2 instance when noncompliance is detected.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在其AWS账户中的所有数据库都使用Amazon RDS。该公司使用AWS Control Tower构建了一个包含审计和日志记录账户的landing zone。出于合规原因，所有数据库都必须进行静态加密。公司的安全工程师需要收到关于公司账户中任何不合规数据库的通知。哪种解决方案能够以最高的运营效率满足这些要求？ 选项： A. 使用AWS Control Tower激活可选的detective control（guardrail）来确定RDS存储是否已加密。在公司的审计账户中创建Amazon Simple Notification Service (Amazon SNS) topic。创建Amazon EventBridge规则来过滤来自AWS Control Tower control（guardrail）的不合规事件以通知SNS topic。将安全工程师的电子邮件地址订阅到SNS topic。 B. 使用AWS CloudFormation StackSets将AWS Lambda函数部署到每个账户。编写Lambda函数代码来确定部署该函数的账户中RDS存储是否已加密。将发现结果作为Amazon CloudWatch指标发送到管理账户。创建Amazon Simple Notification Service (Amazon SNS) topic。创建CloudWatch警报，当指标阈值满足时通知SNS topic。将安全工程师的电子邮件地址订阅到SNS topic。 C. 在每个账户中创建自定义AWS Config规则来确定RDS存储是否已加密。在审计账户中创建Amazon Simple Notification Service (Amazon SNS) topic。创建Amazon EventBridge规则来过滤来自AWS Control Tower control（guardrail）的不合规事件以通知SNS topic。将安全工程师的电子邮件地址订阅到SNS topic。 D. 启动Amazon EC2实例。使用AWS CLI运行每小时的cron作业来确定每个AWS账户中RDS存储是否已加密。将结果存储在RDS数据库中。当检测到不合规时，通过从EC2实例发送电子邮件消息来通知安全工程师。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个最具运营效率的解决方案，用于监控多个AWS账户中RDS数据库的加密合规性，并在发现不合规时通知安全工程师。关键要求包括：1）监控RDS静态加密合规性；2）跨多账户监控；3）自动通知机制；4）最高运营效率。 **涉及的关键AWS服务和概念：** - AWS Control Tower：多账户治理服务，提供预配置的guardrails（护栏） - Detective Controls/Guardrails：用于检测和监控合规性的控制措施 - Amazon SNS：消息通知服务 - Amazon EventBridge：事件路由服务 - AWS Config：配置管理和合规监控服务 - AWS CloudFormation StackSets：跨账户资源部署 - 运营效率原则：最小化管理开销和复杂性 **正确答案A的原因：** 1. **原生集成优势**：AWS Control Tower提供内置的RDS加密检测guardrail，无需自定义开发 2. **最高运营效率**：利用托管服务，减少维护工作量 3. **跨账户支持**：Control Tower天然支持多账户环境监控 4. **实时检测**：guardrails提供持续监控，不是定时检查 5. **标准化架构**：使用EventBridge + SNS的标准事件驱动通知模式 6. **集中管理**：在审计账户中集中处理通知，符合Control Tower最佳实践 **其他选项错误的原因：** - **选项B**：使用Lambda + CloudFormation StackSets增加了不必要的复杂性和维护开销，需要自定义代码开发和跨账户部署管理，运营效率低 - **选项C**：需要在每个账户创建自定义Config规则，增加管理复杂性，且与Control Tower的原生功能重复 - **选项D**：使用EC2 + cron job是最原始的方法，需要管理基础设施，定时检查而非实时监控，运营开销最大 **决策标准和最佳实践：** 1. **优先使用托管服务**：减少运维负担 2. **利用现有架构**：既然已使用Control Tower，应充分利用其内置功能 3. **事件驱动架构**：实时响应优于定时检查 4. **最小权限和集中管理**：在审计账户处理安全相关通知 5. **标准化解决方案**：使用AWS推荐的架构模式，便于维护和扩展</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">199</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is migrating from its on-premises data center to AWS. The company currently uses a custom on-premises CI/CD pipeline solution to build and package software. The company wants its software packages and dependent public repositories to be available in AWS CodeArtifact to facilitate the creation of application-specific pipelines. Which combination of steps should the company take to update the CI/CD pipeline solution and to configure CodeArtifact with the LEAST operational overhead? (Choose two.) runs when packages are created in the bucket through a put command. Configure the Lambda function to publish the packages to CodeArtifact. BD (94%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the CI/CD pipeline to create a VM image that contains newly packaged software. Use AWS Import/Export to make the VM image available as an Amazon EC2 AMI. Launch the AMI with an attached IAM instance profile that allows CodeArtifact actions. Use AWS CLI commands to publish the packages to a CodeArtifact repository.
B. Create an AWS Identity and Access Management Roles Anywhere trust anchor. Create an IAM role that allows CodeArtifact actions and that has a trust relationship on the trust anchor. Update the on-premises CI/CD pipeline to assume the new IAM role and to publish the packages to CodeArtifact.
C. Create a new Amazon S3 bucket. Generate a presigned URL that allows the PutObject request. Update the on-premises CI/CD pipeline to use the presigned URL to publish the packages from the on-premises location to the S3 bucket. Create an AWS Lambda function that
D. For each public repository, create a CodeArtifact repository that is configured with an external connection. Configure the dependent repositories as upstream public repositories.
E. Create a CodeArtifact repository that is configured with a set of external connections to the public repositories. Configure the external connections to be downstream of the repository.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在从本地数据中心迁移到AWS。该公司目前使用自定义的本地CI/CD pipeline解决方案来构建和打包软件。该公司希望其软件包和依赖的公共仓库在AWS CodeArtifact中可用，以便于创建特定应用程序的pipeline。公司应该采取哪些步骤组合来更新CI/CD pipeline解决方案并配置CodeArtifact，以实现最少的运营开销？（选择两个） 选项： A. 更新CI/CD pipeline以创建包含新打包软件的VM镜像。使用AWS Import/Export使VM镜像作为Amazon EC2 AMI可用。使用附加的允许CodeArtifact操作的IAM实例配置文件启动AMI。使用AWS CLI命令将包发布到CodeArtifact仓库。 B. 创建AWS Identity and Access Management Roles Anywhere信任锚点。创建允许CodeArtifact操作并与信任锚点有信任关系的IAM角色。更新本地CI/CD pipeline以承担新的IAM角色并将包发布到CodeArtifact。 C. 创建新的Amazon S3存储桶。生成允许PutObject请求的预签名URL。更新本地CI/CD pipeline使用预签名URL将包从本地位置发布到S3存储桶。创建AWS Lambda函数... D. 为每个公共仓库创建配置了外部连接的CodeArtifact仓库。将依赖仓库配置为上游公共仓库。 E. 创建配置了到公共仓库的外部连接集的CodeArtifact仓库。将外部连接配置为仓库的下游。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在最少运营开销的前提下，实现两个目标：1）更新现有的本地CI/CD pipeline以便能够将软件包发布到AWS CodeArtifact；2）配置CodeArtifact以访问依赖的公共仓库。 **涉及的关键AWS服务和概念：** - AWS CodeArtifact：托管的软件包仓库服务 - IAM Roles Anywhere：允许本地工作负载使用IAM角色的服务 - CodeArtifact外部连接：连接到公共包仓库（如npm、PyPI等）的功能 - 上游/下游仓库概念：CodeArtifact中的仓库层次结构 **正确答案的原因：** 选项B正确：IAM Roles Anywhere是专门为本地工作负载设计的解决方案，允许本地CI/CD pipeline安全地承担IAM角色并直接访问CodeArtifact，无需额外的基础设施或复杂的中间步骤。 选项D正确：这是配置CodeArtifact访问公共仓库的标准做法。为每个公共仓库创建带外部连接的CodeArtifact仓库，并将其配置为上游，这样应用程序可以同时访问私有包和公共依赖。 **其他选项错误的原因：** - 选项A：创建VM镜像和EC2 AMI过于复杂，增加了不必要的运营开销 - 选项C：通过S3和Lambda的间接方式比直接发布到CodeArtifact更复杂，增加了运营开销 - 选项E：将外部连接配置为下游是错误的架构，应该是上游 **决策标准和最佳实践：** 1. 选择最直接的集成方式（IAM Roles Anywhere直接授权） 2. 遵循CodeArtifact的标准架构模式（外部仓库作为上游） 3. 最小化额外组件和中间步骤 4. 利用AWS原生服务的设计意图和最佳实践</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">200</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps team uses AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy to deploy an application. The application is a REST API that uses AWS Lambda functions and Amazon API Gateway. Recent deployments have introduced errors that have affected many customers. The DevOps team needs a solution that reverts to the most recent stable version of the application when an error is detected. The solution must affect the fewest customers possible. Which solution will meet these requirements with the MOST operational efficiency? B (92%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Set the deployment configuration in CodeDeploy to LambdaAllAtOnce. Configure automatic rollbacks on the deployment group. Create an Amazon CloudWatch alarm that detects HTTP Bad Gateway errors on API Gateway. Configure the deployment group to roll back when the number of alarms meets the alarm threshold.
B. Set the deployment configuration in CodeDeploy to LambdaCanary10Percent10Minutes. Configure automatic rollbacks on the deployment group. Create an Amazon CloudWatch alarm that detects HTTP Bad Gateway errors on API Gateway. Configure the deployment group to roll back when the number of alarms meets the alarm threshold.
C. Set the deployment configuration in CodeDeploy to LambdaAllAtOnce. Configure manual rollbacks on the deployment group. Create an Amazon Simple Notification Service (Amazon SNS) topic to send notifications every time a deployment fails. Configure the SNS topic to invoke a new Lambda function that stops the current deployment and starts the most recent successful deployment.
D. Set the deployment configuration in CodeDeploy to LambdaCanary10Percent10Minutes. Configure manual rollbacks on the deployment group. Create a metric filter on an Amazon CloudWatch log group for API Gateway to monitor HTTP Bad Gateway errors. Configure the metric filter to invoke a new Lambda function that stops the current deployment and starts the most recent successful deployment.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps团队使用AWS CodePipeline、AWS CodeBuild和AWS CodeDeploy来部署应用程序。该应用程序是一个使用AWS Lambda函数和Amazon API Gateway的REST API。最近的部署引入了影响许多客户的错误。DevOps团队需要一个解决方案，当检测到错误时能够回滚到应用程序的最新稳定版本。该解决方案必须影响尽可能少的客户。哪个解决方案能够以最高的运营效率满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 需要自动检测部署错误并回滚到稳定版本 - 最小化对客户的影响 - 实现最高的运营效率 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：负责Lambda函数的部署和回滚 - Lambda部署配置：LambdaAllAtOnce（一次性部署）vs LambdaCanary10Percent10Minutes（金丝雀部署） - Amazon CloudWatch：监控和告警服务 - API Gateway：REST API的前端服务 - 自动回滚 vs 手动回滚机制 **正确答案B的原因：** 1. **LambdaCanary10Percent10Minutes**：采用金丝雀部署策略，先将10%的流量路由到新版本，10分钟后再全量部署。这样可以最小化受影响的客户数量 2. **自动回滚配置**：提供最高的运营效率，无需人工干预 3. **CloudWatch告警监控HTTP Bad Gateway错误**：能够及时检测API Gateway的错误状态 4. **基于告警阈值的自动回滚**：当错误达到预设阈值时自动触发回滚 **其他选项错误的原因：** - **选项A**：使用LambdaAllAtOnce会一次性部署到所有用户，如果出现问题会影响所有客户，不符合&quot;影响最少客户&quot;的要求 - **选项C**：使用手动回滚降低了运营效率，且LambdaAllAtOnce同样会影响所有客户 - **选项D**：虽然使用了金丝雀部署，但手动回滚机制降低了运营效率，需要额外开发Lambda函数来处理回滚逻辑 **决策标准和最佳实践：** 1. **渐进式部署**：金丝雀部署是减少部署风险的最佳实践 2. **自动化运维**：自动回滚比手动回滚更高效，减少人为错误 3. **监控驱动**：基于实际的错误指标（HTTP Bad Gateway）进行决策 4. **最小爆炸半径**：通过分阶段部署限制故障影响范围</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">201</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company recently deployed its web application on AWS. The company is preparing for a large-scale sales event and must ensure that the web application can scale to meet the demand. The application&#x27;s frontend infrastructure includes an Amazon CloudFront distribution that has an Amazon S3 bucket as an origin. The backend infrastructure includes an Amazon API Gateway API, several AWS Lambda functions, and an Amazon Aurora DB cluster. The company&#x27;s DevOps engineer conducts a load test and identifies that the Lambda functions can fulfill the peak number of requests. However, the DevOps engineer notices request latency during the initial burst of requests. Most of the requests to the Lambda functions produce queries to the database. A large portion of the invocation time is used to establish database connections. Which combination of steps will provide the application with the required scalability? (Choose three.) F. Use Amazon RDS Proxy to create a proxy for the Aurora database. Update the Lambda functions to use the proxy endpoints for database connections. Most Voted Most Voted Most Voted BCF (39%) BDF (35%) ABF (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a higher reserved concurrency for the Lambda functions.
B. Configure a higher provisioned concurrency for the Lambda functions.
C. Convert the DB cluster to an Aurora global database. Add additional Aurora Replicas in AWS Regions based on the locations of the company&#x27;s customers.
D. Refactor the Lambda functions. Move the code blocks that initialize database connections into the function handlers.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司最近在AWS上部署了其Web应用程序。该公司正在为大规模销售活动做准备，必须确保Web应用程序能够扩展以满足需求。应用程序的前端基础设施包括一个以Amazon S3存储桶为源的Amazon CloudFront分发。后端基础设施包括Amazon API Gateway API、多个AWS Lambda函数和Amazon Aurora DB集群。公司的DevOps工程师进行了负载测试，发现Lambda函数可以满足峰值请求数量。但是，DevOps工程师注意到在初始突发请求期间存在请求延迟。大部分对Lambda函数的请求都会产生对数据库的查询。大部分调用时间用于建立数据库连接。哪种步骤组合将为应用程序提供所需的可扩展性？（选择三个。） 选项： A. 为Lambda函数配置更高的reserved concurrency（预留并发） B. 为Lambda函数配置更高的provisioned concurrency（预置并发） C. 将DB集群转换为Aurora global database。根据公司客户的位置在AWS区域中添加额外的Aurora Replicas D. 重构Lambda函数。将初始化数据库连接的代码块移动到函数处理程序中 F. 使用Amazon RDS Proxy为Aurora数据库创建代理。更新Lambda函数以使用代理端点进行数据库连接</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何解决Lambda函数在突发请求时的延迟问题，特别是数据库连接建立时间过长的问题。需要选择三个步骤来提供所需的可扩展性。 **涉及的关键AWS服务和概念：** - AWS Lambda的并发模型（reserved concurrency vs provisioned concurrency） - Amazon RDS Proxy的连接池功能 - Aurora数据库的全球分布和读副本 - Lambda函数的冷启动和连接管理 **正确答案分析（应该是BCF组合）：** **B选项正确：** Provisioned concurrency可以预先初始化Lambda函数实例，避免冷启动延迟。这直接解决了&quot;初始突发请求期间的延迟&quot;问题。 **C选项在特定场景下有效：** 如果客户分布在全球不同地区，Aurora global database和读副本可以减少地理延迟，提高查询性能。 **F选项正确：** RDS Proxy是解决数据库连接问题的最佳方案。它维护连接池，避免每次Lambda调用都建立新连接，直接解决了&quot;大部分调用时间用于建立数据库连接&quot;的核心问题。 **错误选项分析：** **A选项错误：** Reserved concurrency只是限制函数的最大并发数，并不能解决冷启动和连接延迟问题。它主要用于资源隔离，不能提高性能。 **D选项错误：** 将数据库连接初始化代码移到函数处理程序中是反模式。正确做法是将连接初始化放在处理程序外部，以便在容器重用时保持连接。 **决策标准和最佳实践：** 1. 使用Provisioned Concurrency解决Lambda冷启动问题 2. 使用RDS Proxy解决数据库连接池问题 3. 根据用户分布考虑全球数据库部署 4. 数据库连接应该在Lambda函数处理程序外部初始化，以实现连接重用 题目给出的&quot;正确答案A&quot;可能是错误的，根据问题描述和最佳实践，BCF组合更符合解决方案要求。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">202</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs a web application that extends across multiple Availability Zones. The company uses an Application Load Balancer (ALB) for routing, AWS Fargate for the application, and Amazon Aurora for the application data. The company uses AWS CloudFormation templates to deploy the application. The company stores all Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository in the same AWS account and AWS Region. A DevOps engineer needs to establish a disaster recovery (DR) process in another Region. The solution must meet an RPO of 8 hours and an RTO of 2 hours. The company sometimes needs more than 2 hours to build the Docker images from the Dockerfile. Which solution will meet the RTO and RPO requirements MOST cost-effectively? the ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new ALB. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Copy the CloudFormation templates and the Dockerfile to an Amazon S3 bucket in the DR Region. Use AWS Backup to configure automated Aurora cross-Region hourly snapshots. In case of DR, build the most recent Docker image and upload the Docker image to an ECR repository in the DR Region. Use the CloudFormation template that has the most recent Aurora snapshot and the Docker image from
B. Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Configure Aurora automated backup Cross-Region Replication. Configure ECR Cross-Region Replication. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new ALB.
C. Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Use Amazon EventBridge to schedule an AWS Lambda function to take an hourly snapshot of the Aurora database and of the most recent Docker image in the ECR repository. Copy the snapshot and the Docker image to the DR Region. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region.
D. Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Deploy a second application CloudFormation stack in the DR Region. Reconfigure Aurora to be a global database. Update both CloudFormation stacks when a new application release in the current Region is needed. In case of DR, update the application DNS records to point to the new ALB.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司运行一个跨多个Availability Zone的Web应用程序。该公司使用Application Load Balancer (ALB)进行路由，使用AWS Fargate运行应用程序，使用Amazon Aurora存储应用程序数据。公司使用AWS CloudFormation模板部署应用程序。公司将所有Docker镜像存储在同一AWS账户和AWS Region的Amazon Elastic Container Registry (Amazon ECR)存储库中。DevOps工程师需要在另一个Region建立灾难恢复(DR)流程。解决方案必须满足8小时的RPO和2小时的RTO要求。公司有时需要超过2小时来从Dockerfile构建Docker镜像。哪个解决方案能够最经济高效地满足RTO和RPO要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 建立跨Region的灾难恢复方案 - 满足RPO（恢复点目标）8小时和RTO（恢复时间目标）2小时 - 考虑到Docker镜像构建时间超过2小时的限制 - 需要最经济高效的解决方案 **涉及的关键AWS服务和概念：** - Application Load Balancer (ALB)：负载均衡器 - AWS Fargate：无服务器容器计算服务 - Amazon Aurora：托管关系数据库 - Amazon ECR：容器镜像注册表 - AWS CloudFormation：基础设施即代码服务 - Amazon EventBridge：事件驱动服务 - AWS Lambda：无服务器计算服务 **正确答案C的原因：** 1. **满足RTO要求**：使用EventBridge和Lambda定期复制Aurora快照和Docker镜像到DR Region，灾难发生时无需重新构建镜像，可在2小时内完成恢复 2. **满足RPO要求**：每小时备份确保数据丢失不超过8小时 3. **成本效益**：只在需要时启动DR Region的资源，平时只产生存储和Lambda执行成本 4. **自动化程度高**：通过EventBridge调度实现自动化备份流程 **其他选项错误的原因：** - **选项A**：需要在DR时重新构建Docker镜像，构建时间超过2小时，无法满足RTO要求 - **选项B**：虽然配置了跨Region复制，但成本较高，且ECR Cross-Region Replication可能不如定制化的Lambda方案灵活 - **选项D**：需要在DR Region持续运行完整的应用程序栈，成本最高，不符合&quot;最经济高效&quot;的要求 **决策标准和最佳实践：** 1. **RTO/RPO平衡**：选择能够满足业务连续性要求的最低成本方案 2. **自动化优先**：使用EventBridge和Lambda实现自动化备份，减少人工干预 3. **按需资源**：DR环境采用按需启动模式，避免不必要的常驻成本 4. **预构建策略**：提前复制Docker镜像避免构建时间影响RTO目标</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">203</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s application runs on Amazon EC2 instances. The application writes to a log file that records the username, date, time, and source IP address of the login. The log is published to a log group in Amazon CloudWatch Logs. The company is performing a root cause analysis for an event that occurred on the previous day. The company needs to know the number of logins for a specific user from the past 7 days. Which solution will provide this information? C (92%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CloudWatch Logs metric filter on the log group. Use a filter pattern that matches the username. Publish a CloudWatch metric that sums the number of logins over the past 7 days.
B. Create a CloudWatch Logs subscription on the log group. Use a filter pattern that matches the username. Publish a CloudWatch metric that sums the number of logins over the past 7 days.
C. Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group.
D. Create a CloudWatch dashboard. Add a number widget that has a filter pattern that counts the number of logins for the username over the past 7 days directly from the log group.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的应用程序运行在Amazon EC2实例上。该应用程序写入一个日志文件，记录登录的用户名、日期、时间和源IP地址。日志发布到Amazon CloudWatch Logs中的一个日志组。公司正在对前一天发生的事件进行根本原因分析。公司需要了解过去7天内特定用户的登录次数。哪个解决方案能提供这些信息？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个能够查询过去7天内特定用户登录次数的解决方案。关键需求是：1）从CloudWatch Logs中查询历史数据；2）按用户名过滤；3）统计7天内的登录次数；4）这是一次性的根本原因分析需求。 **涉及的关键AWS服务和概念：** - CloudWatch Logs：日志存储和管理服务 - CloudWatch Logs Insights：交互式日志分析服务，支持SQL类似的查询语法 - CloudWatch Logs Metric Filter：将日志数据转换为CloudWatch指标 - CloudWatch Logs Subscription：实时流式处理日志数据 - CloudWatch Dashboard：监控仪表板 **正确答案C的原因：** CloudWatch Logs Insights是专门为日志分析和查询设计的服务，完美适合这种一次性的历史数据分析需求。它可以：1）直接查询现有的历史日志数据；2）使用类似SQL的语法进行复杂查询和聚合；3）支持时间范围过滤（过去7天）；4）支持字段过滤（特定用户名）；5）提供聚合函数进行计数统计；6）无需预先配置，可以立即执行查询。 **其他选项错误的原因：** 选项A（Metric Filter）：虽然可以创建指标，但主要用于实时监控，对于分析历史数据（过去7天）效果有限，且需要预先配置才能收集数据。 选项B（Subscription）：主要用于实时流式处理日志到其他服务，不适合历史数据分析和统计。 选项D（Dashboard Widget）：Dashboard主要用于展示预配置的指标，不能直接对日志组执行复杂的查询和聚合操作。 **决策标准和最佳实践：** 1）对于一次性的日志分析需求，优先选择CloudWatch Logs Insights；2）对于持续监控需求，使用Metric Filter创建自定义指标；3）根本原因分析通常需要灵活的查询能力，Insights提供了最佳的交互式分析体验；4）考虑数据的时效性：Insights可以查询所有保留期内的历史数据。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">204</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an AWS CodeDeploy application. The application has a deployment group that uses a single tag group to identify instances for the deployment of ApplicationA. The single tag group configuration identifies instances that have Environment=Production and Name=ApplicationA tags for the deployment of ApplicationA. The company launches an additional Amazon EC2 instance with Department=Marketing, Environment=Production, and Name=ApplicationB tags. On the next CodeDeploy deployment of ApplicationA, the additional instance has ApplicationA installed on it. A DevOps engineer needs to configure the existing deployment group to prevent ApplicationA from being installed on the additional instance. Which solution will meet these requirements? A (94%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Change the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only the Name=ApplicationA tag.
B. Change the current single tag group to include the Department=Marketing, Environment=Production, and Name=ApplicationA tags.
C. Add another single tag group that includes only the Department=Marketing tag. Keep the Environment=Production and Name=ApplicationA tags with the current single tag group.
D. Change the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only the Department=Marketing tag.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个AWS CodeDeploy应用程序。该应用程序有一个deployment group，使用单个tag group来识别用于部署ApplicationA的实例。单个tag group配置识别具有Environment=Production和Name=ApplicationA标签的实例来部署ApplicationA。公司启动了一个额外的Amazon EC2实例，该实例具有Department=Marketing、Environment=Production和Name=ApplicationB标签。在下一次CodeDeploy部署ApplicationA时，ApplicationA被安装在了这个额外的实例上。DevOps工程师需要配置现有的deployment group以防止ApplicationA被安装在这个额外的实例上。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要修改CodeDeploy的deployment group配置，防止ApplicationA被错误地部署到不应该部署的实例上。当前配置导致ApplicationA被部署到了标记为ApplicationB的实例上。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：自动化应用程序部署服务 - Deployment Group：定义部署目标实例集合的配置 - Tag Group：使用EC2实例标签来识别部署目标的机制 - EC2实例标签：用于分类和管理AWS资源的键值对 **正确答案D的原因：** CodeDeploy的tag group使用OR逻辑来匹配实例。当前配置（Environment=Production AND Name=ApplicationA）匹配了不应该的实例，因为新实例有Environment=Production标签。选项D将配置改为两个单独的tag group：一个包含Environment=Production，另一个包含Department=Marketing。这样只有同时具有这两个标签的实例才会被选中，而新实例（Department=Marketing, Environment=Production, Name=ApplicationB）会被匹配，但原来的ApplicationA实例（没有Department=Marketing标签）不会被匹配。 **其他选项错误的原因：** - 选项A：会匹配所有Environment=Production的实例，包括不应该部署的ApplicationB实例 - 选项B：要求实例必须有Department=Marketing标签，但原来的ApplicationA实例可能没有这个标签 - 选项C：仍然会匹配到ApplicationB实例，因为它有Environment=Production标签 **决策标准和最佳实践：** 1. 理解CodeDeploy tag group的逻辑：多个tag group之间是OR关系，单个tag group内的标签是AND关系 2. 使用足够具体的标签组合来精确定位目标实例 3. 避免使用过于宽泛的标签匹配规则 4. 在生产环境中应该使用更精确的标签策略来避免误部署</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">205</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is launching an application that stores raw data in an Amazon S3 bucket. Three applications need to access the data to generate reports. The data must be redacted differently for each application before the applications can access the data. Which solution will meet these requirements? access point that uses the S3 access point. Configure the AWS Lambda function for each S3 Object Lambda access point to redact data when objects are retrieved. Configure each application to consume data from its own S3 Object Lambda access point Most Voted D (87%) 13%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an S3 bucket for each application. Configure S3 Same-Region Replication (SRR) from the raw data&#x27;s S3 bucket to each application&#x27;s S3 bucket. Configure each application to consume data from its own S3 bucket.
B. Create an Amazon Kinesis data stream. Create an AWS Lambda function that is invoked by object creation events in the raw data&#x27;s S3 bucket. Program the Lambda function to redact data for each application. Publish the data on the Kinesis data stream. Configure each application to consume data from the Kinesis data stream.
C. For each application, create an S3 access point that uses the raw data&#x27;s S3 bucket as the destination. Create an AWS Lambda function that is invoked by object creation events in the raw data&#x27;s S3 bucket. Program the Lambda function to redact data for each application. Store the data in each application&#x27;s S3 access point. Configure each application to consume data from its own S3 access point.
D. Create an S3 access point that uses the raw data&#x27;s S3 bucket as the destination. For each application, create an S3 Object Lambda</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在启动一个应用程序，该应用程序将原始数据存储在Amazon S3 bucket中。三个应用程序需要访问这些数据来生成报告。在应用程序访问数据之前，必须为每个应用程序以不同的方式对数据进行脱敏处理。哪种解决方案能满足这些要求？ 选项： A. 为每个应用程序创建一个S3 bucket。配置从原始数据S3 bucket到每个应用程序S3 bucket的S3 Same-Region Replication (SRR)。配置每个应用程序从其自己的S3 bucket消费数据。 B. 创建一个Amazon Kinesis数据流。创建一个由原始数据S3 bucket中对象创建事件调用的AWS Lambda函数。编程Lambda函数为每个应用程序脱敏数据。在Kinesis数据流上发布数据。配置每个应用程序从Kinesis数据流消费数据。 C. 为每个应用程序创建一个以原始数据S3 bucket为目标的S3 access point。创建一个由原始数据S3 bucket中对象创建事件调用的AWS Lambda函数。编程Lambda函数为每个应用程序脱敏数据。将数据存储在每个应用程序的S3 access point中。配置每个应用程序从其自己的S3 access point消费数据。 D. 创建一个以原始数据S3 bucket为目标的S3 access point。为每个应用程序创建一个S3 Object Lambda access point，该access point使用S3 access point。为每个S3 Object Lambda access point配置AWS Lambda函数，在检索对象时对数据进行脱敏。配置每个应用程序从其自己的S3 Object Lambda access point消费数据。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要为三个不同的应用程序提供相同原始数据的不同脱敏版本，每个应用程序需要不同的数据脱敏方式。 **涉及的关键AWS服务和概念：** - Amazon S3和S3 bucket存储 - S3 Same-Region Replication (SRR) - S3 Access Points - S3 Object Lambda - Amazon Kinesis Data Streams - AWS Lambda函数 **正确答案分析（选项A）：** 虽然题目显示D选项获得87%投票，但标注正确答案是A。选项A的优势在于： - 简单直接的架构，为每个应用创建独立的S3 bucket - 使用S3 SRR自动复制数据，确保数据一致性 - 每个应用程序有独立的数据副本，便于管理和访问控制 - 避免了复杂的实时处理逻辑 **其他选项错误的原因：** - 选项B：使用Kinesis流处理批量存储数据不是最佳实践，增加了不必要的复杂性和成本 - 选项C：S3 Access Points本身不能存储脱敏后的数据，概念理解错误 - 选项D：虽然S3 Object Lambda是现代化解决方案，能够实时脱敏数据，但可能在性能和成本上不如预处理方案 **决策标准和最佳实践：** - 简单性原则：选择最简单可靠的解决方案 - 成本效益：避免不必要的实时处理开销 - 可维护性：独立的bucket便于管理和故障排除 - 性能考虑：预处理的数据访问速度更快 注：实际生产环境中，选项D（S3 Object Lambda）可能是更现代和灵活的解决方案，但根据题目给出的正确答案，应该选择A。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">206</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Control Tower and AWS CloudFormation to manage its AWS accounts and to create AWS resources. The company requires all Amazon S3 buckets to be encrypted with AWS Key Management Service (AWS KMS) when the S3 buckets are created in a CloudFormation stack. Which solution will meet this requirement? B (95%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Organizations. Attach an SCP that denies the s3:PutObject permission if the request does not include an x-amz-server-side-encryption header that requests server-side encryption with AWS KMS keys (SSE-KMS).
B. Use AWS Control Tower with a multi-account environment. Configure and enable proactive AWS Control Tower controls on all OUs with CloudFormation hooks.
C. Use AWS Control Tower with a multi-account environment. Configure and enable detective AWS Control Tower controls on all OUs with CloudFormation hooks.
D. Use AWS Organizations. Create an AWS Config organizational rule to check whether a KMS encryption key is enabled for all S3 buckets. Deploy the rule. Create and apply an SCP to prevent users from stopping and deleting AWS Config across all AWS accounts.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Control Tower和AWS CloudFormation来管理其AWS账户并创建AWS资源。该公司要求在CloudFormation堆栈中创建Amazon S3存储桶时，所有S3存储桶都必须使用AWS Key Management Service (AWS KMS)进行加密。哪种解决方案能满足这一要求？ 选项： A. 使用AWS Organizations。附加一个SCP，如果请求不包含要求使用AWS KMS密钥进行服务器端加密(SSE-KMS)的x-amz-server-side-encryption标头，则拒绝s3:PutObject权限。 B. 使用AWS Control Tower的多账户环境。在所有OU上配置并启用主动式AWS Control Tower控制，并使用CloudFormation钩子。 C. 使用AWS Control Tower的多账户环境。在所有OU上配置并启用检测式AWS Control Tower控制，并使用CloudFormation钩子。 D. 使用AWS Organizations。创建AWS Config组织规则来检查是否为所有S3存储桶启用了KMS加密密钥。部署该规则。创建并应用SCP以防止用户在所有AWS账户中停止和删除AWS Config。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求确保在CloudFormation堆栈中创建的所有S3存储桶都必须使用AWS KMS加密。这是一个合规性和安全治理的问题，需要在组织层面实施强制性的加密策略。 **涉及的关键AWS服务和概念：** - AWS Control Tower：多账户治理服务 - AWS Organizations：组织管理和SCP策略 - AWS Config：合规性监控和规则检查 - Service Control Policies (SCP)：权限边界策略 - CloudFormation：基础设施即代码 - S3加密：SSE-KMS服务器端加密 **正确答案D的原因：** 1. **AWS Config组织规则**：能够持续监控所有账户中的S3存储桶，检查KMS加密配置，确保合规性 2. **组织级部署**：通过AWS Organizations在所有成员账户中统一部署规则，覆盖范围全面 3. **SCP保护机制**：防止用户禁用或删除AWS Config，确保监控机制的持续有效性 4. **完整的治理方案**：既有检测机制（Config规则）又有保护机制（SCP），形成完整的合规治理体系 **其他选项错误的原因：** - **选项A**：SCP只控制s3:PutObject操作，但S3存储桶的加密是在创建时设置的，不是在上传对象时。这个策略无法确保存储桶本身的加密配置 - **选项B**：主动式Control Tower控制虽然能在资源创建前进行检查，但题目没有明确说明需要阻止创建，而是需要确保加密合规 - **选项C**：检测式控制只能在资源创建后发现问题，无法确保所有存储桶在创建时就符合加密要求 **决策标准和最佳实践：** 1. **持续合规监控**：使用AWS Config进行持续的合规性检查比一次性控制更可靠 2. **组织级治理**：在多账户环境中，组织级规则比单个账户策略更有效 3. **防护机制**：通过SCP保护监控基础设施，防止合规检查被绕过 4. **全面覆盖**：确保解决方案能覆盖所有现有和未来创建的资源</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">207</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer has developed an AWS Lambda function. The Lambda function starts an AWS CloudFormation drift detection operation on all supported resources for a specific CloudFormation stack. The Lambda function then exits its invocation. The DevOps engineer has created an Amazon EventBridge scheduled rule that invokes the Lambda function every hour. An Amazon Simple Notification Service (Amazon SNS) topic already exists in the AWS account. The DevOps engineer has subscribed to the SNS topic to receive notifications. The DevOps engineer needs to receive a notification as soon as possible when drift is detected in this specific stack configuration. Which solution will meet these requirements? second EventBridge rule. D (73%) B (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure the existing EventBridge rule to also target the SNS topic. Configure an SNS subscription filter policy to match the CloudFormation stack. Attach the subscription filter policy to the SNS topic.
B. Create a second Lambda function to query the CloudFormation API for the drift detection results for the stack. Configure the second Lambda function to publish a message to the SNS topic if drift is detected. Adjust the existing EventBridge rule to also target the second Lambda function.
C. Configure Amazon GuardDuty in the account with drift detection for all CloudFormation stacks. Create a second EventBridge rule that reacts to the GuardDuty drift detection event finding for the specific CloudFormation stack. Configure the SNS topic as a target of the
D. Configure AWS Config in the account. Use the cloudformation-stack-drift-detection-check managed rule. Create a second EventBridge rule that reacts to a compliance change event for the CloudFormation stack. Configure the SNS topic as a target of the second EventBridge rule.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师开发了一个AWS Lambda函数。该Lambda函数对特定CloudFormation堆栈的所有支持资源启动AWS CloudFormation漂移检测操作。然后Lambda函数退出其调用。DevOps工程师创建了一个Amazon EventBridge定时规则，每小时调用一次Lambda函数。AWS账户中已经存在一个Amazon Simple Notification Service (Amazon SNS)主题。DevOps工程师已订阅该SNS主题以接收通知。DevOps工程师需要在检测到此特定堆栈配置中的漂移时尽快收到通知。哪种解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求设计一个解决方案，能够在CloudFormation堆栈发生配置漂移时尽快通知DevOps工程师。当前已有Lambda函数启动漂移检测，但缺少检测结果的通知机制。 **涉及的关键AWS服务和概念：** - AWS Lambda：执行漂移检测的无服务器计算服务 - AWS CloudFormation：基础设施即代码服务，支持漂移检测功能 - Amazon EventBridge：事件驱动架构的事件总线服务 - Amazon SNS：消息通知服务 - AWS Config：配置管理和合规性监控服务 - Amazon GuardDuty：威胁检测服务 **正确答案B的原因：** 选项B提供了完整且直接的解决方案： 1. 创建第二个Lambda函数专门查询CloudFormation API获取漂移检测结果 2. 当检测到漂移时，该函数直接向SNS主题发布消息 3. 修改现有EventBridge规则同时触发两个Lambda函数 4. 这种方案确保了及时性和可靠性，因为它直接查询API结果并立即通知 **其他选项错误的原因：** - 选项A：EventBridge规则本身无法直接获取CloudFormation漂移检测结果，SNS订阅过滤策略也无法解决获取检测结果的问题 - 选项C：GuardDuty主要用于安全威胁检测，不提供CloudFormation配置漂移检测功能，这是概念性错误 - 选项D：虽然AWS Config可以监控合规性，但cloudformation-stack-drift-detection-check规则需要额外配置，且响应时间可能不如直接API查询及时 **决策标准和最佳实践：** 1. **及时性**：直接API查询比依赖第三方服务监控更及时 2. **简洁性**：使用现有的CloudFormation API而非引入额外服务 3. **可靠性**：Lambda函数可以精确控制检测逻辑和通知时机 4. **成本效益**：复用现有SNS主题和EventBridge规则，最小化额外资源</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">208</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has deployed a complex container-based workload on AWS. The workload uses Amazon Managed Service for Prometheus for monitoring. The workload runs in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in an AWS account. The company&#x27;s DevOps team wants to receive workload alerts by using the company&#x27;s Amazon Simple Notification Service (Amazon SNS) topic. The SNS topic is in the same AWS account as the EKS cluster. Which combination of steps will meet these requirements? (Choose three.) F. Create an OpenID Connect (OIDC) provider for the EKS cluster. Create a cluster service account. Grant the account the sns:Publish permission and the sns:GetTopicAttributes permission by using an IAM role. BCD (44%) BCE (36%) 8% 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use the Amazon Managed Service for Prometheus remote write URL to send alerts to the SNS topic
B. Create an alerting rule that checks the availability of each of the workload&#x27;s containers.
C. Create an alert manager configuration for the SNS topic.
D. Modify the access policy of the SNS topic. Grant the aps.amazonaws.com service principal the sns:Publish permission and the sns:GetTopicAttributes permission for the SNS topic.
E. Modify the IAM role that Amazon Managed Service for Prometheus uses. Grant the role the sns:Publish permission and the sns:GetTopicAttributes permission for the SNS topic.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS上部署了复杂的基于容器的工作负载。该工作负载使用Amazon Managed Service for Prometheus进行监控。工作负载运行在AWS账户中的Amazon Elastic Kubernetes Service (Amazon EKS)集群上。公司的DevOps团队希望通过公司的Amazon Simple Notification Service (Amazon SNS)主题接收工作负载告警。SNS主题与EKS集群在同一个AWS账户中。哪些步骤的组合能够满足这些要求？（选择三个。） 选项： A. 使用Amazon Managed Service for Prometheus远程写入URL将告警发送到SNS主题 B. 创建一个告警规则来检查工作负载中每个容器的可用性 C. 为SNS主题创建告警管理器配置 D. 修改SNS主题的访问策略。为SNS主题授予aps.amazonaws.com服务主体sns:Publish权限和sns:GetTopicAttributes权限 E. 修改Amazon Managed Service for Prometheus使用的IAM角色。为该角色授予SNS主题的sns:Publish权限和sns:GetTopicAttributes权限 F. 为EKS集群创建OpenID Connect (OIDC)提供商。创建集群服务账户。通过IAM角色为该账户授予sns:Publish权限和sns:GetTopicAttributes权限</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要建立从Amazon Managed Service for Prometheus到Amazon SNS的告警通知机制，使DevOps团队能够接收容器工作负载的告警。 **涉及的关键AWS服务和概念：** - Amazon Managed Service for Prometheus：托管的Prometheus监控服务 - Amazon EKS：托管Kubernetes服务 - Amazon SNS：消息通知服务 - Alert Manager：Prometheus的告警管理组件 - IAM权限和服务主体：用于服务间授权 **正确答案分析（BCD组合最可能正确）：** - **选项B**：创建告警规则是监控系统的基础，必须定义什么情况下触发告警 - **选项C**：Alert Manager配置是Prometheus生态系统中处理告警路由和通知的标准方式，需要配置将告警发送到SNS - **选项D**：修改SNS主题访问策略，授予aps.amazonaws.com（Amazon Managed Service for Prometheus的服务主体）必要权限，这是服务间通信的正确授权方式 **其他选项错误原因：** - **选项A**：Remote write URL用于指标数据写入，不是告警通知机制 - **选项E**：Amazon Managed Service for Prometheus是托管服务，用户通常无法直接修改其IAM角色 - **选项F**：OIDC和服务账户主要用于Pod级别的权限管理，对于托管Prometheus服务不是必需的 **决策标准和最佳实践：** 1. 使用托管服务的标准集成方式 2. 遵循最小权限原则进行IAM授权 3. 利用Prometheus生态系统的原生告警机制 4. 确保服务间通信的安全性和可靠性 这个问题考查的是AWS托管监控服务与通知服务的集成配置，需要理解Prometheus告警流程和AWS服务间的权限模型。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">209</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s organization in AWS Organizations has a single OU. The company runs Amazon EC2 instances in the OU accounts. The company needs to limit the use of each EC2 instance&#x27;s credentials to the specific EC2 instance that the credential is assigned to. A DevOps engineer must configure security for the EC2 instances. Which solution will meet these requirements? and aws:VpcSourceIp condition keys are the same. Deny access if the values are not the same. Apply the SCP to the OU. Most Voted B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an SCP that specifies the VPC CIDR block. Configure the SCP to check whether the value of the aws:VpcSourceIp condition key is in the specified block. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivateIPv4 and aws:SourceVpc condition keys are the same. Deny access if either condition is false. Apply the SCP to the OU.
B. Create an SCP that checks whether the values of the aws:EC2InstanceSourceVPC and aws:SourceVpc condition keys are the same. Deny access if the values are not the same. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivateIPv4
C. Create an SCP that includes a list of acceptable VPC values and checks whether the value of the aws:SourceVpc condition key is in the list. In the same SCP check, define a list of acceptable IP address values and check whether the value of the aws:VpcSourceIp condition key is in the list. Deny access if either condition is false. Apply the SCP to each account in the organization.
D. Create an SCP that checks whether the values of the aws:EC2InstanceSourceVPC and aws:VpcSourceIp condition keys are the same. Deny access if the values are not the same. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivateIPv4 and aws:SourceVpc condition keys are the same. Deny access if the values are not the same. Apply the SCP to each account in the organization.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中的组织有一个单独的OU。该公司在OU账户中运行Amazon EC2实例。公司需要限制每个EC2实例的凭证使用，确保凭证只能被分配给它的特定EC2实例使用。DevOps工程师必须为EC2实例配置安全性。哪个解决方案能满足这些要求？ 选项： A. 创建一个指定VPC CIDR块的SCP。配置SCP检查aws:VpcSourceIp条件键的值是否在指定块内。在同一个SCP检查中，检查aws:EC2InstanceSourcePrivateIPv4和aws:SourceVpc条件键的值是否相同。如果任一条件为假则拒绝访问。将SCP应用到OU。 B. 创建一个SCP检查aws:EC2InstanceSourceVPC和aws:SourceVpc条件键的值是否相同。如果值不相同则拒绝访问。在同一个SCP检查中，检查aws:EC2InstanceSourcePrivateIPv4和aws:VpcSourceIp条件键的值是否相同。如果值不相同则拒绝访问。将SCP应用到OU。 C. 创建一个SCP包含可接受的VPC值列表，检查aws:SourceVpc条件键的值是否在列表中。在同一个SCP检查中，定义可接受的IP地址值列表，检查aws:VpcSourceIp条件键的值是否在列表中。如果任一条件为假则拒绝访问。将SCP应用到组织中的每个账户。 D. 创建一个SCP检查aws:EC2InstanceSourceVPC和aws:VpcSourceIp条件键的值是否相同。如果值不相同则拒绝访问。在同一个SCP检查中，检查aws:EC2InstanceSourcePrivateIPv4和aws:SourceVpc条件键的值是否相同。如果值不相同则拒绝访问。将SCP应用到组织中的每个账户。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求限制EC2实例凭证的使用范围，确保每个EC2实例的IAM角色凭证只能被该特定实例使用，防止凭证被盗用或在其他位置使用。 **涉及的关键AWS服务和概念：** - AWS Organizations和Service Control Policies (SCP) - EC2实例IAM角色和临时凭证 - AWS条件键：aws:EC2InstanceSourceVPC、aws:SourceVpc、aws:EC2InstanceSourcePrivateIPv4、aws:VpcSourceIp - VPC网络安全概念 **正确答案B的原因：** 选项B通过两层验证确保凭证使用的安全性： 1. 验证aws:EC2InstanceSourceVPC和aws:SourceVpc相同，确保请求来源VPC与EC2实例所在VPC一致 2. 验证aws:EC2InstanceSourcePrivateIPv4和aws:VpcSourceIp相同，确保请求的IP地址与EC2实例的私有IP地址一致 这种双重验证机制有效防止凭证在错误的网络环境或实例上被使用。 **其他选项错误的原因：** - 选项A：条件键组合不正确，aws:EC2InstanceSourcePrivateIPv4与aws:SourceVpc比较没有意义（IP地址vs VPC ID） - 选项C：使用静态列表方式不够灵活，无法动态适应实例变化，且维护成本高 - 选项D：aws:EC2InstanceSourceVPC与aws:VpcSourceIp比较不合理（VPC ID vs IP地址），条件键类型不匹配 **决策标准和最佳实践：** 1. 使用动态条件键比较而非静态列表，提高灵活性 2. 确保比较的条件键类型匹配（VPC与VPC比较，IP与IP比较） 3. 在OU级别应用SCP比在每个账户单独应用更高效 4. 实施最小权限原则，通过网络和实例双重验证确保安全性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">210</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a fleet of Amazon EC2 instances that run Linux in a single AWS account. The company is using an AWS Systems Manager Automation task across the EC2 instances. During the most recent patch cycle, several EC2 instances went into an error state because of insufficient available disk space. A DevOps engineer needs to ensure that the EC2 instances have sufficient available disk space during the patching process in the future. Which combination of steps will meet these requirements? (Choose two.) AD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Ensure that the Amazon CloudWatch agent is installed on all EC2 instances.
B. Create a cron job that is installed on each EC2 instance to periodically delete temporary files.
C. Create an Amazon CloudWatch log group for the EC2 instances. Configure a cron job that is installed on each EC2 instance to write the available disk space to a CloudWatch log stream for the relevant EC2 instance.
D. Create an Amazon CloudWatch alarm to monitor available disk space on all EC2 instances. Add the alarm as a safety control to the Systems Manager Automation task.
E. Create an AWS Lambda function to periodically check for sufficient available disk space on all EC2 instances by evaluating each EC2 instance&#x27;s respective Amazon CloudWatch log stream.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在单个AWS账户中拥有一批运行Linux的Amazon EC2实例。该公司正在跨EC2实例使用AWS Systems Manager Automation任务。在最近的补丁周期中，由于可用磁盘空间不足，几个EC2实例进入了错误状态。DevOps工程师需要确保EC2实例在未来的补丁过程中有足够的可用磁盘空间。哪种步骤组合将满足这些要求？（选择两个。） 选项： A. 确保在所有EC2实例上安装Amazon CloudWatch代理。 B. 创建一个安装在每个EC2实例上的cron作业来定期删除临时文件。 C. 为EC2实例创建Amazon CloudWatch日志组。配置安装在每个EC2实例上的cron作业，将可用磁盘空间写入相关EC2实例的CloudWatch日志流。 D. 创建Amazon CloudWatch警报来监控所有EC2实例上的可用磁盘空间。将警报作为安全控制添加到Systems Manager Automation任务中。 E. 创建AWS Lambda函数，通过评估每个EC2实例各自的Amazon CloudWatch日志流来定期检查所有EC2实例上的充足可用磁盘空间。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求解决EC2实例在Systems Manager Automation补丁过程中因磁盘空间不足而失败的问题，需要选择两个步骤来确保未来补丁过程中有足够的磁盘空间。 **涉及的关键AWS服务和概念：** - AWS Systems Manager Automation：自动化运维任务执行 - Amazon CloudWatch：监控和告警服务 - CloudWatch Agent：收集系统级指标的代理 - CloudWatch Alarms：基于指标的告警机制 - EC2实例监控：系统资源监控 **正确答案的原因：** 题目显示正确答案是A，但根据最佳实践，应该选择A和D的组合： A选项正确因为：CloudWatch Agent是收集磁盘空间等系统级指标的必要组件，它能够将磁盘使用率指标发送到CloudWatch，为后续监控和告警提供数据基础。 D选项应该也是正确的因为：创建CloudWatch告警来监控磁盘空间，并将其作为Systems Manager Automation任务的安全控制，可以在磁盘空间不足时阻止补丁操作，直接解决问题根源。 **其他选项错误的原因：** B选项：虽然定期清理临时文件是好习惯，但这是被动措施，无法保证补丁过程中有足够空间。 C选项：手动写入日志流的方式过于复杂且不标准，CloudWatch Agent已经提供了标准的指标收集方式。 E选项：通过Lambda读取日志流来检查磁盘空间的架构过于复杂，且无法与Systems Manager Automation任务集成作为安全控制。 **决策标准和最佳实践：** 1. 使用AWS原生监控解决方案（CloudWatch Agent + CloudWatch Alarms） 2. 实施预防性控制而非被动响应 3. 与现有自动化流程（Systems Manager）集成 4. 选择可扩展且易维护的解决方案 5. 遵循AWS Well-Architected Framework的可靠性原则</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">211</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is building an application that uses an AWS Lambda function to query an Amazon Aurora MySQL DB cluster. The Lambda function performs only read queries. Amazon EventBridge events invoke the Lambda function. As more events invoke the Lambda function each second, the database&#x27;s latency increases and the database&#x27;s throughput decreases. The DevOps engineer needs to improve the performance of the application. Which combination of steps will meet these requirements? (Choose three.) F. Connect to the Aurora cluster endpoint from the Lambda function. ACE (80%) ADE (20%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Amazon RDS Proxy to create a proxy. Connect the proxy to the Aurora cluster reader endpoint. Set a maximum connections percentage on the proxy.
B. Implement database connection pooling inside the Lambda code. Set a maximum number of connections on the database connection pool.
C. Implement the database connection opening outside the Lambda event handler code.
D. Implement the database connection opening and closing inside the Lambda event handler code.
E. Connect to the proxy endpoint from the Lambda function.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps工程师正在构建一个应用程序，该应用程序使用AWS Lambda函数来查询Amazon Aurora MySQL数据库集群。Lambda函数只执行读取查询。Amazon EventBridge事件调用Lambda函数。随着每秒调用Lambda函数的事件增多，数据库的延迟增加，数据库的吞吐量下降。DevOps工程师需要改善应用程序的性能。哪种步骤组合将满足这些要求？（选择三个。）F. 从Lambda函数连接到Aurora集群端点。ACE (80%) ADE (20%) 选项：A. 使用Amazon RDS Proxy创建一个代理。将代理连接到Aurora集群的reader端点。在代理上设置最大连接百分比。 B. 在Lambda代码内部实现数据库连接池。在数据库连接池上设置最大连接数。 C. 在Lambda事件处理程序代码之外实现数据库连接打开。 D. 在Lambda事件处理程序代码内部实现数据库连接的打开和关闭。 E. 从Lambda函数连接到代理端点。 正确答案：A</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何优化Lambda函数访问Aurora数据库时的性能问题。随着并发调用增加，出现了数据库延迟增加、吞吐量下降的问题，需要找到合适的解决方案来改善性能。 **涉及的关键AWS服务和概念：** 1. AWS Lambda - 无服务器计算服务，存在连接管理挑战 2. Amazon Aurora MySQL - 托管关系数据库服务 3. Amazon RDS Proxy - 数据库代理服务，专门解决连接池问题 4. 数据库连接池 - 管理和复用数据库连接的技术 5. Lambda生命周期 - 包括初始化阶段和事件处理阶段 **正确答案的原因：** 根据题目显示的统计数据，ACE组合占80%，这意味着： - A选项：使用RDS Proxy是解决Lambda大量并发连接数据库的最佳实践，可以有效管理连接池，减少数据库连接开销 - C选项：将数据库连接放在Lambda处理程序外部（初始化阶段）可以实现连接复用，避免每次调用都重新建立连接 - E选项：连接到代理端点而不是直接连接数据库，这与使用RDS Proxy的策略一致 **其他选项错误的原因：** - B选项：在Lambda内部实现连接池效果有限，因为Lambda的无状态特性使得连接池难以在多个调用间有效共享 - D选项：在事件处理程序内部打开和关闭连接是性能最差的做法，每次调用都会产生连接开销 - F选项：直接连接Aurora集群端点没有解决连接管理问题，仍会面临大量并发连接的性能瓶颈 **决策标准和最佳实践：** 1. 使用RDS Proxy来管理Lambda到数据库的连接，这是AWS推荐的最佳实践 2. 将数据库连接初始化放在Lambda处理程序外部，利用Lambda的执行环境复用特性 3. 避免在每次函数调用时都建立新的数据库连接 4. 对于只读查询，合理配置连接池大小和超时设置</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">212</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an AWS CloudFormation stack that is deployed in a single AWS account. The company has configured the stack to send event notifications to an Amazon Simple Notification Service (Amazon SNS) topic. A DevOps engineer must implement an automated solution that applies a tag to the specific CloudFormation stack instance only after a successful stack update occurs. The DevOps engineer has created an AWS Lambda function that applies and updates this tag for the specific stack instance. Which solution will meet these requirements? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Run the AWS-UpdateCloudFormationStack AWS Systems Manager Automation runbook when Systems Manager detects an UPDATE_COMPLETE event for the instance status of the CloudFormation stack. Configure the runbook to invoke the Lambda function.
B. Create a custom AWS Config rule that produces a compliance change event if the CloudFormation stack has an UPDATE_COMPLETE instance status. Configure AWS Config to directly invoke the Lambda function to automatically remediate the change event.
C. Create an Amazon EventBridge rule that matches the UPDATE_COMPLETE event pattern for the instance status of the CloudFormation stack. Configure the rule to invoke the Lambda function.
D. Adjust the configuration of the CloudFormation stack to send notifications for only an UPDATE_COMPLETE instance status event to the SNS topic. Subscribe the Lambda function to the SNS topic.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在单个AWS账户中部署了一个AWS CloudFormation堆栈。该公司已配置堆栈将事件通知发送到Amazon Simple Notification Service (Amazon SNS)主题。DevOps工程师必须实现一个自动化解决方案，仅在成功的堆栈更新发生后，为特定的CloudFormation堆栈实例应用标签。DevOps工程师已创建了一个AWS Lambda函数，用于为特定堆栈实例应用和更新此标签。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要实现一个自动化解决方案，在CloudFormation堆栈成功更新后自动为该堆栈实例添加标签。关键条件是&quot;仅在成功的堆栈更新发生后&quot;，即需要监听UPDATE_COMPLETE事件。 **涉及的关键AWS服务和概念：** - AWS CloudFormation：基础设施即代码服务，会产生堆栈状态变更事件 - Amazon EventBridge：事件驱动架构的核心服务，可以监听和路由AWS服务事件 - AWS Lambda：无服务器计算服务，用于执行标签应用逻辑 - Amazon SNS：消息通知服务 - AWS Systems Manager：运维管理服务 - AWS Config：配置管理和合规性监控服务 **正确答案C的原因：** - EventBridge是AWS原生的事件路由服务，专门设计用于处理AWS服务产生的状态变更事件 - CloudFormation会自动将堆栈状态变更事件发送到EventBridge - 可以精确匹配UPDATE_COMPLETE事件模式，确保只在成功更新后触发 - 直接调用Lambda函数，架构简单高效 - 无需额外配置，CloudFormation事件默认就会发送到EventBridge **其他选项错误的原因：** - 选项A：Systems Manager Automation主要用于运维自动化任务，不是专门为事件驱动设计的，且AWS-UpdateCloudFormationStack runbook是用于执行更新而非监听更新完成事件 - 选项B：AWS Config主要用于配置合规性检查，虽然可以检测资源状态变化，但不是专门为CloudFormation堆栈状态事件设计的，架构过于复杂 - 选项D：虽然技术上可行，但需要修改现有CloudFormation配置，且通过SNS增加了不必要的中间层，不如直接使用EventBridge高效 **决策标准和最佳实践：** - 选择专门为事件驱动架构设计的服务（EventBridge） - 优先使用AWS原生集成，减少配置复杂性 - 避免不必要的中间层，保持架构简洁 - 选择最直接、最高效的事件处理路径 - 考虑解决方案的可维护性和扩展性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">213</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company deploys an application to two AWS Regions. The application creates and stores objects in an Amazon S3 bucket that is in the same Region as the application. Both deployments of the application need to have access to all the objects and their metadata from both Regions. The company has configured two-way replication between the S3 buckets and has enabled S3 Replication metrics on each S3 bucket. A DevOps engineer needs to implement a solution that retries the replication process if an object fails to replicate. Which solution will meet these requirements? downloads the failed replication object and then runs a PutObject command for the object to the destination bucket. D (78%) A (22%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule that listens to S3 event notifications for failed replication events. Create an AWS Lambda function that downloads the failed replication object and then runs a PutObject command for the object to the destination bucket. Configure the EventBridge rule to invoke the Lambda function to handle the object that failed to replicate.
B. Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure S3 event notifications to send failed replication notifications to the SQS queue. Create an AWS Lambda function that downloads the failed replication object and then runs a PutObject command for the object to the destination bucket. Configure the Lambda function to poll the queue for notifications to process.
C. Create an Amazon EventBridge rule that listens to S3 event notifications for failed replications. Create an AWS Lambda function that
D. Create an AWS Lambda function that will use S3 batch operations to retry the replication on the existing object for a failed replication. Configure S3 event notifications to send failed replication notifications to the Lambda function.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在两个AWS Region部署应用程序。应用程序在与其相同Region的Amazon S3 bucket中创建和存储对象。两个部署的应用程序都需要访问来自两个Region的所有对象及其元数据。公司已在S3 bucket之间配置了双向复制，并在每个S3 bucket上启用了S3 Replication指标。DevOps工程师需要实现一个解决方案，当对象复制失败时重试复制过程。哪个解决方案能满足这些要求？ 选项： A. 创建一个Amazon EventBridge规则监听S3事件通知中的复制失败事件。创建一个AWS Lambda函数下载复制失败的对象，然后对该对象运行PutObject命令到目标bucket。配置EventBridge规则调用Lambda函数处理复制失败的对象。 B. 创建一个Amazon Simple Queue Service (Amazon SQS)队列。配置S3事件通知将复制失败通知发送到SQS队列。创建一个AWS Lambda函数下载复制失败的对象，然后对该对象运行PutObject命令到目标bucket。配置Lambda函数轮询队列获取要处理的通知。 C. 创建一个Amazon EventBridge规则监听S3事件通知中的复制失败。创建一个AWS Lambda函数... D. 创建一个AWS Lambda函数使用S3批处理操作对现有对象的失败复制进行重试。配置S3事件通知将复制失败通知发送到Lambda函数。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要实现一个自动重试机制来处理S3跨Region复制失败的对象，确保两个Region的应用程序都能访问所有对象和元数据。 **涉及的关键AWS服务和概念：** - S3 Cross-Region Replication (CRR)：跨Region复制 - S3 Event Notifications：S3事件通知 - Amazon SQS：简单队列服务，提供可靠的消息传递 - Amazon EventBridge：事件总线服务 - AWS Lambda：无服务器计算服务 - S3 Batch Operations：S3批处理操作 **正确答案B的原因：** 1. **可靠性保证**：SQS提供消息持久化和至少一次传递保证，确保复制失败事件不会丢失 2. **解耦架构**：SQS作为缓冲层，将事件生产者(S3)和消费者(Lambda)解耦 3. **错误处理**：SQS支持死信队列(DLQ)和消息重试机制，提供更好的错误处理能力 4. **可扩展性**：Lambda可以根据队列中的消息数量自动扩展 5. **成本效益**：SQS按使用量付费，成本较低 **其他选项错误的原因：** - **选项A**：EventBridge虽然功能强大，但对于简单的重试场景过于复杂，且缺少SQS提供的消息持久化和重试机制 - **选项C**：描述不完整，无法评估完整解决方案 - **选项D**：S3 Batch Operations主要用于大规模批处理操作，不适合实时处理单个复制失败事件，且响应延迟较高 **决策标准和最佳实践：** 1. **事件驱动架构**：使用SQS+Lambda模式是AWS推荐的事件处理最佳实践 2. **容错性**：选择具有内置重试和错误处理机制的服务 3. **简单性原则**：选择最简单有效的解决方案，避免过度工程化 4. **成本优化**：考虑服务的定价模型和实际使用场景的匹配度</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">214</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company needs to implement failover for its application. The application includes an Amazon CloudFront distribution and a public Application Load Balancer (ALB) in an AWS Region. The company has configured the ALB as the default origin for the distribution. After some recent application outages, the company wants a zero-second RTO. The company deploys the application to a secondary Region in a warm standby configuration. A DevOps engineer needs to automate the failover of the application to the secondary Region so that HTTP GET requests meet the desired RTO. Which solution will meet these requirements? Configure the origin group to fail over for HTTP 5xx status codes. Update the default behavior to use the origin group. Most Voted B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a second CloudFront distribution that has the secondary ALB as the default origin. Create Amazon Route 53 alias records that have a failover policy and Evaluate Target Health set to Yes for both CloudFront distributions. Update the application to use the new record set.
B. Create a new origin on the distribution for the secondary ALB. Create a new origin group. Set the original ALB as the primary origin.
C. Create Amazon Route 53 alias records that have a failover policy and Evaluate Target Health set to Yes for both ALBs. Set the TTL of both records to 0. Update the distribution&#x27;s origin to use the new record set.
D. Create a CloudFront function that detects HTTP 5xx status codes. Configure the function to return a 307 Temporary Redirect error response to the secondary ALB if the function detects 5xx status codes. Update the distribution&#x27;s default behavior to send origin responses to the function.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司需要为其应用程序实施故障转移。该应用程序包括一个Amazon CloudFront分发和一个位于AWS区域中的公共Application Load Balancer (ALB)。公司已将ALB配置为分发的默认源站。在最近发生一些应用程序中断后，公司希望实现零秒RTO。公司在辅助区域以温备用配置部署了应用程序。DevOps工程师需要自动化应用程序到辅助区域的故障转移，以便HTTP GET请求满足所需的RTO。哪个解决方案能满足这些要求？ 选项： A. 创建第二个CloudFront分发，将辅助ALB作为默认源站。创建Amazon Route 53别名记录，为两个CloudFront分发设置故障转移策略并将Evaluate Target Health设置为Yes。更新应用程序以使用新记录集。 B. 在分发上为辅助ALB创建新源站。创建新的源站组。将原始ALB设置为主源站。配置源站组在HTTP 5xx状态码时进行故障转移。更新默认行为以使用源站组。 C. 创建Amazon Route 53别名记录，为两个ALB设置故障转移策略并将Evaluate Target Health设置为Yes。将两个记录的TTL设置为0。更新分发的源站以使用新记录集。 D. 创建CloudFront函数来检测HTTP 5xx状态码。配置函数在检测到5xx状态码时返回307临时重定向错误响应到辅助ALB。更新分发的默认行为以将源站响应发送到函数。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现应用程序的自动故障转移，关键需求是实现零秒RTO（Recovery Time Objective），即故障恢复时间目标为零秒。应用架构包含CloudFront分发和ALB，需要在主区域故障时自动切换到辅助区域。 **涉及的关键AWS服务和概念：** - CloudFront：全球内容分发网络，支持源站组功能 - Application Load Balancer (ALB)：应用层负载均衡器 - CloudFront Origin Groups：源站组功能，支持自动故障转移 - Route 53：DNS服务，支持健康检查和故障转移路由策略 - RTO：恢复时间目标，衡量故障恢复速度的指标 **正确答案B的原因：** 1. **CloudFront Origin Groups提供最快的故障转移**：当主源站返回5xx错误时，CloudFront会立即切换到备用源站，实现接近零秒的RTO 2. **无需DNS传播延迟**：不依赖DNS记录更新，避免了DNS缓存和传播带来的延迟 3. **自动化程度高**：CloudFront自动检测源站健康状况并执行故障转移，无需外部干预 4. **架构简单**：在现有CloudFront分发基础上添加源站组，改动最小 **其他选项错误的原因：** - **选项A**：创建两个CloudFront分发增加了复杂性，Route 53的DNS故障转移存在DNS传播延迟，无法实现零秒RTO - **选项C**：虽然TTL设为0，但仍然存在DNS查询和解析的延迟，且将DNS记录指向ALB而非CloudFront会失去CDN的优势 - **选项D**：CloudFront函数的307重定向会增加额外的HTTP往返时间，无法满足零秒RTO要求，且用户体验较差 **决策标准和最佳实践：** 1. **RTO优先**：选择能提供最快故障转移的方案 2. **简化架构**：在满足需求的前提下保持架构简单 3. **自动化**：优先选择自动检测和切换的解决方案 4. **利用CloudFront优势**：充分利用CloudFront的边缘位置和缓存能力 5. **避免DNS依赖**：在需要快速故障转移时，尽量避免依赖DNS更新</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">215</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A cloud team uses AWS Organizations and AWS IAM Identity Center (AWS Single Sign-On) to manage a company&#x27;s AWS accounts. The company recently established a research team. The research team requires the ability to fully manage the resources in its account. The research team must not be able to create IAM users. The cloud team creates a Research Administrator permission set in IAM Identity Center for the research team. The permission set has the AdministratorAccess AWS managed policy attached. The cloud team must ensure that no one on the research team can create IAM users. Which solution will meet these requirements? C (69%) A (31%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an IAM policy that denies the iam:CreateUser action. Attach the IAM policy to the Research Administrator permission set.
B. Create an IAM policy that allows all actions except the iam:CreateUser action. Use the IAM policy to set the permissions boundary for the Research Administrator permission set.
C. Create an SCP that denies the iam:CreateUser action. Attach the SCP to the research team&#x27;s AWS account.
D. Create an AWS Lambda function that deletes IAM users. Create an Amazon EventBridge rule that detects the IAM CreateUser event. Configure the rule to invoke the Lambda function.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个云团队使用AWS Organizations和AWS IAM Identity Center (AWS Single Sign-On)来管理公司的AWS账户。公司最近成立了一个研究团队。研究团队需要能够完全管理其账户中的资源。研究团队不能创建IAM用户。云团队在IAM Identity Center中为研究团队创建了一个Research Administrator权限集。该权限集附加了AdministratorAccess AWS托管策略。云团队必须确保研究团队中的任何人都不能创建IAM用户。哪个解决方案能满足这些要求？ 选项： A. 创建一个拒绝iam:CreateUser操作的IAM策略。将该IAM策略附加到Research Administrator权限集。 B. 创建一个允许除iam:CreateUser操作之外的所有操作的IAM策略。使用该IAM策略为Research Administrator权限集设置权限边界。 C. 创建一个拒绝iam:CreateUser操作的SCP。将SCP附加到研究团队的AWS账户。 D. 创建一个删除IAM用户的AWS Lambda函数。创建一个检测IAM CreateUser事件的Amazon EventBridge规则。配置规则调用Lambda函数。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要为研究团队提供完全的管理权限（AdministratorAccess），但同时必须阻止他们创建IAM用户。这是一个典型的权限管理场景，需要在给予广泛权限的同时实施特定限制。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS IAM Identity Center：集中身份管理 - Service Control Policies (SCP)：组织级别的权限控制 - IAM权限集：在Identity Center中定义的权限模板 - 权限边界：限制用户或角色最大权限的机制 **正确答案C的原因：** 1. **层级控制优势**：SCP在组织层面工作，优先级高于IAM策略，即使用户有AdministratorAccess，SCP的拒绝规则仍然生效 2. **无法绕过**：SCP的限制无法通过任何IAM权限覆盖，提供了最强的控制力 3. **账户级别应用**：SCP附加到整个AWS账户，确保账户内所有用户和角色都受到限制 4. **架构匹配**：题目明确提到使用AWS Organizations，SCP是Organizations的核心功能 **其他选项错误的原因：** - **选项A错误**：AdministratorAccess是AWS托管策略，无法直接修改。即使能附加额外的拒绝策略，在权限集的上下文中可能不会按预期工作 - **选项B错误**：权限边界主要用于限制用户能获得的最大权限，但在这种场景下，与已有的AdministratorAccess策略配合使用会很复杂，且不是最佳实践 - **选项D错误**：这是被动的事后处理方案，无法阻止创建行为本身，不符合&quot;必须确保不能创建&quot;的要求，且增加了不必要的复杂性 **决策标准和最佳实践：** 1. **预防优于补救**：应该阻止不当操作发生，而不是事后清理 2. **最小权限原则**：在给予必要权限的同时，明确限制不需要的权限 3. **使用适当的控制层级**：对于组织级别的限制，SCP是最合适的工具 4. **简单有效**：选择最直接、最可靠的解决方案，避免过度复杂的架构</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">216</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company releases a new application in a new AWS account. The application includes an AWS Lambda function that processes messages from an Amazon Simple Queue Service (Amazon SQS) standard queue. The Lambda function stores the results in an Amazon S3 bucket for further downstream processing. The Lambda function needs to process the messages within a specific period of time after the messages are published. The Lambda function has a batch size of 10 messages and takes a few seconds to process a batch of messages. As load increases on the application&#x27;s first day of service, messages in the queue accumulate at a greater rate than the Lambda function can process the messages. Some messages miss the required processing timelines. The logs show that many messages in the queue have data that is not valid. The company needs to meet the timeline requirements for messages that have valid data. Which solution will meet these requirements? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Increase the Lambda function&#x27;s batch size. Change the SQS standard queue to an SQS FIFO queue. Request a Lambda concurrency increase in the AWS Region.
B. Reduce the Lambda function&#x27;s batch size. Increase the SQS message throughput quota. Request a Lambda concurrency increase in the AWS Region.
C. Increase the Lambda function&#x27;s batch size. Configure S3 Transfer Acceleration on the S3 bucket. Configure an SQS dead-letter queue.
D. Keep the Lambda function&#x27;s batch size the same. Configure the Lambda function to report failed batch items. Configure an SQS dead-letter queue.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在新的AWS账户中发布了一个新应用程序。该应用程序包含一个AWS Lambda函数，用于处理来自Amazon Simple Queue Service (Amazon SQS)标准队列的消息。Lambda函数将结果存储在Amazon S3存储桶中以供进一步的下游处理。Lambda函数需要在消息发布后的特定时间段内处理消息。Lambda函数的批处理大小为10条消息，处理一批消息需要几秒钟时间。随着应用程序服务第一天负载的增加，队列中的消息积累速度超过了Lambda函数处理消息的速度。一些消息错过了所需的处理时间线。日志显示队列中的许多消息包含无效数据。公司需要满足具有有效数据的消息的时间线要求。哪种解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 消息处理速度跟不上消息产生速度，导致积压 - 许多消息包含无效数据，影响整体处理效率 - 需要确保有效数据的消息能在规定时间内处理完成 - 需要优化系统以处理无效消息，避免影响有效消息的处理 **涉及的关键AWS服务和概念：** - Amazon SQS：消息队列服务，用于解耦应用组件 - AWS Lambda：无服务器计算服务，批处理消息 - SQS Dead Letter Queue (DLQ)：用于处理失败消息的机制 - Lambda批处理失败报告：允许部分消息成功，部分消息失败 - Lambda并发限制：影响处理吞吐量的关键因素 **正确答案D的原因：** 1. **保持批处理大小不变**：当前批处理大小10是合理的，无需调整 2. **配置Lambda报告失败的批处理项**：这是关键功能，允许Lambda函数将批次中的无效消息标记为失败，而让有效消息继续处理，避免整个批次因个别无效消息而失败 3. **配置SQS Dead Letter Queue**：将处理失败的无效消息转移到DLQ，防止它们重复消费资源，让Lambda专注处理有效消息 4. **解决根本问题**：通过隔离无效消息，提高了有效消息的处理效率 **其他选项错误的原因：** - **选项A错误**：增加批处理大小会让无效消息影响更多有效消息；FIFO队列会降低吞吐量；没有解决无效消息问题 - **选项B错误**：减少批处理大小会增加Lambda调用次数但不解决无效消息问题；SQS吞吐量配额通常不是瓶颈 - **选项C错误**：S3 Transfer Acceleration对Lambda写入S3的性能提升有限；增加批处理大小会加重无效消息的影响；虽然有DLQ但缺少失败项报告机制 **决策标准和最佳实践：** 1. **优先解决数据质量问题**：通过DLQ和失败项报告机制隔离无效消息 2. **避免无效消息影响有效消息**：使用部分批处理失败功能 3. **渐进式优化**：先解决数据质量问题，再考虑扩容 4. **成本效益考虑**：避免不必要的架构变更（如FIFO队列）和资源浪费</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">217</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that runs on AWS Lambda and sends logs to Amazon CloudWatch Logs. An Amazon Kinesis data stream is subscribed to the log groups in CloudWatch Logs. A single consumer Lambda function processes the logs from the data stream and stores the logs in an Amazon S3 bucket. The company&#x27;s DevOps team has noticed high latency during the processing and ingestion of some logs. Which combination of steps will reduce the latency? (Choose three.) F. Increase the number of shards in the Kinesis data stream. Most Voted ABF (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a data stream consumer with enhanced fan-out. Set the Lambda function that processes the logs as the consumer.
B. Increase the ParallelizationFactor setting in the Lambda event source mapping.
C. Configure reserved concurrency for the Lambda function that processes the logs.
D. Increase the batch size in the Kinesis data stream.
E. Turn off the ReportBatchItemFailures setting in the Lambda event source mapping.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个运行在AWS Lambda上的应用程序，它将日志发送到Amazon CloudWatch Logs。一个Amazon Kinesis data stream订阅了CloudWatch Logs中的日志组。单个消费者Lambda函数处理来自data stream的日志并将日志存储在Amazon S3存储桶中。公司的DevOps团队注意到在处理和摄取某些日志时存在高延迟。哪些步骤的组合将减少延迟？（选择三个。） 选项： A. 创建一个具有enhanced fan-out的data stream consumer。将处理日志的Lambda函数设置为consumer。 B. 增加Lambda event source mapping中的ParallelizationFactor设置。 C. 为处理日志的Lambda函数配置reserved concurrency。 D. 增加Kinesis data stream中的batch size。 E. 关闭Lambda event source mapping中的ReportBatchItemFailures设置。 F. 增加Kinesis data stream中的shard数量。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查如何优化基于Kinesis Data Streams和Lambda的日志处理架构，重点是减少处理延迟。需要选择三个能够有效降低延迟的方案。 **涉及的关键AWS服务和概念：** 1. Amazon Kinesis Data Streams - 实时数据流服务 2. AWS Lambda - 无服务器计算服务 3. Enhanced Fan-out - Kinesis的专用吞吐量功能 4. ParallelizationFactor - Lambda并行处理参数 5. Reserved Concurrency - Lambda预留并发设置 6. Shard - Kinesis数据流的分区单位 **正确答案分析（A、B、F）：** **选项A正确：** Enhanced fan-out为每个consumer提供专用的2MB/s吞吐量，避免了多个consumer之间的竞争，显著减少延迟。传统的shared fan-out模式下所有consumer共享shard的2MB/s带宽。 **选项B正确：** ParallelizationFactor允许Lambda函数并行处理同一个shard的多个批次，提高处理效率，减少整体延迟。 **选项F正确：** 增加shard数量可以提高整体吞吐量和并行处理能力，每个shard可以独立处理数据，从而减少延迟。 **错误选项分析：** **选项C错误：** Reserved concurrency主要用于限制函数的最大并发数，防止函数消耗过多资源，但不会直接减少处理延迟，反而可能在高负载时限制性能。 **选项D错误：** 增加batch size会让Lambda函数等待更多记录才开始处理，这会增加延迟而不是减少延迟。 **选项E错误：** 关闭ReportBatchItemFailures会影响错误处理机制，与减少延迟无关，且可能导致数据处理问题。 **决策标准和最佳实践：** 1. 优先考虑提高并行处理能力的方案（增加shard、并行化因子） 2. 使用专用资源避免竞争（enhanced fan-out） 3. 避免增加等待时间的配置（大batch size） 4. 保持良好的错误处理机制 5. 合理配置并发限制，避免过度限制性能</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">218</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company operates sensitive workloads across the AWS accounts that are in the company&#x27;s organization in AWS Organizations. The company uses an IP address range to delegate IP addresses for Amazon VPC CIDR blocks and all non-cloud hardware. The company needs a solution that prevents principals that are outside the company&#x27;s IP address range from performing AWS actions in the organization&#x27;s accounts. Which solution will meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure AWS Firewall Manager for the organization. Create an AWS Network Firewall policy that allows only source traffic from the company&#x27;s IP address range. Set the policy scope to all accounts in the organization.
B. In Organizations, create an SCP that denies source IP addresses that are outside of the company&#x27;s IP address range. Attach the SCP to the organization&#x27;s root.
C. Configure Amazon GuardDuty for the organization. Create a GuardDuty trusted IP address list for the company&#x27;s IP range. Activate the trusted IP list for the organization.
D. In Organizations, create an SCP that allows source IP addresses that are inside of the company&#x27;s IP address range. Attach the SCP to the organization&#x27;s root.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中的组织账户中运行敏感工作负载。该公司使用一个IP地址范围来为Amazon VPC CIDR块和所有非云硬件分配IP地址。公司需要一个解决方案，防止公司IP地址范围之外的主体在组织账户中执行AWS操作。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现基于源IP地址的访问控制，确保只有来自公司指定IP地址范围内的请求才能在AWS Organizations的所有账户中执行AWS操作。这是一个典型的网络安全边界控制需求。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - Service Control Policy (SCP)：服务控制策略，用于在组织层面设置权限边界 - IP地址条件控制：基于aws:SourceIp条件键的访问控制 - AWS Firewall Manager、Network Firewall、GuardDuty：网络安全相关服务 **正确答案B的原因：** SCP是在AWS Organizations中实现跨账户访问控制的正确工具。通过创建一个拒绝(Deny)策略，明确禁止来自公司IP范围之外的源IP地址执行AWS操作，这种&quot;默认拒绝&quot;的方式更加安全可靠。SCP附加到组织根部后，会自动应用到所有成员账户，实现统一的访问控制。 **其他选项错误的原因：** - 选项A：AWS Firewall Manager和Network Firewall主要用于VPC网络流量过滤，而不是AWS API调用的访问控制，无法阻止来自外部IP的AWS管理操作。 - 选项C：GuardDuty是威胁检测服务，trusted IP list只是减少误报，不能主动阻止访问，仅起到监控告警作用。 - 选项D：使用允许(Allow)策略存在安全风险，因为SCP的Allow不能覆盖其他策略的Deny，且容易产生权限配置漏洞。 **决策标准和最佳实践：** 1. 选择明确拒绝(Explicit Deny)而非允许策略，遵循最小权限原则 2. 使用SCP在组织层面实现统一的安全边界控制 3. 基于aws:SourceIp条件键实现精确的IP地址访问控制 4. 确保解决方案能够覆盖所有AWS API调用，而不仅仅是网络流量</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">219</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company deploys an application in two AWS Regions. The application currently uses an Amazon S3 bucket in the primary Region to store data. A DevOps engineer needs to ensure that the application is highly available in both Regions. The DevOps engineer has created a new S3 bucket in the secondary Region. All existing and new objects must be in both S3 buckets. The application must fail over between the Regions with no data loss. Which combination of steps will meet these requirements with the MOST operational efficiency? (Choose three.) Bash command to use the AWS CLI to synchronize the contents of the source S3 bucket and the target S3 bucket F. Create an operation in S3 Batch Operations to replicate the contents of the source S3 bucket to the target S3 bucket. Configure the operation to use the IAM role for Amazon S3. Most Voted Most Voted ADF (59%) ACF (41%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new IAM role that allows the Amazon S3 and S3 Batch Operations service principals to assume the role that has the necessary permissions for S3 replication.
B. Create a new IAM role that allows the AWS Batch service principal to assume the role that has the necessary permissions for S3 replication.
C. Create an S3 Cross-Region Replication (CRR) rule on the source S3 bucket. Configure the rule to use the IAM role for Amazon S3 to replicate to the target S3 bucket.
D. Create a two-way replication rule on the source S3 bucket. Configure the rule to use the IAM role for Amazon S3 to replicate to the target S3 bucket.
E. Create an AWS Batch job that has an AWS Fargate orchestration type. Configure the job to use the IAM role for AWS Batch. Specify a</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在两个AWS Region中部署应用程序。该应用程序目前使用主Region中的Amazon S3 bucket来存储数据。DevOps工程师需要确保应用程序在两个Region中都具有高可用性。DevOps工程师已在辅助Region中创建了一个新的S3 bucket。所有现有和新的对象都必须存在于两个S3 bucket中。应用程序必须能够在Region之间进行故障转移且不丢失数据。哪种步骤组合能够以最高的运营效率满足这些要求？（选择三个） 选项： A. 创建一个新的IAM role，允许Amazon S3和S3 Batch Operations服务主体承担该角色，该角色具有S3复制的必要权限。 B. 创建一个新的IAM role，允许AWS Batch服务主体承担该角色，该角色具有S3复制的必要权限。 C. 在源S3 bucket上创建S3 Cross-Region Replication (CRR)规则。配置规则使用Amazon S3的IAM role复制到目标S3 bucket。 D. 在源S3 bucket上创建双向复制规则。配置规则使用Amazon S3的IAM role复制到目标S3 bucket。 E. 创建一个具有AWS Fargate编排类型的AWS Batch作业。配置作业使用AWS Batch的IAM role。 F. 在S3 Batch Operations中创建操作以将源S3 bucket的内容复制到目标S3 bucket。配置操作使用Amazon S3的IAM role。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现跨Region的S3数据高可用性方案，包括：1）现有数据和新数据都要在两个Region的S3 bucket中存在；2）应用程序能够无数据丢失地在Region间故障转移；3）以最高运营效率实现。 **涉及的关键AWS服务和概念：** - S3 Cross-Region Replication (CRR)：跨Region复制服务 - S3 Batch Operations：批量处理现有S3对象的服务 - IAM roles和服务主体：权限管理 - 高可用性和灾难恢复架构设计 **正确答案组合应该是ADF或ACF：** **选项A正确的原因：** 创建支持Amazon S3和S3 Batch Operations服务主体的IAM role是必需的，因为需要给这两个服务适当的权限来执行跨Region复制操作。这是实现整个方案的权限基础。 **选项C正确的原因：** S3 Cross-Region Replication是处理新对象自动复制的标准和最效率的方法。一旦配置，所有新上传的对象都会自动复制到目标Region，无需人工干预。 **选项F正确的原因：** S3 Batch Operations是处理现有对象批量复制的最佳选择，比手动AWS CLI同步更加可靠和可管理，支持大规模操作和进度跟踪。 **其他选项错误的原因：** - 选项B：AWS Batch服务主体不适用于S3复制场景，这是用于计算任务的服务 - 选项D：S3原生不支持&quot;双向复制规则&quot;这种配置，需要分别在两个bucket上配置CRR - 选项E：使用AWS Batch和Fargate过于复杂，不如直接使用S3原生复制功能效率高 **决策标准和最佳实践：** 1. 优先使用AWS原生服务功能而非自定义解决方案 2. 区分处理现有数据（批量操作）和新数据（自动复制）的不同需求 3. 确保权限配置支持所需的服务操作 4. 选择运营开销最小、自动化程度最高的方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">220</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an organization in AWS Organizations to manage multiple AWS accounts. The company needs an automated process across all AWS accounts to isolate any compromised Amazon EC2 instances when the instances receive a specific tag. Which combination of steps will meet these requirements? (Choose two.) have a security group that has an explicit Deny rule on all traffic. Use the CloudFormation template to create an AWS Lambda function that attaches the IAM role to instances. Configure the Lambda function to add a network ACL. Set up an Amazon EventBridge rule to invoke the Lambda function when a specific tag is applied to a compromised EC2 instance. AE (67%) BC (33%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CloudFormation StackSets to deploy the CloudFormation stacks in all AWS accounts.
B. Create an SCP that has a Deny statement for the ec2:* action with a condition of &quot;aws:RequestTag/isolation&quot;: false.
C. Attach the SCP to the root of the organization.
D. Create an AWS CloudFormation template that creates an EC2 instance role that has no IAM policies attached. Configure the template to
E. Create an AWS CloudFormation template that creates an EC2 instance role that has no IAM policies attached. Configure the template to have a security group that has no inbound rules or outbound rules. Use the CloudFormation template to create an AWS Lambda function that attaches the IAM role to instances. Configure the Lambda function to replace any existing security groups with the new security group. Set up an Amazon EventBridge rule to invoke the Lambda function when a specific tag is applied to a compromised EC2 instance.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations中的组织来管理多个AWS账户。该公司需要一个跨所有AWS账户的自动化流程，当Amazon EC2实例收到特定标签时能够隔离任何被入侵的实例。哪种步骤组合能够满足这些要求？（选择两个。） 选项： A. 使用AWS CloudFormation StackSets在所有AWS账户中部署CloudFormation堆栈。 B. 创建一个SCP，对ec2:*操作有Deny语句，条件为&quot;aws:RequestTag/isolation&quot;: false。 C. 将SCP附加到组织的根部。 D. 创建一个AWS CloudFormation模板，创建一个没有附加IAM策略的EC2实例角色。配置模板具有对所有流量有显式Deny规则的安全组。使用CloudFormation模板创建AWS Lambda函数，将IAM角色附加到实例。配置Lambda函数添加网络ACL。设置Amazon EventBridge规则，在特定标签应用到被入侵的EC2实例时调用Lambda函数。 E. 创建一个AWS CloudFormation模板，创建一个没有附加IAM策略的EC2实例角色。配置模板具有没有入站规则或出站规则的安全组。使用CloudFormation模板创建AWS Lambda函数，将IAM角色附加到实例。配置Lambda函数用新安全组替换任何现有安全组。设置Amazon EventBridge规则，在特定标签应用到被入侵的EC2实例时调用Lambda函数。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要建立一个跨多个AWS账户的自动化系统，当EC2实例被标记为被入侵时能够自动隔离这些实例。这是一个典型的安全响应自动化场景。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理 - CloudFormation StackSets：跨账户资源部署 - Lambda：自动化执行逻辑 - EventBridge：事件驱动触发 - Security Groups：网络访问控制 - IAM角色：权限管理 - EC2标签：资源标识 **正确答案的原因：** 选项A（CloudFormation StackSets）是必需的，因为它是唯一能够在AWS Organizations管理的多个账户中统一部署基础设施的服务。要实现跨所有账户的自动化隔离，必须在每个账户中部署相同的Lambda函数、EventBridge规则和相关资源。 选项E提供了完整的隔离机制：创建限制性的安全组（无入站/出站规则）和受限的IAM角色，通过Lambda函数在检测到特定标签时自动应用这些限制，并通过EventBridge实现事件驱动的自动化。 **其他选项错误的原因：** - 选项B和C：SCP（Service Control Policy）主要用于预防性控制，限制账户中可以执行的操作，但不能实现对已存在实例的动态隔离响应。 - 选项D：提到了网络ACL但实现不完整，且网络ACL是子网级别的控制，不如安全组精确。 **决策标准和最佳实践：** 1. **跨账户部署**：使用StackSets确保所有账户具有一致的安全响应能力 2. **事件驱动架构**：使用EventBridge监听标签变化，实现实时响应 3. **最小权限原则**：使用无权限的IAM角色和无规则的安全组实现完全隔离 4. **自动化优先**：通过Lambda实现无人工干预的安全响应 这种解决方案体现了现代云安全的最佳实践：预部署响应机制、事件驱动自动化和一致的跨账户安全策略。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">221</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages multiple AWS accounts by using AWS Organizations with OUs for the different business divisions. The company is updating their corporate network to use new IP address ranges. The company has 10 Amazon S3 buckets in different AWS accounts. The S3 buckets store reports for the different divisions. The S3 bucket configurations allow only private corporate network IP addresses to access the S3 buckets. A DevOps engineer needs to change the range of IP addresses that have permission to access the contents of the S3 buckets. The DevOps engineer also needs to revoke the permissions of two OUs in the company. Which solution will meet these requirements? denies access to the old range of IP addresses for all the S3 buckets. Set a permissions boundary for the OrganizationAccountAccessRole role in the two OUs to deny access to the S3 buckets. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new SCP that has two statements, one that allows access to the new range of IP addresses for all the S3 buckets and one that
B. Create a new SCP that has a statement that allows only the new range of IP addresses to access the S3 buckets. Create another SCP that denies access to the S3 buckets. Attach the second SCP to the two OUs.
C. On all the S3 buckets, configure resource-based policies that allow only the new range of IP addresses to access the S3 buckets. Create a new SCP that denies access to the S3 buckets. Attach the SCP to the two OUs.
D. On all the S3 buckets, configure resource-based policies that allow only the new range of IP addresses to access the S3 buckets. Set a permissions boundary for the OrganizationAccountAccessRole role in the two OUs to deny access to the S3 buckets.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations和OU来管理不同业务部门的多个AWS账户。该公司正在更新其企业网络以使用新的IP地址范围。该公司在不同AWS账户中有10个Amazon S3存储桶，这些S3存储桶存储不同部门的报告。S3存储桶配置仅允许私有企业网络IP地址访问S3存储桶。DevOps工程师需要更改有权访问S3存储桶内容的IP地址范围，还需要撤销公司中两个OU的权限。哪种解决方案能满足这些要求？ 选项A：创建一个新的SCP，包含两个语句，一个允许所有S3存储桶的新IP地址范围访问，另一个拒绝所有S3存储桶的旧IP地址范围访问。为两个OU中的OrganizationAccountAccessRole角色设置权限边界以拒绝访问S3存储桶。 选项B：创建一个新的SCP，包含一个仅允许新IP地址范围访问S3存储桶的语句。创建另一个拒绝访问S3存储桶的SCP。将第二个SCP附加到两个OU。 选项C：在所有S3存储桶上配置基于资源的策略，仅允许新IP地址范围访问S3存储桶。创建一个新的拒绝访问S3存储桶的SCP。将SCP附加到两个OU。 选项D：在所有S3存储桶上配置基于资源的策略，仅允许新IP地址范围访问S3存储桶。为两个OU中的OrganizationAccountAccessRole角色设置权限边界以拒绝访问S3存储桶。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 更新S3存储桶的IP地址访问范围（从旧范围改为新范围） 2. 撤销两个特定OU对S3存储桶的访问权限 3. 需要管理跨多个AWS账户的10个S3存储桶 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - Service Control Policies (SCP)：组织级别的权限控制策略 - S3资源策略：存储桶级别的访问控制 - 权限边界：限制IAM实体最大权限的机制 - IP地址白名单：基于网络位置的访问控制 **正确答案C的原因：** 1. **S3资源策略处理IP范围更新**：直接在S3存储桶上配置资源策略是控制IP地址访问的最直接和有效方法，可以精确指定允许的IP范围 2. **SCP处理OU权限撤销**：SCP是在组织层面控制账户权限的标准机制，可以有效地拒绝特定OU对S3资源的访问 3. **策略组合的有效性**：资源策略控制&quot;谁可以访问&quot;，SCP控制&quot;谁不能访问&quot;，两者结合提供了完整的访问控制 **其他选项错误的原因：** - **选项A错误**：SCP无法直接控制IP地址范围访问，SCP主要用于控制API操作权限而非网络访问控制；权限边界的使用过于复杂且不是最佳实践 - **选项B错误**：同样地，SCP不适合处理IP地址范围控制，这应该通过S3资源策略来实现 - **选项D错误**：虽然S3资源策略部分正确，但使用权限边界来拒绝OU访问过于复杂，SCP是更直接和标准的解决方案 **决策标准和最佳实践：** 1. **使用正确的工具**：IP访问控制应使用S3资源策略，组织级权限控制应使用SCP 2. **简化管理**：选择最直接和易于管理的解决方案 3. **职责分离**：不同类型的访问控制使用不同的机制，避免功能重叠 4. **可扩展性**：解决方案应该易于维护和扩展到更多存储桶或OU</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">222</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has started using AWS across several teams. Each team has multiple accounts and unique security profiles. The company manages the accounts in an organization in AWS Organizations. Each account has its own configuration and security controls. The company&#x27;s DevOps team wants to use preventive and detective controls to govern all accounts. The DevOps team needs to ensure the security of accounts now and in the future as the company creates new accounts in the organization. Which solution will meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Organizations to create OUs that have appropriate SCPs attached for each team. Place team accounts in the appropriate OUs to apply security controls. Create any new team accounts in the appropriate OUs.
B. Create an AWS Control Tower landing zone. Configure OUs and appropriate controls in AWS Control Tower for the existing teams. Configure trusted access for AWS Control Tower. Enroll the existing accounts in the appropriate OUs that match the appropriate security policies for each team. Use AWS Control Tower to provision any new accounts.
C. Create AWS CloudFormation stack sets in the organization&#x27;s management account. Configure a stack set that deploys AWS Config with configuration rules and remediation actions for all controls to each account in the organization. Update the stack sets to deploy to new accounts as the accounts are created.
D. Configure AWS Config to manage the AWS Config rules across all AWS accounts in the organization. Deploy conformance packs that provide AWS Config rules and remediation actions across the organization.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司已经开始在多个团队中使用AWS。每个团队都有多个账户和独特的安全配置文件。公司在AWS Organizations中的一个组织里管理这些账户。每个账户都有自己的配置和安全控制。公司的DevOps团队希望使用预防性和检测性控制来治理所有账户。DevOps团队需要确保账户现在和未来的安全性，因为公司会在组织中创建新账户。哪种解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为多团队、多账户的AWS环境建立统一的治理机制，需要同时提供预防性和检测性控制，并且能够自动应用到未来新创建的账户上。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Control Tower：账户治理和合规管理服务 - Service Control Policies (SCPs)：预防性控制策略 - Organizational Units (OUs)：组织单元 - AWS Config：配置合规检查服务 - CloudFormation StackSets：跨账户资源部署 **正确答案B的原因：** 1. **全面的治理解决方案**：Control Tower专门设计用于多账户环境的治理，提供预防性控制（通过SCPs）和检测性控制（通过Config规则和CloudTrail） 2. **自动化新账户管理**：Control Tower的Account Factory可以自动为新账户应用预定义的安全基线和控制措施 3. **现有账户集成**：支持将现有账户注册到适当的OUs中，无需重新创建 4. **统一管理界面**：提供集中化的合规仪表板和治理视图 **其他选项错误的原因：** - **选项A**：仅提供预防性控制（SCPs），缺乏检测性控制，且没有自动化的新账户配置机制 - **选项C**：虽然能部署Config规则，但缺乏预防性控制，且StackSets管理复杂度高，不如Control Tower的integrated approach - **选项D**：只提供检测性控制，完全缺乏预防性控制措施 **决策标准和最佳实践：** 1. **治理优先**：对于多账户环境，应选择专门的治理服务而非拼凑解决方案 2. **自动化要求**：新账户必须自动继承安全控制，避免人工配置错误 3. **双重控制**：同时需要预防性和检测性控制来构建纵深防御 4. **可扩展性**：解决方案必须能够随组织增长而扩展 Control Tower是AWS推荐的多账户治理最佳实践，特别适合需要标准化安全控制和合规要求的企业环境。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">223</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an AWS CodeCommit repository to store its source code and corresponding unit tests. The company has configured an AWS CodePipeline pipeline that includes an AWS CodeBuild project that runs when code is merged to the main branch of the repository. The company wants the CodeBuild project to run the unit tests. If the unit tests pass, the CodeBuild project must tag the most recent commit. How should the company configure the CodeBuild project to meet these requirements? run the unit tests. Configure the project to use AWS CLI commands to create a new Git tag in the repository if the code passes the unit tests. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure the CodeBuild project to use native Git to clone the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use native Git to create a tag and to push the Git tag to the repository if the code passes the unit tests.
B. Configure the CodeBuild project to use native Git to clone the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use AWS CLI commands to create a new repository tag in the repository if the code passes the unit tests.
C. Configure the CodeBuild project to use AWS CLI commands to copy the code from the CodeCommit repository. Configure the project to
D. Configure the CodeBuild project to use AWS CLI commands to copy the code from the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use AWS CLI commands to create a new repository tag in the repository if the code passes the unit tests.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS CodeCommit存储库来存储其源代码和相应的单元测试。该公司已配置了一个AWS CodePipeline管道，其中包含一个AWS CodeBuild项目，当代码合并到存储库的主分支时运行。公司希望CodeBuild项目运行单元测试。如果单元测试通过，CodeBuild项目必须标记最新的提交。公司应该如何配置CodeBuild项目以满足这些要求？ 选项： A. 配置CodeBuild项目使用原生Git克隆CodeCommit存储库。配置项目运行单元测试。如果代码通过单元测试，配置项目使用原生Git创建标签并将Git标签推送到存储库。 B. 配置CodeBuild项目使用原生Git克隆CodeCommit存储库。配置项目运行单元测试。如果代码通过单元测试，配置项目使用AWS CLI命令在存储库中创建新的存储库标签。 C. 配置CodeBuild项目使用AWS CLI命令从CodeCommit存储库复制代码。配置项目运行单元测试。如果代码通过单元测试，配置项目使用AWS CLI命令在存储库中创建新的Git标签。 D. 配置CodeBuild项目使用AWS CLI命令从CodeCommit存储库复制代码。配置项目运行单元测试。如果代码通过单元测试，配置项目使用AWS CLI命令在存储库中创建新的存储库标签。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在CodeBuild项目中实现以下功能：1）获取CodeCommit中的源代码；2）运行单元测试；3）如果测试通过，则对最新提交创建标签。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：Git存储库服务 - AWS CodeBuild：持续集成服务，用于编译和测试代码 - AWS CodePipeline：持续交付服务 - Git标签：用于标记特定的代码提交点 - AWS CLI：命令行工具，用于与AWS服务交互 **正确答案的原因：** 选项C是正确的，因为： 1. 使用AWS CLI命令从CodeCommit获取代码是在AWS环境中的标准做法，更安全且集成度更高 2. 使用AWS CLI创建Git标签（codecommit create-tag）是AWS推荐的方式 3. 在CodeBuild环境中，AWS CLI已预装并配置了适当的权限 **其他选项错误的原因：** - 选项A：使用原生Git需要额外的认证配置，在CodeBuild中不是最佳实践 - 选项B：混合使用原生Git和AWS CLI，且&quot;存储库标签&quot;概念不准确 - 选项D：虽然使用AWS CLI获取代码，但&quot;存储库标签&quot;术语错误，应该是&quot;Git标签&quot; **决策标准和最佳实践：** 1. 在AWS环境中优先使用AWS CLI而非原生Git命令 2. CodeBuild项目应该利用AWS的集成权限管理 3. 使用正确的AWS API调用（如codecommit create-tag）来操作CodeCommit资源 4. 保持一致的工具选择（全部使用AWS CLI）以简化配置和权限管理</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">224</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer manages a company&#x27;s Amazon Elastic Container Service (Amazon ECS) cluster. The cluster runs on several Amazon EC2 instances that are in an Auto Scaling group. The DevOps engineer must implement a solution that logs and reviews all stopped tasks for errors. Which solution will meet these requirements? A (83%) C (17%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule to capture task state changes. Send the event to Amazon CloudWatch Logs. Use CloudWatch Logs Insights to investigate stopped tasks.
B. Configure tasks to write log data in the embedded metric format. Store the logs in Amazon CloudWatch Logs. Monitor the ContainerInstanceCount metric for changes.
C. Configure the EC2 instances to store logs in Amazon CloudWatch Logs. Create a CloudWatch Contributor Insights rule that uses the EC2 instance log data. Use the Contributor Insights rule to investigate stopped tasks.
D. Configure an EC2 Auto Scaling lifecycle hook for the EC2_INSTANCE_TERMINATING scale-in event. Write the SystemEventLog file to Amazon S3. Use Amazon Athena to query the log file for errors.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师管理公司的Amazon Elastic Container Service (Amazon ECS)集群。该集群运行在位于Auto Scaling组中的多个Amazon EC2实例上。DevOps工程师必须实现一个解决方案来记录和审查所有已停止的任务以查找错误。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现一个解决方案来监控和分析ECS集群中所有停止的任务，以便发现和调查错误。关键需求是：1）记录所有停止的任务；2）能够审查这些任务的错误信息。 **涉及的关键AWS服务和概念：** - Amazon ECS：容器编排服务，需要监控任务状态变化 - Amazon EventBridge：事件驱动架构的核心服务，可以捕获AWS服务的状态变化 - Amazon CloudWatch Logs：日志存储和分析服务 - CloudWatch Logs Insights：日志查询和分析工具 - Auto Scaling：自动扩缩容服务 **正确答案A的原因：** 选项A是最佳解决方案，因为：1）EventBridge可以精确捕获ECS任务的状态变化事件，包括任务停止事件；2）将事件发送到CloudWatch Logs提供了持久化存储；3）CloudWatch Logs Insights提供了强大的查询和分析能力，可以有效调查停止任务的原因；4）这是一个完整的端到端解决方案，直接针对任务级别的监控。 **其他选项错误的原因：** 选项B错误：虽然提到了日志记录，但监控ContainerInstanceCount指标无法提供任务停止的具体信息，且嵌入式指标格式主要用于性能监控而非错误分析。选项C错误：在EC2实例级别收集日志无法准确捕获ECS任务级别的状态变化，Contributor Insights主要用于分析高基数数据而非任务状态监控。选项D错误：Auto Scaling生命周期钩子只能捕获实例终止事件，无法监控单个任务的停止情况，且SystemEventLog不包含ECS任务的详细错误信息。 **决策标准和最佳实践：** 选择监控解决方案时应考虑：1）监控粒度要匹配需求（任务级别vs实例级别）；2）事件捕获的准确性和实时性；3）日志存储和查询的便利性；4）解决方案的完整性和可维护性。EventBridge + CloudWatch Logs的组合是AWS推荐的事件驱动监控模式，特别适合容器化应用的状态监控。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">225</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to deploy a workload on several hundred Amazon EC2 instances. The company will provision the EC2 instances in an Auto Scaling group by using a launch template. The workload will pull files from an Amazon S3 bucket, process the data, and put the results into a different S3 bucket. The EC2 instances must have least-privilege permissions and must use temporary security credentials. Which combination of steps will meet these requirements? (Choose two.) AB (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an IAM role that has the appropriate permissions for S3 buckets. Add the IAM role to an instance profile.
B. Update the launch template to include the IAM instance profile.
C. Create an IAM user that has the appropriate permissions for Amazon S3. Generate a secret key and token.
D. Create a trust anchor and profile. Attach the IAM role to the profile.
E. Update the launch template. Modify the user data to use the new secret key and token.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司希望在数百个Amazon EC2实例上部署工作负载。该公司将使用launch template在Auto Scaling group中配置EC2实例。工作负载将从Amazon S3 bucket中拉取文件，处理数据，并将结果放入不同的S3 bucket中。EC2实例必须具有最小权限，并且必须使用临时安全凭证。哪种步骤组合将满足这些要求？（选择两个。） 选项： A. 创建一个具有S3 bucket适当权限的IAM role。将IAM role添加到instance profile中。 B. 更新launch template以包含IAM instance profile。 C. 创建一个具有Amazon S3适当权限的IAM user。生成secret key和token。 D. 创建trust anchor和profile。将IAM role附加到profile上。 E. 更新launch template。修改user data以使用新的secret key和token。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为大规模EC2实例部署配置安全的S3访问权限，关键要求包括：1）使用临时安全凭证；2）遵循最小权限原则；3）适用于Auto Scaling group和launch template的架构。 **涉及的关键AWS服务和概念：** - IAM Role和Instance Profile：为EC2实例提供临时凭证的标准机制 - Launch Template：定义EC2实例配置的模板，支持Auto Scaling - Auto Scaling Group：自动管理EC2实例数量的服务 - S3权限管理：需要配置适当的读写权限 - 临时安全凭证：通过STS自动轮换的凭证，比长期凭证更安全 **正确答案的原因：** 选项A和B是正确的组合，因为： - 选项A：创建IAM role并添加到instance profile是EC2实例获取临时凭证的标准做法，role可以配置最小权限的S3访问策略 - 选项B：在launch template中指定instance profile确保所有通过Auto Scaling启动的实例都自动获得相同的权限配置 **其他选项错误的原因：** - 选项C：IAM user使用长期凭证（access key/secret key），不符合&quot;临时安全凭证&quot;要求，且在大规模部署中管理复杂 - 选项D：Trust anchor和profile是AWS IAM Roles Anywhere的概念，用于外部系统，不适用于EC2实例 - 选项E：在user data中硬编码凭证违反安全最佳实践，且无法提供临时凭证的自动轮换 **决策标准和最佳实践：** 1. 安全性：优先使用临时凭证而非长期凭证 2. 可扩展性：解决方案必须适用于数百个实例的自动化部署 3. 管理简便性：通过launch template统一配置，避免手动管理每个实例 4. 最小权限原则：IAM role可以精确控制S3访问权限范围 5. AWS原生集成：利用EC2与IAM的原生集成机制，无需额外配置</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">226</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using AWS CodeDeploy to automate software deployment. The deployment must meet these requirements: • A number of instances must be available to serve traffic during the deployment. Traffic must be balanced across those instances, and the instances must automatically heal in the event of failure. • A new fleet of instances must be launched for deploying a new revision automatically, with no manual provisioning. • Traffic must be rerouted to the new environment to half of the new instances at a time. The deployment should succeed if traffic is rerouted to at least half of the instances; otherwise, it should fail. • Before routing traffic to the new fleet of instances, the temporary files generated during the deployment process must be deleted. • At the end of a successful deployment, the original instances in the deployment group must be deleted immediately to reduce costs. How can a DevOps engineer meet these requirements? target group with the deployment group. Use the Automatically copy Auto Scaling group option, create a custom deployment configuration with minimum healthy hosts defined as 50%, and assign the configuration to the deployment group. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BeforeBlockTraffic hook within appspec.yml to delete the temporary files. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.OneAtATime as the deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the AllowTraffic hook within appspec.yml to delete the temporary files.
B. Use an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group and Application Load Balancer
C. Use an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.HalfAtATime as the deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BeforeAllowTraffic hook within appspec.yml to delete the temporary files.
D. Use an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.AllAtOnce as a deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BlockTraffic hook within appspec.yml to delete the temporary files.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS CodeDeploy来自动化软件部署。部署必须满足以下要求：• 在部署过程中必须有一定数量的实例可用来服务流量。流量必须在这些实例之间进行负载均衡，并且实例必须在发生故障时自动修复。• 必须自动启动新的实例集群来部署新版本，无需手动配置。• 流量必须重新路由到新环境，每次路由到一半的新实例。如果流量成功路由到至少一半的实例，部署应该成功；否则应该失败。• 在将流量路由到新实例集群之前，必须删除部署过程中生成的临时文件。• 在成功部署结束时，部署组中的原始实例必须立即删除以降低成本。DevOps工程师如何满足这些要求？ 选项：A. 使用Application Load Balancer和就地部署。将Auto Scaling组与部署组关联。使用自动复制Auto Scaling组选项，并使用CodeDeployDefault.OneAtATime作为部署配置。指示AWS CodeDeploy终止部署组中的原始实例，并使用appspec.yml中的AllowTraffic钩子删除临时文件。 B. 使用Application Load Balancer和蓝/绿部署。将Auto Scaling组和Application Load Balancer目标组与部署组关联。使用自动复制Auto Scaling组选项，创建最小健康主机定义为50%的自定义部署配置，并将配置分配给部署组。指示AWS CodeDeploy终止部署组中的原始实例，并使用appspec.yml中的BeforeBlockTraffic钩子删除临时文件。 C. 使用Application Load Balancer和蓝/绿部署。将Auto Scaling组和Application Load Balancer目标组与部署组关联。使用自动复制Auto Scaling组选项，并使用CodeDeployDefault.HalfAtATime作为部署配置。指示AWS CodeDeploy终止部署组中的原始实例，并使用appspec.yml中的BeforeAllowTraffic钩子删除临时文件。 D. 使用Application Load Balancer和就地部署。将Auto Scaling组和Application Load Balancer目标组与部署组关联。使用自动复制Auto Scaling组选项，并使用CodeDeployDefault.AllAtOnce作为部署配置。指示AWS CodeDeploy终止部署组中的原始实例，并使用appspec.yml中的BlockTraffic钩子删除临时文件。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodeDeploy的部署策略选择，特别是蓝/绿部署与就地部署的区别，以及相应的配置选项。题目要求实现零停机部署，自动实例管理，渐进式流量切换，临时文件清理和成本优化。 **涉及的关键AWS服务和概念：** 1. AWS CodeDeploy - 自动化部署服务 2. Application Load Balancer - 负载均衡器 3. Auto Scaling Group - 自动扩缩容组 4. 蓝/绿部署 vs 就地部署策略 5. CodeDeploy部署配置（CodeDeployDefault.HalfAtATime等） 6. AppSpec.yml生命周期钩子 **正确答案的原因（选项B）：** 虽然选项B的内容被截断，但从上下文可以推断它应该是蓝/绿部署方案。蓝/绿部署完全符合题目要求： - 创建全新的实例集群（绿环境），满足&quot;新fleet自动启动&quot;要求 - 保持原有实例（蓝环境）继续服务，确保部署期间服务可用性 - 支持渐进式流量切换和快速回滚 - 部署成功后自动终止原实例，实现成本控制 **其他选项错误的原因：** - 选项A：使用就地部署和OneAtATime配置，无法满足&quot;新fleet启动&quot;和&quot;一半实例流量切换&quot;的要求 - 选项C：虽然使用蓝/绿部署，但CodeDeployDefault.HalfAtATime是就地部署配置，不适用于蓝/绿部署；BeforeAllowTraffic钩子时机不当 - 选项D：就地部署无法满足新实例集群要求，AllAtOnce配置风险过高 **决策标准和最佳实践：** 1. 需要零停机和新实例集群时选择蓝/绿部署 2. 蓝/绿部署应配合Auto Scaling组的自动复制功能 3. 临时文件清理应在流量切换前完成，使用BeforeBlockTraffic或类似的早期钩子 4. 成本优化通过自动终止原实例实现 5. 渐进式部署降低风险，满足至少50%成功率要求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">227</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company needs to adopt a multi-account strategy to deploy its applications and the associated CI/CD infrastructure. The company has created an organization in AWS Organizations that has all features enabled. The company has configured AWS Control Tower and has set up a landing zone. The company needs to use AWS Control Tower controls (guardrails) in all AWS accounts in the organization. The company must create the accounts for a multi-environment application and must ensure that all accounts are configured to an initial baseline. Which solution will meet these requirements with the LEAST operational overhead? CloudFormation StackSets to apply the baseline configuration to the new accounts. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS Control Tower Account Factory Customization (AFC) blueprint that uses the baseline configuration. Use AWS Control Tower Account Factory to provision a dedicated AWS account for each environment and a CI/CD account by using the blueprint.
B. Use AWS Control Tower Account Factory to provision a dedicated AWS account for each environment and a CI/CD account. Use AWS CloudFormation StackSets to apply the baseline configuration to the new accounts.
C. Use Organizations to provision a multi-environment AWS account and a CI/CD account. In the Organizations management account, create an AWS Lambda function that assumes the Organizations access role to apply the baseline configuration to the new accounts.
D. Use Organizations to provision a dedicated AWS account for each environment, an audit account, and a CI/CD account. Use AWS</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司需要采用多账户策略来部署其应用程序和相关的CI/CD基础设施。该公司已在AWS Organizations中创建了一个启用所有功能的组织。公司已配置了AWS Control Tower并设置了landing zone。公司需要在组织中的所有AWS账户中使用AWS Control Tower控制措施(guardrails)。公司必须为多环境应用程序创建账户，并确保所有账户都配置为初始基线。哪种解决方案能以最少的运营开销满足这些要求？ 选项： A. 创建一个使用基线配置的AWS Control Tower Account Factory Customization (AFC) blueprint。使用AWS Control Tower Account Factory为每个环境和CI/CD账户配置专用的AWS账户，并使用该blueprint。 B. 使用AWS Control Tower Account Factory为每个环境和CI/CD账户配置专用的AWS账户。使用AWS CloudFormation StackSets将基线配置应用到新账户。 C. 使用Organizations配置多环境AWS账户和CI/CD账户。在Organizations管理账户中，创建一个AWS Lambda函数，该函数承担Organizations访问角色，将基线配置应用到新账户。 D. 使用Organizations为每个环境、审计账户和CI/CD账户配置专用的AWS账户。使用AWS 正确答案：B</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 需要多账户策略部署应用和CI/CD基础设施 - 已启用AWS Control Tower和landing zone - 需要在所有账户中应用Control Tower控制措施(guardrails) - 确保所有新账户都有初始基线配置 - 要求最少的运营开销 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Control Tower：提供预配置的多账户环境和guardrails - AWS Control Tower Account Factory：自动化账户创建和配置 - AWS CloudFormation StackSets：跨多个账户和区域部署CloudFormation模板 - Account Factory Customization (AFC)：Control Tower的定制化功能 **正确答案B的原因：** 1. 使用Control Tower Account Factory创建账户确保自动应用Control Tower的guardrails和基本安全配置 2. CloudFormation StackSets是成熟、稳定的服务，专门用于跨多账户应用配置 3. 这种组合提供了良好的关注点分离：Control Tower处理账户创建和基本治理，StackSets处理自定义基线配置 4. 运营开销相对较低，且解决方案可靠性高 **其他选项错误的原因：** - 选项A：AFC blueprint功能相对较新，可能不如StackSets成熟稳定，在某些复杂场景下可能存在限制 - 选项C：直接使用Organizations创建账户会绕过Control Tower，无法自动获得guardrails和Control Tower的治理优势；Lambda方案增加了复杂性和运营开销 - 选项D：描述不完整，但如果也是直接使用Organizations，会有与选项C相同的问题 **决策标准和最佳实践：** 1. 优先使用AWS托管服务减少运营开销 2. 充分利用已部署的Control Tower功能 3. 选择成熟稳定的服务组合 4. 保持架构简单性和可维护性 5. 确保安全和合规要求得到满足</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">228</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps team has created a Custom Lambda rule in AWS Config. The rule monitors Amazon Elastic Container Repository (Amazon ECR) policy statements for ecr:* actions. When a noncompliant repository is detected, Amazon EventBridge uses Amazon Simple Notification Service (Amazon SNS) to route the notification to a security team. When the custom AWS Config rule is evaluated, the AWS Lambda function fails to run. Which solution will resolve the issue? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the Lambda function&#x27;s resource policy to grant AWS Config permission to invoke the function.
B. Modify the SNS topic policy to include configuration changes for EventBridge to publish to the SNS topic.
C. Modify the Lambda function&#x27;s execution role to include configuration changes for custom AWS Config rules.
D. Modify all the ECR repository policies to grant AWS Config access to the necessary ECR API actions.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps团队在AWS Config中创建了一个自定义Lambda规则。该规则监控Amazon Elastic Container Repository (Amazon ECR)策略语句中的ecr:*操作。当检测到不合规的存储库时，Amazon EventBridge使用Amazon Simple Notification Service (Amazon SNS)将通知路由到安全团队。当自定义AWS Config规则被评估时，AWS Lambda函数无法运行。哪个解决方案能解决这个问题？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS Config自定义规则中Lambda函数执行失败的故障排除。问题的关键在于Lambda函数在AWS Config规则评估时无法正常运行，需要找出根本原因并提供解决方案。 **涉及的关键AWS服务和概念：** - AWS Config：配置管理和合规性监控服务 - AWS Lambda：无服务器计算服务，用于执行自定义Config规则 - Amazon ECR：容器镜像注册表服务 - Amazon EventBridge：事件路由服务 - Amazon SNS：消息通知服务 - IAM权限：Lambda执行角色和资源策略 **正确答案C的原因：** Lambda函数的执行角色需要具备适当的权限才能执行AWS Config自定义规则的相关操作。当Lambda函数作为Config规则运行时，它需要： 1. 读取Config配置项的权限 2. 访问ECR API来检查策略语句的权限 3. 向Config服务返回合规性评估结果的权限 如果执行角色缺少这些权限，Lambda函数就会执行失败。 **其他选项错误的原因：** - 选项A：资源策略主要控制谁可以调用Lambda函数，但题目显示的是函数执行失败，而不是调用失败 - 选项B：SNS主题策略问题不会导致Lambda函数执行失败，这是通知链路的下游问题 - 选项D：修改ECR存储库策略虽然可能解决访问问题，但不是最佳实践，应该通过Lambda执行角色来管理权限 **决策标准和最佳实践：** 1. 遵循最小权限原则，通过IAM角色为Lambda函数分配必要权限 2. Lambda函数执行失败通常与执行角色权限不足相关 3. 在AWS Config自定义规则中，Lambda函数需要特定的权限来读取配置项和评估合规性 4. 优先考虑通过IAM角色管理权限，而不是修改资源策略</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">229</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A developer is creating a proof of concept for a new software as a service (SaaS) application. The application is in a shared development AWS account that is part of an organization in AWS Organizations. The developer needs to create service-linked IAM roles for the AWS services that are being considered for the proof of concept. The solution needs to give the developer the ability to create and configure the service-linked roles only. Which solution will meet these requirements? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an IAM user for the developer in the organization&#x27;s management account. Configure a cross-account role in the development account for the developer to use. Limit the scope of the cross-account role to common services.
B. Add the developer to an IAM group. Attach the PowerUserAccess managed policy to the IAM group. Enforce multi-factor authentication (MFA) on the user account.
C. Add an SCP to the development account in Organizations. Configure the SCP with a Deny rule for iam:* to limit the developer&#x27;s access.
D. Create an IAM role that has the necessary IAM access to allow the developer to create policies and roles. Create and attach a permissions boundary to the role. Grant the developer access to assume the role.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名开发者正在为新的软件即服务(SaaS)应用程序创建概念验证。该应用程序位于一个共享的开发AWS账户中，该账户是AWS Organizations中某个组织的一部分。开发者需要为概念验证中考虑使用的AWS服务创建service-linked IAM roles。解决方案需要给开发者仅创建和配置service-linked roles的能力。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 开发者需要在共享开发账户中创建service-linked IAM roles - 权限范围必须严格限制，仅允许创建和配置service-linked roles - 需要实现最小权限原则，避免过度授权 **涉及的关键AWS服务和概念：** - Service-linked roles：AWS服务自动创建和管理的特殊IAM角色 - IAM permissions boundary：权限边界，用于限制实体的最大权限范围 - AWS Organizations：多账户管理服务 - Cross-account roles：跨账户角色访问 - SCP (Service Control Policy)：服务控制策略 **正确答案D的原因：** - 创建专门的IAM角色提供必要的IAM访问权限 - 使用permissions boundary作为安全护栏，确保开发者只能在预定义的权限范围内操作 - 通过assume role机制提供临时、受控的权限访问 - 完美实现了最小权限原则，既满足功能需求又保证安全性 **其他选项错误的原因：** - 选项A：跨账户角色过于复杂，且&quot;限制到通用服务&quot;的描述过于宽泛，无法精确控制service-linked roles的创建权限 - 选项B：PowerUserAccess权限过大，包含了超出需求的广泛权限，违反最小权限原则 - 选项C：SCP的Deny规则会阻止所有IAM操作，包括创建service-linked roles，完全无法满足需求 **决策标准和最佳实践：** - 优先选择能精确控制权限范围的解决方案 - 使用permissions boundary作为额外的安全层 - 避免使用过于宽泛的managed policies - 在共享环境中特别注意权限隔离和控制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">230</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Organizations to manage its AWS accounts. The company wants its monitoring system to receive an alert when a root user logs in. The company also needs a dashboard to display any log activity that the root user generates. Which combination of steps will meet these requirements? (Choose three.) F. Create an Amazon CloudWatch dashboard that uses a CloudWatch Logs Insights query. Most Voted CEF (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable AWS Config with a multi-account aggregator. Configure log forwarding to Amazon CloudWatch Logs.
B. Create an Amazon QuickSight dashboard that uses an Amazon CloudWatch Logs query.
C. Create an Amazon CloudWatch Logs metric filter to match root user login events. Configure a CloudWatch alarm and an Amazon Simple Notification Service (Amazon SNS) topic to send alerts to the company&#x27;s monitoring system.
D. Create an Amazon CloudWatch Logs subscription filter to match root user login events. Configure the filter to forward events to an Amazon Simple Notification Service (Amazon SNS) topic. Configure the SNS topic to send alerts to the company&#x27;s monitoring system.
E. Create an AWS CloudTrail organization trail. Configure the organization trail to send events to Amazon CloudWatch Logs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations来管理其AWS账户。该公司希望其监控系统在root用户登录时收到警报。该公司还需要一个仪表板来显示root用户生成的任何日志活动。哪种步骤组合将满足这些要求？（选择三个。） 选项： A. 启用带有多账户聚合器的AWS Config。配置日志转发到Amazon CloudWatch Logs。 B. 创建一个使用Amazon CloudWatch Logs查询的Amazon QuickSight仪表板。 C. 创建Amazon CloudWatch Logs指标过滤器来匹配root用户登录事件。配置CloudWatch警报和Amazon Simple Notification Service (Amazon SNS)主题向公司的监控系统发送警报。 D. 创建Amazon CloudWatch Logs订阅过滤器来匹配root用户登录事件。配置过滤器将事件转发到Amazon Simple Notification Service (Amazon SNS)主题。配置SNS主题向公司的监控系统发送警报。 E. 创建AWS CloudTrail组织跟踪。配置组织跟踪将事件发送到Amazon CloudWatch Logs。 F. 创建使用CloudWatch Logs Insights查询的Amazon CloudWatch仪表板。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现两个主要功能：1）当root用户登录时发送警报到监控系统；2）创建仪表板显示root用户的日志活动。需要选择三个正确的步骤组合。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS CloudTrail：API调用和用户活动审计服务 - Amazon CloudWatch Logs：日志管理和监控服务 - CloudWatch Logs指标过滤器和订阅过滤器：用于处理日志数据 - Amazon SNS：消息通知服务 - CloudWatch仪表板：数据可视化工具 **正确答案分析：** 根据题目显示，正确答案应该是CEF的组合（尽管题目中标注B为正确答案，但这可能是标注错误）： - **选项C**：创建CloudWatch Logs指标过滤器匹配root用户登录事件，配置CloudWatch警报和SNS主题发送警报。这满足了警报需求。 - **选项E**：创建CloudTrail组织跟踪并发送事件到CloudWatch Logs。这是基础设施，为监控root用户活动提供数据源。 - **选项F**：创建使用CloudWatch Logs Insights查询的CloudWatch仪表板。这满足了仪表板显示需求。 **其他选项错误的原因：** - **选项A**：AWS Config主要用于配置合规性监控，不是监控用户登录活动的最佳选择。 - **选项B**：QuickSight主要用于商业智能分析，对于实时日志监控不如CloudWatch仪表板合适。 - **选项D**：订阅过滤器虽然可以转发事件，但指标过滤器更适合创建警报。 **决策标准和最佳实践：** 1. 使用CloudTrail作为审计日志的标准数据源 2. 利用CloudWatch Logs的指标过滤器创建基于日志内容的警报 3. 使用CloudWatch仪表板进行运维监控可视化 4. 在多账户环境中使用组织级别的CloudTrail确保全面覆盖 5. 通过SNS实现灵活的通知机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">231</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Organizations to manage its AWS accounts. A DevOps engineer must ensure that all users who access the AWS Management Console are authenticated through the company&#x27;s corporate identity provider (IdP). Which combination of steps will meet these requirements? (Choose two.) BE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Amazon GuardDuty with a delegated administrator account. Use GuardDuty to enforce denial of IAM user logins.
B. Use AWS IAM Identity Center to configure identity federation with SAML 2.0.
C. Create a permissions boundary in AWS IAM Identity Center to deny password logins for IAM users.
D. Create IAM groups in the Organizations management account to apply consistent permissions for all IAM users.
E. Create an SCP in Organizations to deny password creation for IAM users.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations来管理其AWS账户。DevOps工程师必须确保所有访问AWS Management Console的用户都通过公司的企业身份提供商(IdP)进行身份验证。哪种步骤组合将满足这些要求？(选择两个。) 选项： A. 使用Amazon GuardDuty与委托管理员账户。使用GuardDuty强制拒绝IAM用户登录。 B. 使用AWS IAM Identity Center配置SAML 2.0身份联合。 C. 在AWS IAM Identity Center中创建权限边界以拒绝IAM用户的密码登录。 D. 在Organizations管理账户中创建IAM组，为所有IAM用户应用一致的权限。 E. 在Organizations中创建SCP以拒绝IAM用户创建密码。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现企业级身份管理，确保所有用户通过公司的企业IdP进行身份验证，而不是使用AWS原生的IAM用户密码登录。这是一个典型的企业身份联合场景。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS IAM Identity Center (原AWS SSO)：企业身份管理和单点登录服务 - SAML 2.0：安全断言标记语言，用于身份联合 - SCP (Service Control Policy)：服务控制策略，用于组织级权限控制 - 身份联合：将外部身份提供商与AWS集成 **正确答案的原因：** 选项B正确：AWS IAM Identity Center是专门为企业身份管理设计的服务，支持SAML 2.0联合，可以直接与企业IdP集成，实现单点登录和统一身份验证。这是AWS推荐的企业身份管理最佳实践。 根据题目显示应该选择两个答案，第二个正确答案应该是选项E：通过SCP拒绝IAM用户创建密码，这样可以从组织层面强制禁用传统的IAM用户密码登录方式。 **其他选项错误的原因：** - 选项A：GuardDuty是安全监控服务，不是身份管理工具，无法强制执行身份验证策略 - 选项C：权限边界主要用于限制权限范围，不是控制身份验证方式的正确工具 - 选项D：创建IAM组只是权限管理，并不能解决身份验证来源的问题 **决策标准和最佳实践：** 1. 企业身份联合应优先使用IAM Identity Center 2. 使用SCP在组织层面统一控制安全策略 3. 采用SAML 2.0等标准协议确保兼容性 4. 从源头禁用不安全的认证方式（如IAM用户密码） 5. 实现集中化的身份管理和访问控制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">232</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has deployed a new platform that runs on Amazon Elastic Kubernetes Service (Amazon EKS). The new platform hosts web applications that users frequently update. The application developers build the Docker images for the applications and deploy the Docker images manually to the platform. The platform usage has increased to more than 500 users every day. Frequent updates, building the updated Docker images for the applications, and deploying the Docker images on the platform manually have all become difficult to manage. The company needs to receive an Amazon Simple Notification Service (Amazon SNS) notification if Docker image scanning returns any HIGH or CRITICAL findings for operating system or programming language package vulnerabilities. Which combination of steps will meet these requirements? (Choose two.) BD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CodeCommit repository to store the Dockerfile and Kubernetes deployment files. Create a pipeline in AWS CodePipeline. Use an Amazon S3 event to invoke the pipeline when a newer version of the Dockerfile is committed. Add a step to the pipeline to initiate the AWS CodeBuild project.
B. Create an AWS CodeCommit repository to store the Dockerfile and Kubernetes deployment files. Create a pipeline in AWS CodePipeline. Use an Amazon EventBridge event to invoke the pipeline when a newer version of the Dockerfile is committed. Add a step to the pipeline to initiate the AWS CodeBuild project.
C. Create an AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Turn on basic scanning for the ECR repository. Create an Amazon EventBridge rule that monitors Amazon GuardDuty events. Configure the EventBridge rule to send an event to an SNS topic when the finding-severity-counts parameter is more than 0 at a CRITICAL or HIGH level.
D. Create an AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Turn on enhanced scanning for the ECR repository. Create an Amazon EventBridge rule that monitors ECR image scan events. Configure the EventBridge rule to send an event to an SNS topic when the finding-severity-counts parameter is more than 0 at a CRITICAL or HIGH level.
E. Create an AWS CodeBuild project that scans the Dockerfile. Configure the project to build the Docker images and store the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository if the scan is successful. Configure an SNS topic to provide notification if the scan returns any vulnerabilities.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Amazon Elastic Kubernetes Service (Amazon EKS)上部署了一个新平台。该新平台托管用户经常更新的Web应用程序。应用程序开发人员构建应用程序的Docker镜像并手动将Docker镜像部署到平台上。平台使用量已增加到每天超过500个用户。频繁更新、为应用程序构建更新的Docker镜像以及在平台上手动部署Docker镜像都变得难以管理。公司需要在Docker镜像扫描返回操作系统或编程语言包漏洞的任何HIGH或CRITICAL发现时收到Amazon Simple Notification Service (Amazon SNS)通知。哪种步骤组合将满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 自动化Docker镜像构建和部署流程，解决手动管理困难的问题 2. 实现Docker镜像安全扫描功能 3. 当扫描发现HIGH或CRITICAL级别的漏洞时，通过SNS发送通知 **涉及的关键AWS服务和概念：** - AWS CodeCommit：源代码版本控制 - AWS CodePipeline：CI/CD管道服务 - AWS CodeBuild：构建服务 - Amazon ECR：容器镜像仓库 - Amazon EventBridge：事件驱动服务 - Amazon SNS：通知服务 - ECR镜像扫描：基础扫描vs增强扫描 **正确答案的原因：** 选项B和D是正确组合： - **选项B**：建立了完整的CI/CD流程。使用CodeCommit存储代码，CodePipeline创建管道，EventBridge监听代码提交事件触发管道，CodeBuild执行构建任务。这是标准的自动化部署架构。 - **选项D**：实现了镜像安全扫描和通知机制。使用ECR存储镜像，启用增强扫描（enhanced scanning）提供更全面的漏洞检测，通过EventBridge监听ECR扫描事件，当发现HIGH或CRITICAL级别漏洞时触发SNS通知。 **其他选项错误的原因：** - **选项A**：使用S3事件触发管道是错误的。CodeCommit的代码提交应该通过EventBridge事件触发，而不是S3事件。 - **选项C**：使用GuardDuty监听事件是错误的。GuardDuty是威胁检测服务，不负责ECR镜像扫描结果。应该监听ECR的扫描事件。 - **选项E**：描述的是在CodeBuild中扫描Dockerfile，这不是标准做法。镜像扫描应该在ECR中进行，而且没有提到CI/CD管道的建立。 **决策标准和最佳实践：** 1. **自动化优先**：使用CodePipeline建立完整的CI/CD流程，避免手动操作 2. **事件驱动架构**：使用EventBridge实现松耦合的事件驱动自动化 3. **安全左移**：在镜像存储阶段就进行安全扫描，及早发现漏洞 4. **增强扫描vs基础扫描**：增强扫描提供更全面的漏洞检测，包括操作系统和应用程序包漏洞 5. **监控和通知**：建立完善的监控和告警机制，确保安全问题能及时响应</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">233</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company groups its AWS accounts in OUs in an organization in AWS Organizations. The company has deployed a set of Amazon API Gateway APIs in one of the Organizations accounts. The APIs are bound to the account&#x27;s VPC and have no existing authentication mechanism. Only principals in a specific OU can have permissions to invoke the APIs. The company applies the following policy to the API Gateway interface VPC endpoint: The company also updates the API Gateway resource policies to deny invocations that do not come through the interface VPC endpoint. After the updates, the following error message appears during attempts to use the interface VPC endpoint URL to invoke an API: &quot;User: anonymous is not authorized.&quot; Which combination of steps will solve this problem? (Choose two.) AE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable IAM authentication on all API methods by setting AWS IAM as the authorization method.
B. Create a token-based AWS Lambda authorizer that passes the caller&#x27;s identity in a bearer token.
C. Create a request parameter-based AWS Lambda authorizer that passes the caller&#x27;s identity in a combination of headers, query string parameters, stage variables, and $context variables.
D. Use Amazon Cognito user pools as the authorizer to control access to the API.
E. Verify the identity of the requester by using Signature Version 4 to sign client requests by using AWS credentials.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中将其AWS账户分组到OU中。该公司在Organizations的一个账户中部署了一组Amazon API Gateway API。这些API绑定到账户的VPC，并且没有现有的身份验证机制。只有特定OU中的主体才能拥有调用这些API的权限。公司将以下策略应用到API Gateway接口VPC端点：公司还更新了API Gateway资源策略，以拒绝不通过接口VPC端点的调用。更新后，在尝试使用接口VPC端点URL调用API时出现以下错误消息：&quot;User: anonymous is not authorized.&quot;哪种步骤组合将解决此问题？（选择两个）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在AWS Organizations环境下，通过VPC端点访问API Gateway时遇到&quot;User: anonymous is not authorized&quot;错误的解决方案。问题的关键在于API没有身份验证机制，但需要识别调用者身份以验证其是否来自特定OU。 **涉及的关键AWS服务和概念：** - AWS Organizations和OU（组织单位） - API Gateway资源策略和授权机制 - VPC端点和网络访问控制 - Lambda授权器（自定义授权） - IAM身份验证和Signature Version 4 - Amazon Cognito用户池 **正确答案的原因：** 选项B（Lambda授权器with bearer token）是正确的，因为： 1. 错误信息显示用户被识别为&quot;anonymous&quot;，说明缺乏身份验证机制 2. Lambda授权器可以自定义身份验证逻辑，验证调用者是否来自特定OU 3. Bearer token方式可以在请求头中传递身份信息，Lambda授权器可以解析并验证 4. 这种方式灵活性高，可以实现复杂的OU级别的访问控制逻辑 题目要求选择两个答案，但给出的正确答案只有B，可能还需要选择A或E作为配套方案。 **其他选项错误的原因：** - 选项A（IAM身份验证）：虽然可以解决身份验证问题，但单独使用无法实现基于OU的细粒度控制 - 选项C（请求参数授权器）：相比bearer token方式复杂且不够安全 - 选项D（Cognito用户池）：适用于用户身份管理，但不适合AWS账户/OU级别的访问控制 - 选项E（Signature Version 4）：这是AWS API调用的标准签名方式，但需要与其他身份验证机制配合使用 **决策标准和最佳实践：** 1. 选择能够支持自定义授权逻辑的方案（Lambda授权器） 2. 确保身份验证机制能够识别调用者的OU归属 3. 使用安全的token传递方式（bearer token优于查询参数） 4. 考虑方案的可扩展性和维护性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">234</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to decrease the time it takes to develop new features. The company uses AWS CodeBuild and AWS CodeDeploy to build and deploy its applications. The company uses AWS CodePipeline to deploy each microservice with its own CI/CD pipeline. The company needs more visibility into the average time between the release of new features and the average time to recover after a failed deployment. Which solution will provide this visibility with the LEAST configuration effort? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Program an AWS Lambda function that creates Amazon CloudWatch custom metrics with information about successful runs and failed runs for each pipeline. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. Use the metrics to build a CloudWatch dashboard.
B. Program an AWS Lambda function that creates Amazon CloudWatch custom metrics with information about successful runs and failed runs for each pipeline. Create an Amazon EventBridge rule to invoke the Lambda function after every successful run and after every failed run. Use the metrics to build a CloudWatch dashboard.
C. Program an AWS Lambda function that writes information about successful runs and failed runs to Amazon DynamoDB. Create an Amazon EventBridge rule to invoke the Lambda function after every successful run and after every failed run. Build an Amazon QuickSight dashboard to show the information from DynamoDB.
D. Program an AWS Lambda function that writes information about successful runs and failed runs to Amazon DynamoDB. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. Build an Amazon QuickSight dashboard to show the information from DynamoDB.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司希望减少开发新功能所需的时间。该公司使用AWS CodeBuild和AWS CodeDeploy来构建和部署应用程序。公司使用AWS CodePipeline为每个微服务部署各自的CI/CD pipeline。公司需要更多可见性来了解新功能发布的平均时间和部署失败后的平均恢复时间。哪种解决方案能够以最少的配置工作量提供这种可见性？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案来监控CI/CD pipeline的性能指标，特别是功能发布时间和故障恢复时间，并且要求配置工作量最少。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务 - AWS Lambda：无服务器计算服务 - Amazon CloudWatch：监控和可观测性服务 - Amazon EventBridge：事件驱动架构服务 - Amazon DynamoDB：NoSQL数据库服务 - Amazon QuickSight：商业智能和数据可视化服务 **正确答案B的原因：** 1. **实时触发**：使用EventBridge规则在每次成功或失败运行后立即触发Lambda函数，确保数据的实时性和准确性 2. **最少配置**：CloudWatch自带丰富的监控和仪表板功能，无需额外配置复杂的数据存储和可视化服务 3. **原生集成**：CloudWatch与CodePipeline有原生集成，更容易获取pipeline状态信息 4. **自动化程度高**：基于事件驱动，无需定时轮询，减少不必要的资源消耗 **其他选项错误的原因：** - **选项A**：使用5分钟定时触发不够实时，可能错过某些事件，数据准确性较差 - **选项C**：使用DynamoDB + QuickSight增加了额外的配置复杂性，需要设置数据库表结构和QuickSight数据源连接 - **选项D**：结合了A和C的缺点，既有定时触发的不准确性，又有额外服务的配置复杂性 **决策标准和最佳实践：** 1. **最少配置原则**：优先选择AWS原生集成度高的服务组合 2. **实时性要求**：对于CI/CD监控，事件驱动比定时轮询更合适 3. **成本效益**：CloudWatch作为AWS核心监控服务，在成本和功能上更平衡 4. **可维护性**：减少服务依赖链，降低系统复杂度和维护成本</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">235</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has developed a static website hosted on an Amazon S3 bucket. The website is deployed using AWS CloudFormation. The CloudFormation template defines an S3 bucket and a custom resource that copies content into the bucket from a source location. The company has decided that it needs to move the website to a new location, so the existing CloudFormation stack must be deleted and re-created. However, CloudFormation reports that the stack could not be deleted cleanly. What is the MOST likely cause and how can the DevOps engineer mitigate this problem for this and future versions of the website? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Deletion has failed because the S3 bucket has an active website configuration. Modify the CloudFormation template to remove the WebsiteConfiguration property from the S3 bucket resource.
B. Deletion has failed because the S3 bucket is not empty. Modify the custom resource&#x27;s AWS Lambda function code to recursively empty the bucket when RequestType is Delete.
C. Deletion has failed because the custom resource does not define a deletion policy. Add a DeletionPolicy property to the custom resource definition with a value of RemoveOnDeletion.
D. Deletion has failed because the S3 bucket is not empty. Modify the S3 bucket resource in the CloudFormation template to add a DeletionPolicy property with a value of Empty.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司开发了一个托管在Amazon S3存储桶上的静态网站。该网站使用AWS CloudFormation部署。CloudFormation模板定义了一个S3存储桶和一个自定义资源，该自定义资源将内容从源位置复制到存储桶中。公司决定需要将网站迁移到新位置，因此必须删除并重新创建现有的CloudFormation堆栈。但是，CloudFormation报告无法干净地删除堆栈。最可能的原因是什么，DevOps工程师如何为当前和未来版本的网站缓解这个问题？ 选项： A. 删除失败是因为S3存储桶有活跃的网站配置。修改CloudFormation模板以从S3存储桶资源中移除WebsiteConfiguration属性。 B. 删除失败是因为S3存储桶不为空。修改自定义资源的AWS Lambda函数代码，当RequestType为Delete时递归清空存储桶。 C. 删除失败是因为自定义资源没有定义删除策略。在自定义资源定义中添加DeletionPolicy属性，值为RemoveOnDeletion。 D. 删除失败是因为S3存储桶不为空。修改CloudFormation模板中的S3存储桶资源，添加值为Empty的DeletionPolicy属性。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是CloudFormation堆栈删除失败的故障排除，特别是涉及S3存储桶和自定义资源的场景。需要理解为什么删除会失败以及如何通过修改模板来解决问题。 **涉及的关键AWS服务和概念：** 1. Amazon S3 - 对象存储服务，非空存储桶无法被删除 2. AWS CloudFormation - 基础设施即代码服务，管理资源的生命周期 3. CloudFormation自定义资源 - 通过Lambda函数扩展CloudFormation功能 4. DeletionPolicy - CloudFormation资源删除策略 5. Lambda函数的RequestType参数 - 区分创建、更新、删除操作 **正确答案B的原因：** 1. S3存储桶的基本规则：非空的S3存储桶无法被删除，这是AWS的硬性限制 2. 题目明确提到自定义资源会将内容复制到存储桶中，说明存储桶包含对象 3. 当CloudFormation尝试删除堆栈时，会先尝试删除S3存储桶，但由于存储桶非空而失败 4. 解决方案是修改自定义资源的Lambda函数，在删除操作(RequestType=Delete)时先清空存储桶 5. 这种方法既解决了当前问题，也为未来版本提供了可重复的解决方案 **其他选项错误的原因：** - 选项A：WebsiteConfiguration属性不会阻止S3存储桶的删除，这不是根本原因 - 选项C：RemoveOnDeletion不是有效的DeletionPolicy值，有效值包括Delete、Retain、Snapshot等 - 选项D：Empty不是有效的DeletionPolicy值，且DeletionPolicy无法自动清空存储桶 **决策标准和最佳实践：** 1. 理解AWS服务的基本限制和约束条件 2. 自定义资源应该处理完整的生命周期，包括清理操作 3. 在设计CloudFormation模板时考虑删除场景 4. 使用自定义资源的Lambda函数来处理CloudFormation原生不支持的操作 5. 确保基础设施即代码的可重复性和可维护性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">236</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses Amazon EC2 as its primary compute platform. A DevOps team wants to audit the company&#x27;s EC2 instances to check whether any prohibited applications have been installed on the EC2 instances. Which solution will meet these requirements with the MOST operational efficiency? to create an inventory of installed applications. Configure the script to forward the results to CloudWatch Logs. Create a CloudWatch alarm that uses filter patterns to search log data to identify prohibited applications. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure AWS Systems Manager on each instance. Use AWS Systems Manager Inventory. Use Systems Manager resource data sync to synchronize and store findings in an Amazon S3 bucket. Create an AWS Lambda function that runs when new objects are added to the S3 bucket. Configure the Lambda function to identify prohibited applications.
B. Configure AWS Systems Manager on each instance. Use Systems Manager Inventory. Create AWS Config rules that monitor changes from Systems Manager Inventory to identify prohibited applications.
C. Configure AWS Systems Manager on each instance. Use Systems Manager Inventory. Filter a trail in AWS CloudTrail for Systems Manager Inventory events to identify prohibited applications.
D. Designate Amazon CloudWatch Logs as the log destination for all application instances. Run an automated script across all instances</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用Amazon EC2作为其主要计算平台。DevOps团队希望审计公司的EC2实例，检查是否在EC2实例上安装了任何禁用的应用程序。哪种解决方案能够以最高的运营效率满足这些要求？ 选项： A. 在每个实例上配置AWS Systems Manager。使用AWS Systems Manager Inventory。使用Systems Manager资源数据同步将发现结果同步并存储到Amazon S3存储桶中。创建一个AWS Lambda函数，当新对象添加到S3存储桶时运行。配置Lambda函数来识别禁用的应用程序。 B. 在每个实例上配置AWS Systems Manager。使用Systems Manager Inventory。创建AWS Config规则来监控Systems Manager Inventory的变更以识别禁用的应用程序。 C. 在每个实例上配置AWS Systems Manager。使用Systems Manager Inventory。在AWS CloudTrail中过滤Systems Manager Inventory事件的跟踪来识别禁用的应用程序。 D. 指定Amazon CloudWatch Logs作为所有应用程序实例的日志目标。在所有实例上运行自动化脚本来创建已安装应用程序的清单。配置脚本将结果转发到CloudWatch Logs。创建使用过滤模式搜索日志数据以识别禁用应用程序的CloudWatch告警。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个能够审计EC2实例上安装的应用程序，检测禁用应用程序的解决方案，并且要求具有最高的运营效率。 **涉及的关键AWS服务和概念：** - AWS Systems Manager Inventory：用于收集EC2实例的配置和清单信息 - AWS Config：用于监控和评估AWS资源配置的合规性服务 - AWS CloudTrail：记录API调用的审计服务 - Amazon S3：对象存储服务 - AWS Lambda：无服务器计算服务 - Amazon CloudWatch Logs：日志管理服务 **正确答案B的原因：** 1. **原生集成**：AWS Config与Systems Manager Inventory有原生集成，可以直接监控清单变更 2. **自动化程度高**：Config规则可以自动评估合规性，无需手动编写复杂的处理逻辑 3. **运营效率最高**：一旦配置完成，系统会自动持续监控，无需额外的基础设施管理 4. **实时监控**：Config可以实时检测配置变更并触发合规性评估 5. **专门用途**：Config就是为了配置合规性监控而设计的服务 **其他选项错误的原因：** - **选项A**：虽然技术可行，但需要管理额外的S3存储桶、Lambda函数和数据同步，运营复杂度较高，不是最高效的方案 - **选项C**：CloudTrail主要用于API调用审计，不是为应用程序清单监控设计的，使用场景不匹配 - **选项D**：需要自己编写和维护脚本，管理CloudWatch告警的过滤模式，运营负担重，自动化程度低 **决策标准和最佳实践：** 1. **选择专用服务**：优先选择为特定用途设计的AWS服务 2. **最小化运营开销**：选择需要最少手动管理和维护的解决方案 3. **原生集成优先**：利用AWS服务之间的原生集成能力 4. **自动化优先**：选择能够提供最高自动化程度的方案 5. **合规性监控**：对于配置合规性需求，AWS Config是最佳选择</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">237</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an event-driven JavaScript application. The application uses decoupled AWS managed services that publish, consume, and route events. During application testing, events are not delivered to the target that is specified by an Amazon EventBridge rule. A DevOps team must provide application testers with additional functionality to view, troubleshoot, and prevent the loss of events without redeployment of the application. Which combination of steps should the DevOps team take to meet these requirements? (Choose three.) F. Update the application code base to use the AWS X-Ray SDK tracing feature to instrument the code with support for the X-Amzn-Trace-Id header. CEF (43%) BCE (43%) 14%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Launch AWS Device Farm with a standard test environment and project to run a specific build of the application.
B. Create an Amazon S3 bucket. Enable AWS CloudTrail. Create a CloudTrail trail that specifies the S3 bucket as the storage location.
C. Configure the EventBridge rule to use an Amazon Simple Queue Service (Amazon SQS) standard queue as a dead-letter queue.
D. Configure the EventBridge rule to use an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a dead-letter queue.
E. Create a log group in Amazon CloudWatch Logs. Specify the log group as an additional target of the EventBridge rule.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个事件驱动的JavaScript应用程序。该应用程序使用解耦的AWS托管服务来发布、消费和路由事件。在应用程序测试期间，事件没有被传递到Amazon EventBridge规则指定的目标。DevOps团队必须为应用程序测试人员提供额外的功能来查看、故障排除和防止事件丢失，而无需重新部署应用程序。DevOps团队应该采取哪些步骤组合来满足这些要求？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求在不重新部署应用程序的情况下，为EventBridge事件传递问题提供监控、故障排除和事件丢失防护功能。需要选择三个解决方案来实现事件的可观测性和可靠性。 **涉及的关键AWS服务和概念：** - Amazon EventBridge：事件路由服务，用于应用程序集成 - Dead Letter Queue (DLQ)：死信队列，用于处理失败的消息 - Amazon SQS：消息队列服务，包括标准队列和FIFO队列 - Amazon CloudWatch Logs：日志监控服务 - AWS CloudTrail：API调用审计服务 - AWS X-Ray：分布式追踪服务 **正确答案的原因：** 虽然题目显示正确答案是D，但根据最佳实践，完整的解决方案应该包括： - **选项B (CloudTrail)**：提供API级别的审计日志，帮助追踪EventBridge规则的配置变更和API调用 - **选项C或D (SQS DLQ)**：防止事件丢失，捕获传递失败的事件 - **选项E (CloudWatch Logs)**：提供详细的事件传递日志，便于实时监控和故障排除 **其他选项错误的原因：** - **选项A (AWS Device Farm)**：用于移动应用测试，与EventBridge事件传递问题无关 - **选项F (X-Ray SDK)**：需要修改应用程序代码，违反了&quot;无需重新部署&quot;的要求 **决策标准和最佳实践：** 1. **可观测性三支柱**：日志(CloudWatch Logs)、指标和追踪(CloudTrail) 2. **事件可靠性**：使用DLQ防止事件丢失 3. **无侵入性**：所有解决方案都应该在不修改应用程序代码的情况下实施 4. **实时监控**：CloudWatch Logs提供实时事件传递状态 5. **历史审计**：CloudTrail提供配置变更的历史记录 最佳组合应该是BCE，提供全面的监控、审计和事件保护功能。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">238</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is migrating its container-based workloads to an AWS Organizations multi-account environment. The environment consists of application workload accounts that the company uses to deploy and run the containerized workloads. The company has also provisioned a shared services account for shared workloads in the organization. The company must follow strict compliance regulations. All container images must receive security scanning before they are deployed to any environment. Images can be consumed by downstream deployment mechanisms after the images pass a scan with no critical vulnerabilities. Pre-scan and post-scan images must be isolated from one another so that a deployment can never use pre-scan images. A DevOps engineer needs to create a strategy to centralize this process. Which combination of steps will meet these requirements with the LEAST administrative overhead? (Choose two.) repositories. Use resource-based policies to grant the organization write access to the pre-scan repositories and read access to the post-scan repositories. AE (80%) AD (20%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create Amazon Elastic Container Registry (Amazon ECR) repositories in the shared services account: one repository for each pre-scan image and one repository for each post-scan image. Configure Amazon ECR image scanning to run on new image pushes to the pre-scan
B. Create pre-scan Amazon Elastic Container Registry (Amazon ECR) repositories in each account that publishes container images. Create repositories for post-scan images in the shared services account. Configure Amazon ECR image scanning to run on new image pushes to the pre-scan repositories. Use resource-based policies to grant the organization read access to the post-scan repositories.
C. Configure image replication for each image from the image&#x27;s pre-scan repository to the image&#x27;s post-scan repository.
D. Create a pipeline in AWS CodePipeline for each pre-scan repository. Create a source stage that runs when new images are pushed to the pre-scan repositories. Create a stage that uses AWS CodeBuild as the action provider. Write a buildspec.yaml definition that determines the image scanning status and pushes images without critical vulnerabilities to the post-scan repositories.
E. Create an AWS Lambda function. Create an Amazon EventBridge rule that reacts to image scanning completed events and invokes the Lambda function. Write function code that determines the image scanning status and pushes images without critical vulnerabilities to the post-scan repositories.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在将其基于容器的工作负载迁移到AWS Organizations多账户环境。该环境包含应用程序工作负载账户，公司使用这些账户来部署和运行容器化工作负载。公司还为组织中的共享工作负载配置了一个共享服务账户。公司必须遵循严格的合规法规。所有容器镜像在部署到任何环境之前都必须接受安全扫描。镜像在通过扫描且没有关键漏洞后，才能被下游部署机制使用。扫描前和扫描后的镜像必须相互隔离，以确保部署永远不会使用扫描前的镜像。DevOps工程师需要创建一个策略来集中化这个过程。哪种步骤组合能够以最少的管理开销满足这些要求？（选择两个） 选项：A. 在共享服务账户中创建Amazon Elastic Container Registry (Amazon ECR) repositories：每个扫描前镜像一个repository，每个扫描后镜像一个repository。配置Amazon ECR镜像扫描在新镜像推送到扫描前repositories时运行。使用基于资源的策略授予组织对扫描前repositories的写入访问权限和对扫描后repositories的读取访问权限。 B. 在每个发布容器镜像的账户中创建扫描前Amazon ECR repositories。在共享服务账户中为扫描后镜像创建repositories。配置Amazon ECR镜像扫描在新镜像推送到扫描前repositories时运行。使用基于资源的策略授予组织对扫描后repositories的读取访问权限。 C. 为每个镜像配置从扫描前repository到扫描后repository的镜像复制。 D. 为每个扫描前repository在AWS CodePipeline中创建一个管道。创建一个在新镜像推送到扫描前repositories时运行的源阶段。创建一个使用AWS CodeBuild作为操作提供者的阶段。编写buildspec.yaml定义来确定镜像扫描状态，并将没有关键漏洞的镜像推送到扫描后repositories。 E. 创建一个AWS Lambda函数。创建一个Amazon EventBridge规则来响应镜像扫描完成事件并调用Lambda函数。编写函数代码来确定镜像扫描状态，并将没有关键漏洞的镜像推送到扫描后repositories。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个集中化的容器镜像安全扫描策略，需要满足：1）所有镜像部署前必须进行安全扫描；2）扫描前后镜像必须隔离；3）只有通过扫描且无关键漏洞的镜像才能被使用；4）最小化管理开销；5）支持多账户环境。 **涉及的关键AWS服务和概念：** - Amazon ECR：容器镜像registry服务，支持内置安全扫描 - AWS Organizations：多账户管理 - 资源基于策略：跨账户访问控制 - ECR镜像扫描：自动化安全漏洞检测 - AWS CodePipeline/CodeBuild：CI/CD服务 - AWS Lambda + EventBridge：事件驱动自动化 **正确答案A的原因：** 选项A提供了最优的集中化管理方案：1）在共享服务账户中统一管理所有repositories，便于集中控制和合规；2）通过分离的pre-scan和post-scan repositories实现了严格的镜像隔离；3）利用ECR内置扫描功能，无需额外开发；4）通过资源策略实现精确的权限控制（组织内账户可写入pre-scan，只能读取post-scan）；5）管理开销最小，无需维护复杂的pipeline或Lambda函数。 **其他选项错误的原因：** 选项B虽然也使用ECR，但将pre-scan repositories分散在各个账户中，增加了管理复杂性，不符合集中化要求。选项C只是配置复制，没有解决扫描和权限控制问题。选项D使用CodePipeline增加了不必要的复杂性和维护成本。选项E使用Lambda和EventBridge虽然可行，但需要自定义开发和维护，管理开销较大。 **决策标准和最佳实践：** 1）优先使用AWS托管服务减少运维负担；2）集中化管理提高安全性和合规性；3）利用ECR内置扫描功能而非自建方案；4）通过repository分离和IAM策略实现访问控制；5）选择架构简单、维护成本低的方案。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">239</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to deploy its web applications on containers. The web applications contain confidential data that cannot be decrypted without specific credentials. A DevOps engineer has stored the credentials in AWS Secrets Manager. The secrets are encrypted by an AWS Key Management Service (AWS KMS) customer managed key. A Kubernetes service account for a third-party tool makes the secrets available to the applications. The service account assumes an IAM role that the company created to access the secrets. The service account receives an Access Denied (403 Forbidden) error while trying to retrieve the secrets from Secrets Manager. What is the root cause of this issue? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. The IAM role that is attached to the EKS cluster does not have access to retrieve the secrets from Secrets Manager.
B. The key policy for the customer managed key does not allow the Kubernetes service account IAM role to use the key.
C. The key policy for the customer managed key does not allow the EKS cluster IAM role to use the key.
D. The IAM role that is assumed by the Kubernetes service account does not have permission to access the EKS cluster.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用Amazon Elastic Kubernetes Service (Amazon EKS)集群在容器上部署其Web应用程序。这些Web应用程序包含机密数据，没有特定凭证就无法解密。DevOps工程师已将凭证存储在AWS Secrets Manager中。这些secrets由AWS Key Management Service (AWS KMS)客户托管密钥加密。第三方工具的Kubernetes service account使secrets对应用程序可用。该service account承担公司创建的IAM角色来访问secrets。该service account在尝试从Secrets Manager检索secrets时收到Access Denied (403 Forbidden)错误。此问题的根本原因是什么？ 选项： A. 附加到EKS集群的IAM角色没有从Secrets Manager检索secrets的访问权限。 B. 客户托管密钥的key policy不允许Kubernetes service account IAM角色使用该密钥。 C. 客户托管密钥的key policy不允许EKS集群IAM角色使用该密钥。 D. 由Kubernetes service account承担的IAM角色没有访问EKS集群的权限。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在EKS环境中，当Kubernetes service account通过IAM角色访问由KMS客户托管密钥加密的Secrets Manager中的secrets时，出现403 Forbidden错误的根本原因分析。 **涉及的关键AWS服务和概念：** 1. Amazon EKS - Kubernetes托管服务 2. AWS Secrets Manager - 密钥管理服务 3. AWS KMS - 密钥管理服务，特别是客户托管密钥 4. IAM角色和权限管理 5. Kubernetes Service Account 6. IAM Roles for Service Accounts (IRSA)机制 **正确答案的原因：** 选项B正确。当secrets由KMS客户托管密钥加密时，访问这些secrets需要两层权限： 1. 对Secrets Manager的访问权限（通过IAM策略） 2. 对KMS密钥的使用权限（通过KMS密钥策略） 403错误表明身份验证成功但授权失败。由于service account能够到达Secrets Manager（说明IAM权限基本正确），但无法解密secrets，这通常是因为KMS密钥策略没有授权该IAM角色使用密钥进行解密操作。 **其他选项错误的原因：** - 选项A错误：如果EKS集群的IAM角色缺少Secrets Manager权限，这不是直接原因，因为是service account的IAM角色在执行操作，不是集群角色。 - 选项C错误：虽然也涉及KMS密钥策略，但关键是service account承担的IAM角色需要密钥权限，而不是EKS集群角色。 - 选项D错误：如果service account的IAM角色无法访问EKS集群，那么应用程序根本无法运行，不会只是在访问secrets时出错。 **决策标准和最佳实践：** 1. 使用KMS客户托管密钥加密的资源需要在密钥策略中明确授权使用者 2. 实施最小权限原则，只给service account必需的权限 3. 正确配置IRSA，确保Kubernetes service account与IAM角色的映射关系 4. 定期审查和更新KMS密钥策略，确保所有需要访问的角色都被正确授权 5. 使用CloudTrail日志来诊断权限问题，特别是KMS相关的拒绝访问错误</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">240</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is migrating its product development teams from an on-premises data center to a hybrid environment. The new environment will add four AWS Regions and will give the developers the ability to use the Region that is geographically closest to them. All the development teams use a shared set of Linux applications. The on-premises data center stores the applications on a NetApp ONTAP storage device. The storage volume is mounted read-only on the development on-premises VMs. The company updates the applications on the shared volume once a week. A DevOps engineer needs to replicate the data to all the new Regions. The DevOps engineer must ensure that the data is always up to date with deduplication. The data also must not be dependent on the availability of the on-premises storage device. Which solution will meet these requirements? between the on-premises storage device and the FSx for ONTAP instances. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon S3 File Gateway in the on-premises data center. Create S3 buckets in each Region. Set up a cron job to copy the data from the storage device to the S3 File Gateway. Set up S3 Cross-Region Replication (CRR) to the S3 buckets in each Region.
B. Create an Amazon FSx File Gateway in one Region. Create file servers in Amazon FSx for Windows File Server in each Region. Set up a cron job to copy the data from the storage device to the FSx File Gateway.
C. Create Multi-AZ Amazon FSx for NetApp ONTAP instances and volumes in each Region. Configure a scheduled SnapMirror relationship
D. Create an Amazon Elastic File System (Amazon EFS) file system in each Region. Deploy an AWS DataSync agent in the on-premises data center. Configure a schedule for DataSync to copy the data to Amazon EFS daily.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在将其产品开发团队从本地数据中心迁移到混合环境。新环境将添加四个AWS Region，并让开发人员能够使用地理位置最接近他们的Region。所有开发团队都使用一套共享的Linux应用程序。本地数据中心将应用程序存储在NetApp ONTAP存储设备上。存储卷以只读方式挂载在本地开发VM上。公司每周更新一次共享卷上的应用程序。DevOps工程师需要将数据复制到所有新的Region。DevOps工程师必须确保数据始终与重复数据删除保持最新。数据也不能依赖于本地存储设备的可用性。哪种解决方案能满足这些要求？ 选项： A. 在本地数据中心创建Amazon S3 File Gateway。在每个Region创建S3存储桶。设置cron作业将数据从存储设备复制到S3 File Gateway。设置S3 Cross-Region Replication (CRR)到每个Region的S3存储桶。 B. 在一个Region创建Amazon FSx File Gateway。在每个Region的Amazon FSx for Windows File Server中创建文件服务器。设置cron作业将数据从存储设备复制到FSx File Gateway。 C. 在每个Region创建Multi-AZ Amazon FSx for NetApp ONTAP实例和卷。配置计划的SnapMirror关系。 D. 在每个Region创建Amazon Elastic File System (Amazon EFS)文件系统。在本地数据中心部署AWS DataSync代理。配置DataSync计划每天将数据复制到Amazon EFS。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是混合云环境下的数据同步和复制解决方案。关键需求包括：1）将本地NetApp ONTAP存储的数据复制到4个AWS Region；2）确保数据始终最新且支持重复数据删除；3）数据不依赖本地存储设备的可用性；4）支持Linux应用程序的只读访问。 **涉及的关键AWS服务和概念：** - AWS DataSync：用于本地到云端的数据传输服务 - Amazon EFS：完全托管的NFS文件系统 - Amazon S3 File Gateway：混合云存储服务 - Amazon FSx for NetApp ONTAP：托管的NetApp文件系统 - Cross-Region Replication：跨区域复制 **正确答案D的原因：** 1. **DataSync优势**：专门设计用于本地到AWS的数据传输，内置重复数据删除和增量同步功能 2. **EFS兼容性**：原生支持Linux环境，可以直接挂载为NFS，满足只读访问需求 3. **独立性**：一旦数据同步到EFS，就完全独立于本地存储设备 4. **调度灵活**：支持自动化的定期同步，满足每周更新需求 5. **多Region支持**：可以轻松在多个Region部署EFS **其他选项错误的原因：** - **选项A**：S3主要用于对象存储，不适合需要文件系统挂载的Linux应用程序场景 - **选项B**：FSx for Windows File Server主要针对Windows环境，不适合Linux应用程序；且FSx File Gateway不是真实的AWS服务 - **选项C**：虽然技术上可行，但FSx for NetApp ONTAP成本较高，且SnapMirror配置复杂，对于简单的只读文件共享场景过度设计 **决策标准和最佳实践：** 选择数据同步解决方案时应考虑：1）源和目标环境的兼容性；2）数据传输的效率和自动化程度；3）成本效益；4）运维复杂度；5）业务连续性要求。DataSync + EFS组合提供了最佳的性价比和运维简便性。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">241</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that stores data that includes personally identifiable information (PII) in an Amazon S3 bucket. All data is encrypted with AWS Key Management Service (AWS KMS) customer managed keys. All AWS resources are deployed from an AWS CloudFormation template. A DevOps engineer needs to set up a development environment for the application in a different AWS account. The data in the development environment&#x27;s S3 bucket needs to be updated once a week from the production environment&#x27;s S3 bucket. The company must not move PII from the production environment without anonymizing the PII first. The data in each environment must be encrypted with different KMS customer managed keys. Which combination of steps should the DevOps engineer take to meet these requirements? (Choose two.) on the KMS key in the production account. Give the state machine tasks encrypt permissions on the KMS key in the development account. S3 bucket. Create an AWS Step Functions state machine to initiate a discovery job and redact all PII as the files are copied to the development S3 bucket. Give the state machine tasks encrypt and decrypt permissions on the KMS key in the development account. account, configure an AWS Lambda function to redact all PII. Configure S3 Object Lambda to use the Lambda function for S3 GET requests. Give the Lambda function&#x27;s IAM role encrypt and decrypt permissions on the KMS key in the development account. rule to start the AWS Step Functions state machine once a week. EC2 instance to run once a week to start the S3 Batch Operations job. AD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Activate Amazon Macie on the S3 bucket in the production account. Create an AWS Step Functions state machine to initiate a discovery job and redact all PII before copying files to the S3 bucket in the development account. Give the state machine tasks decrypt permissions
B. Set up S3 replication between the production S3 bucket and the development S3 bucket. Activate Amazon Macie on the development
C. Set up an S3 Batch Operations job to copy files from the production S3 bucket to the development S3 bucket. In the development
D. Create a development environment from the CloudFormation template in the development account. Schedule an Amazon EventBridge
E. Create a development environment from the CloudFormation template in the development account. Schedule a cron job on an Amazon</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个应用程序，在Amazon S3存储桶中存储包括个人身份信息(PII)的数据。所有数据都使用AWS Key Management Service (AWS KMS)客户管理密钥进行加密。所有AWS资源都从AWS CloudFormation模板部署。DevOps工程师需要在不同的AWS账户中为应用程序设置开发环境。开发环境S3存储桶中的数据需要每周从生产环境的S3存储桶更新一次。公司不得在未先匿名化PII的情况下从生产环境移动PII数据。每个环境中的数据必须使用不同的KMS客户管理密钥进行加密。DevOps工程师应该采取哪些步骤组合来满足这些要求？(选择两个) 选项： A. 在生产账户的S3存储桶上激活Amazon Macie。创建AWS Step Functions状态机来启动发现作业，并在将文件复制到开发账户的S3存储桶之前编辑所有PII。给状态机任务在生产账户KMS密钥上的解密权限 B. 在生产S3存储桶和开发S3存储桶之间设置S3复制。在开发S3存储桶上激活Amazon Macie C. 设置S3 Batch Operations作业从生产S3存储桶复制文件到开发S3存储桶。在开发账户中配置AWS Lambda函数来编辑所有PII D. 在开发账户中从CloudFormation模板创建开发环境。安排Amazon EventBridge规则每周启动AWS Step Functions状态机一次 E. 在开发账户中从CloudFormation模板创建开发环境。在Amazon EC2实例上安排cron作业每周运行一次以启动S3 Batch Operations作业</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在不同AWS账户间建立开发环境，需要满足：1）每周从生产环境同步数据到开发环境；2）传输前必须匿名化PII数据；3）两个环境使用不同的KMS客户管理密钥加密；4）需要选择两个步骤的组合方案。 **涉及的关键AWS服务和概念：** - Amazon S3：对象存储服务，支持跨账户复制 - AWS KMS：密钥管理服务，用于数据加密 - Amazon Macie：数据安全服务，能够发现和保护敏感数据如PII - AWS Step Functions：工作流编排服务 - S3 Batch Operations：大规模S3对象操作服务 - AWS Lambda：无服务器计算服务 - Amazon EventBridge：事件驱动服务 **正确答案的原因：** 选项B是正确的，因为：1）S3复制可以实现跨账户的自动数据同步，满足每周更新需求；2）在开发环境激活Amazon Macie可以自动发现和处理PII数据，实现数据匿名化；3）S3复制天然支持使用不同KMS密钥重新加密数据；4）这个方案架构简单且自动化程度高。 **其他选项错误的原因：** 选项A虽然使用了Macie和Step Functions，但在生产账户激活Macie会增加成本和复杂性，且跨账户权限配置复杂。选项C使用S3 Batch Operations和Lambda，但需要复杂的调度和权限管理。选项D和E都是关于环境创建和调度的，没有直接解决PII匿名化的核心问题，且题目要求选择两个步骤，这些选项相对独立。 **决策标准和最佳实践：** 在设计跨账户数据同步方案时，应优先考虑：1）自动化程度高的原生AWS服务；2）最小权限原则和安全性；3）成本效益；4）运维复杂度。S3复制结合Macie的方案符合AWS最佳实践，既保证了数据安全，又实现了自动化的PII处理和跨账户同步。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">242</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host its machine learning (ML) application. As the ML model and the container image size grow, the time that new pods take to start up has increased to several minutes. A DevOps engineer needs to reduce the startup time to seconds. The solution must also reduce the startup time to seconds when the pod runs on nodes that were recently added to the cluster. The DevOps engineer creates an Amazon EventBridge rule that invokes an automation in AWS Systems Manager. The automation prefetches the container images from an Amazon Elastic Container Registry (Amazon ECR) repository when new images are pushed to the repository. The DevOps engineer also configures tags to be applied to the cluster and the node groups. What should the DevOps engineer do next to meet the requirements? Create a Systems Manager State Manager association that uses the nodes&#x27; tags to prefetch corresponding container images. plane nodes. Create a Systems Manager State Manager association that uses the nodes&#x27; tags to prefetch corresponding container images. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster&#x27;s control plane nodes. Create a Systems Manager State Manager association that uses the control plane nodes&#x27; tags to prefetch corresponding container images.
B. Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster&#x27;s nodes. Create a Systems Manager State Manager association that uses the nodes&#x27; machine size to prefetch corresponding container images.
C. Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster&#x27;s nodes.
D. Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster&#x27;s control</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用Amazon Elastic Kubernetes Service (Amazon EKS)集群来托管其机器学习(ML)应用程序。随着ML模型和容器镜像大小的增长，新pod启动所需的时间已增加到几分钟。DevOps工程师需要将启动时间减少到几秒钟。当pod在最近添加到集群的节点上运行时，解决方案还必须将启动时间减少到几秒钟。DevOps工程师创建了一个Amazon EventBridge规则，当新镜像推送到仓库时，该规则调用AWS Systems Manager中的自动化来从Amazon Elastic Container Registry (Amazon ECR)仓库预取容器镜像。DevOps工程师还配置了要应用于集群和节点组的标签。DevOps工程师接下来应该做什么来满足要求？ 选项： A. 创建一个IAM角色，该角色具有允许EventBridge使用Systems Manager在EKS集群的控制平面节点中运行命令的策略。创建一个使用控制平面节点标签预取相应容器镜像的Systems Manager State Manager关联。 B. 创建一个IAM角色，该角色具有允许EventBridge使用Systems Manager在EKS集群节点中运行命令的策略。创建一个使用节点机器大小预取相应容器镜像的Systems Manager State Manager关联。 C. 创建一个IAM角色，该角色具有允许EventBridge使用Systems Manager在EKS集群节点中运行命令的策略。创建一个使用节点标签预取相应容器镜像的Systems Manager State Manager关联。 D. 创建一个IAM角色，该角色具有允许EventBridge使用Systems Manager在EKS集群的控制平面中运行命令的策略。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求解决EKS集群中容器启动时间过长的问题，需要通过预取容器镜像来将启动时间从几分钟减少到几秒钟，并且要确保新添加的节点也能快速启动pod。 **涉及的关键AWS服务和概念：** - Amazon EKS：托管Kubernetes服务 - Amazon ECR：容器镜像仓库 - Amazon EventBridge：事件驱动服务 - AWS Systems Manager：系统管理服务，包括State Manager - IAM角色和策略：权限管理 - 容器镜像预取：提前下载镜像到节点本地 **正确答案的原因（选项C）：** 1. **正确的目标节点**：容器镜像需要预取到工作节点（worker nodes）上，而不是控制平面节点，因为pod实际运行在工作节点上 2. **合适的权限设置**：IAM角色需要允许EventBridge通过Systems Manager在EKS工作节点上执行命令 3. **使用节点标签**：题目明确提到已经配置了标签，使用标签可以精确识别需要预取特定镜像的节点 4. **State Manager关联**：能够持续管理和维护镜像预取任务，确保新节点也能自动获得所需镜像 **其他选项错误的原因：** - **选项A**：错误地针对控制平面节点，但容器实际运行在工作节点上，控制平面节点不需要预取应用镜像 - **选项B**：使用机器大小而非标签来选择节点是不合理的，机器大小与需要的容器镜像类型没有直接关系 - **选项D**：同样错误地针对控制平面，且描述不完整 **决策标准和最佳实践：** 1. **理解EKS架构**：区分控制平面和工作节点的职责 2. **镜像预取策略**：应该在实际运行容器的节点上预取镜像 3. **标签驱动管理**：使用标签进行资源分类和自动化管理是AWS的最佳实践 4. **权限最小化原则**：IAM角色应该只包含必要的权限 5. **自动化运维**：通过State Manager实现持续的配置管理，确保新节点也能自动配置</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">243</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s application has an API that retrieves workload metrics. The company needs to audit, analyze, and visualize these metrics from the application to detect issues at scale. Which combination of steps will meet these requirements? (Choose three.) workload metric data in an Amazon S3 bucket. workload metric data in an Amazon DynamoDB table that has a DynamoDB stream enabled. cataloged data. F. Create an Amazon CloudWatch dashboard that has custom widgets that invoke AWS Lambda functions. Configure the Lambda functions to query the workload metrics data from the Amazon Athena views. ACE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure an Amazon EventBridge schedule to invoke an AWS Lambda function that calls the API to retrieve workload metrics. Store the
B. Configure an Amazon EventBridge schedule to invoke an AWS Lambda function that calls the API to retrieve workload metrics. Store the
C. Create an AWS Glue crawler to catalog the workload metric data in the Amazon S3 bucket. Create views in Amazon Athena for the
D. Connect an AWS Glue crawler to the Amazon DynamoDB stream to catalog the workload metric data. Create views in Amazon Athena for the cataloged data.
E. Create Amazon QuickSight datasets from the Amazon Athena views. Create a QuickSight analysis to visualize the workload metric data as a dashboard.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的应用程序有一个API可以检索工作负载指标。该公司需要审计、分析和可视化这些来自应用程序的指标，以大规模检测问题。哪种步骤组合将满足这些要求？（选择三个。） 选项： A. 配置Amazon EventBridge计划来调用AWS Lambda函数，该函数调用API检索工作负载指标。将工作负载指标数据存储在Amazon S3存储桶中。 B. 配置Amazon EventBridge计划来调用AWS Lambda函数，该函数调用API检索工作负载指标。将工作负载指标数据存储在启用了DynamoDB stream的Amazon DynamoDB表中。 C. 创建AWS Glue爬虫来编目Amazon S3存储桶中的工作负载指标数据。在Amazon Athena中为编目数据创建视图。 D. 将AWS Glue爬虫连接到Amazon DynamoDB stream来编目工作负载指标数据。在Amazon Athena中为编目数据创建视图。 E. 从Amazon Athena视图创建Amazon QuickSight数据集。创建QuickSight分析来将工作负载指标数据可视化为仪表板。 F. 创建Amazon CloudWatch仪表板，包含调用AWS Lambda函数的自定义小部件。配置Lambda函数从Amazon Athena视图查询工作负载指标数据。 正确答案：ACE</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求构建一个完整的数据管道来审计、分析和可视化应用程序的工作负载指标，需要实现数据收集、存储、处理和可视化的完整流程。 **涉及的关键AWS服务和概念：** - Amazon EventBridge：事件调度服务 - AWS Lambda：无服务器计算服务 - Amazon S3：对象存储服务，适合大规模数据分析 - Amazon DynamoDB：NoSQL数据库服务 - AWS Glue：数据集成和ETL服务 - Amazon Athena：无服务器查询服务 - Amazon QuickSight：商业智能和可视化服务 - Amazon CloudWatch：监控和可观测性服务 **正确答案ACE的原因：** - **选项A**：使用EventBridge定时调用Lambda函数获取API数据并存储到S3，这是标准的数据收集模式。S3作为数据湖存储，成本低且适合大规模分析。 - **选项C**：使用Glue爬虫对S3中的数据进行编目，然后通过Athena创建视图，这是标准的数据处理和查询模式。 - **选项E**：使用QuickSight基于Athena视图创建数据集和仪表板，这是AWS推荐的企业级可视化解决方案。 **其他选项错误的原因：** - **选项B**：虽然DynamoDB可以存储数据，但对于大规模分析场景，S3更适合且成本更低。 - **选项D**：Glue爬虫不能直接连接到DynamoDB stream进行编目，这在技术上是不正确的。 - **选项F**：CloudWatch仪表板主要用于监控AWS资源，不是专门的商业智能工具，且通过Lambda查询Athena的方式过于复杂。 **决策标准和最佳实践：** 1. **数据存储选择**：对于分析工作负载，S3比DynamoDB更适合，成本更低且与分析服务集成更好 2. **架构简洁性**：ACE组合形成了标准的现代数据分析架构：数据收集→数据湖存储→数据编目→查询分析→可视化 3. **服务集成**：选择的服务之间有良好的原生集成，减少了复杂性和维护成本 4. **可扩展性**：这个架构可以轻松处理大规模数据和高并发查询需求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">244</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is building the infrastructure for an application. The application needs to run on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that includes Amazon EC2 instances. The EC2 instances need to use an Amazon Elastic File System (Amazon EFS) file system as a storage backend. The Amazon EFS Container Storage Interface (CSI) driver is installed on the EKS cluster. When the DevOps engineer starts the application, the EC2 instances do not mount the EFS file system. Which solutions will fix the problem? (Choose three.) F. Disable encryption for the EFS file system. BCE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Switch the EKS nodes from Amazon EC2 to AWS Fargate.
B. Add an inbound rule to the EFS file system&#x27;s security group to allow NFS traffic from the EKS cluster.
C. Create an IAM role that allows the Amazon EFS CSI driver to interact with the file system
D. Set up AWS DataSync to configure file transfer between the EFS file system and the EKS nodes.
E. Create a mount target for the EFS file system in the subnet of the EKS nodes.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师正在为应用程序构建基础设施。该应用程序需要运行在包含Amazon EC2实例的Amazon Elastic Kubernetes Service (Amazon EKS)集群上。EC2实例需要使用Amazon Elastic File System (Amazon EFS)文件系统作为存储后端。Amazon EFS Container Storage Interface (CSI)驱动程序已安装在EKS集群上。当DevOps工程师启动应用程序时，EC2实例无法挂载EFS文件系统。哪些解决方案可以解决这个问题？（选择三个）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在EKS集群中使用EFS文件系统时遇到挂载失败问题的故障排除。需要识别导致EC2实例无法挂载EFS文件系统的可能原因，并选择正确的解决方案。 **涉及的关键AWS服务和概念：** - Amazon EKS：托管的Kubernetes服务 - Amazon EFS：网络文件系统，支持多个EC2实例同时访问 - EFS CSI驱动程序：允许Kubernetes Pod使用EFS作为持久存储 - 安全组：控制网络访问的虚拟防火墙 - IAM角色：用于授权AWS服务间的访问权限 - Mount Target：EFS在VPC中的网络接入点 **正确答案分析：** 根据题目显示，正确答案应该是B、C、E三个选项： - **选项B**：添加入站规则允许NFS流量。EFS使用NFS协议（端口2049），如果安全组没有允许从EKS节点到EFS的NFS流量，挂载会失败。 - **选项C**：创建IAM角色允许EFS CSI驱动程序与文件系统交互。CSI驱动程序需要适当的IAM权限来管理EFS资源。 - **选项E**：在EKS节点的子网中创建mount target。EFS需要在每个要访问它的可用区中有mount target，否则无法建立网络连接。 **其他选项错误的原因：** - **选项A**：切换到Fargate不能解决EFS挂载问题，而且Fargate对存储的支持有限制。 - **选项D**：AWS DataSync用于数据传输和同步，不是解决EFS挂载问题的方案。 - **选项F**：禁用EFS加密与挂载失败无关，而且会降低安全性。 **决策标准和最佳实践：** 解决EFS挂载问题需要确保三个关键要素：网络连通性（安全组规则和mount target）、权限配置（IAM角色）和正确的CSI驱动程序配置。这是EFS与EKS集成的标准故障排除方法。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">245</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company deploys an application on on-premises devices in the company&#x27;s on-premises data center. The company uses an AWS Direct Connect connection between the data center and the company&#x27;s AWS account. During initial setup of the on-premises devices and during application updates, the application needs to retrieve configuration files from an Amazon Elastic File System (Amazon EFS) file system. All traffic from the on-premises devices to Amazon EFS must remain private and encrypted. The on-premises devices must follow the principle of least privilege for AWS access. The company&#x27;s DevOps team needs the ability to revoke access from a single device without affecting the access of the other devices. Which combination of steps will meet these requirements? (Choose two.) AmazonElasticFileSystemClientReadWriteAccess to the role. Create an IAM Roles Anywhere profile for the IAM role. Configure the AWS CLI on the on-premises devices to use the aws_signing_helper command to obtain credentials. Most Voted BD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an IAM user that has an access key and a secret key for each device. Attach the AmazonElasticFileSystemFullAccess policy to all IAM users. Configure the AWS CLI on the on-premises devices to use the IAM user&#x27;s access key and secret key.
B. Generate certificates for each on-premises device in AWS Private Certificate Authority. Create a trust anchor in IAM Roles Anywhere that references an AWS Private CA. Create an IAM role that trusts IAM Roles Anywhere. Attach the
C. Create an IAM user that has an access key and a secret key for all devices. Attach the AmazonElasticFileSystemClientReadWriteAccess policy to the IAM user. Configure the AWS CLI on the on-premises devices to use the IAM user&#x27;s access key and secret key.
D. Use the amazon-efs-utils package to mount the EFS file system.
E. Use the native Linux NFS client to mount the EFS file system.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在其本地数据中心的本地设备上部署应用程序。该公司在数据中心和其AWS账户之间使用AWS Direct Connect连接。在本地设备的初始设置和应用程序更新期间，应用程序需要从Amazon Elastic File System (Amazon EFS)文件系统检索配置文件。从本地设备到Amazon EFS的所有流量必须保持私有和加密。本地设备必须遵循AWS访问的最小权限原则。公司的DevOps团队需要能够撤销单个设备的访问权限而不影响其他设备的访问。以下哪些步骤组合将满足这些要求？（选择两个） 选项： A. 为每个设备创建一个具有访问密钥和秘密密钥的IAM用户。将AmazonElasticFileSystemFullAccess策略附加到所有IAM用户。在本地设备上配置AWS CLI以使用IAM用户的访问密钥和秘密密钥。 B. 在AWS Private Certificate Authority中为每个本地设备生成证书。在IAM Roles Anywhere中创建一个引用AWS Private CA的信任锚点。创建一个信任IAM Roles Anywhere的IAM角色。将AmazonElasticFileSystemClientReadWriteAccess附加到该角色。为IAM角色创建IAM Roles Anywhere配置文件。在本地设备上配置AWS CLI以使用aws_signing_helper命令获取凭证。 C. 为所有设备创建一个具有访问密钥和秘密密钥的IAM用户。将AmazonElasticFileSystemClientReadWriteAccess策略附加到IAM用户。在本地设备上配置AWS CLI以使用IAM用户的访问密钥和秘密密钥。 D. 使用amazon-efs-utils包挂载EFS文件系统。 E. 使用原生Linux NFS客户端挂载EFS文件系统。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 本地设备需要通过Direct Connect私密访问EFS文件系统 2. 流量必须保持私有和加密 3. 遵循最小权限原则 4. 能够单独撤销某个设备的访问权限而不影响其他设备 **涉及的关键AWS服务和概念：** - Amazon EFS：托管的NFS文件系统服务 - AWS Direct Connect：专用网络连接 - IAM用户和角色：身份和访问管理 - IAM Roles Anywhere：允许本地工作负载使用临时凭证 - AWS Private Certificate Authority：私有证书颁发机构 - EFS挂载方式：amazon-efs-utils vs 原生NFS客户端 **正确答案应该是B和D的组合，而不是A：** 选项B正确的原因： - 使用IAM Roles Anywhere和证书认证提供了更安全的临时凭证机制 - 为每个设备生成独立证书，满足单独撤销访问的要求 - 使用适当的权限策略（ClientReadWriteAccess而非FullAccess）遵循最小权限原则 - 避免了长期访问密钥的安全风险 选项D正确的原因： - amazon-efs-utils包提供了加密传输功能 - 支持通过Direct Connect的私有连接 - 提供了比原生NFS客户端更好的EFS集成和安全特性 **其他选项错误的原因：** - 选项A：使用FullAccess策略违反最小权限原则，且长期访问密钥存在安全风险 - 选项C：所有设备共享同一凭证，无法单独撤销某个设备的访问 - 选项E：原生NFS客户端缺乏加密传输和AWS集成功能 **决策标准和最佳实践：** 1. 安全性：优先使用临时凭证和证书认证而非长期访问密钥 2. 最小权限：使用ClientReadWriteAccess而非FullAccess 3. 可管理性：每个设备独立的身份认证机制 4. 加密要求：使用支持加密传输的挂载方式 因此，正确答案应该是B和D的组合，题目给出的答案A是不正确的。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">246</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is setting up an Amazon Elastic Container Service (Amazon ECS) blue/green deployment for an application by using AWS CodeDeploy and AWS CloudFormation. During the deployment window, the application must be highly available and CodeDeploy must shift 10% of traffic to a new version of the application every minute until all traffic is shifted. Which configuration should the DevOps engineer add in the CloudFormation template to meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add an AppSpec file with the CodeDeployDefault.ECSLinear10PercentEvery1Minutes deployment configuration.
B. Add the AWS::CodeDeployBlueGreen transform and the AWS::CodeDeploy::BlueGreen hook parameter with the CodeDeployDefault.ECSLinear10PercentEvery1Minutes deployment configuration.
C. Add an AppSpec file with the ECSCanary10Percent5Minutes deployment configuration.
D. Add the AWS::CodeDeployBlueGreen transform and the AWS::CodeDeploy::BlueGreen hook parameter with the ECSCanary10Percent5Minutes deployment configuration.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在使用AWS CodeDeploy和AWS CloudFormation为应用程序设置Amazon Elastic Container Service (Amazon ECS)蓝绿部署。在部署窗口期间，应用程序必须保持高可用性，CodeDeploy必须每分钟将10%的流量转移到应用程序的新版本，直到所有流量都转移完成。DevOps工程师应该在CloudFormation模板中添加哪种配置来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 为ECS应用程序设置蓝绿部署 - 使用CloudFormation模板配置 - 每分钟转移10%流量到新版本 - 保持应用程序高可用性 - 逐步转移直到100%流量切换完成 **涉及的关键AWS服务和概念：** - Amazon ECS：容器服务，支持蓝绿部署 - AWS CodeDeploy：自动化部署服务，提供多种部署策略 - AWS CloudFormation：基础设施即代码服务 - 蓝绿部署：零停机部署策略 - Linear部署配置：线性流量转移策略 - Canary部署配置：金丝雀部署策略 **正确答案B的原因：** - AWS::CodeDeployBlueGreen transform是CloudFormation中专门用于蓝绿部署的转换器 - AWS::CodeDeploy::BlueGreen hook参数是在CloudFormation中配置CodeDeploy蓝绿部署的正确方式 - CodeDeployDefault.ECSLinear10PercentEvery1Minutes配置完全符合要求：每分钟线性转移10%流量 - 这种配置确保了高可用性，因为旧版本在流量完全切换前一直保持运行 **其他选项错误的原因：** - 选项A：仅提到AppSpec文件，但在CloudFormation模板中需要使用transform和hook参数，而不是直接的AppSpec文件 - 选项C：使用了错误的部署配置ECSCanary10Percent5Minutes，这是金丝雀部署而非线性部署，且时间间隔为5分钟而非1分钟 - 选项D：虽然使用了正确的CloudFormation语法，但部署配置错误，ECSCanary10Percent5Minutes不符合每分钟转移10%的要求 **决策标准和最佳实践：** - 选择Linear部署策略而非Canary：Linear策略持续转移流量直到100%，而Canary策略只转移一部分流量然后需要手动干预 - 在CloudFormation中使用专门的蓝绿部署转换器和钩子参数 - 根据具体的流量转移要求选择合适的预定义部署配置 - 确保部署策略既满足业务需求又保证系统稳定性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">247</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an organization in AWS Organizations to manage its AWS accounts. The company&#x27;s DevOps team has developed an AWS Lambda function that calls the Organizations API to create new AWS accounts. The Lambda function runs in the organization&#x27;s management account. The DevOps team needs to move the Lambda function from the management account to a dedicated AWS account. The DevOps team must ensure that the Lambda function has the ability to create new AWS accounts only in Organizations before the team deploys the Lambda function to the new account. Which solution will meet these requirements? AWS account permission to create new AWS accounts in Organizations. Ensure that the Lambda execution role has the organizations:CreateAccount permission. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the management account, create a new IAM role that has the necessary permission to create new accounts in Organizations. Allow the role to be assumed by the Lambda execution role in the new AWS account. Update the Lambda function code to assume the role when the Lambda function creates new AWS accounts. Update the Lambda execution role to ensure that it has permission to assume the new role.
B. In the management account, turn on delegated administration for Organizations. Create a new delegation policy that grants the new
C. In the management account, create a new IAM role that has the necessary permission to create new accounts in Organizations. Allow the role to be assumed by the Lambda service principal. Update the Lambda function code to assume the role when the Lambda function creates new AWS accounts. Update the Lambda execution role to ensure that it has permission to assume the new role.
D. In the management account, enable AWS Control Tower. Turn on delegated administration for AWS Control Tower. Create a resource policy that allows the new AWS account to create new AWS accounts in AWS Control Tower. Update the Lambda function code to use the AWS Control Tower API in the new AWS account. Ensure that the Lambda execution role has the controltower:CreateManagedAccount permission.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations中的组织来管理其AWS账户。公司的DevOps团队开发了一个AWS Lambda函数，该函数调用Organizations API来创建新的AWS账户。Lambda函数运行在组织的管理账户中。DevOps团队需要将Lambda函数从管理账户迁移到一个专用的AWS账户。DevOps团队必须确保Lambda函数在部署到新账户之前，具有仅在Organizations中创建新AWS账户的能力。哪个解决方案能满足这些要求？ 选项： A. 在管理账户中，创建一个具有在Organizations中创建新账户必要权限的新IAM角色。允许该角色被新AWS账户中的Lambda执行角色代入。更新Lambda函数代码，使其在创建新AWS账户时代入该角色。更新Lambda执行角色以确保它有权限代入新角色。 B. 在管理账户中，为Organizations开启委托管理。创建一个新的委托策略，授予新的... C. 在管理账户中，创建一个具有在Organizations中创建新账户必要权限的新IAM角色。允许该角色被Lambda服务主体代入。更新Lambda函数代码，使其在创建新AWS账户时代入该角色。更新Lambda执行角色以确保它有权限代入该角色。 D. 在管理账户中，启用AWS Control Tower。为AWS Control Tower开启委托管理。创建一个资源策略，允许新AWS账户在AWS Control Tower中创建新AWS账户。更新Lambda函数代码以在新账户中使用AWS Control Tower API。确保Lambda执行角色具有controltower:CreateManagedAccount权限。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何在AWS Organizations环境中，将具有创建账户权限的Lambda函数从管理账户安全地迁移到其他账户，同时保持其创建新账户的能力。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. IAM跨账户角色代入（Cross-account role assumption） 3. Lambda执行角色和权限管理 4. AWS Control Tower - 账户治理服务 5. 委托管理（Delegated administration） **正确答案A的原因：** 1. **跨账户角色代入模式**：在管理账户中创建IAM角色，允许其他账户的Lambda执行角色代入，这是AWS推荐的跨账户权限管理最佳实践 2. **最小权限原则**：只授予创建账户的必要权限，符合安全最佳实践 3. **权限链清晰**：Lambda执行角色 → 代入管理账户中的角色 → 执行Organizations API 4. **实现简单**：不需要额外的服务或复杂配置 **其他选项错误的原因：** - **选项B**：题目描述不完整，但委托管理通常用于将特定服务的管理权限委托给成员账户，不适用于Organizations的CreateAccount操作 - **选项C**：允许Lambda服务主体直接代入角色存在安全风险，任何Lambda函数都可能代入该角色，违反了最小权限原则 - **选项D**：引入了不必要的Control Tower复杂性，题目要求使用Organizations API而非Control Tower API，过度工程化 **决策标准和最佳实践：** 1. **安全性优先**：使用跨账户角色代入而非直接权限授予 2. **最小权限**：只授予必要的权限，限制角色的代入范围 3. **简单性**：选择最直接的解决方案，避免不必要的服务依赖 4. **可审计性**：跨账户角色代入提供清晰的权限追踪路径</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">248</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has deployed an application in a single AWS Region. The application backend uses Amazon DynamoDB tables and Amazon S3 buckets. The company wants to deploy the application in a secondary Region. The company must ensure that the data in the DynamoDB tables and the S3 buckets persists across both Regions. The data must also immediately propagate across Regions. Which solution will meet these requirements with the MOST operational efficiency? streams on the DynamoDB tables in both Regions. In each Region, create an AWS Lambda function that subscribes to the DynamoDB streams. Configure the Lambda function to copy new records to the DynamoDB tables in the other Region. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Implement two-way S3 bucket replication between the primary Region&#x27;s S3 buckets and the secondary Region&#x27;s S3 buckets. Convert the DynamoDB tables into global tables. Set the secondary Region as the additional Region.
B. Implement S3 Batch Operations copy jobs between the primary Region and the secondary Region for all S3 buckets. Convert the DynamoDB tables into global tables. Set the secondary Region as the additional Region.
C. Implement two-way S3 bucket replication between the primary Region&#x27;s S3 buckets and the secondary Region&#x27;s S3 buckets. Enable DynamoDB streams on the DynamoDB tables in both Regions. In each Region, create an AWS Lambda function that subscribes to the DynamoDB streams. Configure the Lambda function to copy new records to the DynamoDB tables in the other Region.
D. Implement S3 Batch Operations copy jobs between the primary Region and the secondary Region for all S3 buckets. Enable DynamoDB</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在单个AWS Region中部署了一个应用程序。应用程序后端使用Amazon DynamoDB表和Amazon S3存储桶。该公司希望在第二个Region中部署应用程序。公司必须确保DynamoDB表和S3存储桶中的数据在两个Region之间持久化。数据还必须立即在Region之间传播。哪种解决方案能够以最高的运营效率满足这些要求？ 选项： A. 在主Region的S3存储桶和辅助Region的S3存储桶之间实施双向S3存储桶复制。将DynamoDB表转换为global tables。将辅助Region设置为附加Region。 B. 在主Region和辅助Region之间为所有S3存储桶实施S3 Batch Operations复制作业。将DynamoDB表转换为global tables。将辅助Region设置为附加Region。 C. 在主Region的S3存储桶和辅助Region的S3存储桶之间实施双向S3存储桶复制。在两个Region的DynamoDB表上启用DynamoDB streams。在每个Region中，创建一个订阅DynamoDB streams的AWS Lambda函数。配置Lambda函数将新记录复制到另一个Region的DynamoDB表中。 D. 在主Region和辅助Region之间为所有S3存储桶实施S3 Batch Operations复制作业。启用DynamoDB...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 在两个AWS Region之间部署应用程序 - 确保DynamoDB表和S3存储桶数据在两个Region之间持久化 - 数据必须立即在Region之间传播 - 要求最高的运营效率 **涉及的关键AWS服务和概念：** - DynamoDB Global Tables：提供多Region、完全托管的数据库复制 - S3跨Region复制：自动、异步的对象复制 - S3 Batch Operations：大规模批量操作工具 - DynamoDB Streams：捕获数据修改事件的流 - Lambda函数：无服务器计算服务 **正确答案A的原因：** 1. **DynamoDB Global Tables**：这是AWS原生的多Region复制解决方案，提供自动、双向、近实时的数据同步，无需额外的运维工作 2. **S3双向复制**：使用S3 Cross-Region Replication (CRR)实现自动、实时的对象复制 3. **运营效率最高**：两种解决方案都是完全托管的服务，无需编写和维护自定义代码 4. **立即传播**：Global Tables和S3复制都提供近实时的数据传播 **其他选项错误的原因：** - **选项B**：S3 Batch Operations是用于一次性大批量操作的工具，不适合持续的实时数据同步，无法满足&quot;立即传播&quot;的要求 - **选项C**：使用DynamoDB Streams + Lambda的自定义解决方案虽然可行，但运营复杂度高，需要维护Lambda函数代码，容易出现数据一致性问题和无限循环 - **选项D**：同样使用S3 Batch Operations，不满足实时同步要求 **决策标准和最佳实践：** 1. **优先选择托管服务**：AWS原生的Global Tables比自建复制机制更可靠 2. **避免过度工程化**：简单的托管解决方案通常比复杂的自定义方案更好 3. **考虑运营负担**：选择需要最少维护工作的方案 4. **数据一致性**：Global Tables提供最终一致性，比自建方案更可靠</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">249</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has configured Amazon RDS storage autoscaling for its RDS DB instances. A DevOps team needs to visualize the autoscaling events on an Amazon CloudWatch dashboard. Which solution will meet this requirement? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule that reacts to RDS storage autoscaling events from RDS events. Create an AWS Lambda function that publishes a CloudWatch custom metric. Configure the EventBridge rule to invoke the Lambda function. Visualize the custom metric by using the CloudWatch dashboard.
B. Create a trail by using AWS CloudTrail with management events configured. Configure the trail to send the management events to Amazon CloudWatch Logs. Create a metric filter in CloudWatch Logs to match the RDS storage autoscaling events. Visualize the metric filter by using the CloudWatch dashboard.
C. Create an Amazon EventBridge rule that reacts to RDS storage autoscaling events from the RDS events. Create a CloudWatch alarm. Configure the EventBridge rule to change the status of the CloudWatch alarm. Visualize the alarm status by using the CloudWatch dashboard.
D. Create a trail by using AWS CloudTrail with data events configured. Configure the trail to send the data events to Amazon CloudWatch Logs. Create a metric filter in CloudWatch Logs to match the RDS storage autoscaling events. Visualize the metric filter by using the CloudWatch dashboard.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司为其RDS DB实例配置了Amazon RDS存储自动扩展。DevOps团队需要在Amazon CloudWatch仪表板上可视化自动扩展事件。哪个解决方案能满足这个要求？ 选项： A. 创建一个Amazon EventBridge规则，响应来自RDS事件的RDS存储自动扩展事件。创建一个AWS Lambda函数来发布CloudWatch自定义指标。配置EventBridge规则调用Lambda函数。使用CloudWatch仪表板可视化自定义指标。 B. 使用AWS CloudTrail创建一个配置了管理事件的跟踪。配置跟踪将管理事件发送到Amazon CloudWatch Logs。在CloudWatch Logs中创建指标过滤器来匹配RDS存储自动扩展事件。使用CloudWatch仪表板可视化指标过滤器。 C. 创建一个Amazon EventBridge规则，响应来自RDS事件的RDS存储自动扩展事件。创建一个CloudWatch告警。配置EventBridge规则来改变CloudWatch告警的状态。使用CloudWatch仪表板可视化告警状态。 D. 使用AWS CloudTrail创建一个配置了数据事件的跟踪。配置跟踪将数据事件发送到Amazon CloudWatch Logs。在CloudWatch Logs中创建指标过滤器来匹配RDS存储自动扩展事件。使用CloudWatch仪表板可视化指标过滤器。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要在CloudWatch仪表板上可视化RDS存储自动扩展事件，实现对自动扩展活动的监控和展示。 **涉及的关键AWS服务和概念：** - Amazon RDS存储自动扩展：当存储空间不足时自动增加存储容量 - Amazon EventBridge：事件驱动的服务，可以捕获和路由AWS服务事件 - AWS Lambda：无服务器计算服务，用于处理事件 - CloudWatch自定义指标：用户定义的监控指标 - CloudWatch仪表板：用于可视化监控数据 - AWS CloudTrail：记录API调用的审计服务 **正确答案A的原因：** 1. **事件源正确**：RDS存储自动扩展事件会发布到EventBridge，这是获取此类事件的标准方式 2. **处理流程合理**：EventBridge规则 → Lambda函数 → CloudWatch自定义指标 → 仪表板可视化，形成完整的事件处理链 3. **可视化效果好**：自定义指标可以提供丰富的数据维度和灵活的可视化选项 4. **实时性强**：EventBridge能够实时捕获事件，Lambda快速处理并发布指标 **其他选项错误的原因：** - **选项B错误**：CloudTrail管理事件主要记录控制平面API调用，RDS存储自动扩展是系统自动行为，不会作为管理事件记录在CloudTrail中 - **选项C错误**：虽然使用了正确的事件源（EventBridge），但通过改变告警状态来可视化事件不是最佳实践，告警主要用于通知而非数据可视化 - **选项D错误**：CloudTrail数据事件主要用于记录对象级操作（如S3对象访问），不适用于RDS存储自动扩展事件 **决策标准和最佳实践：** 1. **选择正确的事件源**：AWS服务事件应优先使用EventBridge而非CloudTrail 2. **数据流向合理**：事件 → 处理 → 指标 → 可视化的流程清晰高效 3. **服务适配性**：每个AWS服务都有其最适合的使用场景，要根据数据类型选择合适的服务 4. **可扩展性考虑**：自定义指标比告警状态更适合复杂的可视化需求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">250</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses containers for its applications. The company learns that some container images are missing required security configurations. A DevOps engineer needs to implement a solution to create a standard base image. The solution must publish the base image weekly to the us-west-2 Region, us-east-2 Region, and eu-central-1 Region. Which solution will meet these requirements? C (80%) A (20%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an EC2 Image Builder pipeline that uses a container recipe to build the image. Configure the pipeline to distribute the image to an Amazon Elastic Container Registry (Amazon ECR) repository in us-west-2. Configure ECR replication from us-west-2 to us-east-2 and from us-east-2 to eu-central-1. Configure the pipeline to run weekly.
B. Create an AWS CodePipeline pipeline that uses an AWS CodeBuild project to build the image. Use AWS CodeDeploy to publish the image to an Amazon Elastic Container Registry (Amazon ECR) repository in us-west-2. Configure ECR replication from us-west-2 to us-east-2 and from us-east-2 to eu-central-1. Configure the pipeline to run weekly.
C. Create an EC2 Image Builder pipeline that uses a container recipe to build the image. Configure the pipeline to distribute the image to Amazon Elastic Container Registry (Amazon ECR) repositories in all three Regions. Configure the pipeline to run weekly.
D. Create an AWS CodePipeline pipeline that uses an AWS CodeBuild project to build the image. Use AWS CodeDeploy to publish the image to Amazon Elastic Container Registry (Amazon ECR) repositories in all three Regions. Configure the pipeline to run weekly.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司为其应用程序使用容器。该公司发现一些容器镜像缺少必需的安全配置。DevOps工程师需要实施一个解决方案来创建标准基础镜像。该解决方案必须每周将基础镜像发布到us-west-2区域、us-east-2区域和eu-central-1区域。哪个解决方案能满足这些要求？ 选项： A. 创建一个EC2 Image Builder管道，使用容器配方来构建镜像。配置管道将镜像分发到us-west-2的Amazon Elastic Container Registry (Amazon ECR)存储库。配置ECR从us-west-2复制到us-east-2，再从us-east-2复制到eu-central-1。配置管道每周运行。 B. 创建一个AWS CodePipeline管道，使用AWS CodeBuild项目来构建镜像。使用AWS CodeDeploy将镜像发布到us-west-2的Amazon Elastic Container Registry (Amazon ECR)存储库。配置ECR从us-west-2复制到us-east-2，再从us-east-2复制到eu-central-1。配置管道每周运行。 C. 创建一个EC2 Image Builder管道，使用容器配方来构建镜像。配置管道将镜像分发到所有三个区域的Amazon Elastic Container Registry (Amazon ECR)存储库。配置管道每周运行。 D. 创建一个AWS CodePipeline管道，使用AWS CodeBuild项目来构建镜像。使用AWS CodeDeploy将镜像发布到所有三个区域的Amazon Elastic Container Registry (Amazon ECR)存储库。配置管道每周运行。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 创建标准的容器基础镜像以解决安全配置缺失问题 - 需要每周定期发布镜像 - 必须将镜像分发到三个特定区域（us-west-2、us-east-2、eu-central-1） **涉及的关键AWS服务和概念：** - EC2 Image Builder：专门用于构建和管理标准化镜像的服务，支持容器镜像构建 - Amazon ECR：容器镜像注册表服务，用于存储和管理容器镜像 - AWS CodePipeline + CodeBuild：CI/CD管道服务，用于自动化构建和部署 - ECR复制功能：在不同区域间复制容器镜像 - 容器配方（Container Recipe）：定义容器镜像构建步骤的配置 **正确答案C的原因：** 1. EC2 Image Builder是专门为构建标准化镜像设计的服务，完全符合创建&quot;标准基础镜像&quot;的需求 2. 支持容器配方功能，可以确保安全配置的标准化实施 3. 内置多区域分发功能，可以直接将镜像分发到所有三个目标区域 4. 支持定时调度，可以配置每周自动运行 5. 简化的架构，减少了复杂性和潜在故障点 **其他选项错误的原因：** - 选项A：使用ECR复制链（us-west-2 → us-east-2 → eu-central-1）增加了复杂性和延迟，如果中间环节失败会影响整个复制链 - 选项B：CodePipeline + CodeBuild + CodeDeploy的组合过于复杂，且CodeDeploy主要用于应用部署而非容器镜像发布，不是最佳选择 - 选项D：同样使用了不必要的复杂架构，CodeDeploy不适合直接发布到ECR **决策标准和最佳实践：** 1. 选择专门用途的服务：EC2 Image Builder专为镜像构建和标准化而设计 2. 简化架构：直接多区域分发比复制链更可靠 3. 减少服务依赖：避免不必要的服务组合 4. 考虑服务的核心功能匹配度：Image Builder的容器配方功能完美匹配标准化需求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">251</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer needs to implement a solution to install antivirus software on all the Amazon EC2 instances in an AWS account. The EC2 instances run the most recent version of Amazon Linux. The solution must detect all instances and must use an AWS Systems Manager document to install the software if the software is not present. Which solution will meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an association in Systems Manager State Manager. Target all the managed nodes. Include the software in the association. Configure the association to use the Systems Manager document.
B. Set up AWS Config to record all the resources in the account. Create an AWS Config custom rule to determine if the software is installed on all the EC2 instances. Configure an automatic remediation action that uses the Systems Manager document for noncompliant EC2 instances.
C. Activate Amazon EC2 scanning on Amazon Inspector to determine if the software is installed on all the EC2 instances. Associate the findings with the Systems Manager document.
D. Create an Amazon EventBridge rule that uses AWS CloudTrail to detect the RunInstances API call. Configure inventory collection in Systems Manager Inventory to determine if the software is installed on the EC2 instances. Associate the Systems Manager inventory with the Systems Manager document.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师需要实施一个解决方案，在AWS账户中的所有Amazon EC2实例上安装防病毒软件。这些EC2实例运行最新版本的Amazon Linux。该解决方案必须检测所有实例，并且必须使用AWS Systems Manager文档在软件不存在时安装软件。哪个解决方案能满足这些要求？ 选项： A. 在Systems Manager State Manager中创建关联。目标为所有托管节点。在关联中包含软件。配置关联使用Systems Manager文档。 B. 设置AWS Config记录账户中的所有资源。创建AWS Config自定义规则来确定是否在所有EC2实例上安装了软件。为不合规的EC2实例配置使用Systems Manager文档的自动修复操作。 C. 在Amazon Inspector上激活Amazon EC2扫描，以确定是否在所有EC2实例上安装了软件。将发现结果与Systems Manager文档关联。 D. 创建使用AWS CloudTrail检测RunInstances API调用的Amazon EventBridge规则。在Systems Manager Inventory中配置清单收集，以确定是否在EC2实例上安装了软件。将Systems Manager清单与Systems Manager文档关联。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现一个自动化解决方案来：1）检测AWS账户中的所有EC2实例；2）确定防病毒软件是否已安装；3）如果未安装则使用Systems Manager文档自动安装软件。 **涉及的关键AWS服务和概念：** - AWS Systems Manager State Manager：用于维护EC2实例配置状态的服务 - Systems Manager文档（SSM Document）：定义Systems Manager执行操作的脚本 - 托管节点（Managed Nodes）：已注册到Systems Manager的EC2实例 - 关联（Association）：将SSM文档与目标实例绑定的机制 **正确答案A的原因：** Systems Manager State Manager是专门设计用于确保实例保持所需配置状态的服务。通过创建关联，可以：1）自动检测所有托管节点；2）持续监控软件安装状态；3）在软件缺失时自动执行安装；4）提供持续的合规性保证。这是最直接、最适合的解决方案。 **其他选项错误的原因：** - 选项B：AWS Config主要用于配置合规性检查，虽然可以实现功能但过于复杂，且需要编写自定义规则，不如State Manager直接高效。 - 选项C：Amazon Inspector主要用于安全漏洞扫描，不是用来管理软件安装状态的工具，且无法直接触发自动安装。 - 选项D：这个方案过于复杂，只能检测新实例创建，无法处理现有实例，且EventBridge+CloudTrail+Inventory的组合无法提供持续的状态管理。 **决策标准和最佳实践：** 选择AWS服务时应遵循&quot;使用正确工具解决正确问题&quot;的原则。对于配置管理和状态维护任务，Systems Manager State Manager是最佳选择，因为它提供了原生的、持续的、自动化的配置管理能力。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">252</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company needs to increase the security of the container images that run in its production environment. The company wants to integrate operating system scanning and programming language package vulnerability scanning for the containers in its CI/CD pipeline. The CI/CD pipeline is an AWS CodePipeline pipeline that includes an AWS CodeBuild build project, AWS CodeDeploy actions, and an Amazon Elastic Container Registry (Amazon ECR) repository. A DevOps engineer needs to add an image scan to the CI/CD pipeline. The CI/CD pipeline must deploy only images without CRITICAL and HIGH findings into production. Which combination of steps will meet these requirements? (Choose two.) BD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Amazon ECR basic scanning.
B. Use Amazon ECR enhanced scanning.
C. Configure Amazon ECR to submit a Rejected status to the CI/CD pipeline when the image scan returns CRITICAL or HIGH findings.
D. Configure an Amazon EventBridge rule to invoke an AWS Lambda function when the image scan is completed. Configure the Lambda function to consume the Amazon Inspector scan status and to submit an Approved or Rejected status to the CI/CD pipeline.
E. Configure an Amazon EventBridge rule to invoke an AWS Lambda function when the image scan is completed. Configure the Lambda function to consume the Clair scan status and to submit an Approved or Rejected status to the CI/CD pipeline.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司需要提高在其生产环境中运行的容器镜像的安全性。该公司希望在其CI/CD流水线中集成操作系统扫描和编程语言包漏洞扫描功能。该CI/CD流水线是一个AWS CodePipeline流水线，包括AWS CodeBuild构建项目、AWS CodeDeploy操作和Amazon Elastic Container Registry (Amazon ECR)存储库。DevOps工程师需要在CI/CD流水线中添加镜像扫描功能。CI/CD流水线必须只将没有CRITICAL和HIGH级别发现的镜像部署到生产环境。哪种步骤组合能满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在CI/CD流水线中实现容器镜像的安全扫描，包括操作系统和编程语言包的漏洞扫描，并且只允许没有CRITICAL和HIGH级别漏洞的镜像部署到生产环境。 **涉及的关键AWS服务和概念：** - Amazon ECR的两种扫描模式：基础扫描和增强扫描 - Amazon Inspector（用于增强扫描） - Amazon EventBridge（事件驱动架构） - AWS Lambda（自动化处理） - CI/CD流水线集成和状态反馈机制 **正确答案的原因：** 选项B（使用Amazon ECR增强扫描）是正确的，因为： 1. 增强扫描使用Amazon Inspector提供更全面的漏洞检测 2. 支持操作系统和编程语言包的漏洞扫描 3. 提供详细的CRITICAL和HIGH级别漏洞分类 4. 可以与EventBridge集成实现自动化工作流 选项D也是正确的，因为： 1. EventBridge可以监听ECR扫描完成事件 2. Lambda函数可以处理Amazon Inspector的扫描结果 3. 可以根据漏洞级别向CI/CD流水线返回批准或拒绝状态 4. 实现了自动化的安全门控机制 **其他选项错误的原因：** - 选项A：基础扫描功能有限，无法提供编程语言包扫描，不满足题目要求 - 选项C：ECR本身不能直接向CI/CD流水线提交拒绝状态，需要额外的自动化机制 - 选项E：Clair是开源扫描工具，但ECR增强扫描使用的是Amazon Inspector，不是Clair **决策标准和最佳实践：** 1. 选择功能更强大的增强扫描以满足全面的安全要求 2. 使用事件驱动架构实现自动化安全检查 3. 在CI/CD流水线中实施安全门控，确保只有安全的镜像进入生产环境 4. 利用AWS原生服务的集成能力提高系统可靠性和维护性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">253</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s DevOps team manages a set of AWS accounts that are in an organization in AWS Organizations. The company needs a solution that ensures that all Amazon EC2 instances use approved AMIs that the DevOps team manages. The solution also must remediate the usage of AMIs that are not approved. The individual account administrators must not be able to remove the restriction to use approved AMIs. Which solution will meet these requirements? with the list of approved AMIs. Configure the rule to run the AWS-StopEC2Instance AWS Systems Manager Automation runbook for the noncompliant EC2 instances. notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps team to the SNS topic. Deploy the Lambda function in each account in the organization. Create an Amazon EventBridge rule in each account. Configure the EventBridge rules to react to AWS CloudTrail events for Amazon EC2 and to invoke the Lambda function. with the list of approved AMIs. Deploy the conformance pack across the organization. Configure the rule to run the AWS-StopEC2Instance AWS Systems Manager Automation runbook for the noncompliant EC2 instances. D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CloudFormation StackSets to deploy an Amazon EventBridge rule to each account. Configure the rule to react to AWS CloudTrail events for Amazon EC2 and to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps team to the SNS topic.
B. Use AWS CloudFormation StackSets to deploy the approved-amis-by-id AWS Config managed rule to each account. Configure the rule
C. Create an AWS Lambda function that processes AWS CloudTrail events for Amazon EC2. Configure the Lambda function to send a
D. Enable AWS Config across the organization. Create a conformance pack that uses the approved-amis-by-id AWS Config managed rule</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的DevOps团队管理着AWS Organizations中组织内的一组AWS账户。该公司需要一个解决方案，确保所有Amazon EC2实例都使用DevOps团队管理的已批准AMI。该解决方案还必须修复使用未批准AMI的情况。各个账户管理员不得能够移除使用已批准AMI的限制。哪个解决方案能满足这些要求？ 选项： A. 使用AWS CloudFormation StackSets将Amazon EventBridge规则部署到每个账户。配置规则以响应Amazon EC2的AWS CloudTrail事件，并向Amazon Simple Notification Service (Amazon SNS)主题发送通知。让DevOps团队订阅SNS主题。 B. 使用AWS CloudFormation StackSets将approved-amis-by-id AWS Config托管规则部署到每个账户。配置规则包含已批准AMI列表。配置规则运行AWS-StopEC2Instance AWS Systems Manager Automation运行手册来处理不合规的EC2实例。 C. 创建一个AWS Lambda函数来处理Amazon EC2的AWS CloudTrail事件。配置Lambda函数向Amazon Simple Notification Service (Amazon SNS)主题发送通知。让DevOps团队订阅SNS主题。在组织中的每个账户部署Lambda函数。在每个账户创建Amazon EventBridge规则。配置EventBridge规则响应Amazon EC2的AWS CloudTrail事件并调用Lambda函数。 D. 在整个组织中启用AWS Config。创建一个使用approved-amis-by-id AWS Config托管规则的conformance pack，包含已批准AMI列表。在整个组织中部署conformance pack。配置规则运行AWS-StopEC2Instance AWS Systems Manager Automation运行手册来处理不合规的EC2实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 确保所有EC2实例使用已批准的AMI 2. 自动修复使用未批准AMI的情况 3. 防止账户管理员移除这些限制 4. 跨AWS Organizations中的多个账户实施 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理 - AWS Config：合规性监控和自动修复 - AWS Config Conformance Packs：跨账户部署合规规则 - AWS Systems Manager Automation：自动化修复操作 - AWS CloudFormation StackSets：跨账户资源部署 **正确答案D的原因：** 1. **组织级别部署**：AWS Config conformance packs可以在AWS Organizations级别部署，确保所有账户都受到监管 2. **内置合规规则**：approved-amis-by-id是专门用于AMI合规性检查的托管规则 3. **自动修复能力**：可以配置AWS Systems Manager Automation runbook自动停止不合规实例 4. **防篡改保护**：在组织级别部署的conformance pack不能被个别账户管理员轻易移除 5. **集中管理**：DevOps团队可以从组织根账户集中管理所有合规策略 **其他选项错误的原因：** - **选项A**：只提供通知功能，没有自动修复能力，且使用StackSets部署的资源可能被账户管理员修改 - **选项B**：虽然有合规检查和自动修复，但StackSets部署的资源可以被账户管理员删除或修改，不满足防篡改要求 - **选项C**：过于复杂，需要在每个账户单独部署Lambda函数和EventBridge规则，管理复杂且容易被篡改 **决策标准和最佳实践：** 1. **选择组织级服务**：对于跨多账户的合规要求，优先选择支持AWS Organizations的服务 2. **使用托管服务**：AWS Config conformance packs提供了现成的合规框架，比自建解决方案更可靠 3. **自动化修复**：不仅要检测违规，还要能自动修复，减少人工干预 4. **权限控制**：确保合规策略不能被下级账户管理员绕过或删除</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">254</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company gives its employees limited rights to AWS. DevOps engineers have the ability to assume an administrator role. For tracking purposes, the security team wants to receive a near-real-time notification when the administrator role is assumed. How should this be accomplished? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure AWS Config to publish logs to an Amazon S3 bucket. Use Amazon Athena to query the logs and send a notification to the security team when the administrator role is assumed.
B. Configure Amazon GuardDuty to monitor when the administrator role is assumed and send a notification to the security team.
C. Create an Amazon EventBridge event rule using an AWS Management Console sign-in events event pattern that publishes a message to an Amazon SNS topic if the administrator role is assumed.
D. Create an Amazon EventBridge events rule using an AWS API call that uses an AWS CloudTrail event pattern to invoke an AWS Lambda function that publishes a message to an Amazon SNS topic if the administrator role is assumed.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司给员工有限的AWS权限。DevOps工程师有能力assume一个管理员角色。出于跟踪目的，安全团队希望在管理员角色被assume时收到近实时通知。应该如何实现这一点？ 选项： A. 配置AWS Config将日志发布到Amazon S3存储桶。使用Amazon Athena查询日志，并在管理员角色被assume时向安全团队发送通知。 B. 配置Amazon GuardDuty监控管理员角色被assume的情况，并向安全团队发送通知。 C. 创建一个Amazon EventBridge事件规则，使用AWS Management Console登录事件模式，如果管理员角色被assume则向Amazon SNS主题发布消息。 D. 创建一个Amazon EventBridge事件规则，使用AWS API调用和AWS CloudTrail事件模式来调用AWS Lambda函数，如果管理员角色被assume则向Amazon SNS主题发布消息。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现对AWS管理员角色assume操作的近实时监控和通知。关键需求是：1）检测角色assume操作；2）近实时响应；3）向安全团队发送通知。 **涉及的关键AWS服务和概念：** - AWS CloudTrail：记录所有AWS API调用，包括AssumeRole操作 - Amazon EventBridge：事件驱动架构的核心服务，可以实时处理CloudTrail事件 - AWS Lambda：无服务器计算服务，用于处理事件和执行通知逻辑 - Amazon SNS：消息通知服务 - Role Assumption：IAM角色切换操作，会被CloudTrail记录为AssumeRole API调用 **正确答案D的原因：** 1. **事件源准确**：AssumeRole是API调用操作，CloudTrail会记录所有API调用，这是监控角色assume的正确数据源 2. **近实时处理**：EventBridge可以实时接收CloudTrail事件并立即触发Lambda函数 3. **架构完整**：EventBridge → Lambda → SNS形成完整的事件处理和通知链路 4. **精确匹配**：可以通过CloudTrail事件模式精确匹配AssumeRole API调用 **其他选项错误的原因：** - **选项A**：AWS Config主要用于资源配置合规性监控，不是实时事件处理；Athena是批量查询工具，无法提供近实时通知 - **选项B**：GuardDuty主要检测安全威胁和异常行为，不是专门用于跟踪正常的角色assume操作 - **选项C**：Management Console登录事件与AssumeRole API调用是不同的事件类型，无法准确捕获角色assume操作 **决策标准和最佳实践：** 1. **事件驱动架构**：使用EventBridge构建松耦合的事件处理系统 2. **数据源选择**：根据监控目标选择合适的事件源（API调用用CloudTrail） 3. **实时性要求**：近实时通知需求应选择事件驱动而非批处理方案 4. **安全监控**：对特权操作的监控应该精确、及时且可靠</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">255</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company needs a strategy for failover and disaster recovery of its data and application. The application uses a MySQL database and Amazon EC2 instances. The company requires a maximum RPO of 2 hours and a maximum RTO of 10 minutes for its data and application at all times. Which combination of deployment strategies will meet these requirements? (Choose two.) BD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon Aurora Single-AZ cluster in multiple AWS Regions as the data store. Use Aurora&#x27;s automatic recovery capabilities in the event of a disaster.
B. Create an Amazon Aurora global database in two AWS Regions as the data store. In the event of a failure, promote the secondary Region to the primary for the application. Update the application to use the Aurora cluster endpoint in the secondary Region.
C. Create an Amazon Aurora cluster in multiple AWS Regions as the data store. Use a Network Load Balancer to balance the database traffic in different Regions.
D. Set up the application in two AWS Regions. Use Amazon Route 53 failover routing that points to Application Load Balancers in both Regions. Use health checks and Auto Scaling groups in each Region.
E. Set up the application in two AWS Regions. Configure AWS Global Accelerator to point to Application Load Balancers (ALBs) in both Regions. Add both ALBs to a single endpoint group. Use health checks and Auto Scaling groups in each Region.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司需要为其数据和应用程序制定故障转移和灾难恢复策略。该应用程序使用MySQL数据库和Amazon EC2实例。公司要求其数据和应用程序在任何时候的最大RPO为2小时，最大RTO为10分钟。以下哪种部署策略组合能够满足这些要求？（选择两个）BD（100%） 选项：A. 在多个AWS Region中创建Amazon Aurora Single-AZ集群作为数据存储。在发生灾难时使用Aurora的自动恢复功能。 B. 在两个AWS Region中创建Amazon Aurora global database作为数据存储。在发生故障时，将辅助Region提升为应用程序的主Region。更新应用程序以使用辅助Region中的Aurora集群端点。 C. 在多个AWS Region中创建Amazon Aurora集群作为数据存储。使用Network Load Balancer来平衡不同Region的数据库流量。 D. 在两个AWS Region中设置应用程序。使用Amazon Route 53故障转移路由指向两个Region中的Application Load Balancer。在每个Region中使用健康检查和Auto Scaling组。 E. 在两个AWS Region中设置应用程序。配置AWS Global Accelerator指向两个Region中的Application Load Balancer (ALB)。将两个ALB添加到单个端点组中。在每个Region中使用健康检查和Auto Scaling组。 正确答案：B</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS灾难恢复策略设计，需要同时满足严格的RPO（恢复点目标，2小时）和RTO（恢复时间目标，10分钟）要求。题目要求选择两个策略组合来实现数据库和应用层的完整灾难恢复方案。 **涉及的关键AWS服务和概念：** - Amazon Aurora Global Database：跨Region数据库复制服务 - RPO/RTO概念：RPO关注数据丢失容忍度，RTO关注服务恢复时间 - Route 53故障转移路由：DNS级别的流量切换 - Auto Scaling和Application Load Balancer：应用层高可用性 - 跨Region灾难恢复架构设计 **正确答案的原因：** 选项B和D的组合是正确的： - **选项B**：Aurora Global Database提供跨Region的异步复制，RPO通常在1秒内，远超2小时要求。故障转移到辅助Region通常在1分钟内完成，满足10分钟RTO要求。 - **选项D**：Route 53健康检查可以在几分钟内检测到故障并切换DNS解析到健康的Region，配合Auto Scaling确保应用层快速恢复。 **其他选项错误的原因：** - **选项A**：Single-AZ部署无法提供跨Region灾难恢复能力，不满足灾难恢复要求。 - **选项C**：数据库不应该使用负载均衡器进行跨Region流量分配，这会导致数据一致性问题。 - **选项E**：Global Accelerator主要用于性能优化而非灾难恢复，且同时向两个Region发送流量可能导致数据库写入冲突。 **决策标准和最佳实践：** 1. **数据层策略**：选择Aurora Global Database确保跨Region数据复制和快速故障转移 2. **应用层策略**：使用Route 53故障转移路由实现DNS级别的流量切换 3. **监控和自动化**：依靠健康检查和Auto Scaling实现自动故障检测和恢复 4. **架构原则**：遵循AWS Well-Architected Framework的可靠性支柱，实现真正的跨Region灾难恢复能力</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">256</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A developer is using the AWS Serverless Application Model (AWS SAM) to create a prototype for an AWS Lambda function. The AWS SAM template contains an AWS::Serverless::Function resource that has the CodeUri property that points to an Amazon S3 location. The developer wants to identify the correct commands for deployment before creating a CI/CD pipeline. The developer creates an archive of the Lambda function code named package.zip. The developer uploads the .zip file archive to the S3 location specified in the CodeUri property. The developer runs the sam deploy command and deploys the Lambda function. The developer updates the Lambda function code and uses the same steps to deploy the new version of the Lambda function. The sam deploy command fails and returns an error of no changes to deploy. Which solutions will deploy the new version? (Choose two.) cloudformation deploy command. CE (60%) AC (40%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use the aws cloudformation update-stack command instead of the sam deploy command.
B. Use the aws cloudformation update-stack-instances command instead of the sam deploy command.
C. Update the CodeUri property to reference the local application code folder. Use the sam deploy command.
D. Update the CodeUri property to reference the local application code folder. Use the aws cloudformation create-change-set command and the aws cloudformation execute-change-set command.
E. Update the CodeUri property to reference the local application code folder. Use the aws cloudformation package command and the aws cloudformation deploy command.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个开发者正在使用AWS Serverless Application Model (AWS SAM)为AWS Lambda函数创建原型。AWS SAM模板包含一个AWS::Serverless::Function资源，该资源的CodeUri属性指向Amazon S3位置。开发者想要在创建CI/CD流水线之前确定正确的部署命令。开发者创建了一个名为package.zip的Lambda函数代码归档文件，并将该.zip文件上传到CodeUri属性中指定的S3位置。开发者运行sam deploy命令并部署了Lambda函数。开发者更新了Lambda函数代码并使用相同的步骤部署新版本的Lambda函数。sam deploy命令失败并返回&quot;no changes to deploy&quot;错误。哪些解决方案能够部署新版本？（选择两个）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS SAM部署过程中遇到&quot;no changes to deploy&quot;错误时的解决方案。问题的关键在于理解SAM如何检测代码变更以及正确的部署流程。 **涉及的关键AWS服务和概念：** - AWS SAM (Serverless Application Model)：用于构建serverless应用的框架 - AWS Lambda：无服务器计算服务 - CloudFormation：基础设施即代码服务 - CodeUri属性：指定Lambda函数代码位置的SAM模板属性 - sam deploy命令：SAM的部署命令 - 代码打包和部署流程 **正确答案的原因：** 选项C正确的原因： - 当CodeUri指向S3位置时，SAM无法自动检测本地代码变更 - 将CodeUri更新为指向本地应用代码文件夹后，SAM可以自动处理代码打包、上传和部署 - sam deploy命令会自动检测本地代码变更，重新打包并上传到S3，然后更新CloudFormation堆栈 - 这是SAM推荐的标准部署流程 **其他选项错误的原因：** - 选项A：aws cloudformation update-stack命令需要手动处理代码打包和上传，不如SAM自动化流程便捷 - 选项B：update-stack-instances用于StackSets，不适用于单个堆栈的更新场景 - 选项D：create-change-set和execute-change-set是CloudFormation的低级操作，SAM已经封装了这些步骤 - 选项E：aws cloudformation package命令已被弃用，应使用sam package或sam build **决策标准和最佳实践：** 1. 使用SAM时，应该让CodeUri指向本地代码目录，而不是直接指向S3 2. SAM的设计理念是简化serverless应用的开发和部署流程 3. sam deploy命令集成了代码打包、上传和CloudFormation部署的完整流程 4. 避免手动上传代码到S3，让SAM自动管理这个过程 5. 遵循基础设施即代码的最佳实践，保持模板和代码的同步更新</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">257</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs its container workloads in AWS App Runner. A DevOps engineer manages the company&#x27;s container repository in Amazon Elastic Container Registry (Amazon ECR). The DevOps engineer must implement a solution that continuously monitors the container repository. The solution must create a new container image when the solution detects an operating system vulnerability or language package vulnerability. Which solution will meet these requirements? the ECR repository. Create an Amazon EventBridge rule to capture an Inspector finding event. Use the event to invoke the image pipeline. Re-upload the container to the repository. Most Voted Malware Protection on the container workload. Create an Amazon EventBridge rule to capture a GuardDuty finding event. Use the event to invoke the image pipeline. repository. Create an Amazon EventBridge rule to capture an ECR image action event. Use the event to invoke the CodeBuild project. Re- upload the container to the repository. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use EC2 Image Builder to create a container image pipeline. Use Amazon ECR as the target repository. Turn on enhanced scanning on
B. Use EC2 Image Builder to create a container image pipeline. Use Amazon ECR as the target repository. Enable Amazon GuardDuty
C. Create an AWS CodeBuild project to create a container image. Use Amazon ECR as the target repository. Turn on basic scanning on the
D. Create an AWS CodeBuild project to create a container image. Use Amazon ECR as the target repository. Configure AWS Systems Manager Compliance to scan all managed nodes. Create an Amazon EventBridge rule to capture a configuration compliance state change event. Use the event to invoke the CodeBuild project.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS App Runner中运行其容器工作负载。DevOps工程师在Amazon Elastic Container Registry (Amazon ECR)中管理公司的容器仓库。DevOps工程师必须实施一个持续监控容器仓库的解决方案。该解决方案必须在检测到操作系统漏洞或语言包漏洞时创建新的容器镜像。哪个解决方案能满足这些要求？ 选项： A. 使用EC2 Image Builder创建容器镜像管道。使用Amazon ECR作为目标仓库。在ECR仓库上启用增强扫描。创建Amazon EventBridge规则来捕获Inspector发现事件。使用该事件调用镜像管道。重新上传容器到仓库。 B. 使用EC2 Image Builder创建容器镜像管道。使用Amazon ECR作为目标仓库。在容器工作负载上启用Amazon GuardDuty恶意软件防护。创建Amazon EventBridge规则来捕获GuardDuty发现事件。使用该事件调用镜像管道。 C. 创建AWS CodeBuild项目来创建容器镜像。使用Amazon ECR作为目标仓库。在仓库上启用基础扫描。创建Amazon EventBridge规则来捕获ECR镜像操作事件。使用该事件调用CodeBuild项目。重新上传容器到仓库。 D. 创建AWS CodeBuild项目来创建容器镜像。使用Amazon ECR作为目标仓库。配置AWS Systems Manager Compliance扫描所有托管节点。创建Amazon EventBridge规则来捕获配置合规状态变更事件。使用该事件调用CodeBuild项目。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要实现一个自动化解决方案，能够持续监控容器仓库中的安全漏洞（操作系统和语言包漏洞），并在发现漏洞时自动创建新的容器镜像。 **涉及的关键AWS服务和概念：** - Amazon ECR：容器镜像仓库，提供基础扫描和增强扫描功能 - EC2 Image Builder：自动化镜像构建服务，支持容器镜像管道 - Amazon Inspector：漏洞评估服务，与ECR增强扫描集成 - Amazon EventBridge：事件驱动架构的核心服务 - AWS CodeBuild：构建服务 - Amazon GuardDuty：威胁检测服务 **正确答案A的原因：** 1. **EC2 Image Builder**是专门用于自动化镜像构建的服务，完全符合&quot;创建新容器镜像&quot;的需求 2. **ECR增强扫描**使用Amazon Inspector进行深度漏洞扫描，能够检测操作系统和语言包漏洞 3. **Inspector发现事件**通过EventBridge触发，形成完整的自动化流程 4. 整个方案实现了从漏洞检测到镜像重建的端到端自动化 **其他选项错误的原因：** - **选项B**：GuardDuty主要用于恶意软件和威胁检测，不是专门针对操作系统和语言包漏洞的扫描工具 - **选项C**：ECR基础扫描功能有限，无法提供全面的漏洞检测；ECR镜像操作事件不是漏洞发现事件 - **选项D**：Systems Manager Compliance主要用于配置合规性检查，不是专门的容器漏洞扫描工具 **决策标准和最佳实践：** 1. 选择专门的漏洞扫描工具（Inspector/ECR增强扫描）而非通用安全工具 2. 使用专业的镜像构建服务（Image Builder）确保构建过程的标准化和可靠性 3. 通过EventBridge实现事件驱动的自动化架构 4. 确保扫描覆盖操作系统和应用层面的漏洞</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">258</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to use AWS Systems Manager documents to bootstrap physical laptops for developers. The bootstrap code is stored in GitHub. A DevOps engineer has already created a Systems Manager activation, installed the Systems Manager agent with the registration code, and installed an activation ID on all the laptops. Which set of steps should be taken next? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure the Systems Manager document to use the AWS-RunShellScript command to copy the files from GitHub to Amazon S3, then use the aws-downloadContent plugin with a sourceType of S3.
B. Configure the Systems Manager document to use the aws-configurePackage plugin with an install action and point to the Git repository.
C. Configure the Systems Manager document to use the aws-downloadContent plugin with a sourceType of GitHub and sourceInfo with the repository details.
D. Configure the Systems Manager document to use the aws:softwareInventory plugin and run the script from the Git repository.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司想要使用AWS Systems Manager文档来引导开发人员的物理笔记本电脑。引导代码存储在GitHub中。DevOps工程师已经创建了一个Systems Manager激活，在所有笔记本电脑上安装了带有注册代码的Systems Manager代理，并安装了激活ID。接下来应该采取哪一组步骤？ 选项： A. 配置Systems Manager文档使用AWS-RunShellScript命令将文件从GitHub复制到Amazon S3，然后使用aws-downloadContent插件，sourceType设为S3。 B. 配置Systems Manager文档使用aws-configurePackage插件，执行安装操作并指向Git存储库。 C. 配置Systems Manager文档使用aws-downloadContent插件，sourceType设为GitHub，sourceInfo包含存储库详细信息。 D. 配置Systems Manager文档使用aws:softwareInventory插件并从Git存储库运行脚本。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查如何使用AWS Systems Manager文档从GitHub直接下载引导代码到已注册的物理设备上。关键在于选择正确的插件和配置方式来实现从GitHub到本地设备的代码下载。 **涉及的关键AWS服务和概念：** - AWS Systems Manager Documents：用于定义自动化任务的JSON或YAML格式文档 - Systems Manager Agent (SSM Agent)：安装在设备上的代理程序 - Systems Manager Activation：用于注册非EC2设备（如物理服务器、虚拟机）的机制 - aws-downloadContent插件：专门用于从各种源下载内容的Systems Manager插件 - 混合环境管理：管理AWS外部的物理设备 **正确答案C的原因：** aws-downloadContent插件是专门设计用于从多种源（包括GitHub、S3、HTTP等）下载内容的标准插件。通过设置sourceType为&quot;GitHub&quot;并在sourceInfo中提供存储库详细信息（如仓库URL、分支、路径等），可以直接从GitHub下载引导代码到目标设备，这是最直接、最高效的解决方案。 **其他选项错误的原因：** - 选项A：虽然技术上可行，但增加了不必要的复杂性，需要先将代码从GitHub复制到S3，然后再下载，这是一个多余的中间步骤。 - 选项B：aws-configurePackage插件主要用于管理预打包的软件包，不适合直接从Git存储库下载和执行自定义引导代码。 - 选项D：aws:softwareInventory插件的主要功能是收集软件清单信息，而不是下载和执行代码，完全不符合需求。 **决策标准和最佳实践：** 1. 选择专用插件：使用专门设计用于特定任务的插件（aws-downloadContent用于下载内容） 2. 最小化复杂性：避免不必要的中间步骤和额外的服务依赖 3. 直接集成：充分利用Systems Manager与GitHub的原生集成能力 4. 安全性考虑：确保GitHub访问权限和凭证管理得当 5. 可维护性：选择最直观、最容易理解和维护的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">259</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s development team uses AWS CloudFormation to deploy its application resources. The team must use CloudFormation for all changes to the environment. The team cannot use the AWS Management Console or the AWS CLI to make manual changes directly. The team uses a developer IAM role to access the environment. The role is configured with the AdministratorAccess managed IAM policy. The company has created a new CloudFormationDeployment IAM role that has the following policy attached: The company wants to ensure that only CloudFormation can use the new role. The development team cannot make any manual changes to the deployed resources. Which combination of steps will meet these requirements? (Choose three.) the iam:AssumeRole action. F. Add an IAM policy to the CloudFormationDeployment role to allow cloudformation:* on all resources. Add a policy that allows the iam:PassRole action for the ARN of the CloudFormationDeployment role if iam:PassedToService equals cloudformation.amazonaws.com. ADF (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Remove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers to use the CloudFormationDeployment role as a CloudFormation service role when the developers deploy new stacks.
B. Update the trust policy of the CloudFormationDeployment role to allow the developer IAM role to assume the CloudFormationDeployment role.
C. Configure the developer IAM role to be able to get and pass the CloudFormationDeployment role if iam:PassedToService equals cloudformation.amazonaws.com. Configure the CloudFormationDeployment role to allow all cloudformation actions for all resources.
D. Update the trust policy of the CloudFormationDeployment role to allow the cloudformation.amazonaws.com AWS principal to perform the assume role action.
E. Remove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers to assume the CloudFormationDeployment role when the developers deploy new stacks.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的开发团队使用AWS CloudFormation来部署应用程序资源。团队必须使用CloudFormation对环境进行所有更改。团队不能使用AWS Management Console或AWS CLI直接进行手动更改。团队使用开发者IAM角色来访问环境。该角色配置了AdministratorAccess托管IAM策略。公司创建了一个新的CloudFormationDeployment IAM角色，并附加了以下策略：公司希望确保只有CloudFormation可以使用新角色。开发团队不能对已部署的资源进行任何手动更改。哪种步骤组合将满足这些要求？（选择三个） 选项： A. 移除AdministratorAccess策略。将ReadOnlyAccess托管IAM策略分配给开发者角色。指示开发者在部署新堆栈时使用CloudFormationDeployment角色作为CloudFormation服务角色。 B. 更新CloudFormationDeployment角色的信任策略，允许开发者IAM角色assume CloudFormationDeployment角色。 C. 配置开发者IAM角色能够获取和传递CloudFormationDeployment角色，如果iam:PassedToService等于cloudformation.amazonaws.com。配置CloudFormationDeployment角色允许所有cloudformation操作对所有资源。 D. 更新CloudFormationDeployment角色的信任策略，允许cloudformation.amazonaws.com AWS主体执行assume role操作。 E. 移除AdministratorAccess策略。将ReadOnlyAccess托管IAM策略分配给开发者角色。指示开发者在部署新堆栈时assume CloudFormationDeployment角色。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个IAM权限架构，确保：1）开发者只能通过CloudFormation进行资源部署和修改；2）防止开发者直接手动修改AWS资源；3）只有CloudFormation服务可以使用特定的部署角色。 **涉及的关键AWS服务和概念：** - CloudFormation服务角色（Service Role）：CloudFormation代表用户执行操作时使用的角色 - IAM角色信任策略：定义谁可以assume该角色 - iam:PassRole权限：允许将角色传递给AWS服务 - 最小权限原则：给予完成任务所需的最小权限 **正确答案的原因：** 选项A是正确的，因为： 1. 移除AdministratorAccess并分配ReadOnlyAccess，限制了开发者的直接操作权限，防止手动修改资源 2. 使用CloudFormationDeployment作为CloudFormation服务角色，确保只有CloudFormation服务可以执行实际的资源操作 3. 这种方式实现了权限分离：开发者只能读取和传递角色给CloudFormation，而实际的资源操作由CloudFormation使用服务角色执行 **其他选项错误的原因：** - 选项B：允许开发者直接assume部署角色，违背了&quot;只有CloudFormation可以使用新角色&quot;的要求 - 选项C：虽然有iam:PassedToService条件，但仍然给了开发者过多权限 - 选项D：这是必要的配置，但单独不足以满足所有要求 - 选项E：让开发者直接assume角色进行部署，同样违背了限制要求 **决策标准和最佳实践：** 1. 实施最小权限原则，开发者只获得必要的读取权限 2. 使用CloudFormation服务角色模式，将实际操作权限委托给AWS服务 3. 通过架构设计而非仅依赖策略条件来强制执行安全要求 4. 确保权限边界清晰，防止权限升级攻击</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">260</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is developing a web application&#x27;s infrastructure using AWS CloudFormation. The database engineering team maintains the database resources in a CloudFormation template, and the software development team maintains the web application resources in a separate CloudFormation template. As the scope of the application grows, the software development team needs to use resources maintained by the database engineering team. However, both teams have their own review and lifecycle management processes that they want to keep. Both teams also require resource-level change-set reviews. The software development team would like to deploy changes to this template using their CI/CD pipeline. Which solution will meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.
B. Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks.
C. Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks.
D. Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS CloudFormation开发Web应用程序的基础设施。数据库工程团队在一个CloudFormation模板中维护数据库资源，软件开发团队在另一个单独的CloudFormation模板中维护Web应用程序资源。随着应用程序范围的扩大，软件开发团队需要使用数据库工程团队维护的资源。但是，两个团队都有自己的审查和生命周期管理流程，他们希望保持这些流程。两个团队还需要资源级别的change-set审查。软件开发团队希望使用他们的CI/CD pipeline来部署对此模板的更改。哪种解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在保持团队独立性的前提下，如何实现CloudFormation模板之间的资源共享。关键需求包括：1）两个团队维护独立的模板和流程；2）软件开发团队需要引用数据库团队的资源；3）保持各自的审查和生命周期管理；4）支持资源级别的change-set审查；5）支持CI/CD pipeline部署。 **涉及的关键AWS服务和概念：** - CloudFormation Stack Exports/Imports：允许一个stack导出值供其他stack使用 - CloudFormation Nested Stacks：父子stack关系，用于模板重用 - CloudFormation Stack Sets：用于跨多个账户和区域部署相同stack - Change Sets：用于预览stack更改的功能 **正确答案的原因：** 选项A（Stack Export/Import）是最佳解决方案，因为： 1）完全保持两个团队的独立性 - 各自维护自己的stack和流程 2）数据库团队可以通过Outputs导出资源引用，软件团队通过ImportValue函数导入 3）支持资源级别的change-set审查 - 每个team都可以独立进行change-set操作 4）完全兼容CI/CD pipeline - 不改变现有部署流程 5）松耦合设计 - 只要导出的值名称不变，两个stack可以独立演进 **其他选项错误的原因：** - 选项B（Nested Stack）：会创建父子依赖关系，违背了团队独立管理的要求，且需要一个团队管理父stack - 选项C（Stack Sets）：主要用于跨账户/区域的相同stack部署，不适用于不同模板间的资源共享场景 - 选项D（Input Parameters）：需要手动传递参数，增加了运维复杂度，且容易出错，不如自动化的export/import机制 **决策标准和最佳实践：** 在选择跨stack资源共享方案时，应优先考虑：1）团队自治性和独立性；2）自动化程度和易维护性；3）与现有CI/CD流程的兼容性；4）松耦合架构设计。Stack Export/Import是CloudFormation中实现跨stack引用的标准最佳实践，既保证了技术可行性，又满足了组织管理需求。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">261</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an organization in AWS Organizations. A DevOps engineer needs to maintain multiple AWS accounts that belong to different OUs in the organization. All resources, including IAM policies and Amazon S3 policies within an account, are deployed through AWS CloudFormation. All templates and code are maintained in an AWS CodeCommit repository. Recently, some developers have not been able to access an S3 bucket from some accounts in the organization. The following policy is attached to the S3 bucket: What should the DevOps engineer do to resolve this access issue? permissions boundaries. Use an AWS Config recorder in the individual developer accounts that are experiencing the issue to revert any changes that are blocking access. Commit the fix back into the CodeCommit repository. Invoke deployment through CloudFormation to apply the changes. D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the S3 bucket policy. Turn off the S3 Block Public Access setting on the S3 bucket. In the S3 policy, add the aws:SourceAccount condition. Add the AWS account IDs of all developers who are experiencing the issue.
B. Verify that no IAM permissions boundaries are denying developers access to the S3 bucket. Make the necessary changes to IAM
C. Configure an SCP that stops anyone from modifying IAM resources in developer OUs. In the S3 policy, add the aws:SourceAccount condition. Add the AWS account IDs of all developers who are experiencing the issue. Commit the fix back into the CodeCommit repository. Invoke deployment through CloudFormation to apply the changes.
D. Ensure that no SCP is blocking access for developers to the S3 bucket. Ensure that no IAM policy permissions boundaries are denying access to developer IAM users. Make the necessary changes to the SCP and IAM policy permissions boundaries in the CodeCommit repository. Invoke deployment through CloudFormation to apply the changes.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中有一个组织。DevOps工程师需要维护属于组织中不同OU的多个AWS账户。账户内的所有资源，包括IAM策略和Amazon S3策略，都通过AWS CloudFormation部署。所有模板和代码都维护在AWS CodeCommit存储库中。最近，一些开发人员无法从组织中的某些账户访问S3存储桶。以下策略附加到S3存储桶上：DevOps工程师应该做什么来解决这个访问问题？ 选项： A. 修改S3存储桶策略。关闭S3存储桶上的S3 Block Public Access设置。在S3策略中，添加aws:SourceAccount条件。添加所有遇到问题的开发人员的AWS账户ID。 B. 验证没有IAM permissions boundaries拒绝开发人员访问S3存储桶。对IAM permissions boundaries进行必要的更改。在遇到问题的个别开发人员账户中使用AWS Config recorder来恢复任何阻止访问的更改。将修复提交回CodeCommit存储库。通过CloudFormation调用部署以应用更改。 C. 配置一个SCP来阻止任何人修改开发人员OU中的IAM资源。在S3策略中，添加aws:SourceAccount条件。添加所有遇到问题的开发人员的AWS账户ID。将修复提交回CodeCommit存储库。通过CloudFormation调用部署以应用更改。 D. 确保没有SCP阻止开发人员访问S3存储桶。确保没有IAM policy permissions boundaries拒绝开发人员IAM用户的访问。在CodeCommit存储库中对SCP和IAM policy permissions boundaries进行必要的更改。通过CloudFormation调用部署以应用更改。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在AWS Organizations环境中排查和解决S3访问权限问题的能力。开发人员无法访问S3存储桶，需要DevOps工程师系统性地排查可能的权限限制并通过正确的流程修复问题。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. Service Control Policies (SCP) - 组织级别的权限边界 3. IAM permissions boundaries - 用户级别的权限边界 4. S3 bucket policies - 资源级别的访问控制 5. AWS CloudFormation - 基础设施即代码 6. AWS CodeCommit - 代码版本控制 **正确答案D的原因：** 1. **全面排查**：同时检查SCP和IAM permissions boundaries这两个可能限制访问的层面 2. **系统性方法**：在AWS Organizations环境中，权限问题可能来自多个层级的限制 3. **正确的修复流程**：通过CodeCommit和CloudFormation进行更改，符合Infrastructure as Code的最佳实践 4. **完整性**：涵盖了组织级别(SCP)和用户级别(permissions boundaries)的权限控制 **其他选项错误的原因：** - **选项A**：关闭S3 Block Public Access是不安全的做法，且只关注S3层面忽略了其他可能的权限限制 - **选项B**：只检查permissions boundaries，忽略了SCP可能造成的限制；使用AWS Config recorder来恢复更改不是正确的权限管理方式 - **选项C**：添加新的SCP限制是在制造问题而不是解决问题；且只关注账户ID而忽略了现有的权限边界问题 **决策标准和最佳实践：** 1. 在AWS Organizations环境中排查权限问题时，需要从上到下检查所有权限层级 2. 使用Infrastructure as Code进行权限管理，确保更改可追踪和可重现 3. 权限问题的排查应该是系统性的，不能只关注单一层面 4. 避免为了解决访问问题而降低安全性（如关闭Block Public Access）</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">262</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an organization in AWS Organizations for its multi-account environment. A DevOps engineer is developing an AWS CodeArtifact based strategy for application package management across the organization. Each application team at the company has its own account in the organization. Each application team also has limited access to a centralized shared services account. Each application team needs full access to download, publish, and grant access to its own packages. Some common library packages that the application teams use must also be shared with the entire organization. Which combination of steps will meet these requirements with the LEAST administrative overhead? (Choose three.) F. Set the other application teams&#x27; repositories as upstream repositories. BCD (75%) BDE (25%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a domain in each application team&#x27;s account. Grant each application team&#x27;s account full read access and write access to the application team&#x27;s domain.
B. Create a domain in the shared services account. Grant the organization read access and CreateRepository access.
C. Create a repository in each application team&#x27;s account. Grant each application team&#x27;s account full read access and write access to its own repository.
D. Create a repository in the shared services account. Grant the organization read access to the repository in the shared services account. Set the repository as the upstream repository in each application team&#x27;s repository.
E. For teams that require shared packages, create resource-based policies that allow read access to the repository from other application teams&#x27; accounts.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中有一个组织来管理其多账户环境。一名DevOps工程师正在开发基于AWS CodeArtifact的策略，用于在整个组织中进行应用程序包管理。公司的每个应用团队在组织中都有自己的账户。每个应用团队对集中式共享服务账户也有有限的访问权限。每个应用团队需要完全访问权限来下载、发布和授权访问自己的包。应用团队使用的一些通用库包也必须与整个组织共享。哪种步骤组合能够以最少的管理开销满足这些要求？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个AWS CodeArtifact包管理策略，需要满足：1）每个应用团队对自己的包有完全控制权；2）通用库包需要在整个组织间共享；3）最小化管理开销。 **涉及的关键AWS服务和概念：** - AWS CodeArtifact：AWS的软件包管理服务 - AWS Organizations：多账户管理服务 - Domain和Repository概念：CodeArtifact中的层次结构 - Upstream Repository：上游仓库机制，用于包共享 - 资源策略和跨账户访问控制 **正确答案BCD的原因：** - **选项B**：在共享服务账户中创建domain，授予组织读取和CreateRepository权限。这提供了集中管理和最小管理开销 - **选项C**：每个团队账户创建自己的repository，确保团队对自己包的完全控制 - **选项D**：共享服务账户的repository作为上游仓库，实现通用库包的组织级共享，这是CodeArtifact的最佳实践 **其他选项错误的原因：** - **选项A**：每个账户创建domain会增加管理复杂性，违背最小管理开销原则 - **选项E**：使用资源策略进行逐一授权会显著增加管理开销，不如upstream机制高效 **决策标准和最佳实践：** CodeArtifact最佳实践是使用集中式domain配合upstream repository机制。这种架构既保证了团队自主性，又通过上游仓库实现了包共享，同时将管理开销降到最低。集中式domain简化了权限管理，upstream机制自动处理包的继承和共享。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">263</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company deploys an application to Amazon EC2 instances. The application runs Amazon Linux 2 and uses AWS CodeDeploy. The application has the following file structure for its code repository: The appspec.yml file has the following contents in the files section: What will the result be for the deployment of the config.txt file? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. The config.txt file will be deployed to only /var/www/html/config/config.txt.
B. The config.txt file will be deployed to /usr/local/src/config.txt and to /var/www/html/config/config.txt.
C. The config.txt file will be deployed to only /usr/local/src/config.txt.
D. The config.txt file will be deployed to /usr/local/src/config.txt and to /var/www/html/application/web/config.txt.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司将应用程序部署到Amazon EC2实例上。该应用程序运行Amazon Linux 2并使用AWS CodeDeploy。应用程序的代码仓库具有以下文件结构：appspec.yml文件在files部分包含以下内容：config.txt文件的部署结果将是什么？ 选项： A. config.txt文件将仅部署到/var/www/html/config/config.txt B. config.txt文件将部署到/usr/local/src/config.txt和/var/www/html/config/config.txt C. config.txt文件将仅部署到/usr/local/src/config.txt D. config.txt文件将部署到/usr/local/src/config.txt和/var/www/html/application/web/config.txt 正确答案：C</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查AWS CodeDeploy中appspec.yml文件的files部分配置规则，特别是文件映射和部署路径的匹配逻辑。需要理解CodeDeploy如何根据源文件路径和目标路径配置来确定文件的最终部署位置。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：自动化应用程序部署服务 - appspec.yml：CodeDeploy的应用程序规范文件，定义部署过程 - files部分：指定源文件到目标位置的映射关系 - 文件路径匹配规则：CodeDeploy使用特定的路径匹配算法 **正确答案的原因：** 选项C正确，因为CodeDeploy的files部分遵循&quot;最具体匹配&quot;原则。当appspec.yml中有多个文件映射规则时，CodeDeploy会选择最具体、最精确匹配源文件路径的规则。config.txt文件只会根据最匹配的规则部署到一个目标位置/usr/local/src/config.txt，而不会同时部署到多个位置。 **其他选项错误的原因：** - 选项A错误：假设了错误的目标路径，没有正确理解appspec.yml中的映射规则 - 选项B和D错误：误认为一个源文件可以同时部署到多个目标位置。CodeDeploy不会将同一个源文件复制到多个目标路径，它只会使用最匹配的规则进行单一部署 **决策标准和最佳实践：** 1. 理解CodeDeploy文件映射的&quot;最具体匹配&quot;原则 2. 在设计appspec.yml时避免路径规则冲突 3. 使用明确的源路径和目标路径映射以避免歧义 4. 测试部署配置以确保文件被部署到预期位置 5. 遵循最小权限原则，确保CodeDeploy有适当的权限访问目标路径</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">264</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has set up AWS CodeArtifact repositories with public upstream repositories. The company&#x27;s development team consumes open source dependencies from the repositories in the company&#x27;s internal network. The company&#x27;s security team recently discovered a critical vulnerability in the most recent version of a package that the development team consumes. The security team has produced a patched version to fix the vulnerability. The company needs to prevent the vulnerable version from being downloaded. The company also needs to allow the security team to publish the patched version. Which combination of steps will meet these requirements? (Choose two.) CD (60%) BD (40%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the status of the affected CodeArtifact package version to unlisted.
B. Update the status of the affected CodeArtifact package version to deleted.
C. Update the status of the affected CodeArtifact package version to archived.
D. Update the CodeArtifact package origin control settings to allow direct publishing and to block upstream operations.
E. Update the CodeArtifact package origin control settings to block direct publishing and to allow upstream operations.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司已经设置了带有公共上游仓库的AWS CodeArtifact仓库。公司的开发团队在公司内部网络中从这些仓库消费开源依赖项。公司的安全团队最近在开发团队消费的某个包的最新版本中发现了一个严重漏洞。安全团队已经制作了一个修补版本来修复该漏洞。公司需要防止下载有漏洞的版本，同时还需要允许安全团队发布修补版本。哪种步骤组合能满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodeArtifact中包版本管理和安全控制的最佳实践。需要同时实现两个目标：1）阻止下载有漏洞的包版本；2）允许安全团队发布修补后的版本。 **涉及的关键AWS服务和概念：** - AWS CodeArtifact：企业级包管理服务 - 包版本状态管理：包括unlisted、deleted、archived等状态 - Origin Control设置：控制包的发布来源和上游操作 - 上游仓库：从外部公共仓库（如npm、PyPI等）获取包的机制 **正确答案的原因：** 选项B（将包版本状态更新为deleted）是正确的，因为： - deleted状态会完全阻止该版本被下载和访问 - 这是处理安全漏洞最彻底的方法 - 符合安全最佳实践，确保有漏洞的代码无法被意外使用 还需要选择一个关于origin control的选项。正确的第二个答案应该是D，因为： - 允许direct publishing使安全团队能够直接发布修补版本 - 阻止upstream operations防止从上游仓库重新获取有漏洞的版本 **其他选项错误的原因：** - 选项A（unlisted）：包仍然可以通过直接引用下载，安全风险依然存在 - 选项C（archived）：通常用于长期保存，不是处理安全漏洞的适当方式 - 选项E：阻止direct publishing会妨碍安全团队发布修补版本 **决策标准和最佳实践：** 处理包安全漏洞时应遵循：1）立即阻断风险源；2）确保修补机制畅通；3）采用最严格的安全控制措施；4）平衡安全性与开发效率。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">265</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is running a custom-built application that processes records. All the components run on Amazon EC2 instances that run in an Auto Scaling group. Each record&#x27;s processing is a multistep sequential action that is compute-intensive. Each step is always completed in 5 minutes or less. A limitation of the current system is that if any steps fail, the application has to reprocess the record from the beginning. The company wants to update the architecture so that the application must reprocess only the failed steps. What is the MOST operationally efficient solution that meets these requirements? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a web application to write records to Amazon S3. Use S3 Event Notifications to publish to an Amazon Simple Notification Service (Amazon SNS) topic. Use an EC2 instance to poll Amazon SNS and start processing. Save intermediate results to Amazon S3 to pass on to the next step.
B. Perform the processing steps by using logic in the application. Convert the application code to run in a container. Use AWS Fargate to manage the container instances. Configure the container to invoke itself to pass the state from one step to the next.
C. Create a web application to pass records to an Amazon Kinesis data stream. Decouple the processing by using the Kinesis data stream and AWS Lambda functions.
D. Create a web application to pass records to AWS Step Functions. Decouple the processing into Step Functions tasks and AWS Lambda functions.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在运行一个自定义构建的应用程序来处理记录。所有组件都运行在Auto Scaling组中的Amazon EC2实例上。每个记录的处理是一个多步骤的顺序操作，计算密集型。每个步骤总是在5分钟或更短时间内完成。当前系统的一个限制是，如果任何步骤失败，应用程序必须从头开始重新处理记录。公司希望更新架构，使应用程序只需要重新处理失败的步骤。什么是满足这些要求的最具运营效率的解决方案？ 选项： A. 创建一个web应用程序将记录写入Amazon S3。使用S3 Event Notifications发布到Amazon Simple Notification Service (Amazon SNS) topic。使用EC2实例轮询Amazon SNS并开始处理。将中间结果保存到Amazon S3以传递给下一步。 B. 使用应用程序中的逻辑执行处理步骤。将应用程序代码转换为在容器中运行。使用AWS Fargate管理容器实例。配置容器调用自身以将状态从一个步骤传递到下一个步骤。 C. 创建一个web应用程序将记录传递给Amazon Kinesis数据流。使用Kinesis数据流和AWS Lambda函数解耦处理。 D. 创建一个web应用程序将记录传递给AWS Step Functions。将处理解耦为Step Functions任务和AWS Lambda函数。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个架构来解决多步骤顺序处理中的容错问题。关键需求包括：1）多步骤顺序处理；2）每步5分钟内完成；3）失败时只重新执行失败的步骤，而不是从头开始；4）最具运营效率的解决方案。 **涉及的关键AWS服务和概念：** - AWS Step Functions：工作流编排服务，支持状态管理和错误处理 - AWS Lambda：无服务器计算服务，适合短时间任务 - Amazon Kinesis：实时数据流处理服务 - AWS Fargate：无服务器容器平台 - Amazon S3和SNS：存储和通知服务 **正确答案D的原因：** AWS Step Functions是专门为解决这类问题设计的工作流编排服务。它提供：1）内置的状态管理和错误处理机制；2）可以精确控制在哪个步骤重试，而不需要从头开始；3）与Lambda完美集成，每个步骤可以是独立的Lambda函数；4）可视化工作流管理，运营效率最高；5）自动处理步骤间的状态传递和错误恢复。 **其他选项错误的原因：** 选项A：使用S3+SNS+EC2的方案过于复杂，需要自己实现状态管理和错误处理逻辑，运营效率低。选项B：Fargate容器方案仍然需要自己实现复杂的状态管理和重试逻辑，且容器自调用的设计不够优雅。选项C：Kinesis主要用于流数据处理，不是为多步骤工作流设计的，缺乏内置的步骤级错误处理能力。 **决策标准和最佳实践：** 选择工作流编排服务时应考虑：1）是否有内置的状态管理；2）错误处理和重试机制的灵活性；3）运营复杂度和维护成本；4）与其他AWS服务的集成程度。Step Functions作为托管的工作流服务，在处理多步骤顺序任务时是最佳选择，特别是需要精细化错误处理的场景。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">266</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is migrating its on-premises Windows applications and Linux applications to AWS. The company will use automation to launch Amazon EC2 instances to mirror the on-premises configurations. The migrated applications require access to shared storage that uses SMB for Windows and NFS for Linux. The company is also creating a pilot light disaster recovery (DR) environment in another AWS Region. The company will use automation to launch and configure the EC2 instances in the DR Region. The company needs to replicate the storage to the DR Region. Which storage solution will meet these requirements? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Amazon S3 for the application storage. Create an S3 bucket in the primary Region and an S3 bucket in the DR Region. Configure S3 Cross-Region Replication (CRR) from the primary Region to the DR Region.
B. Use Amazon Elastic Block Store (Amazon EBS) for the application storage. Create a backup plan in AWS Backup that creates snapshots of the EBS volumes that are in the primary Region and replicates the snapshots to the DR Region.
C. Use a Volume Gateway in AWS Storage Gateway for the application storage. Configure Cross-Region Replication (CRR) of the Volume Gateway from the primary Region to the DR Region.
D. Use Amazon FSx for NetApp ONTAP for the application storage. Create an FSx for ONTAP instance in the DR Region. Configure NetApp SnapMirror replication from the primary Region to the DR Region.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在将其本地Windows应用程序和Linux应用程序迁移到AWS。该公司将使用自动化来启动Amazon EC2实例以镜像本地配置。迁移的应用程序需要访问共享存储，Windows使用SMB协议，Linux使用NFS协议。该公司还在另一个AWS Region创建了一个pilot light灾难恢复(DR)环境。该公司将使用自动化在DR Region启动和配置EC2实例。该公司需要将存储复制到DR Region。哪种存储解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个存储解决方案，需要同时满足以下条件： 1. 支持SMB协议（Windows应用程序） 2. 支持NFS协议（Linux应用程序） 3. 提供共享存储功能 4. 支持跨Region复制到灾难恢复环境 5. 适合pilot light DR架构 **涉及的关键AWS服务和概念：** - Amazon FSx for NetApp ONTAP：完全托管的文件系统服务 - NetApp SnapMirror：数据复制技术 - SMB和NFS协议：文件共享协议 - Pilot light DR：轻量级灾难恢复策略 - 跨Region复制 **正确答案D的原因：** 1. **协议支持完整**：FSx for NetApp ONTAP原生支持SMB和NFS协议，可以同时为Windows和Linux应用程序提供文件共享服务 2. **共享存储**：提供真正的共享文件系统，多个EC2实例可以同时访问 3. **复制能力强**：NetApp SnapMirror提供高效的跨Region数据复制功能 4. **企业级功能**：支持快照、克隆、数据去重等企业级存储功能 5. **完全托管**：AWS负责基础设施管理，简化运维 **其他选项错误的原因：** - **选项A (S3 + CRR)**：S3是对象存储，不支持SMB/NFS协议，无法作为应用程序的共享文件系统使用 - **选项B (EBS + AWS Backup)**：EBS是块存储，只能挂载到单个EC2实例，不支持多实例共享访问，且不直接支持SMB/NFS协议 - **选项C (Volume Gateway + CRR)**：Volume Gateway主要提供iSCSI协议的块存储，不支持SMB/NFS文件共享协议 **决策标准和最佳实践：** 1. **协议匹配**：选择支持应用程序所需协议的存储服务 2. **共享需求**：区分块存储、文件存储和对象存储的使用场景 3. **DR要求**：选择具备原生跨Region复制能力的服务 4. **运维简化**：优先选择完全托管的服务减少管理负担 5. **性能考虑**：文件系统服务通常比通过网关的解决方案性能更好</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">267</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s application uses a fleet of Amazon EC2 On-Demand Instances to analyze and process data. The EC2 instances are in an Auto Scaling group. The Auto Scaling group is a target group for an Application Load Balancer (ALB). The application analyzes critical data that cannot tolerate interruption. The application also analyzes noncritical data that can withstand interruption. The critical data analysis requires quick scalability in response to real-time application demand. The noncritical data analysis involves memory consumption. A DevOps engineer must implement a solution that reduces scale-out latency for the critical data. The solution also must process the noncritical data. Which combination of steps will meet these requirements? (Choose two.) successfully. Ensure that the application on the instances is ready to accept traffic before the instances are registered. Create a new version of the launch template that has detailed monitoring enabled. BD (80%) BE (20%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. For the critical data, modify the existing Auto Scaling group. Create a warm pool instance in the stopped state. Define the warm pool size. Create a new version of the launch template that has detailed monitoring enabled. Use Spot Instances.
B. For the critical data, modify the existing Auto Scaling group. Create a warm pool instance in the stopped state. Define the warm pool size. Create a new version of the launch template that has detailed monitoring enabled. Use On-Demand Instances.
C. For the critical data, modify the existing Auto Scaling group. Create a lifecycle hook to ensure that bootstrap scripts are completed
D. For the noncritical data, create a second Auto Scaling group that uses a launch template. Configure the launch template to install the unified Amazon CloudWatch agent and to configure the CloudWatch agent with a custom memory utilization metric. Use Spot Instances. Add the new Auto Scaling group as the target group for the ALB. Modify the application to use two target groups for critical data and noncritical data.
E. For the noncritical data, create a second Auto Scaling group. Choose the predefined memory utilization metric type for the target tracking scaling policy. Use Spot Instances. Add the new Auto Scaling group as the target group for the ALB. Modify the application to use two target groups for critical data and noncritical data.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的应用程序使用Amazon EC2 On-Demand实例集群来分析和处理数据。EC2实例位于Auto Scaling组中。Auto Scaling组是Application Load Balancer (ALB)的目标组。应用程序分析不能容忍中断的关键数据，也分析可以承受中断的非关键数据。关键数据分析需要快速扩展以响应实时应用程序需求。非关键数据分析涉及内存消耗。DevOps工程师必须实施一个解决方案，减少关键数据的横向扩展延迟，同时也必须处理非关键数据。哪种步骤组合将满足这些要求？（选择两个） 选项： A. 对于关键数据，修改现有的Auto Scaling组。创建处于停止状态的warm pool实例。定义warm pool大小。创建启用详细监控的launch template新版本。使用Spot实例。 B. 对于关键数据，修改现有的Auto Scaling组。创建处于停止状态的warm pool实例。定义warm pool大小。创建启用详细监控的launch template新版本。使用On-Demand实例。 C. 对于关键数据，修改现有的Auto Scaling组。创建lifecycle hook以确保bootstrap脚本完成。 D. 对于非关键数据，创建使用launch template的第二个Auto Scaling组。配置launch template安装统一的Amazon CloudWatch agent并配置CloudWatch agent使用自定义内存利用率指标。使用Spot实例。将新的Auto Scaling组添加为ALB的目标组。修改应用程序为关键数据和非关键数据使用两个目标组。 E. 对于非关键数据，创建第二个Auto Scaling组。为target tracking scaling policy选择预定义的内存利用率指标类型。使用Spot实例。将新的Auto Scaling组添加为ALB的目标组。修改应用程序为关键数据和非关键数据使用两个目标组。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个解决方案来处理两种不同类型的数据工作负载： 1. 关键数据：不能容忍中断，需要快速扩展响应实时需求，要求减少scale-out延迟 2. 非关键数据：可以承受中断，涉及内存消耗监控 **涉及的关键AWS服务和概念：** - Auto Scaling Groups和warm pools（预热池） - EC2实例类型（On-Demand vs Spot） - Application Load Balancer目标组 - CloudWatch监控和自定义指标 - Launch Templates和lifecycle hooks **正确答案的原因：** 选项B正确，因为： - Warm pool可以预先准备处于停止状态的实例，当需要扩展时可以快速启动，显著减少scale-out延迟 - 对于关键数据使用On-Demand实例确保可用性和稳定性，避免Spot实例的中断风险 - 详细监控提供更精确的指标数据，支持更好的扩展决策 还需要选择D或E中的一个来处理非关键数据。选项D更合适，因为它提供了自定义内存监控指标的完整解决方案。 **其他选项错误的原因：** - 选项A：对关键数据使用Spot实例不合适，因为Spot实例可能被中断，与&quot;不能容忍中断&quot;的要求冲突 - 选项C：Lifecycle hook虽然有用，但不如warm pool能有效减少scale-out延迟 - 选项E：预定义的内存利用率指标可能不够精确，自定义指标更适合特定的内存消耗分析需求 **决策标准和最佳实践：** 1. 关键工作负载应使用On-Demand实例确保可用性 2. 使用warm pools是减少Auto Scaling延迟的最佳实践 3. 非关键工作负载可以使用成本更低的Spot实例 4. 根据工作负载特性选择合适的监控指标（内存消耗需要专门的CloudWatch agent配置） 5. 分离不同类型的工作负载到不同的Auto Scaling组可以提供更好的控制和优化</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">268</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company recently migrated its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that uses Amazon EC2 instances. The company configured the application to automatically scale based on CPU utilization. The application produces memory errors when it experiences heavy loads. The application also does not scale out enough to handle the increased load. The company needs to collect and analyze memory metrics for the application over time. Which combination of steps will meet these requirements? (Choose three.) agent to the AMI for any new EC2 instances that are added to the cluster. Most Voted F. Analyze the node_memory_utilization Amazon CloudWatch metric in the ContainerInsights namespace by using the ClusterName dimension. ACE (63%) BCE (25%) 13%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Attach the CloudWatchAgentServerPolicy managed IAM policy to the IAM instance profile that the cluster uses.
B. Attach the CloudWatchAgentServerPolicy managed IAM policy to a service account role for the cluster.
C. Collect performance metrics by deploying the unified Amazon CloudWatch agent to the existing EC2 instances in the cluster. Add the
D. Collect performance logs by deploying the AWS Distro for OpenTelemetry collector as a DaemonSet.
E. Analyze the pod_memory_utilization Amazon CloudWatch metric in the ContainerInsights namespace by using the Service dimension.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司最近将其应用程序迁移到使用Amazon EC2实例的Amazon Elastic Kubernetes Service (Amazon EKS)集群。该公司配置应用程序根据CPU利用率自动扩展。当应用程序遇到重负载时会产生内存错误。应用程序也没有充分扩展以处理增加的负载。公司需要收集和分析应用程序随时间变化的内存指标。以下哪些步骤的组合将满足这些要求？（选择三个。） 选项： A. 将CloudWatchAgentServerPolicy托管IAM策略附加到集群使用的IAM实例配置文件。 B. 将CloudWatchAgentServerPolicy托管IAM策略附加到集群的服务账户角色。 C. 通过将统一的Amazon CloudWatch代理部署到集群中现有的EC2实例来收集性能指标。将代理添加到添加到集群的任何新EC2实例的AMI中。 D. 通过将AWS Distro for OpenTelemetry收集器作为DaemonSet部署来收集性能日志。 E. 通过使用Service维度分析ContainerInsights命名空间中的pod_memory_utilization Amazon CloudWatch指标。 F. 通过使用ClusterName维度分析ContainerInsights命名空间中的node_memory_utilization Amazon CloudWatch指标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为EKS集群配置内存监控解决方案，以便收集和分析应用程序的内存使用情况，帮助解决内存错误和扩展不足的问题。需要选择三个正确的步骤。 **涉及的关键AWS服务和概念：** - Amazon EKS (Elastic Kubernetes Service) - Amazon CloudWatch和Container Insights - CloudWatch Agent - IAM权限管理 - Kubernetes监控指标 **正确答案分析（A、C、E）：** 选项A正确：CloudWatchAgentServerPolicy是CloudWatch代理运行所需的IAM权限。在EKS环境中，EC2实例需要通过实例配置文件获得这些权限才能发送指标到CloudWatch。 选项C正确：部署统一CloudWatch代理是收集详细内存指标的标准方法。代理可以收集操作系统级别和应用程序级别的内存指标，并将其发送到CloudWatch进行分析。 选项E正确：pod_memory_utilization指标提供了Pod级别的内存使用情况，这对于分析应用程序的内存使用模式和识别内存瓶颈非常重要。Service维度允许按服务分组分析。 **其他选项错误的原因：** 选项B错误：虽然服务账户角色是Kubernetes的最佳实践，但对于EC2节点上的CloudWatch代理，通常使用实例配置文件更直接有效。 选项D错误：题目要求的是内存指标收集，而OpenTelemetry收集器主要用于收集性能日志，不是最适合的解决方案。 选项F部分正确但不是最佳选择：node_memory_utilization提供节点级别的内存信息，但Pod级别的指标（选项E）对于应用程序内存问题的诊断更有价值。 **决策标准和最佳实践：** 1. 权限配置：确保CloudWatch代理有适当的IAM权限 2. 监控粒度：选择Pod级别监控以获得应用程序特定的洞察 3. 数据收集：使用CloudWatch代理获得全面的系统和应用程序指标 4. 可扩展性：确保监控解决方案能够随集群扩展而扩展</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">269</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s video streaming platform usage has increased from 10,000 users each day to 50,000 users each day in multiple countries. The company deploys the streaming platform on Amazon Elastic Kubernetes Service (Amazon EKS). The EKS workload scales up to thousands of nodes during peak viewing time. The company&#x27;s users report occurrences of unauthorized logins. Users also report sudden interruptions and logouts from the platform. The company wants additional security measures for the entire platform. The company also needs a summarized view of the resource behaviors and interactions across the company&#x27;s entire AWS environment. The summarized view must show login attempts, API calls, and network traffic. The solution must permit network traffic analysis while minimizing the overhead of managing logs. The solution must also quickly investigate any potential malicious behavior that is associated with the EKS workload. Which solution will meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable AWS CloudTrail logs. Store the EKS audit logs and CloudTrail log files in an Amazon S3 bucket. Use Amazon Athena to create an external table. Use Amazon QuickSight to create a dashboard.
B. Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable Amazon Detective in the company&#x27;s AWS account. Enable EKS audit logs from optional source packages in Detective.
C. Enable Amazon CloudWatch Container Insights. Enable AWS CloudTrail logs. Store the EKS audit logs and CloudTrail log files in an Amazon S3 bucket. Use Amazon Athena to create an external table. Use Amazon QuickSight to create a dashboard.
D. Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable Amazon CloudWatch Container Insights and VPC Flow Logs. Enable AWS CloudTrail logs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的视频流媒体平台使用量从每天10,000用户增长到多个国家的每天50,000用户。该公司在Amazon Elastic Kubernetes Service (Amazon EKS)上部署流媒体平台。EKS工作负载在高峰观看时间扩展到数千个节点。公司用户报告出现未经授权的登录。用户还报告平台突然中断和登出。公司希望为整个平台增加额外的安全措施。公司还需要一个汇总视图来显示整个AWS环境中资源行为和交互。汇总视图必须显示登录尝试、API调用和网络流量。解决方案必须允许网络流量分析，同时最小化管理日志的开销。解决方案还必须能够快速调查与EKS工作负载相关的任何潜在恶意行为。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 解决未经授权登录和突然中断问题 2. 为整个平台提供额外安全措施 3. 提供汇总视图显示登录尝试、API调用和网络流量 4. 支持网络流量分析且最小化日志管理开销 5. 快速调查EKS工作负载的潜在恶意行为 **涉及的关键AWS服务和概念：** - Amazon GuardDuty：威胁检测服务，专门用于识别恶意活动 - EKS Audit Log Monitoring：监控Kubernetes API服务器的审计日志 - Amazon Detective：安全调查服务，提供可视化分析 - CloudWatch Container Insights：容器监控服务 - VPC Flow Logs：网络流量日志 - AWS CloudTrail：API调用审计服务 **正确答案D的原因：** 1. **GuardDuty for EKS Audit Log Monitoring**：提供自动威胁检测，识别未经授权的访问和恶意行为 2. **CloudWatch Container Insights**：提供容器级别的监控和可视化，满足汇总视图需求 3. **VPC Flow Logs**：提供网络流量分析能力，满足网络流量监控要求 4. **CloudTrail**：记录API调用，满足审计要求 5. 这个组合提供了完整的安全监控覆盖，且CloudWatch和GuardDuty都是托管服务，最小化了管理开销 **其他选项错误的原因：** - **选项A**：使用S3+Athena+QuickSight需要手动管理和查询日志，增加了管理开销，不符合&quot;最小化日志管理开销&quot;的要求 - **选项B**：Detective主要用于事后调查，缺少实时监控和网络流量分析能力，无法满足网络流量分析需求 - **选项C**：缺少GuardDuty威胁检测功能，无法有效识别恶意行为；同样存在手动管理日志的开销问题 **决策标准和最佳实践：** 1. 选择托管服务以减少运维开销 2. 威胁检测需要专业的安全服务（GuardDuty） 3. 网络安全监控需要多层防护（应用层+网络层） 4. 容器环境需要专门的监控工具（Container Insights） 5. 综合使用多个AWS安全服务形成完整的安全监控体系</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">270</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Organizations to manage hundreds of AWS accounts. The company has a team that is responsible for AWS Identity and Access Management (IAM). The IAM team wants to implement AWS IAM Identity Center (AWS Single Sign-On). The IAM team must have only the minimum needed permissions to manage IAM Identity Center. The IAM team must not be able to gain unneeded access to the Organizations management account. The IAM team must be able to provision new IAM Identity Center permission sets and assignments for existing and new member accounts. Which combination of steps will meet these requirements? (Choose three.) Organizations management account, register the new account as a delegated administrator for IAM Identity Center. F. Assign the permission set to the new AWS account. Allow the IAM team group to use the permission set. Most Voted ADF (67%) BCF (33%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new AWS account for the IAM team. In the new account, enable IAM Identity Center. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center.
B. Create a new AWS account for the IAM team. In the Organizations management account, enable IAM Identity Center. In the
C. In IAM Identity Center, create users and a group for the IAM team. Add the users to the group. Create a new permission set. Attach the AWSSSODirectoryAdministrator managed IAM policy to the group.
D. In IAM Identity Center, create users and a group for the IAM team. Add the users to the group. Create a new permission set. Attach the AWSSSOMemberAccountAdministrator managed IAM policy to the group.
E. Assign the permission set to the Organizations management account. Allow the IAM team group to use the permission set.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations来管理数百个AWS账户。该公司有一个负责AWS Identity and Access Management (IAM)的团队。IAM团队想要实施AWS IAM Identity Center (AWS Single Sign-On)。IAM团队必须只拥有管理IAM Identity Center所需的最小权限。IAM团队不能获得对Organizations管理账户的不必要访问权限。IAM团队必须能够为现有和新的成员账户配置新的IAM Identity Center权限集和分配。哪种步骤组合将满足这些要求？（选择三个。） 选项： A. 为IAM团队创建一个新的AWS账户。在新账户中启用IAM Identity Center。在Organizations管理账户中，将新账户注册为IAM Identity Center的委托管理员。 B. 为IAM团队创建一个新的AWS账户。在Organizations管理账户中启用IAM Identity Center。在Organizations管理账户中，将新账户注册为IAM Identity Center的委托管理员。 C. 在IAM Identity Center中，为IAM团队创建用户和组。将用户添加到组中。创建一个新的权限集。将AWSSSODirectoryAdministrator托管IAM策略附加到组。 D. 在IAM Identity Center中，为IAM团队创建用户和组。将用户添加到组中。创建一个新的权限集。将AWSSSOMemberAccountAdministrator托管IAM策略附加到组。 E. 将权限集分配给Organizations管理账户。允许IAM团队组使用权限集。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为IAM团队设置AWS IAM Identity Center管理权限，需要满足三个关键要求：1）最小权限原则；2）不能访问Organizations管理账户；3）能够管理权限集和分配。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS IAM Identity Center：集中身份管理和单点登录服务 - 委托管理员（Delegated Administrator）：允许成员账户管理特定服务 - 权限集（Permission Sets）：定义用户在目标账户中的权限 - 托管策略：AWS预定义的权限策略 **正确答案的原因：** 根据题目描述，这应该是一个&quot;选择三个&quot;的题目，但只给出了正确答案C。选项C正确的原因是： - 在IAM Identity Center中创建用户和组是标准做法 - AWSSSODirectoryAdministrator策略提供了管理IAM Identity Center目录、用户、组和权限集的权限 - 这个策略符合最小权限原则，专门用于IAM Identity Center管理 **其他选项分析：** - 选项A：在新账户中启用IAM Identity Center是错误的，应该在管理账户中启用 - 选项B：正确描述了委托管理员的设置方式，可能是正确答案之一 - 选项D：AWSSSOMemberAccountAdministrator主要用于成员账户管理，权限范围不够 - 选项E：将权限集分配给管理账户违反了&quot;不能访问管理账户&quot;的要求 **决策标准和最佳实践：** 1. 使用委托管理员模式避免直接访问管理账户 2. 选择合适的托管策略确保最小权限 3. 在专门的账户中设置IAM团队，实现职责分离 4. IAM Identity Center应在管理账户中启用，但通过委托管理员在其他账户中管理</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">271</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an organization in AWS Organizations that has all features enabled. The company uses AWS Backup in a primary account and uses an AWS Key Management Service (AWS KMS) key to encrypt the backups. The company needs to automate a cross-account backup of the resources that AWS Backup backs up in the primary account. The company configures cross-account backup in the Organizations management account. The company creates a new AWS account in the organization and configures an AWS Backup backup vault in the new account. The company creates a KMS key in the new account to encrypt the backups. Finally, the company configures a new backup plan in the primary account. The destination for the new backup plan is the backup vault in the new account. When the AWS Backup job in the primary account is invoked, the job creates backups in the primary account. However, the backups are not copied to the new account&#x27;s backup vault. Which combination of steps must the company take so that backups can be copied to the new account&#x27;s backup vault? (Choose two.) AE (54%) AD (46%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Edit the backup vault access policy in the new account to allow access to the primary account.
B. Edit the backup vault access policy in the primary account to allow access to the new account.
C. Edit the backup vault access policy in the primary account to allow access to the KMS key in the new account.
D. Edit the key policy of the KMS key in the primary account to share the key with the new account.
E. Edit the key policy of the KMS key in the new account to share the key with the primary account.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中使用了一个启用了所有功能的组织。该公司在主账户中使用AWS Backup，并使用AWS Key Management Service (AWS KMS)密钥来加密备份。该公司需要自动化跨账户备份AWS Backup在主账户中备份的资源。公司在Organizations管理账户中配置了跨账户备份。公司在组织中创建了一个新的AWS账户，并在新账户中配置了AWS Backup备份保险库。公司在新账户中创建了一个KMS密钥来加密备份。最后，公司在主账户中配置了一个新的备份计划。新备份计划的目标是新账户中的备份保险库。当主账户中的AWS Backup作业被调用时，作业在主账户中创建了备份。但是，备份没有被复制到新账户的备份保险库中。公司必须采取哪些步骤组合，以便备份可以被复制到新账户的备份保险库？（选择两个）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题考查的是AWS跨账户备份配置中的权限问题。主账户的备份作业能够在本地创建备份，但无法将备份复制到目标账户的备份保险库，需要找出缺失的权限配置。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. AWS Backup - 集中化备份服务 3. AWS KMS - 密钥管理服务 4. 跨账户资源访问权限 5. 备份保险库访问策略 6. KMS密钥策略 **正确答案的原因：** 选项A和E是正确答案： - **选项A**：编辑新账户中的备份保险库访问策略以允许主账户访问。这是必需的，因为目标账户的备份保险库需要明确授权主账户将备份复制到其中。 - **选项E**：编辑新账户中KMS密钥的密钥策略以与主账户共享密钥。由于备份需要在新账户中使用新的KMS密钥进行加密，主账户必须有权限使用这个密钥来加密复制过来的备份。 **其他选项错误的原因：** - **选项B**：主账户的备份保险库访问策略不需要修改，因为备份是从主账户复制出去，而不是复制进来。 - **选项C**：主账户的备份保险库访问策略不需要配置对新账户KMS密钥的访问，这不是正确的权限配置方向。 - **选项D**：不需要将主账户的KMS密钥共享给新账户，因为目标是使用新账户的KMS密钥来加密备份。 **决策标准和最佳实践：** 1. 跨账户备份需要目标账户明确授权源账户访问其备份保险库 2. 当使用目标账户的KMS密钥加密时，源账户需要有使用该密钥的权限 3. 权限配置应遵循最小权限原则，只授予必要的访问权限 4. 在AWS Organizations环境中，跨账户资源访问仍需要明确的权限配置</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">272</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs an application that uses an Amazon S3 bucket to store images. A DevOps engineer needs to implement a multi-Region strategy for the objects that are stored in the S3 bucket. The company needs to be able to fail over to an S3 bucket in another AWS Region. When an image is added to either S3 bucket, the image must be replicated to the other S3 bucket within 15 minutes. The DevOps engineer enables two-way replication between the S3 buckets. Which combination of steps should the DevOps engineer take next to meet the requirements? (Choose three.) F. Call the UpdateRoutingControlStates operation in the AWS API when the company needs to fail over to the S3 bucket in the other Region. ABC (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable S3 Replication Time Control (S3 RTC) on each replication rule.
B. Create an S3 Multi-Region Access Point in an active-passive configuration.
C. Call the SubmitMultiRegionAccessPointRoutes operation in the AWS API when the company needs to fail over to the S3 bucket in the other Region.
D. Enable S3 Transfer Acceleration on both S3 buckets.
E. Configure a routing control in Amazon Route 53 Recovery Controller. Add the S3 buckets in an active-passive configuration.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司运行一个使用Amazon S3存储桶来存储图像的应用程序。DevOps工程师需要为存储在S3存储桶中的对象实施多Region策略。公司需要能够故障转移到另一个AWS Region中的S3存储桶。当图像添加到任一S3存储桶时，图像必须在15分钟内复制到另一个S3存储桶。DevOps工程师启用了S3存储桶之间的双向复制。DevOps工程师接下来应该采取哪些步骤组合来满足要求？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现S3的多Region策略，关键需求包括：1）双向复制已启用；2）图像必须在15分钟内完成跨Region复制；3）需要故障转移能力。题目要求选择三个正确答案。 **涉及的关键AWS服务和概念：** - S3 Cross-Region Replication (CRR) - 跨Region复制 - S3 Replication Time Control (S3 RTC) - 复制时间控制 - S3 Multi-Region Access Point - 多Region访问点 - S3 Transfer Acceleration - 传输加速 - Route 53 Application Recovery Controller - 应用恢复控制器 **正确答案分析：** 题目显示正确答案是A，但标注了&quot;ABC (100%)&quot;，这表明A、B、C都是正确答案： A. 启用S3 Replication Time Control (S3 RTC) - 正确，因为S3 RTC专门用于确保复制在15分钟内完成，这直接满足了时间要求。 B. 创建active-passive配置的S3 Multi-Region Access Point - 正确，这提供了统一的全局端点和故障转移能力。 C. 调用SubmitMultiRegionAccessPointRoutes API操作进行故障转移 - 正确，这是控制Multi-Region Access Point路由和实现故障转移的正确方法。 **其他选项错误的原因：** D. S3 Transfer Acceleration - 错误，虽然能加速上传，但不能保证15分钟内的复制时间要求，且不提供故障转移功能。 E. Route 53 Recovery Controller配置 - 错误，这主要用于应用层面的故障转移控制，不是S3对象级别复制的最佳解决方案。 F. UpdateRoutingControlStates操作 - 错误，这是Route 53 Recovery Controller的API，与S3 Multi-Region Access Point的故障转移机制不匹配。 **决策标准和最佳实践：** 1. 使用S3 RTC确保满足严格的复制时间要求（15分钟） 2. 使用S3 Multi-Region Access Point提供统一访问入口和自动故障转移 3. 通过正确的API操作实现可控的故障转移机制 4. 选择专门针对S3服务设计的解决方案，而非通用的网络层解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">273</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses the AWS Cloud Development Kit (AWS CDK) to define its application. The company uses a pipeline that consists of AWS CodePipeline and AWS CodeBuild to deploy the CDK application. The company wants to introduce unit tests to the pipeline to test various infrastructure components. The company wants to ensure that a deployment proceeds if no unit tests result in a failure. Which combination of steps will enforce the testing requirement in the pipeline? (Choose two.) AD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the CodeBuild build phase commands to run the tests then to deploy the application. Set the OnFailure phase property to ABORT.
B. Update the CodeBuild build phase commands to run the tests then to deploy the application. Add the --rollback true flag to the cdk deploy command.
C. Update the CodeBuild build phase commands to run the tests then to deploy the application. Add the --require-approval any-change flag to the cdk deploy command.
D. Create a test that uses the AWS CDK assertions module. Use the template.hasResourceProperties assertion to test that resources have the expected properties.
E. Create a test that uses the cdk diff command. Configure the test to fail if any resources have changed.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Cloud Development Kit (AWS CDK)来定义其应用程序。该公司使用由AWS CodePipeline和AWS CodeBuild组成的流水线来部署CDK应用程序。公司希望在流水线中引入单元测试来测试各种基础设施组件。公司希望确保如果没有单元测试失败，部署就会继续进行。哪种步骤组合将在流水线中强制执行测试要求？（选择两个。） 选项： A. 更新CodeBuild构建阶段命令以运行测试然后部署应用程序。将OnFailure阶段属性设置为ABORT。 B. 更新CodeBuild构建阶段命令以运行测试然后部署应用程序。在cdk deploy命令中添加--rollback true标志。 C. 更新CodeBuild构建阶段命令以运行测试然后部署应用程序。在cdk deploy命令中添加--require-approval any-change标志。 D. 创建一个使用AWS CDK assertions模块的测试。使用template.hasResourceProperties断言来测试资源是否具有预期的属性。 E. 创建一个使用cdk diff命令的测试。配置测试在任何资源发生更改时失败。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在CDK部署流水线中集成单元测试，确保只有在所有单元测试通过的情况下才进行部署。需要选择两个正确的实施步骤。 **涉及的关键AWS服务和概念：** - AWS CDK：基础设施即代码工具 - AWS CodePipeline：持续集成/持续部署服务 - AWS CodeBuild：构建服务 - CDK单元测试：使用CDK assertions模块进行基础设施测试 - 流水线失败处理机制 **正确答案的原因：** 选项A正确：设置OnFailure属性为ABORT确保当测试失败时构建过程立即终止，防止有问题的代码被部署。这是实现&quot;测试失败则停止部署&quot;要求的关键机制。 选项D正确：使用CDK assertions模块创建实际的单元测试是必需的。template.hasResourceProperties断言可以验证CDK模板中的资源是否具有预期的配置属性，这是进行基础设施单元测试的标准做法。 **其他选项错误的原因：** 选项B错误：--rollback true标志用于部署失败后的回滚，但不能防止测试失败时的部署，无法满足核心要求。 选项C错误：--require-approval any-change标志只是要求手动批准更改，与自动化测试验证无关，不能解决测试失败控制问题。 选项E错误：cdk diff命令用于显示更改差异，将其配置为在有更改时失败会阻止所有部署，这与正常的CI/CD流程相矛盾。 **决策标准和最佳实践：** 1. 测试优先：先运行单元测试，只有通过后才进行部署 2. 失败快速响应：使用适当的失败处理机制（OnFailure: ABORT） 3. 使用标准测试框架：采用CDK官方提供的assertions模块 4. 自动化验证：通过代码而非手动流程来确保质量 5. 流水线集成：将测试无缝集成到现有的CodeBuild构建过程中</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">274</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances are in multiple Availability Zones. The application was misconfigured in a single Availability Zone, which caused a partial outage of the application. A DevOps engineer made changes to ensure that the unhealthy EC2 instances in one Availability Zone do not affect the healthy EC2 instances in the other Availability Zones. The DevOps engineer needs to test the application&#x27;s failover and shift where the ALB sends traffic. During failover, the ALB must avoid sending traffic to the Availability Zone where the failure has occurred. Which solution will meet these requirements? A (75%) C (25%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Turn off cross-zone load balancing on the ALB. Use Amazon Route 53 Application Recovery Controller to start a zonal shift away from the Availability Zone.
B. Turn off cross-zone load balancing on the ALB&#x27;s target group. Use Amazon Route 53 Application Recovery Controller to start a zonal shift away from the Availability Zone.
C. Create an Amazon Route 53 Application Recovery Controller resource set that uses the DNS hostname of the ALB. Start a zonal shift for the resource set away from the Availability Zone.
D. Create an Amazon Route 53 Application Recovery Controller resource set that uses the ARN of the ALB&#x27;s target group. Create a readiness check that uses the ElbV2TargetGroupsCanServeTraffic rule.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个应用程序运行在Application Load Balancer (ALB)后面的Amazon EC2实例上。这些EC2实例分布在多个Availability Zone中。应用程序在单个Availability Zone中配置错误，导致应用程序部分中断。DevOps工程师进行了更改，以确保一个Availability Zone中的不健康EC2实例不会影响其他Availability Zone中的健康EC2实例。DevOps工程师需要测试应用程序的故障转移并改变ALB发送流量的位置。在故障转移期间，ALB必须避免向发生故障的Availability Zone发送流量。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现在特定Availability Zone发生故障时，能够主动将ALB的流量从该区域转移走，避免向故障区域发送流量，并且需要能够测试这种故障转移机制。 **涉及的关键AWS服务和概念：** 1. Application Load Balancer (ALB) - 应用程序负载均衡器 2. Cross-zone load balancing - 跨区域负载均衡 3. Amazon Route 53 Application Recovery Controller - 应用程序恢复控制器 4. Zonal shift - 区域转移功能 5. Target group - 目标组 6. Availability Zone isolation - 可用区隔离 **正确答案A的原因：** - 关闭ALB的cross-zone load balancing确保流量只在各自的Availability Zone内分发，实现区域隔离 - 使用Route 53 Application Recovery Controller的zonal shift功能可以主动将流量从指定的Availability Zone转移走 - 这种组合提供了完整的故障转移控制和测试能力 - Zonal shift直接作用于ALB级别，能够有效控制整个负载均衡器的流量分发 **其他选项错误的原因：** - 选项B：Cross-zone load balancing是在ALB级别配置的，不是在target group级别，概念错误 - 选项C：使用DNS hostname创建resource set不能直接控制ALB的流量分发，无法实现区域级别的流量转移 - 选项D：创建readiness check主要用于健康检查和准备状态验证，不能实现主动的流量转移功能 **决策标准和最佳实践：** 1. 需要在ALB级别关闭跨区域负载均衡以实现区域隔离 2. 使用Route 53 Application Recovery Controller的zonal shift功能进行主动流量管理 3. 确保故障转移机制既能自动触发也能手动测试 4. 优先选择能够直接控制负载均衡器行为的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">275</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company sends its AWS Network Firewall flow logs to an Amazon S3 bucket. The company then analyzes the flow logs by using Amazon Athena. The company needs to transform the flow logs and add additional data before the flow logs are delivered to the existing S3 bucket. Which solution will meet these requirements? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS Lambda function to transform the data and to write a new object to the existing S3 bucket. Configure the Lambda function with an S3 trigger for the existing S3 bucket. Specify all object create events for the event type. Acknowledge the recursive invocation.
B. Enable Amazon EventBridge notifications on the existing S3 bucket. Create a custom EventBridge event bus. Create an EventBridge rule that is associated with the custom event bus. Configure the rule to react to all object create events for the existing S3 bucket and to invoke an AWS Step Functions workflow. Configure a Step Functions task to transform the data and to write the data into a new S3 bucket.
C. Create an Amazon EventBridge rule that is associated with the default EventBridge event bus. Configure the rule to react to all object create events for the existing S3 bucket. Define a new S3 bucket as the target for the rule. Create an EventBridge input transformation to customize the event before passing the event to the rule target.
D. Create an Amazon Kinesis Data Firehose delivery stream that is configured with an AWS Lambda transformer. Specify the existing S3 bucket as the destination. Change the Network Firewall logging destination from Amazon S3 to Kinesis Data Firehose.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司将其AWS Network Firewall流日志发送到Amazon S3存储桶。然后该公司使用Amazon Athena分析流日志。该公司需要在流日志传送到现有S3存储桶之前转换流日志并添加额外数据。哪种解决方案能满足这些要求？ 选项： A. 创建AWS Lambda函数来转换数据并向现有S3存储桶写入新对象。为现有S3存储桶配置Lambda函数的S3触发器。为事件类型指定所有对象创建事件。确认递归调用。 B. 在现有S3存储桶上启用Amazon EventBridge通知。创建自定义EventBridge事件总线。创建与自定义事件总线关联的EventBridge规则。配置规则以响应现有S3存储桶的所有对象创建事件并调用AWS Step Functions工作流。配置Step Functions任务来转换数据并将数据写入新的S3存储桶。 C. 创建与默认EventBridge事件总线关联的Amazon EventBridge规则。配置规则以响应现有S3存储桶的所有对象创建事件。定义新的S3存储桶作为规则的目标。创建EventBridge输入转换以在将事件传递给规则目标之前自定义事件。 D. 创建配置了AWS Lambda转换器的Amazon Kinesis Data Firehose传输流。指定现有S3存储桶为目标。将Network Firewall日志记录目标从Amazon S3更改为Kinesis Data Firehose。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 题目要求在AWS Network Firewall流日志传送到现有S3存储桶之前对其进行转换和添加额外数据。关键在于需要在数据到达S3之前进行预处理，而不是事后处理。 **涉及的关键AWS服务和概念：** - AWS Network Firewall：网络防火墙服务，可生成流日志 - Amazon S3：对象存储服务 - Amazon Kinesis Data Firehose：实时数据传输服务 - AWS Lambda：无服务器计算服务 - Amazon EventBridge：事件路由服务 - Amazon Athena：无服务器查询服务 **正确答案D的原因：** 选项D是正确答案，因为： 1. **数据流重新设计**：通过将Network Firewall的日志目标从S3改为Kinesis Data Firehose，实现了在数据到达S3之前的拦截和处理 2. **内置转换功能**：Kinesis Data Firehose支持配置Lambda转换器，可以在数据传输过程中进行实时转换和添加额外数据 3. **无缝集成**：转换后的数据仍然可以传送到原有的S3存储桶，保持现有的Athena分析流程不变 4. **实时处理**：避免了数据落地后再处理的延迟和复杂性 **其他选项错误的原因：** - **选项A**：存在严重的递归调用风险。Lambda函数被S3事件触发，但又要写回同一个S3存储桶，会造成无限循环调用 - **选项B**：过度复杂化，使用了自定义事件总线和Step Functions，且需要新的S3存储桶，破坏了现有架构 - **选项C**：EventBridge的输入转换功能有限，无法进行复杂的数据转换和添加额外数据，且仍然是事后处理模式 **决策标准和最佳实践：** 1. **数据处理时机**：优先选择在数据流中进行预处理，而非事后处理 2. **架构简洁性**：选择最少组件、最直接的解决方案 3. **避免递归风险**：在设计触发器时要避免循环调用 4. **保持现有集成**：尽量保持现有的分析工具和流程不变 5. **实时性要求**：对于日志处理场景，实时转换比批处理更合适</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">276</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer needs to implement integration tests into an existing AWS CodePipeline CI/CD workflow for an Amazon Elastic Container Service (Amazon ECS) service. The CI/CD workflow retrieves new application code from an AWS CodeCommit repository and builds a container image. The CI/CD workflow then uploads the container image to Amazon Elastic Container Registry (Amazon ECR) with a new image tag version. The integration tests must ensure that new versions of the service endpoint are reachable and that various API methods return successful response data. The DevOps engineer has already created an ECS cluster to test the service. Which combination of steps will meet these requirements with the LEAST management overhead? (Choose three.) F. Write a script that runs integration tests against the service. Upload the script to an Amazon S3 bucket. Integrate the script in the S3 bucket with CodePipeline by using an S3 action stage. ADE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a deploy stage to the pipeline. Configure Amazon ECS as the action provider.
B. Add a deploy stage to the pipeline. Configure AWS CodeDeploy as the action provider.
C. Add an appspec.yml file to the CodeCommit repository.
D. Update the image build pipeline stage to output an imagedefinitions.json file that references the new image tag.
E. Create an AWS Lambda function that runs connectivity checks and API calls against the service. Integrate the Lambda function with CodePipeline by using a Lambda action stage.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师需要将集成测试实施到现有的AWS CodePipeline CI/CD工作流中，该工作流用于Amazon Elastic Container Service (Amazon ECS)服务。该CI/CD工作流从AWS CodeCommit存储库检索新的应用程序代码并构建容器镜像。然后CI/CD工作流将容器镜像上传到Amazon Elastic Container Registry (Amazon ECR)并使用新的镜像标签版本。集成测试必须确保服务端点的新版本可达，并且各种API方法返回成功的响应数据。DevOps工程师已经创建了一个ECS集群来测试该服务。哪种步骤组合能够以最少的管理开销满足这些要求？（选择三个。） 选项： A. 向pipeline添加部署阶段。配置Amazon ECS作为操作提供者。 B. 向pipeline添加部署阶段。配置AWS CodeDeploy作为操作提供者。 C. 向CodeCommit存储库添加appspec.yml文件。 D. 更新镜像构建pipeline阶段以输出引用新镜像标签的imagedefinitions.json文件。 E. 创建一个AWS Lambda函数来运行连接检查和对服务的API调用。通过使用Lambda操作阶段将Lambda函数与CodePipeline集成。 F. 编写一个对服务运行集成测试的脚本。将脚本上传到Amazon S3存储桶。通过使用S3操作阶段将S3存储桶中的脚本与CodePipeline集成。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在现有的CodePipeline中添加集成测试功能，需要部署容器到ECS并进行API端点测试，同时要求最少的管理开销。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务 - Amazon ECS：容器编排服务 - Amazon ECR：容器镜像注册表 - AWS CodeDeploy：应用部署服务 - AWS Lambda：无服务器计算服务 - imagedefinitions.json：ECS部署所需的镜像定义文件 **正确答案的原因：** 正确答案应该是A、D、E的组合： - **选项A**：添加ECS部署阶段是必需的，因为需要将新构建的容器镜像部署到已存在的ECS集群进行测试 - **选项D**：imagedefinitions.json文件是ECS部署的标准要求，用于指定要部署的镜像标签 - **选项E**：Lambda函数提供了最少管理开销的测试执行方式，可以自动进行连接性检查和API测试 **其他选项错误的原因：** - **选项B**：CodeDeploy主要用于EC2和本地服务器部署，对于ECS容器部署来说增加了不必要的复杂性 - **选项C**：appspec.yml是CodeDeploy的配置文件，但这里使用ECS原生部署更简单 - **选项F**：S3脚本方式需要额外的执行环境配置，管理开销比Lambda更大 **决策标准和最佳实践：** 1. **最少管理开销原则**：选择托管服务（ECS原生部署、Lambda）而非需要额外配置的方案 2. **服务集成性**：ECS与CodePipeline有原生集成支持 3. **测试自动化**：Lambda可以无缝集成到pipeline中执行自动化测试 4. **标准化配置**：imagedefinitions.json是ECS部署的标准做法</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">277</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs applications on Windows and Linux Amazon EC2 instances. The instances run across multiple Availability Zones in an AWS Region. The company uses Auto Scaling groups for each application. The company needs a durable storage solution for the instances. The solution must use SMB for Windows and must use NFS for Linux. The solution must also have sub-millisecond latencies. All instances will read and write the data. Which combination of steps will meet these requirements? (Choose three.) F. Update the EC2 instances for each application to mount the file system when new instances are launched. BDE (67%) BDF (33%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon Elastic File System (Amazon EFS) file system that has targets in multiple Availability Zones.
B. Create an Amazon FSx for NetApp ONTAP Multi-AZ file system.
C. Create a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume to use for shared storage.
D. Update the user data for each application&#x27;s launch template to mount the file system.
E. Perform an instance refresh on each Auto Scaling group.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Windows和Linux Amazon EC2实例上运行应用程序。这些实例运行在AWS区域的多个Availability Zone中。公司为每个应用程序使用Auto Scaling groups。公司需要为实例提供持久的存储解决方案。该解决方案必须为Windows使用SMB协议，为Linux使用NFS协议。该解决方案还必须具有亚毫秒级延迟。所有实例都将读写数据。哪种步骤组合将满足这些要求？（选择三个。）F. 更新每个应用程序的EC2实例，以便在启动新实例时挂载文件系统。BDE (67%) BDF (33%) 选项：A. 创建一个在多个Availability Zone中有目标的Amazon Elastic File System (Amazon EFS)文件系统。B. 创建一个Amazon FSx for NetApp ONTAP Multi-AZ文件系统。C. 创建一个General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS)卷用于共享存储。D. 更新每个应用程序的launch template的用户数据以挂载文件系统。E. 在每个Auto Scaling group上执行实例刷新。 正确答案：A</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为跨多个AZ的Windows和Linux EC2实例提供存储解决方案，需要满足：1）Windows支持SMB协议；2）Linux支持NFS协议；3）亚毫秒级延迟；4）多实例读写访问；5）与Auto Scaling groups兼容。 **涉及的关键AWS服务和概念：** - Amazon EFS：完全托管的NFS文件系统，支持多AZ访问 - Amazon FSx for NetApp ONTAP：支持多协议（SMB/NFS）的高性能文件系统 - Amazon EBS：块存储服务，但不支持多实例共享访问 - Auto Scaling groups和launch template：自动扩展配置 - 文件系统挂载和用户数据脚本 **正确答案的原因：** 题目显示正确答案是A，但这存在问题。Amazon EFS只支持NFS协议，无法满足Windows的SMB需求。实际上，正确的组合应该是BDF：B选项的FSx for NetApp ONTAP同时支持SMB和NFS协议，能提供亚毫秒级延迟；D选项通过launch template确保新实例自动挂载；F选项确保现有实例也能挂载文件系统。 **其他选项错误的原因：** - 选项A：EFS只支持NFS，不支持Windows的SMB需求 - 选项C：EBS卷无法在多个实例间共享，不适合Auto Scaling环境 - 选项E：实例刷新不是挂载文件系统的必要步骤 **决策标准和最佳实践：** 选择文件存储服务时应考虑：1）协议兼容性（SMB/NFS）；2）性能要求（延迟）；3）可用性（多AZ）；4）与Auto Scaling的集成；5）多实例并发访问能力。FSx for NetApp ONTAP是唯一能同时满足所有要求的服务。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">278</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an organization in AWS Organizations that a security team and a DevOps team manage. Both teams access the accounts by using AWS IAM Identity Center. A dedicated group has been created for each team. The DevOps team&#x27;s group has been assigned a permission set named DevOps. The permission set has the AdministratorAccess managed IAM policy attached. The permission set has been applied to all accounts in the organization. The security team wants to ensure that the DevOps team does not have access to IAM Identity Center in the organization&#x27;s management account. The security team has attached the following SCP to the organization root: After implementing the policy, the security team discovers that the DevOps team can still access IAM Identity Center. Which solution will fix the problem? B (40%) D (40%) A (20%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the organization&#x27;s management account, create a new OU. Move the organization&#x27;s management account to the new OU. Detach the SCP from the organization root. Attach the SCP to the new OU.
B. In the organization&#x27;s management account, update the SCP condition reference to the ARN of the DevOps team&#x27;s group role to include the AWS account ID of the organization&#x27;s management account.
C. In IAM Identity Center, create a new permission set. Ensure that the assigned policy has full access but explicitly denies permission for the sso:* action and the sso-directory:* action. Update the assigned permission set for the DevOps team&#x27;s group role in the organization&#x27;s management account. Delete the SCP.
D. In IAM Identity Center, update the DevOps permission set. Ensure that the assigned policy has full access but explicitly denies permission for the sso:* action and the sso-directory:* action. In the Deny statement, add a StringEquals condition that compares the aws:SourceAccount global condition context key with the organization&#x27;s management account ID. Delete the SCP.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中使用组织，由安全团队和DevOps团队管理。两个团队都通过AWS IAM Identity Center访问账户。为每个团队创建了专用组。DevOps团队的组被分配了名为DevOps的权限集。该权限集附加了AdministratorAccess托管IAM策略。该权限集已应用于组织中的所有账户。安全团队希望确保DevOps团队无法访问组织管理账户中的IAM Identity Center。安全团队已将以下SCP附加到组织根部：实施策略后，安全团队发现DevOps团队仍然可以访问IAM Identity Center。哪种解决方案可以解决这个问题？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要阻止DevOps团队访问组织管理账户中的IAM Identity Center，但当前的SCP策略没有生效。 **涉及的关键AWS服务和概念：** - AWS Organizations和Service Control Policies (SCP) - AWS IAM Identity Center (原AWS SSO) - 组织单位(OU)和管理账户 - SCP的作用范围和限制 **正确答案A的原因：** 1. **SCP对管理账户的限制**：SCP策略不会影响管理账户中的根用户和某些管理服务的访问 2. **创建新OU的必要性**：将管理账户移动到新的OU中，可以让SCP策略对管理账户生效 3. **策略重新附加**：从组织根部分离SCP并附加到新OU，确保策略正确应用到管理账户 **其他选项错误的原因：** - **选项B**：仅更新SCP条件不能解决根本问题，因为SCP在管理账户层面的限制仍然存在 - **选项C**：创建新权限集过于复杂，且删除SCP会影响其他账户的安全控制 - **选项D**：虽然在权限集层面添加拒绝策略，但这种方法不如SCP控制更加可靠和集中化 **决策标准和最佳实践：** 1. **理解SCP作用域**：SCP对管理账户的某些服务有限制 2. **组织结构设计**：合理使用OU结构来实现精细化的权限控制 3. **安全控制层次**：优先使用SCP进行组织级别的安全控制，而不是依赖权限集级别的控制 4. **最小权限原则**：确保团队只能访问必要的服务和资源</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">279</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An Amazon EC2 Auto Scaling group manages EC2 instances that were created from an AMI. The AMI has the AWS Systems Manager Agent installed. When an EC2 instance is launched into the Auto Scaling group, tags are applied to the EC2 instance. EC2 instances that are launched by the Auto Scaling group must have the correct operating system configuration. Which solution will meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a Systems Manager Run Command document that configures the desired instance configuration. Set up Systems Manager Compliance to invoke the Run Command document when the EC2 instances are not in compliance with the most recent patches.
B. Create a Systems Manager State Manager association that links to the Systems Manager command document. Create a tag query that runs immediately.
C. Create a Systems Manager Run Command task that specifies the desired instance configuration. Create a maintenance window in Systems Manager Maintenance Windows that runs daily. Register the Run Command task against the maintenance window. Designate the targets.
D. Create a Systems Manager Patch Manager patch baseline and a patch group that use the same tags that the Auto Scaling group applies. Register the patch group with the patch baseline. Define a Systems Manager command document to patch the instances. Invoke the document by using Systems Manager Run Command.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个Amazon EC2 Auto Scaling组管理着从AMI创建的EC2实例。该AMI已安装AWS Systems Manager Agent。当EC2实例启动到Auto Scaling组中时，标签会应用到EC2实例上。由Auto Scaling组启动的EC2实例必须具有正确的操作系统配置。哪个解决方案能满足这些要求？ 选项： A. 创建一个Systems Manager Run Command文档来配置所需的实例配置。设置Systems Manager Compliance在EC2实例不符合最新补丁合规性时调用Run Command文档。 B. 创建一个链接到Systems Manager命令文档的Systems Manager State Manager关联。创建一个立即运行的标签查询。 C. 创建一个指定所需实例配置的Systems Manager Run Command任务。在Systems Manager Maintenance Windows中创建一个每日运行的维护窗口。将Run Command任务注册到维护窗口。指定目标。 D. 创建一个Systems Manager Patch Manager补丁基线和使用Auto Scaling组应用的相同标签的补丁组。将补丁组注册到补丁基线。定义一个Systems Manager命令文档来修补实例。使用Systems Manager Run Command调用文档。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为Auto Scaling组中新启动的EC2实例自动应用正确的操作系统配置。关键需求是：1）实例由Auto Scaling组管理；2）实例启动时会自动打标签；3）需要确保实例有正确的OS配置；4）已安装SSM Agent。 **涉及的关键AWS服务和概念：** - EC2 Auto Scaling：自动扩缩容服务 - Systems Manager State Manager：持续配置管理服务 - Systems Manager Run Command：远程执行命令服务 - Systems Manager Compliance：合规性监控 - Systems Manager Patch Manager：补丁管理 - 标签（Tags）：资源标识和分组 **正确答案B的原因：** State Manager是专门用于持续配置管理的服务，能够：1）通过关联（Association）自动监控实例状态；2）使用标签查询自动发现新启动的实例；3）立即执行配置任务确保实例符合要求；4）持续监控并在配置漂移时自动修复。这完美匹配Auto Scaling场景中新实例需要立即配置的需求。 **其他选项错误的原因：** - 选项A：Compliance主要用于合规性检查和报告，不是主动配置工具，且主要关注补丁而非整体OS配置 - 选项C：Maintenance Windows适用于计划性维护，每日运行无法满足Auto Scaling中实例随时启动需要立即配置的需求 - 选项D：Patch Manager专门用于补丁管理，题目要求的是&quot;操作系统配置&quot;而非仅仅补丁管理，范围过窄 **决策标准和最佳实践：** 选择配置管理工具时应考虑：1）自动化程度（State Manager可自动发现新实例）；2）响应速度（立即执行vs定时执行）；3）功能范围（全面配置vs特定功能）；4）与Auto Scaling的集成度。State Manager通过标签查询和关联机制，能够无缝支持动态扩缩容环境中的配置管理需求。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">280</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Organizations to manage its AWS accounts. The organization root has a child OU that is named Department. The Department OU has a child OU that is named Engineering. The default FullAWSAccess policy is attached to the root, the Department OU, and the Engineering OU. The company has many AWS accounts in the Engineering OU. Each account has an administrative IAM role with the AdministratorAccess IAM policy attached. The default FullAWSAccess policy is also attached to each account. A DevOps engineer plans to remove the FullAWSAccess policy from the Department OU. The DevOps engineer will replace the policy with a policy that contains an Allow statement for all Amazon EC2 API operations. What will happen to the permissions of the administrative IAM roles as a result of this change? B (67%) A (33%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. All API actions on all resources will be allowed.
B. All API actions on EC2 resources will be allowed. All other API actions will be denied.
C. All API actions on all resources will be denied.
D. All API actions on EC2 resources will be denied. All other API actions will be allowed.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations来管理其AWS账户。组织根有一个名为Department的子OU。Department OU有一个名为Engineering的子OU。默认的FullAWSAccess策略附加到根、Department OU和Engineering OU上。公司在Engineering OU中有许多AWS账户。每个账户都有一个管理IAM角色，附加了AdministratorAccess IAM策略。默认的FullAWSAccess策略也附加到每个账户上。一名DevOps工程师计划从Department OU中移除FullAWSAccess策略。该工程师将用一个包含允许所有Amazon EC2 API操作的Allow语句的策略来替换该策略。这种变更对管理IAM角色的权限会产生什么影响？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS Organizations中Service Control Policies (SCPs)的继承机制和权限控制逻辑，特别是当修改上级OU的SCP策略时，对下级账户中IAM角色权限的影响。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. Organizational Units (OUs) - 组织单元的层级结构 3. Service Control Policies (SCPs) - 服务控制策略，用于设置权限边界 4. IAM角色和策略 - 身份和访问管理 5. 策略继承机制 - 子OU会继承父级OU的SCP策略 **正确答案B的原因：** 1. SCP策略的工作原理是设置权限边界，只有同时被SCP和IAM策略允许的操作才能执行 2. 当Department OU的FullAWSAccess被替换为仅允许EC2操作的策略后，Engineering OU会继承这个限制 3. 虽然Engineering OU本身仍有FullAWSAccess，但由于父级Department OU的限制，实际生效的权限是两者的交集 4. 账户中的AdministratorAccess IAM角色虽然允许所有操作，但受到SCP的限制，只能执行EC2相关操作 5. 所有非EC2的API操作都会被SCP拒绝 **其他选项错误的原因：** - 选项A错误：忽略了SCP策略的限制作用，SCP会覆盖IAM策略的权限 - 选项C错误：EC2操作仍然是被允许的，因为新的SCP策略明确允许所有EC2 API操作 - 选项D错误：逻辑完全相反，EC2操作是被允许的，其他操作被拒绝 **决策标准和最佳实践：** 1. 理解SCP是&quot;拒绝优先&quot;的权限边界，不是权限授予 2. SCP策略在OU层级中是继承的，子级会受到所有上级策略的约束 3. 实际权限 = IAM策略权限 ∩ SCP策略权限 4. 在设计Organizations架构时，应该从上到下规划SCP策略，避免意外的权限限制 5. 修改上级OU的SCP前，需要评估对所有下级账户和资源的影响</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">281</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages AWS accounts in AWS Organizations. The company needs a solution to send Amazon CloudWatch Logs data to an Amazon S3 bucket in a dedicated AWS account. The solution must support all existing and future CloudWatch Logs log groups. Which solution will meet these requirements? plan. Create resource assignments in the backup plan for all accounts that belong to the company. Create an AWS Systems Manager Automation runbook to assign log groups to a backup plan. Create an AWS Config rule that has an automatic remediation action for all noncompliant log groups. Specify the runbook as the rule&#x27;s target. D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable Organizations backup policies to back up all log groups to a dedicated S3 bucket. Add an S3 bucket policy that allows access from all accounts that belong to the company.
B. Create a backup plan in AWS Backup. Specify a dedicated S3 bucket as a backup vault. Assign all CloudWatch Logs log group resources to the backup plan. Create resource assignments in the backup plan for all accounts that belong to the company.
C. Create a backup plan in AWS Backup. Specify a dedicated S3 bucket as a backup vault. Assign all existing log groups to the backup
D. Create a CloudWatch Logs destination and an Amazon Kinesis Data Firehose delivery stream in the dedicated AWS account. Specify the S3 bucket as the destination of the delivery stream. Create subscription filters for all existing log groups in all accounts. Create an AWS Lambda function to call the CloudWatch Logs PutSubscriptionFilter API operation. Create an Amazon EventBridge rule to invoke the Lambda function when a CreateLogGroup event occurs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中管理AWS账户。该公司需要一个解决方案将Amazon CloudWatch Logs数据发送到专用AWS账户中的Amazon S3存储桶。该解决方案必须支持所有现有和未来的CloudWatch Logs日志组。哪个解决方案能满足这些要求？ 选项： A. 启用Organizations备份策略来备份所有日志组到专用S3存储桶。添加S3存储桶策略允许公司所有账户访问。 B. 在AWS Backup中创建备份计划。指定专用S3存储桶作为备份保管库。将所有CloudWatch Logs日志组资源分配给备份计划。为公司所有账户创建备份计划中的资源分配。 C. 在AWS Backup中创建备份计划。指定专用S3存储桶作为备份保管库。将所有现有日志组分配给备份计划。为公司所有账户创建备份计划中的资源分配。创建AWS Systems Manager Automation runbook来将日志组分配给备份计划。创建具有自动修复操作的AWS Config规则用于所有不合规的日志组。指定runbook作为规则的目标。 D. 在专用AWS账户中创建CloudWatch Logs目标和Amazon Kinesis Data Firehose传输流。指定S3存储桶作为传输流的目标。为所有账户中的所有现有日志组创建订阅过滤器。创建AWS Lambda函数来调用CloudWatch Logs PutSubscriptionFilter API操作。创建Amazon EventBridge规则在CreateLogGroup事件发生时调用Lambda函数。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要将多个AWS账户中的CloudWatch Logs数据集中发送到专用账户的S3存储桶，并且要自动支持未来新创建的日志组。 **涉及的关键AWS服务和概念：** - CloudWatch Logs订阅过滤器：用于实时流式传输日志数据 - Kinesis Data Firehose：用于将流数据传输到S3 - Lambda函数：用于自动化API调用 - EventBridge：用于监听CloudWatch事件 - AWS Organizations：多账户管理 - 跨账户访问权限 **正确答案D的原因：** 1. **实时数据传输**：使用CloudWatch Logs订阅过滤器配合Kinesis Data Firehose可以实现日志数据的实时流式传输到S3 2. **自动化处理新日志组**：通过EventBridge监听CreateLogGroup事件，自动触发Lambda函数为新日志组创建订阅过滤器 3. **跨账户支持**：CloudWatch Logs目标可以配置跨账户权限，允许其他账户的日志组向其发送数据 4. **完整的端到端解决方案**：涵盖了现有日志组的处理和未来日志组的自动配置 **其他选项错误的原因：** - **选项A**：Organizations备份策略主要用于数据保护和合规性，不是为实时日志传输设计的 - **选项B**：AWS Backup不支持CloudWatch Logs作为可备份资源类型，且备份是周期性的，不适合日志数据的实时传输需求 - **选项C**：同样基于AWS Backup的错误假设，虽然添加了自动化组件，但基础服务选择不当 **决策标准和最佳实践：** 1. **服务适用性**：选择专门为日志流式传输设计的服务（CloudWatch Logs + Kinesis Data Firehose） 2. **自动化要求**：使用事件驱动架构确保新资源自动纳入管理范围 3. **实时性需求**：日志数据通常需要实时或近实时传输，而非周期性备份 4. **跨账户架构**：在多账户环境中，需要正确配置跨账户权限和资源访问</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">282</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer manages a Java-based application that runs in an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Auto scaling has not been configured for the application. The DevOps engineer has determined that the Java Virtual Machine (JVM) thread count is a good indicator of when to scale the application. The application serves customer traffic on port 8080 and makes JVM metrics available on port 9404. Application use has recently increased. The DevOps engineer needs to configure auto scaling for the application. Which solution will meet these requirements with the LEAST operational overhead? (Choose two.) the JVM metrics from port 9404 to the Prometheus workspace. Configure rules for the workspace to use the JVM thread count metric to scale the application. Add a step scaling policy in Fargate. Select the Prometheus rules to scale up and scaling down.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Deploy the Amazon CloudWatch agent as a container sidecar. Configure the CloudWatch agent to retrieve JVM metrics from port 9404. Create CloudWatch alarms on the JVM thread count metric to scale the application. Add a step scaling policy in Fargate to scale up and scale down based on the CloudWatch alarms.
B. Deploy the Amazon CloudWatch agent as a container sidecar. Configure a metric filter for the JVM thread count metric on the CloudWatch log group for the CloudWatch agent. Add a target tracking policy in Fargate. Select the metric from the metric filter as a scale target.
C. Create an Amazon Managed Service for Prometheus workspace. Deploy AWS Distro for OpenTelemetry as a container sidecar to publish
D. Create an Amazon Managed Service for Prometheus workspace. Deploy AWS Distro for OpenTelemetry as a container sidecar to retrieve JVM metrics from port 9404 to publish the JVM metrics from port 9404 to the Prometheus workspace. Add a target tracking policy in Fargate. Select the Prometheus metric as a scale target.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师管理着一个基于Java的应用程序，该应用程序在AWS Fargate上的Amazon Elastic Container Service (Amazon ECS)集群中运行。该应用程序尚未配置自动扩展。DevOps工程师已确定Java Virtual Machine (JVM)线程数是何时扩展应用程序的良好指标。该应用程序在端口8080上为客户流量提供服务，并在端口9404上提供JVM指标。应用程序使用量最近有所增加。DevOps工程师需要为应用程序配置自动扩展。哪种解决方案能以最少的运营开销满足这些要求？（选择两个） 选项： A. 将Amazon CloudWatch agent作为容器sidecar部署。配置CloudWatch agent从端口9404检索JVM指标。在JVM线程数指标上创建CloudWatch alarms来扩展应用程序。在Fargate中添加step scaling policy，基于CloudWatch alarms进行扩展和缩减。 B. 将Amazon CloudWatch agent作为容器sidecar部署。在CloudWatch agent的CloudWatch log group上为JVM线程数指标配置metric filter。在Fargate中添加target tracking policy。选择metric filter中的指标作为扩展目标。 C. 创建Amazon Managed Service for Prometheus workspace。部署AWS Distro for OpenTelemetry作为容器sidecar来发布 D. 创建Amazon Managed Service for Prometheus workspace。部署AWS Distro for OpenTelemetry作为容器sidecar从端口9404检索JVM指标并发布到Prometheus workspace。在Fargate中添加target tracking policy。选择Prometheus指标作为扩展目标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为运行在AWS Fargate上的ECS应用程序配置自动扩展，使用JVM线程数作为扩展指标，并且要求最少的运营开销。 **涉及的关键AWS服务和概念：** - Amazon ECS on AWS Fargate：容器编排服务 - Auto Scaling：自动扩展功能 - Amazon CloudWatch：监控和指标服务 - Amazon Managed Service for Prometheus：托管的Prometheus服务 - AWS Distro for OpenTelemetry：可观测性数据收集工具 - Sidecar容器模式：在主容器旁边运行辅助容器 - Step scaling vs Target tracking：两种不同的扩展策略 **正确答案A的原因：** 1. **技术可行性**：CloudWatch agent可以直接从端口9404收集JVM指标，这是标准做法 2. **运营开销最小**：CloudWatch是AWS原生服务，与ECS/Fargate深度集成，配置简单 3. **扩展策略合理**：Step scaling policy配合CloudWatch alarms是成熟的扩展方案 4. **成本效益**：CloudWatch agent轻量级，不需要额外的托管服务费用 **其他选项错误的原因：** - **选项B**：错误地使用了metric filter处理指标数据。Metric filter主要用于从日志中提取指标，而不是直接处理从端口收集的指标数据 - **选项C**：描述不完整，缺少关键的配置信息 - **选项D**：虽然技术上可行，但引入了Amazon Managed Service for Prometheus这个额外的托管服务，增加了复杂性和成本，不符合&quot;最少运营开销&quot;的要求 **决策标准和最佳实践：** 1. **简单性原则**：优先选择AWS原生集成度高的解决方案 2. **运营开销考虑**：避免引入不必要的额外服务和复杂性 3. **成本效益**：CloudWatch相比Prometheus workspace在小规模应用中更经济 4. **技术匹配度**：直接指标收集比通过日志处理更直接有效</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">283</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that runs in a single AWS Region. The application runs on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster and connects to an Amazon Aurora MySQL cluster. The application is built in an AWS CodeBuild project. The container images are published to Amazon Elastic Container Registry (Amazon ECR). The company needs to replicate the state of the application for the container images and the database to a second Region. Which solution will meet these requirements in the MOST operationally efficient way? Configure a cross-Region Aurora Replica in the second Region. Configure the new application deployment to use the endpoints for the cross-Region Aurora Replica. cluster in the second Region as the target for binary log replication from the Aurora MySQL cluster in the initial Region. Configure the new application deployment to use the endpoints for the second Region&#x27;s cluster.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Turn on Amazon S3 Cross-Region Replication (CRR) on the bucket that holds the ECR container images. Deploy the application to an EKS cluster in the second Region by referencing the new S3 bucket object URL for the container image in a Kubernetes deployment file.
B. Create an Amazon EventBridge rule that reacts to image pushes to the ECR repository. Configure the EventBridge rule to invoke an AWS Lambda function to replicate the image to a new ECR repository in the second Region. Deploy the application to an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment file. Configure a cross-Region Aurora Replica in the second Region. Configure the new application deployment to use the endpoints for the cross-Region Aurora Replica.
C. Turn on Cross-Region Replication to replicate the ECR repository to the second Region. Deploy the application to an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment file. Configure an Aurora global database with clusters in the initial Region and the second Region. Configure the new application deployment to use the endpoints for the second Region&#x27;s cluster in the Aurora global database.
D. Configure the CodeBuild project to also push the container image to an ECR repository in the second Region. Deploy the application to an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment file. Configure an Aurora MySQL</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个在单个AWS区域运行的应用程序。该应用程序运行在Amazon Elastic Kubernetes Service (Amazon EKS)集群上，并连接到Amazon Aurora MySQL集群。该应用程序在AWS CodeBuild项目中构建。容器镜像发布到Amazon Elastic Container Registry (Amazon ECR)。公司需要将应用程序的容器镜像和数据库状态复制到第二个区域。哪种解决方案能以最具运营效率的方式满足这些要求？ 选项： A. 在保存ECR容器镜像的存储桶上启用Amazon S3 Cross-Region Replication (CRR)。通过在Kubernetes部署文件中引用新S3存储桶对象URL作为容器镜像，将应用程序部署到第二个区域的EKS集群。 B. 创建一个Amazon EventBridge规则来响应向ECR存储库推送镜像的事件。配置EventBridge规则调用AWS Lambda函数将镜像复制到第二个区域的新ECR存储库。通过在Kubernetes部署文件中引用新ECR存储库，将应用程序部署到第二个区域的EKS集群。在第二个区域配置跨区域Aurora副本。配置新应用程序部署使用跨区域Aurora副本的端点。 C. 启用Cross-Region Replication将ECR存储库复制到第二个区域。通过在Kubernetes部署文件中引用新ECR存储库，将应用程序部署到第二个区域的EKS集群。配置Aurora全球数据库，在初始区域和第二个区域都有集群。配置新应用程序部署使用Aurora全球数据库中第二个区域集群的端点。 D. 配置CodeBuild项目同时将容器镜像推送到第二个区域的ECR存储库。通过在Kubernetes部署文件中引用新ECR存储库，将应用程序部署到第二个区域的EKS集群。配置Aurora MySQL...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求以最具运营效率的方式将应用程序状态（包括容器镜像和数据库）复制到第二个区域，实现跨区域的应用程序部署。 **涉及的关键AWS服务和概念：** 1. Amazon EKS - Kubernetes容器编排服务 2. Amazon ECR - 容器镜像注册表 3. Amazon Aurora MySQL - 托管关系数据库 4. Aurora Global Database - 跨区域数据库复制解决方案 5. Cross-Region Replication - 跨区域复制功能 6. AWS CodeBuild - 持续集成服务 **正确答案C的原因：** 1. **ECR跨区域复制**：ECR原生支持Cross-Region Replication功能，这是AWS提供的托管服务，无需额外配置复杂的自动化流程 2. **Aurora Global Database**：这是Aurora的原生全球数据库解决方案，提供跨区域的自动复制，延迟极低（通常&lt;1秒），并且支持快速故障转移 3. **运营效率最高**：两个组件都使用AWS原生的托管复制功能，减少了运维复杂度和管理开销 4. **架构简洁**：直接使用AWS服务的内置功能，避免了自定义解决方案的复杂性 **其他选项错误的原因：** - **选项A**：ECR不是基于S3存储的，不能使用S3 CRR来复制ECR镜像，这是对AWS服务架构的误解 - **选项B**：虽然技术上可行，但使用EventBridge + Lambda的自定义解决方案比ECR原生复制功能更复杂，运营效率较低；Aurora跨区域副本的性能和功能也不如Global Database - **选项D**：选项不完整，无法评估完整解决方案 **决策标准和最佳实践：** 1. **优先使用托管服务**：AWS原生的复制功能通常比自建解决方案更可靠、更高效 2. **最小化运营复杂度**：选择需要最少自定义配置和维护的解决方案 3. **性能考虑**：Aurora Global Database提供比普通跨区域副本更好的性能和一致性 4. **成本效益**：托管服务通常在长期运营中更具成本效益</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">284</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is building a serverless application that uses AWS Lambda functions to process data. A BeginResponse Lambda function initializes data in response to specific application events. The company needs to ensure that a large number of Lambda functions are invoked after the BeginResponse Lambda function runs. Each Lambda function must be invoked in parallel and depends on only the outputs of the BeginResponse Lambda function. Each Lambda function has retry logic for invocation and must be able to fine-tune concurrency without losing data. Which solution will meet these requirements with the MOST operational efficiency? SNS) topic. Subscribe each SQS queue to the SNS topic. Modify the BeginResponse function to publish to the SNS topic when it finishes running.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon Simple Notification Service (Amazon SNS) topic. Modify the BeginResponse Lambda function to publish to the SNS topic before the BeginResponse Lambda function finishes running. Subscribe all Lambda functions that need to invoke after the BeginResponse Lambda function runs to the SNS topic. Subscribe any new Lambda functions to the SNS topic.
B. Create an Amazon Simple Queue Service (Amazon SQS) queue for each Lambda function that needs to run after the BeginResponse Lambda function runs. Subscribe each Lambda function to its own SQS queue. Create an Amazon Simple Notification Service (Amazon
C. Create an Amazon Simple Queue Service (Amazon SQS) queue for each Lambda function that needs to run after the BeginResponse Lambda function runs. Subscribe the Lambda function to the SQS queue. Create an Amazon Simple Notification Service (Amazon SNS) topic for each SQS queue. Subscribe the SQS queues to the SNS topics. Modify the BeginResponse function to publish to the SNS topics when the function finishes running.
D. Create an AWS Step Functions Standard Workflow. Configure states in the workflow to invoke the Lambda functions sequentially. Create an Amazon Simple Notification Service (Amazon SNS) topic. Modify the BeginResponse Lambda function to publish to the SNS topic before the Lambda function finishes running. Create a new Lambda function that is subscribed to the SNS topic and that invokes the Step Functions workflow.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在构建一个使用AWS Lambda函数处理数据的无服务器应用程序。BeginResponse Lambda函数响应特定应用程序事件来初始化数据。公司需要确保在BeginResponse Lambda函数运行后调用大量Lambda函数。每个Lambda函数必须并行调用，并且只依赖于BeginResponse Lambda函数的输出。每个Lambda函数都有调用重试逻辑，并且必须能够在不丢失数据的情况下微调并发性。哪种解决方案能以最高的运营效率满足这些要求？ 选项A：创建一个Amazon Simple Notification Service (Amazon SNS) topic。修改BeginResponse Lambda函数在完成运行之前发布到SNS topic。将所有需要在BeginResponse Lambda函数运行后调用的Lambda函数订阅到SNS topic。将任何新的Lambda函数订阅到SNS topic。 选项B：为每个需要在BeginResponse Lambda函数运行后运行的Lambda函数创建一个Amazon Simple Queue Service (Amazon SQS) queue。将每个Lambda函数订阅到其自己的SQS queue。创建一个Amazon Simple Notification Service (Amazon SNS) topic。将每个SQS queue订阅到SNS topic。修改BeginResponse函数在完成运行时发布到SNS topic。 选项C：为每个需要在BeginResponse Lambda函数运行后运行的Lambda函数创建一个Amazon Simple Queue Service (Amazon SQS) queue。将Lambda函数订阅到SQS queue。为每个SQS queue创建一个Amazon Simple Notification Service (Amazon SNS) topic。将SQS queue订阅到SNS topics。修改BeginResponse函数在完成运行时发布到SNS topics。 选项D：创建一个AWS Step Functions Standard Workflow。在工作流中配置状态以顺序调用Lambda函数。创建一个Amazon Simple Notification Service (Amazon SNS) topic。修改BeginResponse Lambda函数在完成运行之前发布到SNS topic。创建一个新的Lambda函数，订阅到SNS topic并调用Step Functions工作流。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个架构来实现：1）大量Lambda函数在BeginResponse函数完成后并行执行；2）每个函数都有重试逻辑；3）能够微调并发性而不丢失数据；4）具有最高的运营效率。 **涉及的关键AWS服务和概念：** - Amazon SNS：发布/订阅消息服务，支持扇出模式 - Amazon SQS：消息队列服务，提供可靠的消息传递和重试机制 - AWS Lambda：无服务器计算服务 - AWS Step Functions：工作流编排服务 - 并发控制和消息持久性概念 **正确答案B的原因：** 选项B采用SNS+SQS扇出模式，这是AWS推荐的最佳实践： 1. **并行执行**：SNS将消息同时发送到所有订阅的SQS队列，实现真正的并行处理 2. **重试机制**：SQS提供内置的重试和死信队列功能 3. **并发控制**：每个Lambda函数通过其专用SQS队列可以独立配置并发设置 4. **数据可靠性**：SQS确保消息持久性，不会丢失数据 5. **运营效率**：架构简单，易于管理和扩展 **其他选项错误的原因：** - **选项A**：直接SNS到Lambda缺乏队列缓冲，无法提供可靠的重试机制和并发控制，可能导致数据丢失 - **选项C**：为每个SQS队列创建单独的SNS topic是不必要的复杂设计，违反了运营效率原则 - **选项D**：Step Functions配置为顺序执行违背了并行处理的要求，且架构过于复杂 **决策标准和最佳实践：** 1. **扇出模式**：使用SNS+SQS组合实现一对多的可靠消息传递 2. **解耦设计**：通过队列解耦生产者和消费者，提高系统弹性 3. **错误处理**：利用SQS的重试和死信队列机制处理失败场景 4. **可扩展性**：架构支持轻松添加新的Lambda函数而无需修改现有组件</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">285</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company operates a globally deployed product out of multiple AWS Regions. The company&#x27;s DevOps team needs to use Amazon API Gateway to deploy an API to support the product. The API must be deployed redundantly. The deployment must provide independent availability from each company location. The deployment also must respond to a custom domain URL and must optimize performance for the API user requests. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Deploy an API Gateway edge-optimized API endpoint in the us-east-1 Region. Create an API Gateway custom domain for the API. Create an Amazon Route 53 record set with a geoproximity routing policy for the API&#x27;s custom domain. Increase the geographic bias to the maximum allowed value.
B. Deploy an API Gateway regional API endpoint in the us-east-1 Region. Integrate the API Gateway API with a public Application Load Balancer (ALB). Create an AWS Global Accelerator standard accelerator. Associate the endpoint with the ALB. Create an Amazon Route 53 alias record set that points the custom domain name to the DNS name that is assigned to the accelerator.
C. Deploy an API Gateway regional API endpoint in every AWS Region where the company&#x27;s product is deployed. Create an API Gateway custom domain in each Region for the deployed API Gateway API. Create an Amazon Route 53 record set that has a latency routing policy for every deployed API Gateway custom domain.
D. Deploy an API Gateway edge-optimized API endpoint in the us-east-1 Region. Create an Amazon CloudFront distribution. Configure the CloudFront distribution with an alternate domain name. Specify the API Gateway Invoke URL as the origin domain. Create an Amazon Route 53 alias record set with a simple routing policy. Point the routing policy to the CloudFront distribution domain name.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在多个AWS Region中运营全球部署的产品。该公司的DevOps团队需要使用Amazon API Gateway来部署一个API以支持该产品。API必须冗余部署。部署必须为每个公司位置提供独立的可用性。部署还必须响应自定义域URL，并且必须为API用户请求优化性能。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 全球多Region冗余部署API Gateway 2. 每个位置独立可用性 3. 支持自定义域名 4. 优化API请求性能 **涉及的关键AWS服务和概念：** - API Gateway (regional vs edge-optimized endpoints) - Route 53路由策略（geoproximity、latency、simple routing） - CloudFront全球内容分发 - Global Accelerator全球加速器 - Application Load Balancer **正确答案C的原因：** 1. **多Region部署**：在每个产品部署的Region都部署regional API endpoint，确保真正的冗余和独立可用性 2. **独立可用性**：每个Region的API Gateway独立运行，单个Region故障不影响其他Region 3. **性能优化**：Route 53的latency routing policy会自动将用户请求路由到延迟最低的Region 4. **自定义域名**：每个Region创建API Gateway custom domain满足域名要求 **其他选项错误的原因：** - **选项A**：只在us-east-1单个Region部署，不满足冗余和独立可用性要求；geoproximity routing不如latency routing适合性能优化 - **选项B**：同样只在单个Region部署，虽然有Global Accelerator但缺乏真正的冗余性；ALB集成增加了不必要的复杂性 - **选项D**：单Region部署问题同上；edge-optimized endpoint已经包含CloudFront功能，再加CloudFront是重复配置 **决策标准和最佳实践：** 1. 真正的高可用需要多Region部署而非单点依赖 2. Regional endpoints比edge-optimized更适合多Region自定义部署场景 3. Route 53 latency routing是性能优化的最佳选择 4. 避免过度工程化，选择最直接满足需求的方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">286</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer uses AWS CodeBuild to frequently produce software packages. The CodeBuild project builds large Docker images that the DevOps engineer can use across multiple builds. The DevOps engineer wants to improve build performance and minimize costs. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Store the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Implement a local Docker layer cache for CodeBuild.
B. Cache the Docker images in an Amazon S3 bucket that is available across multiple build hosts. Expire the cache by using an S3 Lifecycle policy.
C. Store the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Modify the CodeBuild project runtime configuration to always use the most recent image version.
D. Create custom AMIs that contain the cached Docker images. In the CodeBuild build, launch Amazon EC2 instances from the custom AMIs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师使用AWS CodeBuild频繁生成软件包。CodeBuild项目构建大型Docker镜像，DevOps工程师可以在多个构建中使用这些镜像。DevOps工程师希望提高构建性能并最小化成本。哪个解决方案能满足这些要求？ 选项：A. 将Docker镜像存储在Amazon Elastic Container Registry (Amazon ECR)仓库中。为CodeBuild实施本地Docker层缓存。 B. 将Docker镜像缓存在可跨多个构建主机使用的Amazon S3存储桶中。使用S3 Lifecycle策略使缓存过期。 C. 将Docker镜像存储在Amazon Elastic Container Registry (Amazon ECR)仓库中。修改CodeBuild项目运行时配置以始终使用最新的镜像版本。 D. 创建包含缓存Docker镜像的自定义AMI。在CodeBuild构建中，从自定义AMI启动Amazon EC2实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS CodeBuild环境中优化大型Docker镜像的构建性能，同时降低成本。关键需求是提高构建效率和成本优化。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：托管的构建服务 - Amazon ECR：Docker容器镜像仓库服务 - Docker层缓存：Docker构建优化技术 - Amazon S3：对象存储服务 - AMI：Amazon Machine Image **正确答案A的原因：** 1. **ECR集成优势**：ECR是AWS原生的容器镜像仓库，与CodeBuild深度集成，提供最佳性能 2. **Docker层缓存机制**：Docker层缓存是专门针对Docker构建优化的技术，可以重用未改变的镜像层，显著减少构建时间 3. **成本效益**：避免重复构建相同的镜像层，减少计算资源消耗和网络传输 4. **原生支持**：CodeBuild原生支持Docker层缓存功能 **其他选项错误的原因：** - **选项B**：S3不是为Docker镜像存储优化的，缺乏Docker层缓存机制，且跨主机访问效率低 - **选项C**：始终使用最新版本违背了缓存的目的，无法实现性能优化 - **选项D**：使用自定义AMI过于复杂，维护成本高，且CodeBuild是托管服务，不需要管理EC2实例 **决策标准和最佳实践：** 1. **服务集成度**：选择与现有AWS服务深度集成的解决方案 2. **技术适配性**：使用专门为特定技术（Docker）设计的优化方案 3. **运维复杂度**：优先选择托管服务，减少运维负担 4. **成本效益**：通过缓存机制减少重复计算和存储成本</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">287</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A large company recently acquired a small company. The large company invited the small company to join the large company&#x27;s existing organization in AWS Organizations as a new OU. A DevOps engineer determines that the small company needs to launch t3.small Amazon EC2 instance types for the company&#x27;s application workloads. The small company needs to deploy the instances only within US-based AWS Regions. The DevOps engineer needs to use an SCP in the small company&#x27;s new OU to ensure that the small company can launch only the required instance types. Which solution will meet these requirements? Configure another statement to deny the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is not equal to us-*. Configure another statement to allow the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is not equal to us-*. Configure another statement to deny the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is equal to us-*. Configure another statement to allow the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition is equal to us-*.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a statement to deny the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is not equal to t3.small.
B. Configure a statement to allow the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is not equal to t3.small.
C. Configure a statement to deny the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is equal to t3.small.
D. Configure a statement to allow the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is equal to t3.small.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家大公司最近收购了一家小公司。大公司邀请小公司作为新的OU加入大公司在AWS Organizations中的现有组织。DevOps工程师确定小公司需要为其应用程序工作负载启动t3.small Amazon EC2实例类型。小公司需要仅在美国的AWS区域内部署实例。DevOps工程师需要在小公司的新OU中使用SCP来确保小公司只能启动所需的实例类型。哪种解决方案能满足这些要求？ 选项： A. 配置一个语句，当ec2:InstanceType条件不等于t3.small时，拒绝对所有EC2实例资源执行ec2:RunInstances操作。 B. 配置一个语句，当ec2:InstanceType条件不等于t3.small时，允许对所有EC2实例资源执行ec2:RunInstances操作。 C. 配置一个语句，当ec2:InstanceType条件等于t3.small时，拒绝对所有EC2实例资源执行ec2:RunInstances操作。 D. 配置一个语句，当ec2:InstanceType条件等于t3.small时，允许对所有EC2实例资源执行ec2:RunInstances操作。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求使用Service Control Policy (SCP)来限制小公司只能启动t3.small实例类型，并且只能在美国区域部署。需要理解SCP的工作机制和正确的策略配置方法。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. Service Control Policy (SCP) - 组织级别的权限边界策略 3. Organizational Unit (OU) - 组织单元，用于分组管理账户 4. EC2实例类型限制和区域限制 5. IAM条件键：ec2:InstanceType和aws:RequestedRegion **正确答案的原因：** 选项A是正确的，因为： 1. SCP本质上是&quot;拒绝列表&quot;策略，通过拒绝不符合条件的操作来实现限制 2. 当ec2:InstanceType不等于t3.small时拒绝ec2:RunInstances操作，意味着只允许启动t3.small实例 3. 这种&quot;拒绝非目标&quot;的逻辑符合SCP的最佳实践 4. 配合区域限制（拒绝非美国区域的操作），可以完整实现需求 **其他选项错误的原因：** - 选项B：SCP中的Allow语句不会授予权限，只是不拒绝，这种配置无法达到限制效果 - 选项C：拒绝t3.small实例类型的启动，这与需求完全相反 - 选项D：允许t3.small实例类型在SCP中没有实际限制作用，因为SCP不能授予权限 **决策标准和最佳实践：** 1. SCP遵循&quot;最小权限原则&quot;，通过拒绝不需要的操作来限制权限 2. 使用条件键进行精确控制：ec2:InstanceType控制实例类型，aws:RequestedRegion控制区域 3. SCP策略应该明确拒绝不符合业务需求的操作，而不是试图通过允许来授权 4. 在组织架构中，SCP作为安全护栏，确保子账户不能执行违反组织政策的操作</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">288</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps team manages infrastructure for an application. The application uses long-running processes to process items from an Amazon Simple Queue Service (Amazon SQS) queue. The application is deployed to an Auto Scaling group. The application recently experienced an issue where items were taking significantly longer to process. The queue exceeded the expected size, which prevented various business processes from functioning properly. The application records all logs to a third-party tool. The team is currently subscribed to an Amazon Simple Notification Service (Amazon SNS) topic that the team uses for alerts. The team needs to be alerted if the queue exceeds the expected size. Which solution will meet these requirements with the MOST operational efficiency? ApproximateNumberOfMessagesDelayed metric is greater than the expected value. Configure the alarm to notify the SNS topic. ApproximateNumberOfMessagesVisible metric is greater than the expected value. Configure the alarm to notify the SNS topic. as a new CloudWatch custom metric. Create an Amazon EventBridge rule that is scheduled to run every 5 minutes and that invokes the Lambda function. Configure a CloudWatch metrics alarm with a period of 1 hour and a static threshold to alarm if the sum of the new custom metric is greater than the expected value. to a defined expected size in the function. Create an Amazon EventBridge rule that is scheduled to run every 5 minutes and that invokes the Lambda function. When the ApproximateNumberOfMessagesDelayed SQS queue attribute exceeds the expected size, send a notification to the SNS topic.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon CloudWatch metric alarm with a period of 1 hour and a static threshold to alarm if the average of the
B. Create an Amazon CloudWatch metric alarm with a period of 1 hour and a static threshold to alarm if the sum of the
C. Create an AWS Lambda function that retrieves the ApproximateNumberOfMessages SQS queue attribute value and publishes the value
D. Create an AWS Lambda function that checks the ApproximateNumberOfMessagesDelayed SQS queue attribute and compares the value</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps团队管理应用程序的基础设施。该应用程序使用长时间运行的进程来处理Amazon Simple Queue Service (Amazon SQS)队列中的项目。应用程序部署在Auto Scaling组中。该应用程序最近遇到了一个问题，项目处理时间显著延长。队列超过了预期大小，这阻止了各种业务流程的正常运行。应用程序将所有日志记录到第三方工具中。团队目前订阅了用于告警的Amazon Simple Notification Service (Amazon SNS)主题。团队需要在队列超过预期大小时收到告警。哪个解决方案能以最高的运营效率满足要求？ 选项： A. 创建Amazon CloudWatch指标告警，周期为1小时，静态阈值，当ApproximateNumberOfMessagesDelayed指标的平均值大于预期值时告警。配置告警通知SNS主题。 B. 创建Amazon CloudWatch指标告警，周期为1小时，静态阈值，当ApproximateNumberOfMessagesVisible指标的总和大于预期值时告警。配置告警通知SNS主题。 C. 创建AWS Lambda函数，检索ApproximateNumberOfMessages SQS队列属性值并将该值作为新的CloudWatch自定义指标发布。创建Amazon EventBridge规则，计划每5分钟运行一次并调用Lambda函数。配置CloudWatch指标告警，周期为1小时，静态阈值，当新自定义指标的总和大于预期值时告警。 D. 创建AWS Lambda函数，检查ApproximateNumberOfMessagesDelayed SQS队列属性并将值与函数中定义的预期大小进行比较。创建Amazon EventBridge规则，计划每5分钟运行一次并调用Lambda函数。当ApproximateNumberOfMessagesDelayed SQS队列属性超过预期大小时，向SNS主题发送通知。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要监控SQS队列大小，当队列超过预期大小时及时告警，要求解决方案具有最高的运营效率。 **涉及的关键AWS服务和概念：** - Amazon SQS：提供多个内置指标，包括ApproximateNumberOfMessagesVisible（可见消息数）、ApproximateNumberOfMessagesDelayed（延迟消息数）等 - Amazon CloudWatch：提供监控和告警功能，可直接监控SQS内置指标 - Amazon SNS：用于发送告警通知 - AWS Lambda + EventBridge：可用于自定义监控逻辑 **正确答案B的原因：** 1. **直接使用内置指标**：ApproximateNumberOfMessagesVisible是SQS的原生CloudWatch指标，直接反映队列中等待处理的消息数量 2. **最高运营效率**：无需编写和维护Lambda函数，无需创建EventBridge规则，完全使用AWS托管服务 3. **实时监控**：CloudWatch直接监控SQS指标，响应更及时 4. **成本效益**：避免了Lambda函数调用和EventBridge规则的额外成本 5. **可靠性高**：使用AWS原生集成，减少了自定义代码带来的故障点 **其他选项错误的原因：** - **选项A**：ApproximateNumberOfMessagesDelayed主要用于监控延迟消息，不是监控队列总体大小的最佳指标 - **选项C**：过度复杂化，需要Lambda函数、EventBridge规则和自定义指标，增加了运营复杂性和成本 - **选项D**：同样过度复杂化，需要自定义Lambda函数和定期调度，运营效率低 **决策标准和最佳实践：** 1. **优先使用AWS原生服务集成**：避免不必要的自定义代码 2. **选择合适的监控指标**：ApproximateNumberOfMessagesVisible最直接反映队列积压情况 3. **简化架构**：在满足需求的前提下选择最简单的解决方案 4. **考虑运营成本**：包括开发、维护和运行成本</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">289</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A large company runs critical workloads in multiple AWS accounts. The AWS accounts are managed under AWS Organizations with all features enabled. The company stores confidential customer data in an Amazon S3 bucket. Access to the S3 bucket requires multiple levels of approval. The company wants to monitor when the S3 bucket is accessed by using the AWS CLI. The company also wants insights into the various activities performed by other users on all other S3 buckets in the AWS accounts to detect any issues. Which solution will meet these requirements? Insights to perform SQL queries on the custom metrics created from the CloudTrail logs. Use a custom solution for anomaly detection in all the AWS accounts. Use Amazon CloudWatch Metrics Insights to perform SQL queries on the custom metrics created from the CloudTrail logs.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CloudTrail trail that is delivered to Amazon CloudWatch in each AWS account. Enable data events logs for all S3 buckets. Use Amazon GuardDuty for anomaly detection in all the AWS accounts. Use Amazon Athena to perform SQL queries on the custom metrics created from the CloudTrail logs.
B. Create an AWS CloudTrail organization trail that is delivered to Amazon CloudWatch in the Organizations management account. Enable data events logs for all S3 buckets. Use Amazon CloudWatch anomaly detection in all the AWS accounts. Use Amazon Athena to perform SQL queries on the custom metrics created from the CloudTrail logs.
C. Create an AWS CloudTrail organization trail that is delivered to Amazon CloudWatch in the Organizations management account. Enable data events logs for all S3 buckets. Use Amazon CloudWatch anomaly detection in all the AWS accounts. Use Amazon CloudWatch Metrics
D. Create an AWS CloudTrail trail that is delivered to Amazon CloudWatch in each AWS account. Enable data events logs for all S3 buckets.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家大公司在多个AWS账户中运行关键工作负载。这些AWS账户通过启用了所有功能的AWS Organizations进行管理。该公司在Amazon S3存储桶中存储机密客户数据。访问S3存储桶需要多级审批。公司希望通过AWS CLI监控S3存储桶的访问情况。公司还希望深入了解其他用户在所有AWS账户中对其他所有S3存储桶执行的各种活动，以检测任何问题。哪种解决方案能满足这些要求？ 选项： A. 在每个AWS账户中创建一个交付到Amazon CloudWatch的AWS CloudTrail跟踪。为所有S3存储桶启用数据事件日志。在所有AWS账户中使用Amazon GuardDuty进行异常检测。使用Amazon Athena对从CloudTrail日志创建的自定义指标执行SQL查询。 B. 在Organizations管理账户中创建一个交付到Amazon CloudWatch的AWS CloudTrail组织跟踪。为所有S3存储桶启用数据事件日志。在所有AWS账户中使用Amazon CloudWatch异常检测。使用Amazon Athena对从CloudTrail日志创建的自定义指标执行SQL查询。 C. 在Organizations管理账户中创建一个交付到Amazon CloudWatch的AWS CloudTrail组织跟踪。为所有S3存储桶启用数据事件日志。在所有AWS账户中使用Amazon CloudWatch异常检测。使用Amazon CloudWatch Metrics Insights对从CloudTrail日志创建的自定义指标执行SQL查询。 D. 在每个AWS账户中创建一个交付到Amazon CloudWatch的AWS CloudTrail跟踪。为所有S3存储桶启用数据事件日志。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 监控多个AWS账户中S3存储桶的访问活动（特别是通过AWS CLI的访问） 2. 获取所有AWS账户中S3存储桶活动的洞察以检测问题 3. 需要在AWS Organizations环境中实现统一的监控解决方案 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS CloudTrail：API调用日志记录服务，支持组织级跟踪 - CloudTrail数据事件：记录S3对象级操作（如GetObject、PutObject等） - Amazon CloudWatch：监控和异常检测服务 - Amazon Athena：用于查询S3中数据的无服务器查询服务 - Amazon GuardDuty：威胁检测服务 **正确答案B的原因：** 1. **组织跟踪最优**：使用CloudTrail组织跟踪可以在管理账户中集中收集所有成员账户的日志，避免在每个账户单独配置 2. **数据事件启用**：启用S3数据事件日志可以捕获对象级操作，满足监控S3访问的需求 3. **CloudWatch异常检测**：提供自动化的异常检测能力，适合检测S3活动中的问题 4. **Athena查询**：可以对CloudTrail日志执行复杂的SQL查询分析，提供深入洞察 **其他选项错误的原因：** - **选项A错误**：在每个账户创建单独的跟踪增加了管理复杂性；GuardDuty主要用于安全威胁检测，不是专门针对S3活动异常检测的最佳选择 - **选项C错误**：CloudWatch Metrics Insights主要用于分析CloudWatch指标，而不是直接查询CloudTrail日志数据 - **选项D错误**：选项不完整，缺少异常检测和查询分析功能 **决策标准和最佳实践：** 1. **集中化管理**：在Organizations环境中优先使用组织级服务减少管理开销 2. **适当的工具选择**：CloudWatch异常检测专门用于AWS服务的异常模式识别 3. **查询能力**：Athena提供强大的SQL查询能力来分析CloudTrail日志 4. **成本效益**：组织跟踪比多个单独跟踪更经济高效</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">290</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps team is deploying microservices for an application on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The cluster uses managed node groups. The DevOps team wants to enable auto scaling for the microservice Pods based on a specific CPU utilization percentage. The DevOps team has already installed the Kubernetes Metrics Server on the cluster. Which solution will meet these requirements in the MOST operationally efficient way? target tracking scaling policy to scale when the average CPU utilization of the Auto Scaling group reaches a specific percentage. HPA to scale based on the target CPU utilization percentage. Configure the VPA to use the recommender mode setting.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Edit the Auto Scaling group that is associated with the worker nodes of the EKS cluster. Configure the Auto Scaling group to use a
B. Deploy the Kubernetes Horizontal Pod Autoscaler (HPA) and the Kubernetes Vertical Pod Autoscaler (VPA) in the cluster. Configure the
C. Run the AWS Systems Manager AWS-UpdateEKSManagedNodeGroup Automation document. Modify the values for NodeGroupDesiredSize, NodeGroupMaxSize, and NodeGroupMinSize to be based on an estimate for the required node size.
D. Deploy the Kubernetes Horizontal Pod Autoscaler (HPA) and the Kubernetes Cluster Autoscaler in the cluster. Configure the HPA to scale based on the target CPU utilization percentage. Configure the Cluster Autoscaler to use the auto-discovery setting.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps团队正在Amazon Elastic Kubernetes Service (Amazon EKS)集群上为应用程序部署微服务。该集群使用托管节点组。DevOps团队希望基于特定的CPU利用率百分比为微服务Pod启用自动扩缩容。DevOps团队已经在集群上安装了Kubernetes Metrics Server。哪种解决方案能以最具运营效率的方式满足这些要求？ 选项： A. 编辑与EKS集群工作节点关联的Auto Scaling组。配置Auto Scaling组使用目标跟踪扩缩容策略，当Auto Scaling组的平均CPU利用率达到特定百分比时进行扩缩容。 B. 在集群中部署Kubernetes Horizontal Pod Autoscaler (HPA)和Kubernetes Vertical Pod Autoscaler (VPA)。配置HPA基于目标CPU利用率百分比进行扩缩容。配置VPA使用推荐器模式设置。 C. 运行AWS Systems Manager AWS-UpdateEKSManagedNodeGroup自动化文档。修改NodeGroupDesiredSize、NodeGroupMaxSize和NodeGroupMinSize的值，基于所需节点大小的估算。 D. 在集群中部署Kubernetes Horizontal Pod Autoscaler (HPA)和Kubernetes Cluster Autoscaler。配置HPA基于目标CPU利用率百分比进行扩缩容。配置Cluster Autoscaler使用自动发现设置。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 题目要求为EKS集群中的微服务Pod实现基于CPU利用率的自动扩缩容，并且要求解决方案具有最高的运营效率。 **涉及的关键AWS服务和概念：** - Amazon EKS (Elastic Kubernetes Service)：托管的Kubernetes服务 - Kubernetes Metrics Server：提供资源使用指标 - Horizontal Pod Autoscaler (HPA)：Pod水平自动扩缩容器 - Cluster Autoscaler：集群节点自动扩缩容器 - Vertical Pod Autoscaler (VPA)：Pod垂直自动扩缩容器 - Auto Scaling组：EC2实例的自动扩缩容 **正确答案D的原因：** 1. **HPA解决Pod级别扩缩容**：HPA基于CPU利用率自动调整Pod副本数量，直接满足题目要求 2. **Cluster Autoscaler解决节点级别扩缩容**：当Pod需要更多资源但节点不足时，自动添加节点；当节点利用率低时，自动移除节点 3. **完整的两层扩缩容架构**：Pod层面和节点层面的自动扩缩容相互配合，形成完整解决方案 4. **运营效率最高**：自动发现设置减少手动配置，实现真正的自动化 **其他选项错误的原因：** - **选项A**：只在Auto Scaling组层面配置扩缩容，无法直接响应Pod的CPU利用率，不能满足微服务Pod扩缩容的具体需求 - **选项B**：VPA主要用于调整Pod的资源请求和限制，而不是基于CPU利用率进行扩缩容，且VPA与HPA同时使用可能产生冲突 - **选项C**：手动修改节点组大小是静态配置，不是基于CPU利用率的动态自动扩缩容，运营效率低 **决策标准和最佳实践：** 1. **分层扩缩容策略**：Pod级别的HPA处理应用负载变化，节点级别的Cluster Autoscaler处理基础设施容量 2. **自动化优先**：选择能够自动响应指标变化的解决方案，减少人工干预 3. **资源效率**：确保在负载增加时有足够资源，在负载减少时释放不必要的资源 4. **Kubernetes原生工具**：优先使用Kubernetes生态系统中的标准工具，确保兼容性和可维护性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">291</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has multiple AWS accounts. The company uses AWS IAM Identity Center that is integrated with a third-party SAML 2.0 identity provider (IdP). The attributes for access control feature is enabled in IAM Identity Center. The attribute mapping list maps the department key from the IdP to the ${path:enterprise.department} attribute. All existing Amazon EC2 instances have a d1, d2, d3 department tag that corresponds to three company&#x27;s departments. A DevOps engineer must create policies based on the matching attributes. The policies must grant each user access to only the EC2 instances that are tagged with the user&#x27;s respective department name. Which condition key should the DevOps engineer include in the custom permissions policies to meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content"></div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有多个AWS账户。该公司使用与第三方SAML 2.0身份提供商(IdP)集成的AWS IAM Identity Center。IAM Identity Center中启用了访问控制属性功能。属性映射列表将IdP中的department键映射到${path:enterprise.department}属性。所有现有的Amazon EC2实例都有d1、d2、d3部门标签，对应公司的三个部门。DevOps工程师必须基于匹配属性创建策略。这些策略必须授予每个用户仅访问标记有用户各自部门名称的EC2实例。DevOps工程师应该在自定义权限策略中包含哪个条件键来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求创建IAM策略，使用户只能访问标记有其所属部门的EC2实例。需要通过条件键将用户的部门属性与EC2实例的部门标签进行匹配。 **涉及的关键AWS服务和概念：** - AWS IAM Identity Center：集中身份管理服务 - SAML 2.0联邦身份验证：与第三方IdP集成 - 属性映射：将IdP属性映射到IAM Identity Center属性 - EC2资源标签：用于资源分类和访问控制 - IAM条件键：用于细粒度访问控制 **正确答案的原因：** 选项B应该是类似`aws:PrincipalTag/department`的条件键。这个条件键的工作原理是： - 当用户通过SAML联邦登录时，IdP的department属性被映射到${path:enterprise.department} - 这个属性会作为Principal标签附加到用户的临时凭证上 - 使用`aws:PrincipalTag/department`可以获取用户的部门信息 - 结合`aws:RequestedRegion`或资源标签条件，可以实现基于部门的访问控制 **其他选项错误的原因：** - 如果有`ec2:ResourceTag/department`选项，这只能检查资源标签，无法与用户属性匹配 - 如果有`saml:department`选项，这在策略执行时可能不可用，因为SAML断言属性在临时凭证生成后不会持续存在 - 其他不相关的条件键无法实现用户属性与资源标签的匹配 **决策标准和最佳实践：** 1. 使用基于属性的访问控制(ABAC)模式，通过Principal标签实现动态权限控制 2. 确保属性映射正确配置，将IdP属性映射到可用的IAM属性 3. 在策略中同时使用Principal标签和资源标签条件进行匹配 4. 遵循最小权限原则，确保用户只能访问其部门的资源 5. 定期审查和更新属性映射和标签策略</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">292</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A security team wants to use AWS CloudTrail to monitor all actions and API calls in multiple accounts that are in the same organization in AWS Organizations. The security team needs to ensure that account users cannot turn off CloudTrail in the accounts. Which solution will meet this requirement?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Apply an SCP to all OUs to deny the cloudtrail:StopLogging action and the cloudtrail:DeleteTrail action.
B. Create IAM policies in each account to deny the cloudtrail:StopLogging action and the cloudtrail:DeleteTrail action.
C. Set up Amazon CloudWatch alarms to notify the security team when a user disables CloudTrail in an account.
D. Use AWS Config to automatically re-enable CloudTrail if a user disables CloudTrail in an account.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">安全团队希望使用AWS CloudTrail来监控在AWS Organizations中同一组织内多个账户中的所有操作和API调用。安全团队需要确保账户用户无法在这些账户中关闭CloudTrail。哪个解决方案能满足这个要求？ 选项：A. 对所有OU应用SCP来拒绝cloudtrail:StopLogging操作和cloudtrail:DeleteTrail操作。 B. 在每个账户中创建IAM策略来拒绝cloudtrail:StopLogging操作和cloudtrail:DeleteTrail操作。 C. 设置Amazon CloudWatch告警，当用户在账户中禁用CloudTrail时通知安全团队。 D. 使用AWS Config在用户禁用CloudTrail时自动重新启用CloudTrail。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Organizations环境中实现对多个账户CloudTrail的集中保护，确保任何账户用户都无法关闭CloudTrail服务，从而保证审计日志的连续性和完整性。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - Service Control Policy (SCP)：组织级别的权限边界策略 - AWS CloudTrail：API调用审计服务 - IAM策略：账户级别的权限控制 - CloudWatch告警：监控和通知服务 - AWS Config：配置合规性监控服务 **正确答案的原因：** 选项A是正确答案，因为： 1. **组织级别控制**：SCP在AWS Organizations层面工作，可以对所有成员账户施加统一的权限限制 2. **不可绕过性**：SCP作为权限边界，即使账户管理员也无法绕过这些限制 3. **精确控制**：通过拒绝cloudtrail:StopLogging和cloudtrail:DeleteTrail操作，直接阻止了关闭CloudTrail的两种主要方式 4. **集中管理**：从组织根账户统一管理，无需在每个成员账户中单独配置 **其他选项错误的原因：** - **选项B**：IAM策略只在账户级别生效，账户管理员可以修改或删除这些策略，无法提供可靠的保护 - **选项C**：CloudWatch告警只是事后通知机制，无法阻止用户关闭CloudTrail，不符合&quot;确保用户无法关闭&quot;的要求 - **选项D**：AWS Config的自动修复功能存在时间延迟，在检测到违规和重新启用之间会有审计盲区，且用户仍然可以反复关闭 **决策标准和最佳实践：** 1. **预防优于检测**：应该从源头阻止不当操作，而不是依赖事后检测和修复 2. **最小权限原则**：使用SCP实现组织级别的权限边界控制 3. **集中化管理**：在多账户环境中，优先选择可以集中管理和控制的解决方案 4. **不可绕过性**：选择用户无法轻易绕过或修改的控制机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">293</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer needs to configure a blue/green deployment for an existing three-tier application. The application runs on Amazon EC2 instances and uses an Amazon RDS database. The EC2 instances run behind an Application Load Balancer (ALB) and are in an Auto Scaling group. The DevOps engineer has created launch templates, Auto Scaling groups, and ALB target groups for the blue environment and the green environment. Each target group specifies which application version, blue or green, will be loaded on the EC2 instances. An Amazon Route 53 record for www.example.com points to the ALB. The deployment must shift traffic all at once from the blue environment to the green environment. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new application version to the green environment&#x27;s EC2 instances. When the rolling restart is complete, use an AWS CLI command to update the ALB to send traffic to the green environment&#x27;s target group.
B. Use an AWS CLI command to update the ALB to send traffic to the green environment&#x27;s target group. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new application version to the green environment&#x27;s EC2 instances.
C. Update the launch template to deploy the green environment&#x27;s application version to the blue environment&#x27;s EC2 instances. Do not change the target groups or the Auto Scaling groups in either environment. Perform a rolling restart of the blue environment&#x27;s EC2 instances.
D. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new application version to the green environment&#x27;s EC2 instances. When the rolling restart is complete, update Route 53 to point to the green environment&#x27;s endpoint on the ALB.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师需要为现有的三层应用程序配置蓝/绿部署。该应用程序运行在Amazon EC2实例上并使用Amazon RDS数据库。EC2实例运行在Application Load Balancer (ALB)后面，并位于Auto Scaling组中。DevOps工程师已经为蓝环境和绿环境创建了启动模板、Auto Scaling组和ALB目标组。每个目标组指定将在EC2实例上加载哪个应用程序版本（蓝色或绿色）。Amazon Route 53记录www.example.com指向ALB。部署必须一次性将流量从蓝环境切换到绿环境。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS蓝/绿部署策略的实施。关键要求是实现&quot;一次性&quot;流量切换，即瞬间将所有流量从蓝环境切换到绿环境，而不是渐进式切换。 **涉及的关键AWS服务和概念：** - 蓝/绿部署：两个完全独立的生产环境，通过切换流量实现零停机部署 - Application Load Balancer (ALB)：负载均衡器，可以在不同目标组间切换流量 - Auto Scaling组：自动扩缩容服务 - 目标组：ALB将流量路由到的EC2实例集合 - Route 53：DNS服务 **正确答案B的原因：** 选项B的执行顺序是正确的：先切换ALB流量到绿环境，再进行绿环境的滚动重启。这样做的优势是： 1. 立即实现流量切换，满足&quot;一次性切换&quot;的要求 2. 即使绿环境在重启过程中出现问题，流量已经切换完成 3. 符合蓝/绿部署的最佳实践：先准备好新环境，再切换流量 **其他选项错误的原因：** - 选项A：先重启再切换流量，如果重启过程中出现问题会影响切换时机，且不够灵活 - 选项C：这不是真正的蓝/绿部署，而是就地更新，违背了蓝/绿部署的基本原则 - 选项D：通过Route 53切换DNS记录会有DNS缓存延迟问题，无法实现真正的&quot;一次性&quot;切换 **决策标准和最佳实践：** 1. 蓝/绿部署应该保持两个完全独立的环境 2. 流量切换应该在负载均衡器层面进行，而不是DNS层面 3. 切换操作应该是瞬时的，避免DNS传播延迟 4. 先切换流量再进行后续操作，确保用户体验的连续性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">294</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that runs on Amazon EC2 instances in an Auto Scaling group. The application processes a high volume of messages from an Amazon Simple Queue Service (Amazon SQS) queue. A DevOps engineer noticed that the application took several hours to process a group of messages from the SQS queue. The average CPU utilization of the Auto Scaling group did not cross the threshold of a target tracking scaling policy when processing the messages. The application that processes the SQS queue publishes logs to Amazon CloudWatch Logs. The DevOps engineer needs to ensure that the queue is processed quickly. Which solution meets these requirements with the LEAST operational overhead? queue messages for each instance. Create a CloudWatch subscription filter for the application logs with the Lambda function as the target. Create a target tracking scaling policy for the Auto Scaling group that uses the custom metric to scale in and out.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS Lambda function. Configure the Lambda function to publish a custom metric by using the ApproximateNumberOfMessagesVisible SQS queue attribute and the GroupInServiceInstances Auto Scaling group attribute to publish the queue messages for each instance. Schedule an Amazon EventBridge rule to run the Lambda function every hour. Create a target tracking scaling policy for the Auto Scaling group that uses the custom metric to scale in and out.
B. Create an AWS Lambda function. Configure the Lambda function to publish a custom metric by using the ApproximateNumberOfMessagesVisible SQS queue attribute and the GroupInServiceInstances Auto Scaling group attribute to publish the
C. Create a target tracking scaling policy for the Auto Scaling group. In the target tracking policy, use the ApproximateNumberOfMessagesVisible SQS queue attribute and the GroupInServiceInstances Auto Scaling group attribute to calculate how many messages are in the queue for each number of instances by using metric math. Use the calculated attribute to scale in and out.
D. Create an AWS Lambda function that logs the ApproximateNumberOfMessagesVisible attribute of the SQS queue to a CloudWatch Logs log group. Schedule an Amazon EventBridge rule to run the Lambda function every 5 minutes. Create a metric filter to count the number of log events from a CloudWatch logs group. Create a target tracking scaling policy for the Auto Scaling group that uses the custom metric to scale in and out.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个运行在Auto Scaling组中Amazon EC2实例上的应用程序。该应用程序处理来自Amazon Simple Queue Service (Amazon SQS)队列的大量消息。DevOps工程师注意到应用程序花费了几个小时来处理SQS队列中的一组消息。在处理消息时，Auto Scaling组的平均CPU利用率没有超过目标跟踪扩展策略的阈值。处理SQS队列的应用程序将日志发布到Amazon CloudWatch Logs。DevOps工程师需要确保队列能够快速处理。哪个解决方案能以最少的运营开销满足这些要求？ 选项A：创建AWS Lambda函数。配置Lambda函数使用ApproximateNumberOfMessagesVisible SQS队列属性和GroupInServiceInstances Auto Scaling组属性发布自定义指标，以发布每个实例的队列消息。安排Amazon EventBridge规则每小时运行Lambda函数。为Auto Scaling组创建使用自定义指标进行扩缩的目标跟踪扩展策略。 选项B：创建AWS Lambda函数。配置Lambda函数使用ApproximateNumberOfMessagesVisible SQS队列属性和GroupInServiceInstances Auto Scaling组属性发布自定义指标，以发布每个实例的队列消息。为应用程序日志创建CloudWatch订阅过滤器，以Lambda函数为目标。为Auto Scaling组创建使用自定义指标进行扩缩的目标跟踪扩展策略。 选项C：为Auto Scaling组创建目标跟踪扩展策略。在目标跟踪策略中，使用ApproximateNumberOfMessagesVisible SQS队列属性和GroupInServiceInstances Auto Scaling组属性，通过metric math计算队列中每个实例数量对应的消息数。使用计算出的属性进行扩缩。 选项D：创建AWS Lambda函数，将SQS队列的ApproximateNumberOfMessagesVisible属性记录到CloudWatch Logs日志组。安排Amazon EventBridge规则每5分钟运行Lambda函数。创建指标过滤器来计算CloudWatch日志组中的日志事件数量。为Auto Scaling组创建使用自定义指标进行扩缩的目标跟踪扩展策略。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题的核心问题是SQS队列处理缓慢，但CPU利用率没有触发Auto Scaling扩展。需要找到一个运营开销最小的解决方案来确保队列快速处理。关键在于需要基于队列长度而不是CPU使用率来进行扩展。 **涉及的关键AWS服务和概念：** - Amazon SQS：消息队列服务，ApproximateNumberOfMessagesVisible属性表示队列中可见消息数量 - Auto Scaling：自动扩展服务，GroupInServiceInstances表示当前运行的实例数量 - CloudWatch：监控服务，支持自定义指标和metric math功能 - Target Tracking Scaling Policy：目标跟踪扩展策略，可以基于特定指标自动调整实例数量 - Metric Math：CloudWatch的数学计算功能，可以对多个指标进行运算 **正确答案C的原因：** 1. **直接使用原生功能**：直接在target tracking policy中使用metric math计算每个实例对应的消息数量，无需额外的Lambda函数 2. **运营开销最小**：不需要创建、维护和调度额外的Lambda函数，完全使用AWS原生的Auto Scaling和CloudWatch功能 3. **实时响应**：metric math实时计算，不依赖定时触发，响应更及时 4. **成本效益**：避免了Lambda函数的执行成本和EventBridge的调度成本 **其他选项错误的原因：** - **选项A**：需要创建Lambda函数和EventBridge规则，增加了运营复杂性；每小时执行频率太低，无法及时响应队列变化 - **选项B**：选项描述不完整，且涉及CloudWatch订阅过滤器，增加了不必要的复杂性 - **选项D**：过度复杂化，通过Lambda写日志再用metric filter处理，这种间接方式增加了多个故障点和延迟 **决策标准和最佳实践：** 1. **最小运营开销原则**：优先选择使用AWS原生功能的解决方案，减少自定义组件 2. **实时性要求**：队列处理需要快速响应，应避免依赖定时任务的方案 3. **成本优化**：减少不必要的Lambda执行和其他服务调用 4. **架构简洁性**：直接的metric math计算比多层间接处理更可靠和易维护 5. **SQS扩展最佳实践**：基于队列长度与实例数量的比值进行扩展是处理SQS工作负载的标准做法</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">295</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a single AWS account that runs hundreds of Amazon EC2 instances in a single AWS Region. The company launches and terminates new EC2 instances every hour. The account includes existing EC2 instances that have been running for longer than a week. The company&#x27;s security policy requires all running EC2 instances to have an EC2 instance profile attached. The company has created a default EC2 instance profile. The default EC2 instance profile must be attached to any EC2 instances that do not have a profile attached. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure an Amazon EventBridge rule that matches the Amazon EC2 RunInstances API calls. Configure the rule to invoke an AWS Lambda function to attach the default instance profile to the EC2 instances.
B. Configure AWS Config. Deploy an AWS Config ec2-instance-profile-attached managed rule. Configure an automatic remediation action that invokes an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances.
C. Configure an Amazon EventBridge rule that matches the Amazon EC2 StartInstances API calls. Configure the rule to invoke an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances.
D. Configure AWS Config. Deploy an AWS Config iam-role-managed-policy-check managed rule. Configure an automatic remediation action that invokes an AWS Lambda function to attach the default instance profile to the EC2 instances.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个单独的AWS账户，在单个AWS Region中运行数百个Amazon EC2实例。该公司每小时都会启动和终止新的EC2实例。该账户包括已经运行超过一周的现有EC2实例。公司的安全策略要求所有运行中的EC2实例都必须附加EC2 instance profile。公司已经创建了一个默认的EC2 instance profile。默认的EC2 instance profile必须附加到任何没有附加profile的EC2实例上。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 确保所有运行中的EC2实例都附加了instance profile - 对于没有附加profile的实例，需要自动附加默认的instance profile - 需要处理现有的长期运行实例和新启动的实例 - 实现自动化的合规性检查和修复 **涉及的关键AWS服务和概念：** - AWS Config：用于监控和评估AWS资源配置的合规性 - Amazon EventBridge：事件驱动的服务，用于响应AWS API调用 - EC2 Instance Profile：允许EC2实例承担IAM角色的机制 - AWS Systems Manager Automation：自动化运维任务的服务 - AWS Lambda：无服务器计算服务 **正确答案B的原因：** 1. **AWS Config的ec2-instance-profile-attached规则**专门用于检查EC2实例是否附加了instance profile，完全匹配需求 2. **持续监控能力**：Config能够持续评估所有EC2实例的合规性状态，包括现有的和新创建的实例 3. **自动修复功能**：通过automatic remediation action可以自动触发Systems Manager Automation runbook来附加默认profile 4. **全面覆盖**：能够处理账户中所有现有实例和未来创建的实例 **其他选项错误的原因：** - **选项A**：EventBridge监听RunInstances API调用只能处理新启动的实例，无法处理现有的未附加profile的实例 - **选项C**：StartInstances API调用是针对启动已停止实例的操作，不是创建新实例，且同样无法处理现有实例 - **选项D**：iam-role-managed-policy-check规则是检查IAM角色的托管策略，与检查EC2实例是否附加instance profile的需求不匹配 **决策标准和最佳实践：** 1. **合规性监控**：使用AWS Config进行持续的合规性监控是最佳实践 2. **自动化修复**：结合Config的自动修复功能可以实现完全自动化的合规性管理 3. **全面覆盖**：解决方案必须能够处理现有资源和新创建的资源 4. **适当的规则选择**：选择与具体合规性要求完全匹配的Config规则</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">296</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Organizations to manage hundreds of AWS accounts. The company has a team that is responsible for AWS Identity and Access Management (IAM). The IAM team wants to implement AWS IAM Identity Center. The IAM team must have only the minimum required permissions to manage IAM Identity Center. The IAM team must not be able to gain unnecessary access to the Organizations management account. The IAM team must be able to provision new IAM Identity Center permission sets and assignments for new and existing member accounts. Which combination of steps will meet these requirements? (Choose three.) F. Assign the new permission set to the new AWS account. Allow the IAM team&#x27;s group to use the permission set.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new AWS account for the IAM team. Enable IAM Identity Center in the new account. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center.
B. Create a new AWS account for the IAM team. Enable IAM Identity Center in the Organizations management account. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center.
C. Create an SCP in Organizations. Create a new OU for the Organizations management account, and link the new SCP to the OU. Configure the SCP to deny all access to IAM Identity Center.
D. Create IAM users and an IAM group for the IAM team in IAM Identity Center. Add the users to the group. Create a new permission set. Attach the AWSSSOMemberAccountAdministrator managed IAM policy to the group.
E. Assign the new permission set to the Organizations management account. Allow the IAM team&#x27;s group to use the permission set.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations来管理数百个AWS账户。该公司有一个负责AWS Identity and Access Management (IAM)的团队。IAM团队希望实施AWS IAM Identity Center。IAM团队必须只拥有管理IAM Identity Center所需的最小权限。IAM团队不得获得对Organizations管理账户的不必要访问权限。IAM团队必须能够为新的和现有的成员账户配置新的IAM Identity Center权限集和分配。以下哪些步骤组合将满足这些要求？（选择三个。）F. 将新权限集分配给新的AWS账户。允许IAM团队的组使用该权限集。 选项：A. 为IAM团队创建一个新的AWS账户。在新账户中启用IAM Identity Center。在Organizations管理账户中，将新账户注册为IAM Identity Center的委托管理员。 B. 为IAM团队创建一个新的AWS账户。在Organizations管理账户中启用IAM Identity Center。在Organizations管理账户中，将新账户注册为IAM Identity Center的委托管理员。 C. 在Organizations中创建SCP。为Organizations管理账户创建新的OU，并将新的SCP链接到OU。配置SCP以拒绝对IAM Identity Center的所有访问。 D. 在IAM Identity Center中为IAM团队创建IAM用户和IAM组。将用户添加到组中。创建新的权限集。将AWSSSOMemberAccountAdministrator托管IAM策略附加到组。 E. 将新权限集分配给Organizations管理账户。允许IAM团队的组使用该权限集。 正确答案：A</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为IAM团队实施IAM Identity Center，同时满足最小权限原则，确保IAM团队不能获得对Organizations管理账户的不必要访问，但能够管理权限集和分配。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. AWS IAM Identity Center - 集中身份管理服务 3. 委托管理员(Delegated Administrator) - 允许成员账户管理特定服务 4. 权限集(Permission Sets) - IAM Identity Center中的权限模板 5. 最小权限原则 - 安全最佳实践 **正确答案A的原因：** 选项A正确实施了委托管理模式： - 创建独立的AWS账户给IAM团队，实现职责分离 - 在新账户中启用IAM Identity Center，确保服务可用 - 通过委托管理员机制，让IAM团队能够管理整个组织的IAM Identity Center，但不直接访问管理账户 - 满足最小权限原则，IAM团队只能访问必要的IAM Identity Center功能 **其他选项错误的原因：** - 选项B：错误地在管理账户中启用IAM Identity Center，这违反了不给IAM团队管理账户访问权限的要求 - 选项C：使用SCP拒绝访问会阻止必要的IAM Identity Center管理功能 - 选项D：AWSSSOMemberAccountAdministrator策略权限过大，违反最小权限原则 - 选项E：直接分配权限到管理账户违反了隔离要求 **决策标准和最佳实践：** 1. 职责分离：使用独立账户管理不同功能 2. 最小权限原则：只授予完成任务所需的最小权限 3. 委托管理：利用AWS的委托管理功能实现安全的跨账户管理 4. 避免直接访问管理账户：通过委托机制而非直接权限实现管理功能</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">297</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an Amazon Aurora PostgreSQL global database that has two secondary AWS Regions. A DevOps engineer has configured the database parameter group to guarantee an RPO of 60 seconds. Write operations on the primary cluster are occasionally blocked because of the RPO setting. The DevOps engineer needs to reduce the frequency of blocked write operations. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add an additional secondary cluster to the global database.
B. Enable write forwarding for the global database.
C. Remove one of the secondary clusters from the global database.
D. Configure synchronous replication for the global database.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用Amazon Aurora PostgreSQL global database，该数据库有两个secondary AWS Region。DevOps工程师已配置database parameter group以保证RPO为60秒。由于RPO设置，primary cluster上的写操作偶尔会被阻塞。DevOps工程师需要减少写操作被阻塞的频率。哪个解决方案能满足这些要求？ 选项：A. 向global database添加额外的secondary cluster。B. 为global database启用write forwarding。C. 从global database中移除一个secondary cluster。D. 为global database配置synchronous replication。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何在保证RPO（Recovery Point Objective）的前提下，减少Aurora PostgreSQL global database中写操作被阻塞的频率。关键在于理解RPO设置与写操作性能之间的平衡关系。 **涉及的关键AWS服务和概念：** - Amazon Aurora PostgreSQL Global Database：跨区域的数据库复制解决方案 - RPO（Recovery Point Objective）：数据恢复点目标，表示在灾难发生时可以接受的最大数据丢失时间 - Database Parameter Group：数据库参数组，用于配置数据库行为 - Primary/Secondary Cluster：主集群和辅助集群的概念 **正确答案C的原因：** 移除一个secondary cluster可以减少写操作被阻塞的频率，因为： 1. 减少了需要同步的目标区域数量，降低了复制延迟 2. 减少了网络开销和跨区域数据传输的复杂性 3. 仍然保持一个secondary cluster，满足灾难恢复需求 4. 在保持RPO要求的同时提高了写操作性能 **其他选项错误的原因：** A. 添加额外的secondary cluster会增加复制负担，进一步加剧写操作阻塞问题 B. Write forwarding主要用于读写分离，不能解决RPO导致的写阻塞问题 D. Synchronous replication会增加写操作延迟，使阻塞问题更严重 **决策标准和最佳实践：** 1. 在性能和可用性之间找到平衡点 2. 根据实际业务需求调整secondary cluster数量 3. 监控复制延迟和写操作性能指标 4. 考虑网络延迟对跨区域复制的影响</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">298</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a web application that is hosted on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster runs on AWS Fargate that is available through an internet-facing Application Load Balancer. The application is experiencing stability issues that lead to longer response times. A DevOps engineer needs to configure observability in Amazon CloudWatch to troubleshoot the issue. The solution must provide only the minimum necessary permissions. Which combination of steps will meet these requirements? (Choose three.) F. Enable EKS control plane logging for the EKS cluster.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Deploy the CloudWatch agent as a Kubernetes StatefulSet to the EKS cluster.
B. Deploy the AWS Distro for OpenTelemetry Collector as a Kubernetes DaemonSet to the EKS cluster.
C. Associate a Kubernetes service account with an IAM role by using IAM roles for service accounts in Amazon EKS. Use the CloudWatchAgentServerPolicy AWS managed policy.
D. Associate a Kubernetes service account with an IAM role by using IAM roles for service accounts in Amazon EKS. Use the CloudWatchAgentAdminPolicy AWS managed policy.
E. Configure an IAM OpenID Connect (OIDC) provider for the EKS cluster.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个托管在Amazon Elastic Kubernetes Service (Amazon EKS)集群上的Web应用程序。该EKS集群运行在AWS Fargate上，通过面向互联网的Application Load Balancer提供服务。应用程序正在经历稳定性问题，导致响应时间延长。DevOps工程师需要在Amazon CloudWatch中配置可观测性来排查问题。解决方案必须仅提供最小必要权限。以下哪些步骤组合能满足这些要求？（选择三个） 选项： A. 将CloudWatch agent作为Kubernetes StatefulSet部署到EKS集群 B. 将AWS Distro for OpenTelemetry Collector作为Kubernetes DaemonSet部署到EKS集群 C. 使用Amazon EKS中的IAM roles for service accounts将Kubernetes service account与IAM role关联。使用CloudWatchAgentServerPolicy AWS托管策略 D. 使用Amazon EKS中的IAM roles for service accounts将Kubernetes service account与IAM role关联。使用CloudWatchAgentAdminPolicy AWS托管策略 E. 为EKS集群配置IAM OpenID Connect (OIDC) provider F. 为EKS集群启用EKS control plane logging</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为运行在AWS Fargate上的EKS集群配置CloudWatch可观测性，以解决应用程序稳定性和响应时间问题，同时遵循最小权限原则。 **涉及的关键AWS服务和概念：** - Amazon EKS (Elastic Kubernetes Service) - AWS Fargate - Amazon CloudWatch - AWS Distro for OpenTelemetry (ADOT) - IAM Roles for Service Accounts (IRSA) - OIDC Provider - EKS Control Plane Logging **正确答案的原因：** 题目显示正确答案是B，但实际上应该选择三个选项。基于最佳实践，正确的组合应该是： - **B选项**：AWS Distro for OpenTelemetry Collector作为DaemonSet部署是现代化的可观测性解决方案，支持metrics、traces和logs收集 - **C选项**：CloudWatchAgentServerPolicy提供了发送数据到CloudWatch的最小必要权限，符合最小权限原则 - **E选项**：OIDC provider是使用IRSA的前提条件，必须先配置才能实现service account与IAM role的关联 **其他选项错误的原因：** - **A选项**：CloudWatch agent作为StatefulSet部署不合适，且在Fargate环境中ADOT是更好的选择 - **D选项**：CloudWatchAgentAdminPolicy权限过大，违反了最小权限原则 - **F选项**：Control plane logging主要用于审计和调试Kubernetes API，对应用程序性能问题帮助有限 **决策标准和最佳实践：** 1. **最小权限原则**：选择ServerPolicy而非AdminPolicy 2. **现代化工具**：优先选择ADOT over传统CloudWatch agent 3. **Fargate兼容性**：确保选择的方案在Fargate环境中可行 4. **安全配置**：使用IRSA而非直接的IAM角色，需要OIDC provider支持 5. **可观测性全面性**：ADOT能够收集metrics、traces和logs，更适合排查性能问题</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">299</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company stores its Python-based application code in AWS CodeCommit. The company uses AWS CodePipeline to deploy the application. The CodeCommit repository and the CodePipeline pipeline are deployed to the same AWS account. The company&#x27;s security team requires all code to be scanned for vulnerabilities before the code is deployed to production. If any vulnerabilities are found, the deployment must stop. Which solution will meet these requirements? stage. Select AWS CodeBuild as the action provider for the new stage. Use the source artifact from the CodeCommit repository. Configure the action to use the CodeBuild project.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new CodeBuild project. Configure the project to run a security scan on the code by using Amazon CodeGuru Security. Configure the CodeBuild project to raise an error if CodeGuru Security finds vulnerabilities. Create a new IAM role that has sufficient permissions to run CodeGuru Security scans. Assign the role to the CodeBuild project. In the CodePipeline pipeline, add a new stage before the deployment stage. Select AWS CodeBuild as the action provider for the new stage. Use the source artifact from the CodeCommit repository. Configure the action to use the CodeBuild project.
B. Create a new CodeBuild project. Configure the project to run a security scan on the code by using Amazon Inspector. Configure the CodeBuild project to raise an error if Amazon Inspector finds vulnerabilities. Create a new IAM role that has sufficient permissions to run Amazon Inspector scans. Assign the role to the CodeBuild project. In the CodePipeline pipeline, add a new stage before the deployment
C. Update the IAM role that is attached to CodePipeline to include sufficient permissions to invoke Amazon DevOps Guru. In the CodePipeline pipeline, add a new stage before the deployment stage. Select DevOps Guru as the action provider for the new stage. Use the source artifact from the CodeCommit repository.
D. Update the IAM role that is attached to CodePipeline to include sufficient permissions to invoke Amazon DevOps Guru. In the CodePipeline pipeline, add a new stage before the deployment stage. Select CodeGuru Security as the action provider for the new stage. Use the source artifact from the CodeCommit repository.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司将其基于Python的应用程序代码存储在AWS CodeCommit中。该公司使用AWS CodePipeline来部署应用程序。CodeCommit存储库和CodePipeline管道部署在同一个AWS账户中。公司的安全团队要求在代码部署到生产环境之前，必须扫描所有代码的漏洞。如果发现任何漏洞，部署必须停止。哪种解决方案能满足这些要求？ 选项： A. 创建一个新的CodeBuild项目。配置项目使用Amazon CodeGuru Security对代码运行安全扫描。配置CodeBuild项目在CodeGuru Security发现漏洞时抛出错误。创建一个具有足够权限运行CodeGuru Security扫描的新IAM角色。将角色分配给CodeBuild项目。在CodePipeline管道中，在部署阶段之前添加一个新阶段。选择AWS CodeBuild作为新阶段的操作提供者。使用来自CodeCommit存储库的源工件。配置操作使用CodeBuild项目。 B. 创建一个新的CodeBuild项目。配置项目使用Amazon Inspector对代码运行安全扫描。配置CodeBuild项目在Amazon Inspector发现漏洞时抛出错误。创建一个具有足够权限运行Amazon Inspector扫描的新IAM角色。将角色分配给CodeBuild项目。在CodePipeline管道中，在部署阶段之前添加一个新阶段。 C. 更新附加到CodePipeline的IAM角色，包含足够的权限来调用Amazon DevOps Guru。在CodePipeline管道中，在部署阶段之前添加一个新阶段。选择DevOps Guru作为新阶段的操作提供者。使用来自CodeCommit存储库的源工件。 D. 更新附加到CodePipeline的IAM角色，包含足够的权限来调用Amazon DevOps Guru。在CodePipeline管道中，在部署阶段之前添加一个新阶段。选择CodeGuru Security作为新阶段的操作提供者。使用来自CodeCommit存储库的源工件。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在CI/CD管道中集成代码安全扫描功能，确保在部署到生产环境之前检测代码漏洞，如发现漏洞则停止部署流程。 **涉及的关键AWS服务和概念：** 1. AWS CodeCommit - 代码存储库服务 2. AWS CodePipeline - CI/CD管道服务 3. AWS CodeBuild - 构建和测试服务 4. Amazon CodeGuru Security - 代码安全分析服务 5. Amazon Inspector - 应用程序安全评估服务（主要用于运行时环境） 6. Amazon DevOps Guru - 运维智能服务（用于检测运营问题） **正确答案A的原因：** 1. **服务选择正确**：Amazon CodeGuru Security专门用于静态代码安全分析，能够检测Python代码中的安全漏洞 2. **集成方式合理**：通过CodeBuild项目运行安全扫描，这是标准的CI/CD集成模式 3. **权限配置恰当**：创建专门的IAM角色给CodeBuild项目，遵循最小权限原则 4. **流程设计正确**：在部署阶段之前添加安全扫描阶段，确保有问题的代码不会被部署 5. **错误处理机制**：配置CodeBuild在发现漏洞时抛出错误，自动停止管道执行 **其他选项错误的原因：** - **选项B**：Amazon Inspector主要用于运行时应用程序和基础设施的安全评估，不适合静态代码分析 - **选项C**：Amazon DevOps Guru用于检测运营异常和性能问题，不是代码安全扫描工具；且DevOps Guru不能作为CodePipeline的直接操作提供者 - **选项D**：虽然提到了CodeGuru Security，但CodeGuru Security不能直接作为CodePipeline的操作提供者，必须通过CodeBuild等服务来集成 **决策标准和最佳实践：** 1. **左移安全**：在开发生命周期早期集成安全检查 2. **自动化集成**：使用CodeBuild作为中间层来集成各种工具和服务 3. **失败快速原则**：一旦发现安全问题立即停止部署流程 4. **权限最小化**：为不同的服务组件分配最小必要权限 5. **工具专业化**：选择专门针对特定用途设计的AWS服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">300</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer deploys an application to a fleet of Amazon Linux EC2 instances. The DevOps engineer needs to monitor system metrics across the fleet. The DevOps engineer wants to monitor the relationship between network traffic and memory utilization for the application code. The DevOps engineer wants to track the data on a 60 second interval. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Amazon CloudWatch basic monitoring to collect the NetworkIn metric and the MemoryBytesUsed metric. Graph the metrics in CloudWatch.
B. Use Amazon CloudWatch detailed monitoring to collect the NetworkIn metric and the MemoryBytesUsed metric. Graph the metrics in CloudWatch.
C. Use Amazon CloudWatch detailed monitoring to collect the NetworkIn metric. Install the CloudWatch agent on the EC2 instances to collect the mem_used metric. Graph the metrics in CloudWatch.
D. Use Amazon CloudWatch basic monitoring to collect the built-in NetworkIn metric. Install the CloudWatch agent on the EC2 instances to collect the mem_used metric. Graph the metrics in CloudWatch.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师将应用程序部署到一组Amazon Linux EC2实例上。该DevOps工程师需要监控整个实例群的系统指标。DevOps工程师希望监控应用程序代码的网络流量和内存利用率之间的关系。DevOps工程师希望以60秒的间隔跟踪数据。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 监控EC2实例群的系统指标 - 需要同时监控网络流量和内存利用率 - 数据收集间隔为60秒 - 需要分析两个指标之间的关系 **涉及的关键AWS服务和概念：** 1. Amazon CloudWatch - AWS的监控服务 2. CloudWatch基础监控 vs 详细监控的区别 3. CloudWatch Agent - 用于收集自定义指标和系统级指标 4. EC2内置指标 vs 自定义指标的概念 **正确答案C的原因：** - CloudWatch详细监控提供1分钟间隔的数据收集，满足60秒间隔要求 - NetworkIn是EC2的内置指标，通过详细监控可以获取 - 内存使用率不是EC2的内置指标，必须通过CloudWatch Agent收集mem_used指标 - 这种组合能够同时获取网络和内存数据进行关联分析 **其他选项错误的原因：** - 选项A：基础监控只提供5分钟间隔，不满足60秒要求；且MemoryBytesUsed不是EC2内置指标 - 选项B：虽然详细监控满足时间要求，但MemoryBytesUsed不是EC2的内置指标，无法直接获取 - 选项D：基础监控的5分钟间隔不满足60秒的要求 **决策标准和最佳实践：** 1. 根据监控间隔需求选择基础监控（5分钟）或详细监控（1分钟） 2. 区分EC2内置指标（CPU、网络、磁盘）和需要Agent收集的系统指标（内存、进程等） 3. 对于系统级监控，CloudWatch Agent是获取详细操作系统指标的标准方案 4. 成本考虑：详细监控和自定义指标会产生额外费用，但对于生产环境监控是必要的</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">301</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Systems Manager to manage a fleet of Amazon Linux EC2 instances that have SSM Agent installed. All EC2 instances are configured to use Instance Metadata Service Version 2 (IMDSv2) and are running in the same AWS account and AWS Region. Company policy requires developers to use only Amazon Linux. The company wants to ensure that all new EC2 instances are automatically managed by Systems Manager after creation. Which solution will meet these requirements with the MOST operational efficiency? AmazonSSMManagedInstanceCore policy to the role. Ensure that AWS Config is set up. Use the ec2-instance-profile-attached managed AWS Config rule to validate if an EC2 instance has the role attached. Configure the rule to run on EC2 configuration changes. Configure automatic remediation for the rule to run the AWS-SetupManagedRoleOnEc2Instance SSM document to attach the role to the EC2 instance.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an IAM role that has a trust policy that allows Systems Manager to assume the role. Attach the AmazonSSMManagedEC2InstanceDefaultPolicy policy to the role. Configure the default-ec2-instance-management-role SSM service setting to use the role.
B. Ensure that AWS Config is set up. Create an AWS Config rule that validates if an EC2 instance has SSM Agent installed. Configure the rule to run on EC2 configuration changes. Configure automatic remediation for the rule to run the AWS-InstallSSMAgent SSM document to install SSM Agent.
C. Configure Systems Manager Patch Manager. Create a patch baseline that automatically installs SSM Agent on all new EC2 instances. Create a patch group for all EC2 instances. Attach the patch baseline to the patch group. Create a maintenance window and maintenance window task to start installing SSM Agent daily.
D. Create an EC2 instance role that has a trust policy that allows Amazon EC2 to assume the role. Attach the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Systems Manager来管理一组已安装SSM Agent的Amazon Linux EC2实例。所有EC2实例都配置为使用Instance Metadata Service Version 2 (IMDSv2)，并在同一个AWS账户和AWS区域中运行。公司政策要求开发人员只能使用Amazon Linux。公司希望确保所有新的EC2实例在创建后都能自动被Systems Manager管理。哪个解决方案能以最高的运营效率满足这些要求？ 选项： A. 创建一个IAM角色，该角色具有允许Systems Manager承担该角色的信任策略。将AmazonSSMManagedEC2InstanceDefaultPolicy策略附加到该角色。配置default-ec2-instance-management-role SSM服务设置以使用该角色。 B. 确保AWS Config已设置。创建一个AWS Config规则来验证EC2实例是否安装了SSM Agent。配置规则在EC2配置更改时运行。为规则配置自动修复，运行AWS-InstallSSMAgent SSM文档来安装SSM Agent。 C. 配置Systems Manager Patch Manager。创建一个补丁基线，自动在所有新EC2实例上安装SSM Agent。为所有EC2实例创建补丁组。将补丁基线附加到补丁组。创建维护窗口和维护窗口任务，每天开始安装SSM Agent。 D. 创建一个EC2实例角色，该角色具有允许Amazon EC2承担该角色的信任策略。将AmazonSSMManagedInstanceCore策略附加到该角色。确保AWS Config已设置。使用ec2-instance-profile-attached托管AWS Config规则来验证EC2实例是否附加了该角色。配置规则在EC2配置更改时运行。为规则配置自动修复，运行AWS-SetupManagedRoleOnEc2Instance SSM文档将角色附加到EC2实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到最具运营效率的解决方案，确保所有新创建的EC2实例都能自动被Systems Manager管理。关键词是&quot;自动&quot;和&quot;最高运营效率&quot;。 **涉及的关键AWS服务和概念：** - AWS Systems Manager (SSM)：用于管理EC2实例的服务 - SSM Agent：已经安装在Amazon Linux实例上 - IAM角色和策略：控制权限访问 - default-ec2-instance-management-role：SSM的默认实例管理角色设置 - AWS Config：配置合规性监控服务 - Systems Manager Patch Manager：补丁管理服务 **正确答案A的原因：** 1. **自动化程度最高**：通过配置default-ec2-instance-management-role服务设置，所有新创建的EC2实例都会自动获得指定的IAM角色，无需人工干预 2. **运营效率最佳**：一次性配置后，系统会自动处理所有新实例，无需额外的监控、规则或修复流程 3. **符合AWS最佳实践**：使用AWS原生的默认实例管理功能，这是AWS推荐的标准做法 4. **权限适当**：AmazonSSMManagedEC2InstanceDefaultPolicy提供了SSM管理所需的完整权限 **其他选项错误的原因：** - **选项B**：SSM Agent已经安装在Amazon Linux上，不需要再次安装；且使用Config规则进行监控和修复增加了不必要的复杂性 - **选项C**：Patch Manager是用于补丁管理的，不是用来安装SSM Agent的；且SSM Agent已经预装在Amazon Linux上 - **选项D**：虽然技术上可行，但需要设置Config规则、监控和自动修复流程，运营复杂度高，效率低于选项A **决策标准和最佳实践：** 1. **优先选择AWS原生自动化功能**：default-ec2-instance-management-role是AWS专门为此场景设计的功能 2. **最小化运营开销**：避免不必要的监控、规则和修复流程 3. **利用现有资源**：Amazon Linux已预装SSM Agent，无需重复安装 4. **遵循最少权限原则**：使用适当的托管策略而非过度权限</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">302</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company configured an Amazon S3 event source for an AWS Lambda function. The company needs the Lambda function to run when a new object is created or an existing object is modified in a specific S3 bucket. The Lambda function will use the S3 bucket name and the S3 object key of the incoming event to read the contents of the new or modified S3 object. The Lambda function will parse the contents and save the parsed contents to an Amazon DynamoDB table. The Lambda function&#x27;s execution role has permissions to read from the S3 bucket and to write to the DynamoDB table. During testing, a DevOps engineer discovers that the Lambda function does not run when objects are added to the S3 bucket or when existing objects are modified. Which solution will resolve these problems?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an S3 bucket policy for the S3 bucket that grants the S3 bucket permission to invoke the Lambda function.
B. Create a resource policy for the Lambda function to grant Amazon S3 permission to invoke the Lambda function on the S3 bucket.
C. Configure an Amazon Simple Queue Service (Amazon SQS) queue as an OnFailure destination for the Lambda function. Update the Lambda function to process messages from the SQS queue and the S3 event notifications.
D. Configure an Amazon Simple Queue Service (Amazon SQS) queue as the destination for the S3 bucket event notifications. Update the Lambda function&#x27;s execution role to have permission to read from the SQS queue. Update the Lambda function to consume messages from the SQS queue.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司为AWS Lambda函数配置了Amazon S3事件源。该公司需要Lambda函数在特定S3存储桶中创建新对象或修改现有对象时运行。Lambda函数将使用传入事件的S3存储桶名称和S3对象键来读取新建或修改的S3对象内容。Lambda函数将解析内容并将解析后的内容保存到Amazon DynamoDB表中。Lambda函数的执行角色具有从S3存储桶读取和向DynamoDB表写入的权限。在测试过程中，DevOps工程师发现当对象添加到S3存储桶或修改现有对象时，Lambda函数不会运行。哪个解决方案能解决这些问题？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题的核心是解决S3事件触发Lambda函数失败的问题。尽管已经配置了S3事件源和Lambda函数的执行权限，但Lambda函数无法被S3事件正常触发。 **涉及的关键AWS服务和概念：** 1. Amazon S3事件通知机制 2. AWS Lambda函数和资源策略 3. IAM权限模型中的资源策略vs身份策略 4. S3到Lambda的直接集成 5. Amazon SQS作为事件目标的替代方案 **正确答案B的原因：** Lambda函数需要资源策略来允许S3服务调用它。这是AWS权限模型的关键概念： - 执行角色（身份策略）控制Lambda函数能访问什么资源 - 资源策略控制谁能调用Lambda函数 - 当S3尝试调用Lambda时，需要Lambda函数的资源策略明确授权S3服务这个权限 - 这是跨服务调用的标准安全要求 **其他选项错误的原因：** - 选项A：S3存储桶策略控制对存储桶资源的访问，不能授予调用Lambda函数的权限，这是概念混淆 - 选项C：配置OnFailure目标是处理Lambda执行失败后的情况，但问题是Lambda根本没有被触发，不是执行失败 - 选项D：虽然SQS可以作为替代方案，但这改变了整个架构，增加了不必要的复杂性，而且没有解决根本的权限问题 **决策标准和最佳实践：** 1. 理解AWS权限模型的双向性：身份策略+资源策略 2. 对于跨服务调用，优先检查资源策略配置 3. 选择最简单直接的解决方案，避免过度工程化 4. S3直接触发Lambda是常见模式，应该优先使用而不是引入额外的中间件</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">303</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company recently configured AWS Control Tower in its organization in AWS Organizations. The company enrolled all existing AWS accounts in AWS Control Tower. The company wants to ensure that all new AWS accounts are automatically enrolled in AWS Control Tower. The company has an existing AWS Step Functions workflow that creates new AWS accounts and performs any actions required as part of account creation. The Step Functions workflow is defined in the same AWS account as AWS Control Tower. Which combination of steps should the company add to the Step Functions workflow to meet these requirements? (Choose two.) new AWS account to the detail field of the event.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge event that has an aws.controltower source and a CreateManagedAccount detail-type. Add the details of the new AWS account to the detail field of the event.
B. Create an Amazon EventBridge event that has an aws.controltower source and a SetupLandingZone detail-type. Add the details of the
C. Create an AWSControlTowerExecution role in the new AWS account. Configure the role to allow the AWS Control Tower administrator account to assume the role.
D. Call the AWS Service Catalog ProvisionProduct API operation with the details of the new AWS account.
E. Call the Organizations EnableAWSServiceAccess API operation with the controltower.amazonaws.com service name and the details of the new AWS account.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司最近在其AWS Organizations组织中配置了AWS Control Tower。该公司将所有现有的AWS账户都注册到了AWS Control Tower中。该公司希望确保所有新的AWS账户都能自动注册到AWS Control Tower中。该公司有一个现有的AWS Step Functions工作流，用于创建新的AWS账户并执行账户创建过程中所需的任何操作。Step Functions工作流定义在与AWS Control Tower相同的AWS账户中。公司应该在Step Functions工作流中添加哪些步骤组合来满足这些要求？（选择两个） 选项： A. 创建一个Amazon EventBridge事件，具有aws.controltower源和CreateManagedAccount详细类型。将新AWS账户的详细信息添加到事件的detail字段中。 B. 创建一个Amazon EventBridge事件，具有aws.controltower源和SetupLandingZone详细类型。将新AWS账户的详细信息添加到detail字段中。 C. 在新AWS账户中创建AWSControlTowerExecution角色。配置该角色以允许AWS Control Tower管理员账户承担该角色。 D. 使用新AWS账户的详细信息调用AWS Service Catalog ProvisionProduct API操作。 E. 使用controltower.amazonaws.com服务名称和新AWS账户的详细信息调用Organizations EnableAWSServiceAccess API操作。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在现有的Step Functions工作流中添加步骤，使新创建的AWS账户能够自动注册到AWS Control Tower中。关键是要理解AWS Control Tower的账户注册机制。 **涉及的关键AWS服务和概念：** - AWS Control Tower：用于设置和管理多账户AWS环境的服务 - AWS Step Functions：工作流编排服务 - AWS Service Catalog：管理和部署AWS资源的服务 - AWS Organizations：多账户管理服务 - Amazon EventBridge：事件驱动架构服务 **正确答案的原因：** 选项D是正确的，因为AWS Control Tower通过AWS Service Catalog来管理账户的创建和注册。当需要将新账户注册到Control Tower时，实际上是在调用Service Catalog的ProvisionProduct API来预配置一个&quot;Account Factory&quot;产品。这是AWS Control Tower的标准机制，Account Factory是一个预定义的Service Catalog产品，专门用于创建和配置符合Control Tower要求的AWS账户。 题目提到需要选择两个答案，但只给出了一个正确答案D。根据AWS Control Tower的工作原理，另一个正确答案很可能是选项C，因为新账户需要有适当的IAM角色来允许Control Tower进行管理。 **其他选项错误的原因：** - 选项A：CreateManagedAccount不是Control Tower的标准EventBridge事件类型，Control Tower主要通过Service Catalog而不是直接的EventBridge事件来管理账户 - 选项B：SetupLandingZone是用于设置整个Landing Zone的操作，不是用于注册单个账户的 - 选项E：EnableAWSServiceAccess是在组织级别启用服务访问的API，不是用于注册特定账户到Control Tower的 **决策标准和最佳实践：** 1. 理解AWS Control Tower使用Service Catalog Account Factory来管理账户生命周期 2. 新账户注册到Control Tower需要通过标准的ProvisionProduct API调用 3. 确保新账户具有适当的IAM角色以支持Control Tower的管理功能 4. 遵循AWS Control Tower的标准工作流程，而不是尝试使用非标准的EventBridge事件或API调用</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">304</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s web application uses an Application Load Balancer (ALB) to direct traffic to Amazon EC2 instances across three Availability Zones. The company has deployed a newer version of the application to one Availability Zone for testing. If a problem is detected with the application, the company wants to direct traffic away from the affected Availability Zone until the deployment has been rolled back. The application must remain available and maintain static stability during the rollback. Which solution will meet these requirements with the MOST operational efficiency?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Disable cross-zone load balancing on the ALB&#x27;s target group. Initiate a zonal shift on the ALB to direct traffic away from the affected Availability Zone.
B. Disable cross-zone load balancing on the ALB&#x27;s target group. Manually remove instances in the target group that belong to the affected Availability Zone.
C. Configure cross-zone load balancing on the ALB&#x27;s target group to inherit settings from the ALB. Initiate a zonal shift on the ALB to direct traffic away from the affected Availability Zone.
D. Configure cross-zone load balancing on the ALB&#x27;s target group to inherit settings from the ALB. Remove the subnet that is associated with the affected Availability Zone.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的Web应用程序使用Application Load Balancer (ALB)将流量分发到跨三个Availability Zone的Amazon EC2实例。该公司已将应用程序的新版本部署到一个Availability Zone进行测试。如果检测到应用程序存在问题，公司希望将流量从受影响的Availability Zone转移开，直到部署回滚完成。在回滚过程中，应用程序必须保持可用并维持静态稳定性。哪种解决方案能够以最高的运营效率满足这些要求？ 选项： A. 在ALB的target group上禁用cross-zone load balancing。在ALB上启动zonal shift以将流量从受影响的Availability Zone转移开。 B. 在ALB的target group上禁用cross-zone load balancing。手动移除target group中属于受影响Availability Zone的实例。 C. 配置ALB的target group上的cross-zone load balancing从ALB继承设置。在ALB上启动zonal shift以将流量从受影响的Availability Zone转移开。 D. 配置ALB的target group上的cross-zone load balancing从ALB继承设置。移除与受影响Availability Zone关联的subnet。 正确答案：A</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在检测到应用程序问题时，能够快速将流量从有问题的Availability Zone转移开，同时保持应用程序的可用性和静态稳定性，并且要求最高的运营效率。 **涉及的关键AWS服务和概念：** 1. Application Load Balancer (ALB) - 应用程序负载均衡器 2. Cross-zone load balancing - 跨可用区负载均衡 3. Zonal shift - 可用区转移功能 4. Target group - 目标组 5. Static stability - 静态稳定性（系统在故障时不会进一步恶化） **正确答案A的原因：** 1. **禁用cross-zone load balancing**：确保流量只在各自的Availability Zone内分发，避免跨区域的复杂性 2. **使用zonal shift功能**：这是AWS提供的自动化功能，可以快速、安全地将流量从指定的Availability Zone转移开 3. **运营效率最高**：zonal shift是一键操作，无需手动干预多个组件 4. **保持静态稳定性**：zonal shift是AWS设计用于故障转移的功能，不会引入额外的系统不稳定性 **其他选项错误的原因：** - **选项B**：手动移除实例的方式运营效率低，需要逐个操作，容易出错，且恢复时也需要手动添加回来 - **选项C**：启用cross-zone load balancing会增加复杂性，在故障场景下可能导致流量仍然路由到有问题的区域 - **选项D**：移除subnet是一个更激进和复杂的操作，可能影响其他服务，且恢复复杂度高 **决策标准和最佳实践：** 1. **运营效率优先**：选择自动化程度最高的解决方案 2. **最小影响原则**：使用专门设计的故障转移功能而非手动操作 3. **快速恢复能力**：zonal shift可以快速撤销，便于问题解决后恢复正常 4. **静态稳定性**：避免在故障期间进行可能引入新问题的复杂配置更改</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">305</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has several AWS accounts. An Amazon Connect instance runs in each account. The company uses an Amazon EventBridge default event bus in each account for event handling. A DevOps team needs to receive all the Amazon Connect events in a single DevOps account. Which solution meets these requirements? the accounts. Configure an EventBridge rule in each account that matches Amazon Connect events and has a target of the DevOps account&#x27;s default event bus.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the resource-based policy of the default event bus in each account to allow the DevOps account to replay events. Configure an EventBridge rule in the DevOps account that matches Amazon Connect events and has a target of the default event bus in the other accounts.
B. Update the resource-based policy of the default event bus in each account to allow the DevOps account to receive events. Configure an EventBridge rule in the DevOps account that matches Amazon Connect events and has a target of the default event bus in the other accounts.
C. Update the resource-based policy of the default event bus in the DevOps account. Update the policy to allow events to be received from
D. Update the resource-based policy of the default event bus in the DevOps account. Update the policy to allow events to be replayed by the accounts. Configure an EventBridge rule in each account that matches Amazon Connect events and has a target of the DevOps account&#x27;s default event bus.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有多个AWS账户。每个账户中都运行着一个Amazon Connect实例。该公司在每个账户中使用Amazon EventBridge默认事件总线进行事件处理。DevOps团队需要在单个DevOps账户中接收所有的Amazon Connect事件。哪种解决方案满足这些要求？ 选项： A. 更新每个账户中默认事件总线的基于资源的策略，允许DevOps账户重放事件。在DevOps账户中配置EventBridge规则，匹配Amazon Connect事件并以其他账户的默认事件总线为目标。 B. 更新每个账户中默认事件总线的基于资源的策略，允许DevOps账户接收事件。在DevOps账户中配置EventBridge规则，匹配Amazon Connect事件并以其他账户的默认事件总线为目标。 C. 更新DevOps账户中默认事件总线的基于资源的策略。更新策略以允许从这些账户接收事件。在每个账户中配置EventBridge规则，匹配Amazon Connect事件并以DevOps账户的默认事件总线为目标。 D. 更新DevOps账户中默认事件总线的基于资源的策略。更新策略以允许这些账户重放事件。在每个账户中配置EventBridge规则，匹配Amazon Connect事件并以DevOps账户的默认事件总线为目标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要将多个AWS账户中的Amazon Connect事件集中到单个DevOps账户中进行统一处理，实现跨账户的事件聚合。 **涉及的关键AWS服务和概念：** - Amazon EventBridge：AWS的事件路由服务，支持跨账户事件传输 - 跨账户事件传输：需要配置资源策略和事件规则 - 资源基于策略（Resource-based Policy）：控制谁可以访问EventBridge事件总线 - EventBridge规则：定义事件匹配条件和目标 **正确答案C的原因：** 1. **正确的权限配置方向**：在目标账户（DevOps账户）配置资源策略，允许源账户向其发送事件，这是跨账户EventBridge的标准做法 2. **正确的事件流向**：从源账户（有Amazon Connect的账户）推送事件到目标账户（DevOps账户），符合事件驱动架构的最佳实践 3. **规则配置位置正确**：在源账户配置规则来捕获和转发Amazon Connect事件，这样可以在事件产生的地方进行过滤和路由 **其他选项错误的原因：** - **选项A和B**：错误地在DevOps账户配置规则去拉取其他账户的事件，这不是EventBridge的工作方式。EventBridge是推送模式，不是拉取模式 - **选项D**：使用了&quot;重放事件&quot;概念，这不是标准的跨账户事件传输方式，而且重放通常用于事件回放场景 **决策标准和最佳实践：** 1. **推送vs拉取**：EventBridge采用推送模式，事件在源头被规则捕获并推送到目标 2. **权限最小化原则**：只在目标账户配置接收权限，在源账户配置发送规则 3. **事件流向清晰**：从事件产生地（源账户）流向事件处理地（DevOps账户） 4. **架构简洁性**：避免复杂的双向权限配置，采用单向的事件流设计</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">306</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has deployed an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 node groups. The company&#x27;s DevOps team uses the Kubernetes Horizontal Pod Autoscaler and recently installed a supported EKS cluster Autoscaler. The DevOps team needs to implement a solution to collect metrics and logs of the EKS cluster to establish a baseline for performance. The DevOps team will create an initial set of thresholds for specific metrics and will update the thresholds over time as the cluster is used. The DevOps team must receive an Amazon Simple Notification Service (Amazon SNS) email notification if the initial set of thresholds is exceeded or if the EKS cluster Autoscaler is not functioning properly. The solution must collect cluster, node, and pod metrics. The solution also must capture logs in Amazon CloudWatch. Which combination of steps should the DevOps team take to meet these requirements? (Choose three.) F. Create a CloudWatch alarm to monitor a metric log filter of the Autoscaler deployments for errors. Configure the alarm to send an SNS email notification to the DevOps team if thresholds are exceeded.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Deploy the CloudWatch agent and Fluent Bit to the cluster. Ensure that the EKS cluster has appropriate permissions to send metrics and logs to CloudWatch.
B. Deploy AWS Distro for OpenTelemetry to the cluster. Ensure that the EKS cluster has appropriate permissions to send metrics and logs to CloudWatch.
C. Create CloudWatch alarms to monitor the CPU, memory, and node failure metrics of the cluster. Configure the alarms to send an SNS email notification to the DevOps team if thresholds are exceeded.
D. Create a CloudWatch composite alarm to monitor a metric log filter of the CPU, memory, and node metrics of the cluster. Configure the alarm to send an SNS email notification to the DevOps team when anomalies are detected.
E. Create a CloudWatch alarm to monitor the logs of the Autoscaler deployments for errors. Configure the alarm to send an SNS email notification to the DevOps team if thresholds are exceeded.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司已经部署了一个带有Amazon EC2节点组的Amazon Elastic Kubernetes Service (Amazon EKS)集群。公司的DevOps团队使用Kubernetes Horizontal Pod Autoscaler，并且最近安装了一个受支持的EKS cluster Autoscaler。DevOps团队需要实施一个解决方案来收集EKS集群的指标和日志，以建立性能基线。DevOps团队将为特定指标创建一组初始阈值，并随着集群的使用逐步更新这些阈值。如果超过初始阈值或EKS cluster Autoscaler无法正常工作，DevOps团队必须收到Amazon Simple Notification Service (Amazon SNS)邮件通知。该解决方案必须收集集群、节点和pod指标，还必须在Amazon CloudWatch中捕获日志。DevOps团队应该采取哪些步骤组合来满足这些要求？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为EKS集群建立完整的监控解决方案，包括：1）收集集群、节点和pod的指标；2）捕获日志到CloudWatch；3）建立性能基线和阈值监控；4）监控Autoscaler的运行状态；5）通过SNS发送告警通知。 **涉及的关键AWS服务和概念：** - Amazon EKS：托管的Kubernetes服务 - CloudWatch：监控和日志服务，用于存储指标和日志 - CloudWatch Agent：收集系统级指标的代理 - Fluent Bit：轻量级日志处理器，专门用于容器环境 - AWS Distro for OpenTelemetry：开源可观测性框架 - CloudWatch Alarms：基于指标的告警机制 - SNS：通知服务 **正确答案的原因：** 选项A是正确的，因为： 1. CloudWatch Agent专门用于收集系统级指标（CPU、内存、磁盘等），能够满足收集集群和节点指标的需求 2. Fluent Bit是AWS推荐的容器日志收集工具，轻量级且专为Kubernetes环境优化 3. 这个组合能够全面收集cluster、node和pod级别的指标和日志 4. 需要适当的IAM权限确保数据能够发送到CloudWatch **其他选项错误的原因：** - 选项B：AWS Distro for OpenTelemetry主要用于分布式追踪和应用程序指标，不如CloudWatch Agent + Fluent Bit组合在基础设施监控方面全面 - 选项C：虽然创建CloudWatch告警是必要的，但这个选项没有解决数据收集的根本问题 - 选项D：复合告警和异常检测比较复杂，不适合建立初始基线的场景 - 选项E：监控Autoscaler日志是需要的，但应该基于日志指标过滤器而不是直接监控日志 **决策标准和最佳实践：** 1. 优先选择AWS原生和推荐的监控工具组合 2. 确保解决方案能够覆盖所有要求的监控层面（集群、节点、pod） 3. 选择轻量级、专门为容器环境设计的工具 4. 建立适当的权限模型确保数据收集的安全性 5. 从基础监控开始，逐步建立更复杂的告警机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">307</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company discovers that its production environment and disaster recovery (DR) environment are deployed to the same AWS Region. All the production applications run on Amazon EC2 instances and are deployed by AWS CloudFormation. The applications use an Amazon FSx for NetApp ONTAP volume for application storage. No application data resides on the EC2 instances. A DevOps engineer copies the required AMIs to a new DR Region. The DevOps engineer also updates the CloudFormation code to accept a Region as a parameter. The storage needs to have an RPO of 10 minutes in the DR Region. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon S3 bucket in both Regions. Configure S3 Cross-Region Replication (CRR) for the S3 buckets. Create a scheduled AWS Lambda function to copy any new content from the FSx for ONTAP volume to the S3 bucket in the production Region.
B. Use AWS Backup to create a backup vault and a custom backup plan that has a 10-minute frequency. Specify the DR Region as the target Region. Assign the EC2 instances in the production Region to the backup plan.
C. Create an AWS Lambda function to create snapshots of the instance store volumes that are attached to the EC2 instances. Configure the Lambda function to copy the snapshots to the DR Region and to remove the previous copies. Create an Amazon EventBridge scheduled rule that invokes the Lambda function every 10 minutes.
D. Create an FSx for ONTAP instance in the DR Region. Configure a 5-minute schedule for a volume-level NetApp SnapMirror to replicate the volume from the production Region to the DR Region.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司发现其生产环境和灾难恢复(DR)环境部署在同一个AWS Region中。所有生产应用程序都运行在Amazon EC2实例上，并通过AWS CloudFormation部署。应用程序使用Amazon FSx for NetApp ONTAP卷作为应用程序存储。EC2实例上没有应用程序数据。DevOps工程师将所需的AMI复制到新的DR Region，并更新CloudFormation代码以接受Region作为参数。存储需要在DR Region中具有10分钟的RPO。哪种解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为使用Amazon FSx for NetApp ONTAP存储的应用程序建立跨区域灾难恢复方案，关键要求是实现10分钟的RPO（恢复点目标）。需要注意的是应用数据全部存储在FSx卷上，EC2实例本身不包含应用数据。 **涉及的关键AWS服务和概念：** - Amazon FSx for NetApp ONTAP：企业级NFS/SMB文件系统 - NetApp SnapMirror：NetApp原生的卷级复制技术 - RPO（Recovery Point Objective）：数据恢复点目标，即可接受的最大数据丢失时间 - AWS Backup：AWS托管的备份服务 - S3 Cross-Region Replication：S3跨区域复制 **正确答案D的原因：** 1. **技术匹配性**：NetApp SnapMirror是FSx for ONTAP的原生复制功能，专门设计用于卷级数据复制 2. **RPO满足**：5分钟的复制频率完全满足10分钟RPO要求，提供了额外的安全边际 3. **效率最优**：SnapMirror使用增量复制，只传输变更的数据块，网络效率高 4. **一致性保证**：提供应用程序一致的快照和复制 5. **恢复便利**：在DR区域直接创建可用的FSx卷，恢复时间短 **其他选项错误的原因：** - **选项A错误**：需要额外开发Lambda函数来同步FSx数据到S3，增加复杂性和潜在故障点，且无法保证严格的10分钟RPO - **选项B错误**：AWS Backup主要针对EC2实例备份，而题目明确说明应用数据不在EC2实例上，备份EC2实例无法保护FSx卷中的应用数据 - **选项C错误**：基于错误假设，EC2实例使用的不是instance store卷，且题目已说明无应用数据在EC2实例上 **决策标准和最佳实践：** 1. **服务原生功能优先**：优先使用AWS服务的原生功能而非自定义解决方案 2. **RPO/RTO平衡**：选择能满足业务连续性要求的最简单可靠方案 3. **数据一致性**：确保复制的数据在应用程序级别保持一致性 4. **运维简化**：减少需要维护的组件和潜在故障点</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">308</div>
        <div class="field-label">Question:</div>
        <div class="field-content">During a security audit, a company discovered that some security groups allow SSH traffic from 0.0.0.0/0. A security team must implement a solution to detect and remediate this issue as soon as possible. The company uses one organization in AWS Organizations to manage all the company&#x27;s AWS accounts. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable AWS Config for all AWS accounts. Use a periodic trigger to activate the vpc-sg-port-restriction-check AWS Config rule. Create an AWS Lambda function to remediate any noncompliant rules.
B. Create an AWS Lambda function in each AWS account to delete all the security group rules. Create an Amazon EventBridge rule to match security group update events or creation events. Set the Lambda function in each account as a target for the rule.
C. Enable AWS Config for all AWS accounts. Create a custom AWS Config rule to run on the restricted-ssh configuration change trigger. Configure the rule to invoke an AWS Lambda function to remediate any noncompliant resources.
D. Create an AWS Systems Manager Automation document in each account to inspect all security groups and to delete noncompliant rules. Use an Amazon EventBridge rule to run the Automation document every hour.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">在安全审计期间，一家公司发现某些security groups允许来自0.0.0.0/0的SSH流量。安全团队必须尽快实施一个解决方案来检测和修复这个问题。该公司使用AWS Organizations中的一个organization来管理所有公司的AWS账户。哪个解决方案能满足这些要求？ 选项： A. 为所有AWS账户启用AWS Config。使用周期性触发器来激活vpc-sg-port-restriction-check AWS Config规则。创建一个AWS Lambda函数来修复任何不合规的规则。 B. 在每个AWS账户中创建一个AWS Lambda函数来删除所有security group规则。创建一个Amazon EventBridge规则来匹配security group更新事件或创建事件。将每个账户中的Lambda函数设置为规则的目标。 C. 为所有AWS账户启用AWS Config。创建一个自定义AWS Config规则在restricted-ssh配置变更触发器上运行。配置规则调用AWS Lambda函数来修复任何不合规的资源。 D. 在每个账户中创建一个AWS Systems Manager Automation文档来检查所有security groups并删除不合规的规则。使用Amazon EventBridge规则每小时运行一次Automation文档。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实施一个解决方案来检测和修复允许从0.0.0.0/0进行SSH访问的security groups，需要在AWS Organizations管理的多账户环境中尽快部署。 **涉及的关键AWS服务和概念：** - AWS Config：配置管理和合规性监控服务 - AWS Organizations：多账户管理服务 - Security Groups：EC2安全组，控制入站和出站流量 - AWS Lambda：无服务器计算服务 - Amazon EventBridge：事件驱动架构服务 - AWS Systems Manager：运维管理服务 **正确答案A的原因：** 1. **现成的规则**：vpc-sg-port-restriction-check是AWS Config的预建规则，专门用于检测security groups中的端口限制问题，包括SSH端口22 2. **组织级部署**：AWS Config可以通过AWS Organizations在所有账户中统一启用和管理 3. **周期性检查**：periodic trigger确保定期扫描所有security groups，不会遗漏 4. **自动修复**：Lambda函数可以自动修复检测到的不合规配置 5. **成熟稳定**：使用AWS托管的规则比自定义规则更可靠 **其他选项错误的原因：** - **选项B**：过于激进，删除&quot;所有&quot;security group规则会破坏正常的网络访问，而且需要在每个账户单独部署 - **选项C**：restricted-ssh不是标准的AWS Config触发器，而且自定义规则增加了复杂性和维护成本 - **选项D**：基于时间的触发（每小时）响应不够及时，而且Systems Manager Automation在多账户管理方面不如Config方便 **决策标准和最佳实践：** 1. **使用AWS托管规则**：优先选择AWS预建的Config规则，减少自定义开发 2. **组织级管理**：在多账户环境中，选择支持集中管理的服务 3. **实时响应**：配置变更触发比定时触发更及时 4. **渐进式修复**：针对性修复不合规项，而不是大范围删除 5. **合规性监控**：AWS Config提供持续的合规性监控和历史记录</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">309</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s DevOps engineer must install a software package on 30 on-premises VMs and 15 Amazon EC2 instances. The DevOps engineer needs to ensure that all VMs receive the package in a process that is auditable and that any configuration drift on the VMs is automatically identified and alerted on. The company uses AWS Direct Connect to connect its on-premises data center to AWS. Which solution will meet these requirements with the MOST operational efficiency? the package is not found. Configure the script to send an email message notification to the system administrator if the package is not found. for configuration drift. Use Amazon Simple Notification Service (Amazon SNS) to notify the system administrator if any drift is found. noncompliant. Configure the script to send the list to the system administrator, who will install the package on the noncompliant VMs. changes. Write a script to monitor the on-premises resources.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Write a script that iterates through the list of VMs once a week. Configure the script to check for the package and install the package if
B. Install the AWS Systems Manager Agent (SSM Agent) on all VMs. Use the SSM Agent to install the package. Use AWS Config to monitor
C. Write a script that checks if the package is installed across the environment. Configure the script to create a list of all VMs that are
D. Log in to each VM. Use a local package manager to install the package. Use AWS Config to monitor the AWS resources for configuration</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的DevOps工程师必须在30台本地虚拟机和15台Amazon EC2实例上安装一个软件包。DevOps工程师需要确保所有虚拟机都能通过可审计的流程接收到该软件包，并且能够自动识别和告警虚拟机上的任何配置漂移。该公司使用AWS Direct Connect连接其本地数据中心到AWS。哪种解决方案能以最高的运营效率满足这些要求？ 选项： A. 编写一个脚本，每周遍历虚拟机列表一次。配置脚本检查软件包，如果未找到软件包则安装。如果未找到软件包，配置脚本向系统管理员发送电子邮件通知。 B. 在所有虚拟机上安装AWS Systems Manager Agent (SSM Agent)。使用SSM Agent安装软件包。使用AWS Config监控配置漂移。如果发现任何漂移，使用Amazon Simple Notification Service (Amazon SNS)通知系统管理员。 C. 编写一个脚本检查软件包是否在整个环境中安装。配置脚本创建所有不合规虚拟机的列表。配置脚本将列表发送给系统管理员，由其在不合规的虚拟机上安装软件包。 D. 登录到每台虚拟机。使用本地包管理器安装软件包。使用AWS Config监控AWS资源的配置变更。编写脚本监控本地资源。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在混合环境（本地和云端）中高效地管理软件包安装，同时满足三个关键需求：1）可审计的安装过程；2）自动检测配置漂移；3）最高的运营效率。 **涉及的关键AWS服务和概念：** - AWS Systems Manager (SSM)：统一的运维管理服务，支持混合环境 - SSM Agent：在实例上运行的代理，使Systems Manager能够管理实例 - AWS Config：配置管理和合规性监控服务 - Amazon SNS：通知服务 - AWS Direct Connect：专用网络连接服务 **正确答案B的原因：** 1. **统一管理**：SSM Agent支持本地和EC2实例的统一管理，无需区别对待 2. **可审计性**：Systems Manager提供完整的操作日志和审计跟踪 3. **自动化程度高**：可以批量执行命令和安装软件包，无需手动干预 4. **配置漂移检测**：AWS Config能够持续监控配置状态并自动检测漂移 5. **及时通知**：通过SNS实现自动化告警，响应迅速 6. **运营效率最高**：一次性设置后可实现持续的自动化管理 **其他选项错误的原因：** - **选项A**：依赖定期脚本执行，频率低，不够实时，且缺乏持续的配置监控 - **选项C**：仍需要人工干预安装软件包，运营效率低，不够自动化 - **选项D**：完全手动操作，效率极低，且AWS Config无法直接监控本地资源 **决策标准和最佳实践：** 1. **混合环境管理**：优先选择支持统一管理本地和云端资源的服务 2. **自动化优先**：减少人工干预，提高运营效率和一致性 3. **持续监控**：选择能够提供实时监控和告警的解决方案 4. **可审计性**：确保所有操作都有完整的日志记录 5. **扩展性考虑**：选择能够轻松扩展到更多实例的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">310</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an AWS CodePipeline pipeline in the eu-west-1 Region. The pipeline stores the build artifacts in an Amazon S3 bucket. The pipeline builds and deploys an AWS Lambda function by using an AWS CloudFormation deploy action. A DevOps engineer needs to update the existing pipeline to also deploy the Lambda function to the us-east-1 Region. The pipeline has already been updated to create an additional artifact to deploy to us-east-1. Which combination of steps should the DevOps engineer take to meet these requirements? (Choose two.) in the pipeline. Configure the new deploy action to use the CloudFormation template from the us-east-1 artifact.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the CloudFormation template to include a parameter for the Lambda function code&#x27;s .zip file location. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to pass in the us-east-1 artifact location as a parameter override.
B. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the additional artifact that was created for us-east-1.
C. Create an S3 bucket in us-east-1. Configure the S3 bucket policy to allow CodePipeline to have read and write access.
D. Create an S3 bucket in us-east-1. Configure S3 Cross-Region Replication (CRR) from the S3 bucket in eu-west-1 to the S3 bucket in us-east-1.
E. Modify the pipeline to include the S3 bucket for us-east-1 as an artifact store. Create a new CloudFormation deploy action for us-east-1</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在eu-west-1区域有一个AWS CodePipeline流水线。该流水线将构建工件存储在Amazon S3存储桶中。流水线使用AWS CloudFormation部署操作来构建和部署AWS Lambda函数。DevOps工程师需要更新现有流水线，以便也将Lambda函数部署到us-east-1区域。流水线已经更新为创建一个额外的工件来部署到us-east-1。DevOps工程师应该采取哪些步骤组合来满足这些要求？（选择两个） 选项： A. 修改CloudFormation模板以包含Lambda函数代码.zip文件位置的参数。为us-east-1在流水线中创建新的CloudFormation部署操作。配置新的部署操作，将us-east-1工件位置作为参数覆盖传入。 B. 为us-east-1在流水线中创建新的CloudFormation部署操作。配置新的部署操作使用为us-east-1创建的额外工件中的CloudFormation模板。 C. 在us-east-1创建S3存储桶。配置S3存储桶策略以允许CodePipeline具有读写访问权限。 D. 在us-east-1创建S3存储桶。配置从eu-west-1的S3存储桶到us-east-1的S3存储桶的S3跨区域复制(CRR)。 E. 修改流水线以包含us-east-1的S3存储桶作为工件存储。为us-east-1创建新的CloudFormation部署操作。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在现有的单区域CodePipeline基础上，扩展为跨区域部署Lambda函数到us-east-1区域。关键是要理解跨区域部署的技术要求和最佳实践。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD流水线服务 - AWS CloudFormation：基础设施即代码服务 - AWS Lambda：无服务器计算服务 - Amazon S3：对象存储服务，用于存储工件 - 跨区域部署：将应用部署到多个AWS区域 - 工件存储：CodePipeline中存储构建产物的机制 **正确答案A的原因：** 选项A提供了正确的跨区域部署方法： 1. 修改CloudFormation模板添加参数来接收Lambda代码的S3位置，这样模板可以灵活地从不同区域的S3存储桶获取代码 2. 创建新的CloudFormation部署操作专门针对us-east-1区域 3. 通过参数覆盖传入正确的工件位置，确保部署操作能找到Lambda代码 **其他选项错误的原因：** - 选项B：虽然创建了新的部署操作，但没有解决跨区域工件访问的核心问题 - 选项C：仅创建S3存储桶和配置权限是不够的，没有解决工件同步问题 - 选项D：S3跨区域复制会增加复杂性和延迟，不是CodePipeline跨区域部署的推荐方式 - 选项E：描述不完整，缺少具体的配置细节 **决策标准和最佳实践：** 1. **工件管理**：CodePipeline需要在每个部署区域都有可访问的工件存储 2. **模板参数化**：CloudFormation模板应该参数化以支持不同区域的部署 3. **最小复杂性**：选择最简单有效的解决方案，避免不必要的复制机制 4. **区域独立性**：每个区域的部署应该相对独立，减少跨区域依赖 正确答案应该是A和E的组合，因为需要既要配置跨区域的工件存储(E)，又要正确配置CloudFormation模板参数(A)。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">311</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an AWS Cloud Development Kit (AWS CDK) application for its infrastructure. The AWS CDK application creates AWS Lambda functions and the IAM roles that are attached to the functions. The company also uses AWS Organizations. The company&#x27;s developers can assume the AWS CDK application deployment role. The company&#x27;s security team discovered that the developers and the role used to deploy the AWS CDK application have more permissions than necessary. The security team also discovered that the roles attached to the Lambda functions that the CDK application creates have more permissions than necessary. The developers must not have the ability to grant additional permissions. Which solution will meet these requirements with the LEAST operational overhead? permissions boundary to use the policy.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an SCP that denies the iam:CreateRole action and the iam:UpdateRole action for the developer role and the AWS CDK application deployment role. Centrally create new IAM roles to attach to the Lambda functions for the developers to use to provision Lambda functions.
B. Create an IAM permission boundary policy. Define the maximum actions that the AWS CDK application requires in the policy. Update the account&#x27;s AWS CDK bootstrapping to use the permission boundary. Update the configuration in the AWS CDK application for the default
C. Create an IAM permission boundary policy. Define the maximum actions that the AWS CDK application requires in the policy. Instruct the developers to use the permission boundary policy name when they create a role in the AWS CDK application code.
D. Create an SCP that denies the iam:CreateRole action and the iam:UpdateRole action for the developer role. Give the AWS CDK deployment role access to create roles associated with Lambda functions. Run AWS Identity and Access Management Access Analyzer to verify that the Lambda function&#x27;s role does not have permissions.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Cloud Development Kit (AWS CDK)应用程序来管理其基础设施。该AWS CDK应用程序创建AWS Lambda函数以及附加到这些函数的IAM角色。该公司还使用AWS Organizations。公司的开发人员可以担任AWS CDK应用程序部署角色。公司的安全团队发现开发人员和用于部署AWS CDK应用程序的角色拥有超出必要的权限。安全团队还发现CDK应用程序创建的附加到Lambda函数的角色也拥有超出必要的权限。开发人员不得具有授予额外权限的能力。哪种解决方案能够以最少的运营开销满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求解决权限过度授予的问题，需要限制开发人员和CDK部署角色的权限，同时确保Lambda函数角色也不会获得过多权限，并且要求最少的运营开销。 **涉及的关键AWS服务和概念：** - AWS CDK (Cloud Development Kit): 基础设施即代码工具 - IAM Permission Boundary: 权限边界，定义实体可获得的最大权限 - AWS Organizations和SCP (Service Control Policy): 组织级别的权限控制 - CDK Bootstrap: CDK的初始化配置过程 - IAM Access Analyzer: 权限分析工具 **正确答案B的原因：** 1. **权限边界机制**：Permission Boundary为所有通过CDK创建的角色提供了统一的权限上限控制 2. **CDK Bootstrap集成**：通过更新CDK bootstrapping配置，可以自动为所有新创建的角色应用权限边界 3. **自动化程度高**：一旦配置完成，所有后续的角色创建都会自动受到权限边界限制 4. **运营开销最小**：开发人员无需修改代码或手动指定策略，系统自动应用限制 **其他选项错误的原因：** - **选项A**：完全禁止角色创建会破坏CDK的正常功能，需要手动创建角色增加了大量运营开销 - **选项C**：依赖开发人员手动指定权限边界，容易出现人为错误且运营开销较大 - **选项D**：只限制开发人员但给CDK角色过多权限，仍存在安全风险，且需要持续使用Access Analyzer增加运营成本 **决策标准和最佳实践：** 1. **最小权限原则**：通过权限边界确保角色获得的权限不超过业务需要 2. **自动化优先**：选择能够自动应用安全控制的方案，减少人为干预 3. **深度防御**：在CDK层面设置权限边界比依赖开发人员自觉性更可靠 4. **运营效率**：优先选择一次配置、长期有效的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">312</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses Amazon Elastic Container Registry (Amazon ECR) private registries to store container images. A DevOps team needs to ensure that the container images are regularly scanned for software package vulnerabilities. Which solution will meet this requirement?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable enhanced scanning for private registries in Amazon ECR.
B. Enable basic continuous scanning for private registries in Amazon ECR.
C. Create an AWS Systems Manager Automation document to scan images by using the AWS SDK. Configure the Automation document to run when a new image is pushed to an ECR registry.
D. Create an AWS Lambda function that scans all images in Amazon ECR by using the AWS SDK. Create an Amazon EventBridge rule to invoke the Lambda function each day.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用Amazon Elastic Container Registry (Amazon ECR)私有注册表来存储容器镜像。DevOps团队需要确保容器镜像定期扫描软件包漏洞。哪个解决方案能满足这个要求？ 选项： A. 为Amazon ECR中的私有注册表启用增强扫描。 B. 为Amazon ECR中的私有注册表启用基础持续扫描。 C. 创建AWS Systems Manager Automation文档，使用AWS SDK扫描镜像。配置Automation文档在新镜像推送到ECR注册表时运行。 D. 创建AWS Lambda函数，使用AWS SDK扫描Amazon ECR中的所有镜像。创建Amazon EventBridge规则每天调用Lambda函数。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 题目要求为Amazon ECR私有注册表中的容器镜像提供定期的软件包漏洞扫描解决方案。 **涉及的关键AWS服务和概念：** - Amazon ECR：AWS的容器镜像注册表服务 - ECR漏洞扫描功能：包括基础扫描和增强扫描两种类型 - AWS Systems Manager Automation：自动化运维任务 - AWS Lambda：无服务器计算服务 - Amazon EventBridge：事件驱动服务 **正确答案A的原因：** - Enhanced scanning（增强扫描）是ECR提供的高级漏洞扫描功能 - 使用Amazon Inspector集成，提供更全面和准确的漏洞检测 - 支持持续扫描，当新的漏洞数据库更新时会重新扫描现有镜像 - 提供详细的漏洞报告和修复建议 - 是AWS原生功能，无需额外开发和维护 **其他选项错误的原因：** B. 基础持续扫描功能相对有限，检测能力不如增强扫描全面，且可能不满足&quot;定期扫描&quot;的完整要求 C. 使用Systems Manager需要自定义开发扫描逻辑，增加复杂性和维护成本，且可能无法获得与ECR原生扫描相同的漏洞数据库质量 D. Lambda方案需要自行实现扫描逻辑，缺乏专业的漏洞数据库支持，且每日扫描频率可能不够及时 **决策标准和最佳实践：** - 优先选择AWS原生安全功能而非自建解决方案 - 选择功能最全面的扫描选项以确保安全性 - 考虑维护成本和运营复杂度 - 增强扫描提供了最佳的安全覆盖范围和自动化程度</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">313</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A security team sets up a workflow that invokes an AWS Step Functions workflow when Amazon EventBridge matches specific events. The events can be generated by several AWS services. AWS CloudTrail records user activities. The security team notices that some important events do not invoke the workflow as expected. The CloudTrail logs do not indicate any direct errors related to the missing events. Which combination of steps will identify the root cause of the missing event invocations? (Choose three.) F. Verify that the Step Functions workflow has the correct permissions to be invoked by EventBridge.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable EventBridge schema discovery on the event bus to determine whether the event patterns match the expected schema.
B. Configure Amazon CloudWatch to monitor EventBridge metrics and Step Functions metrics. Set up alerts for anomalies in event patterns and workflow invocations.
C. Configure an AWS Lambda logging function to monitor and log events from EventBridge to provide more details about the processed events.
D. Review the Step Functions execution history for patterns of failures or timeouts that could correlate to the missing event invocations.
E. Review metrics for the EventBridge failed invocations to ensure that the IAM execution role that is attached to the rule has sufficient permissions.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">安全团队设置了一个工作流程，当Amazon EventBridge匹配特定事件时会调用AWS Step Functions工作流程。这些事件可以由多个AWS服务生成。AWS CloudTrail记录用户活动。安全团队注意到一些重要事件没有按预期调用工作流程。CloudTrail日志没有显示与缺失事件相关的任何直接错误。以下哪些步骤组合将识别缺失事件调用的根本原因？（选择三个。） 选项： A. 在事件总线上启用EventBridge架构发现，以确定事件模式是否与预期架构匹配。 B. 配置Amazon CloudWatch来监控EventBridge指标和Step Functions指标。为事件模式和工作流程调用中的异常设置警报。 C. 配置AWS Lambda日志记录函数来监控和记录来自EventBridge的事件，以提供有关已处理事件的更多详细信息。 D. 查看Step Functions执行历史记录，寻找可能与缺失事件调用相关的失败或超时模式。 E. 查看EventBridge失败调用的指标，以确保附加到规则的IAM执行角色具有足够的权限。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在EventBridge触发Step Functions工作流程失败时的故障排除方法。需要选择三个最有效的诊断步骤来识别为什么某些事件没有成功调用工作流程。 **涉及的关键AWS服务和概念：** - Amazon EventBridge：事件路由服务，用于匹配和转发事件 - AWS Step Functions：工作流程编排服务 - CloudWatch：监控和日志服务 - IAM权限：服务间调用的权限控制 - 事件模式匹配：EventBridge如何识别和处理事件 **正确答案的原因：** 题目要求选择三个选项，但只给出了B作为正确答案。基于AWS故障排除最佳实践，应该选择B、D、E： - **选项B**：CloudWatch监控是诊断的基础，能够提供EventBridge和Step Functions的关键指标，包括调用次数、失败率等 - **选项D**：Step Functions执行历史记录能显示工作流程是否被触发以及执行状态 - **选项E**：IAM权限问题是最常见的调用失败原因，检查失败调用指标和权限配置至关重要 **其他选项错误的原因：** - **选项A**：架构发现主要用于开发阶段了解事件结构，对于已配置的生产环境故障排除帮助有限 - **选项C**：虽然Lambda可以提供额外日志，但这增加了系统复杂性，且CloudWatch已能提供必要的监控信息 **决策标准和最佳实践：** 1. **监控优先**：使用CloudWatch获取系统级指标和告警 2. **权限检查**：IAM权限问题是跨服务调用失败的主要原因 3. **执行历史分析**：查看目标服务的执行记录确认是否收到调用 4. **系统化诊断**：从事件源到目标服务的完整链路分析 5. **避免过度复杂化**：优先使用AWS原生监控工具而非自定义解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">314</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s DevOps engineer uses AWS Systems Manager to perform maintenance tasks. The company has a few Amazon EC2 instances that require a restart after notifications from AWS Health. The DevOps engineer must implement an automated solution that uses Amazon EventBridge to remediate the notifications during the company&#x27;s scheduled maintenance windows. How should the DevOps engineer configure an EventBridge rule to meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure an event source of AWS Health. Configure event types that indicate scheduled instance termination and retirement. Target the AWS-RestartEC2Instance Systems Manager Automation runbook to restart the EC2 instances.
B. Configure an event source of Systems Manager. Configure an event type that indicates a maintenance window. Target the AWS-RestartEC2Instance Systems Manager Automation runbook to restart the EC2 instances.
C. Configure an event source of AWS Health. Configure event types that indicate scheduled instance termination and retirement. Target a newly created AWS Lambda function that registers a Systems Manager maintenance window task to restart the EC2 instances.
D. Configure an event source of EC2. Configure an event type that indicates instance state notification. Target a newly created AWS Lambda function that registers a Systems Manager maintenance window task to restart the EC2 instances.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的DevOps工程师使用AWS Systems Manager来执行维护任务。该公司有一些Amazon EC2实例在收到AWS Health通知后需要重启。DevOps工程师必须实现一个自动化解决方案，使用Amazon EventBridge在公司的计划维护窗口期间修复这些通知。DevOps工程师应该如何配置EventBridge规则来满足这些要求？ 选项：A. 配置AWS Health作为事件源。配置指示计划实例终止和退役的事件类型。目标设为AWS-RestartEC2Instance Systems Manager Automation runbook来重启EC2实例。 B. 配置Systems Manager作为事件源。配置指示维护窗口的事件类型。目标设为AWS-RestartEC2Instance Systems Manager Automation runbook来重启EC2实例。 C. 配置AWS Health作为事件源。配置指示计划实例终止和退役的事件类型。目标设为新创建的AWS Lambda函数，该函数注册一个Systems Manager维护窗口任务来重启EC2实例。 D. 配置EC2作为事件源。配置指示实例状态通知的事件类型。目标设为新创建的AWS Lambda函数，该函数注册一个Systems Manager维护窗口任务来重启EC2实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现一个自动化解决方案，当AWS Health发出EC2实例需要重启的通知时，能够在维护窗口期间自动重启这些实例。关键需求包括：1）响应AWS Health通知；2）在维护窗口期间执行；3）自动重启EC2实例。 **涉及的关键AWS服务和概念：** - AWS Health：提供AWS资源和服务的健康状态通知，包括计划维护事件 - Amazon EventBridge：事件驱动的服务，用于连接应用程序与来自各种源的数据 - AWS Systems Manager：提供运维管理功能，包括Automation runbook和维护窗口 - AWS-RestartEC2Instance：预定义的Systems Manager Automation文档，用于重启EC2实例 **正确答案A的原因：** 1. **正确的事件源**：AWS Health是发出实例维护通知的正确源头 2. **适当的事件类型**：计划实例终止和退役事件正是需要响应的通知类型 3. **直接高效的解决方案**：直接调用AWS-RestartEC2Instance runbook，无需额外的Lambda函数 4. **符合维护窗口要求**：可以配置EventBridge规则在特定时间窗口触发 **其他选项错误的原因：** - **选项B**：事件源错误，Systems Manager本身不会发出需要重启实例的初始通知 - **选项C**：虽然事件源正确，但增加了不必要的复杂性，Lambda函数再注册维护窗口任务是多余的步骤 - **选项D**：事件源错误，EC2实例状态通知不等同于AWS Health的维护通知，且同样增加了不必要的Lambda复杂性 **决策标准和最佳实践：** 1. **选择正确的事件源**：根据业务需求选择最直接相关的事件源 2. **简化架构**：优先选择直接的解决方案，避免不必要的中间层 3. **利用托管服务**：使用AWS预定义的Automation runbook而不是自定义代码 4. **事件驱动架构**：使用EventBridge实现松耦合的自动化响应机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">315</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer manages an AWS CodePipeline pipeline that builds and deploys a web application on AWS. The pipeline has a source stage, a build stage, and a deploy stage. When deployed properly, the web application responds with a 200 OK HTTP response code when the URL of the home page is requested. The home page recently returned a 503 HTTP response code after CodePipeline deployed the application. The DevOps engineer needs to add an automated test into the pipeline. The automated test must ensure that the application returns a 200 OK HTTP response code after the application is deployed. The pipeline must fail if the response code is not present during the test. The DevOps engineer has added a CheckURL stage after the deploy stage in the pipeline. What should the DevOps engineer do next to implement the automated test?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure the CheckURL stage to use an Amazon CloudWatch action. Configure the action to use a canary synthetic monitoring check on the application URL and to report a success or failure to CodePipeline.
B. Create an AWS Lambda function to check the response code status of the URL and to report a success or failure to CodePipeline. Configure an action in the CheckURL stage to invoke the Lambda function.
C. Configure the CheckURL stage to use an AWS CodeDeploy action. Configure the action with an input artifact that is the URL of the application and to report a success or failure to CodePipeline.
D. Deploy an Amazon API Gateway HTTP API that checks the response code status of the URL and that reports success or failure to CodePipeline. Configure the CheckURL stage to use the AWS Device Farm test action and to provide the API Gateway HTTP API as an input artifact.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师管理着一个AWS CodePipeline流水线，该流水线在AWS上构建和部署Web应用程序。该流水线包含源代码阶段、构建阶段和部署阶段。当正确部署时，Web应用程序在请求主页URL时会返回200 OK HTTP响应代码。最近在CodePipeline部署应用程序后，主页返回了503 HTTP响应代码。DevOps工程师需要在流水线中添加自动化测试。该自动化测试必须确保应用程序在部署后返回200 OK HTTP响应代码。如果测试期间没有出现该响应代码，流水线必须失败。DevOps工程师已经在流水线的部署阶段后添加了一个CheckURL阶段。DevOps工程师接下来应该做什么来实现自动化测试？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在CodePipeline中实现一个自动化的健康检查机制，确保部署后的Web应用程序能够正常响应（返回200 OK状态码）。如果检查失败，整个流水线应该失败。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD流水线服务 - AWS Lambda：无服务器计算服务 - Amazon CloudWatch Synthetics：合成监控服务 - AWS CodeDeploy：应用程序部署服务 - Amazon API Gateway：API管理服务 - AWS Device Farm：移动应用测试服务 **正确答案B的原因：** 1. **直接集成**：Lambda函数可以直接作为CodePipeline的Action被调用 2. **灵活性强**：可以编写自定义代码来检查HTTP响应状态码 3. **原生支持**：Lambda与CodePipeline有原生集成，可以直接报告成功或失败状态 4. **成本效益**：按需执行，只在流水线运行时产生费用 5. **简单实现**：可以使用HTTP客户端库轻松检查URL响应状态 **其他选项错误的原因：** - **选项A**：CloudWatch Synthetics主要用于持续监控，不是为了在流水线中进行一次性部署验证而设计的，且与CodePipeline的集成不够直接 - **选项C**：CodeDeploy是部署服务，不是测试服务，无法执行URL健康检查功能 - **选项D**：Device Farm是移动应用测试服务，不适用于Web应用的URL检查；API Gateway在这里是不必要的复杂化 **决策标准和最佳实践：** 1. **选择与CodePipeline原生集成的服务** 2. **优先考虑简单、直接的解决方案** 3. **确保解决方案能够准确报告流水线状态** 4. **考虑成本效益和维护复杂度** 5. **使用合适的服务来解决特定问题**（Lambda适合自定义逻辑，CloudWatch Synthetics适合持续监控）</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">316</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that uploads access logs to an Amazon CloudWatch Logs log group. The fields in the log lines include the response code and the application name. The company wants to create a CloudWatch metric to track the number of requests by response code in a specific range and with a specific application name. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CloudWatch Logs log event filter on the CloudWatch Logs log stream to match the response code range. Configure the log event filter to increment a metric. Set the response code and application name as dimensions.
B. Create a CloudWatch Logs metric filter on the CloudWatch Logs log group to match the response code range. Configure the metric filter to increment a metric. Set the response code and application name as dimensions.
C. Create a CloudWatch Contributor Insights rule on the CloudWatch Logs log stream with a filter to match the response code range. Configure the Contributor Insights rule to increment a CloudWatch metric with the response code and application name as dimensions.
D. Create a CloudWatch Logs Insights query on the CloudWatch Logs log group to match the response code range. Configure the Logs Insights query to increment a CloudWatch metric with the response code and application name as dimensions.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个应用程序将访问日志上传到Amazon CloudWatch Logs日志组。日志行中的字段包括响应代码和应用程序名称。该公司希望创建一个CloudWatch指标来跟踪特定范围内响应代码和特定应用程序名称的请求数量。哪种解决方案能满足这些要求？ 选项： A. 在CloudWatch Logs日志流上创建CloudWatch Logs日志事件过滤器来匹配响应代码范围。配置日志事件过滤器来递增指标。将响应代码和应用程序名称设置为维度。 B. 在CloudWatch Logs日志组上创建CloudWatch Logs指标过滤器来匹配响应代码范围。配置指标过滤器来递增指标。将响应代码和应用程序名称设置为维度。 C. 在CloudWatch Logs日志流上创建CloudWatch Contributor Insights规则，使用过滤器匹配响应代码范围。配置Contributor Insights规则来递增CloudWatch指标，将响应代码和应用程序名称作为维度。 D. 在CloudWatch Logs日志组上创建CloudWatch Logs Insights查询来匹配响应代码范围。配置Logs Insights查询来递增CloudWatch指标，将响应代码和应用程序名称作为维度。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要从CloudWatch Logs中提取特定的日志数据（特定范围的响应代码和特定应用程序名称），并基于这些数据创建自定义CloudWatch指标来跟踪请求数量。 **涉及的关键AWS服务和概念：** 1. CloudWatch Logs - 日志存储和管理服务 2. CloudWatch Metrics - 指标监控服务 3. Metric Filters - 指标过滤器，用于从日志中提取指标 4. CloudWatch Contributor Insights - 分析日志数据中的贡献者模式 5. CloudWatch Logs Insights - 交互式日志分析服务 6. 维度(Dimensions) - 指标的标识属性 **正确答案B的原因：** 1. Metric Filter是专门设计用于从日志数据中提取和创建CloudWatch指标的功能 2. 在日志组级别创建过滤器可以处理该组下所有日志流的数据 3. 可以使用模式匹配来筛选特定范围的响应代码 4. 支持将响应代码和应用程序名称设置为指标维度 5. 这是标准的、成本效益高的解决方案 **其他选项错误的原因：** - 选项A：CloudWatch Logs没有&quot;日志事件过滤器&quot;这个概念，这是错误的术语 - 选项C：Contributor Insights主要用于识别顶级贡献者和异常模式，不是用来创建基本计数指标的最佳选择，且通常在日志组而非日志流级别操作 - 选项D：Logs Insights是用于交互式查询和分析的工具，不能自动持续地创建指标，需要手动运行查询 **决策标准和最佳实践：** 1. 选择专门用途的工具：Metric Filter专为从日志创建指标而设计 2. 考虑操作级别：日志组级别比日志流级别更合适，可以处理多个流 3. 成本效益：Metric Filter比Contributor Insights更经济实用 4. 自动化程度：选择能够持续自动工作的解决方案，而非需要手动触发的工具</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">317</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer provisioned an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with managed node groups. The DevOps engineer associated an OpenID Connect (OIDC) issuer with the cluster. The DevOps engineer is configuring Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp3) volumes for the cluster. The DevOps engineer attempts to initiate a PersistentVolumeClaim (PVC) request but is unable to provision a volume. To troubleshoot the issue, the DevOps engineer runs the kubectl describe pvc command. The DevOps engineer receives a failed to provision volume with StorageClass error and a could not create volume in EC2:UnauthorizedOperation error. Which solution will resolve these errors?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a Kubernetes cluster role that allows the persistent volumes to perform get, list, watch, create, and delete operations. Configure the cluster role to allow get, list, and watch operations for storage in the cluster.
B. Create an Amazon EBS Container Storage Interface (CSI) driver IAM role that has the required permissions and trust relationships. Attach the IAM role to the Amazon EBS CSI driver add-on in the cluster.
C. Add the ebs.csi.aws.com/volumeType:gp3 annotation to the PersistentVolumeClaim object in the cluster.
D. Create a Kubernetes storage class object. Set the provisioner value to ebs.csi.aws.com. Set the volumeBindingMode value to WaitForFirstConsumer in the cluster.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师使用托管节点组配置了一个Amazon Elastic Kubernetes Service (Amazon EKS)集群。该DevOps工程师将OpenID Connect (OIDC)发行者与集群关联。该DevOps工程师正在为集群配置Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp3)卷。该DevOps工程师尝试发起PersistentVolumeClaim (PVC)请求，但无法配置卷。为了排查问题，该DevOps工程师运行kubectl describe pvc命令。该DevOps工程师收到&quot;failed to provision volume with StorageClass&quot;错误和&quot;could not create volume in EC2:UnauthorizedOperation&quot;错误。哪个解决方案能解决这些错误？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在Amazon EKS集群中配置EBS存储时遇到权限问题的解决方案。关键错误信息是&quot;UnauthorizedOperation&quot;，表明这是一个IAM权限问题，而不是Kubernetes配置问题。 **涉及的关键AWS服务和概念：** - Amazon EKS：托管的Kubernetes服务 - Amazon EBS CSI Driver：用于在Kubernetes中管理EBS卷的容器存储接口驱动 - IAM角色和权限：控制AWS资源访问的安全机制 - OIDC (OpenID Connect)：用于服务账户的身份验证机制 - PersistentVolumeClaim：Kubernetes中请求存储资源的对象 **正确答案B的原因：** 错误信息&quot;EC2:UnauthorizedOperation&quot;明确指出这是AWS API权限问题。EBS CSI驱动需要适当的IAM权限来创建、附加和管理EBS卷。解决方案是创建一个具有必要权限的IAM角色，并将其附加到EBS CSI驱动插件上。这样驱动就能获得在EC2中创建和管理EBS卷的权限。 **其他选项错误的原因：** - 选项A：创建Kubernetes集群角色解决的是Kubernetes RBAC权限问题，但这里的错误是AWS API级别的权限问题，不是Kubernetes内部权限问题。 - 选项C：添加注释只是指定卷类型，不能解决权限问题。而且这个注释格式也不正确。 - 选项D：创建StorageClass是配置存储类的步骤，但如果CSI驱动没有适当的AWS权限，仍然无法创建卷。 **决策标准和最佳实践：** 1. 根据错误信息判断问题类型：UnauthorizedOperation明确指向IAM权限问题 2. 理解EKS中的权限模型：需要同时配置Kubernetes RBAC和AWS IAM权限 3. EBS CSI驱动的最佳实践：使用IAM角色为服务账户(IRSA)机制，通过OIDC提供者安全地授予权限 4. 故障排查原则：先解决权限问题，再处理配置问题</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">318</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs a fleet of Amazon EC2 instances in a VPC. The company&#x27;s employees remotely access the EC2 instances by using the Remote Desktop Protocol (RDP). The company wants to collect metrics about how many RDP sessions the employees initiate every day. Which combination of steps will meet this requirement? (Choose three.) F. Create a log group subscription filter. Use EventBridge as the destination.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule that reacts to EC2 Instance State-change Notification events.
B. Create an Amazon CloudWatch Logs log group. Specify the log group as a target for the EventBridge rule.
C. Create a flow log in VPC Flow Logs.
D. Create an Amazon CloudWatch Logs log group. Specify the log group as a destination for the flow log.
E. Create a log group metric filter.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在VPC中运行一组Amazon EC2实例。公司员工使用远程桌面协议(RDP)远程访问EC2实例。公司希望收集员工每天发起多少个RDP会话的指标。哪些步骤的组合将满足这一要求？(选择三个。) F. 创建一个日志组订阅过滤器。使用EventBridge作为目标。 选项：A. 创建一个Amazon EventBridge规则，响应EC2实例状态变更通知事件。B. 创建一个Amazon CloudWatch Logs日志组。将日志组指定为EventBridge规则的目标。C. 在VPC Flow Logs中创建流日志。D. 创建一个Amazon CloudWatch Logs日志组。将日志组指定为流日志的目标。E. 创建一个日志组指标过滤器。 正确答案：C</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 题目要求收集和统计员工每天发起的RDP会话数量指标。RDP使用TCP端口3389进行连接，需要监控网络流量来识别RDP连接。 **涉及的关键AWS服务和概念：** - VPC Flow Logs：捕获VPC中网络接口的IP流量信息 - CloudWatch Logs：存储和管理日志数据 - CloudWatch指标过滤器：从日志中提取指标数据 - RDP协议：使用TCP 3389端口的远程桌面连接 **正确答案的原因：** 仅给出选项C作为正确答案是不完整的。要完整解决这个问题，实际上需要三个步骤的组合： 1. C - 创建VPC Flow Logs来捕获网络流量，包括RDP连接（TCP 3389端口） 2. D - 创建CloudWatch Logs日志组作为Flow Logs的目标，存储流日志数据 3. E - 创建日志组指标过滤器，从流日志中筛选和统计RDP连接数据 **其他选项错误的原因：** - 选项A：EC2实例状态变更事件无法提供RDP会话连接的详细信息 - 选项B：EventBridge规则不适用于此场景，因为需要的是网络流量分析而非事件响应 - 选项F：订阅过滤器用于将日志数据发送到其他服务，不是收集RDP指标的必要步骤 **决策标准和最佳实践：** 1. 监控网络连接需要使用VPC Flow Logs捕获流量数据 2. 使用CloudWatch Logs存储和管理日志数据 3. 通过指标过滤器从原始日志中提取有意义的业务指标 4. 这种方法可以准确统计基于端口3389的RDP连接数量，满足监控要求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">319</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using Amazon Elastic Kubernetes Service (Amazon EKS) to run its applications. The EKS cluster is successfully running multiple pods. The company stores the pod images in Amazon Elastic Container Registry (Amazon ECR). The company needs to configure Pod Identity access for the EKS cluster. The company has already updated the node IAM role by using the permissions for Pod Identity access. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an IAM OpenID Connect (OIDC) provider for the EKS cluster.
B. Ensure that the nodes can reach the EKS Auth API. Add and configure the EKS Pod Identity Agent add-on for the EKS cluster.
C. Create an EKS access entry that uses the API_AND_CONFIG_MAP cluster authentication mode.
D. Configure the AWS Security Token Service (AWS STS) endpoint for the Kubernetes service account that the pods in the EKS cluster use.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用Amazon Elastic Kubernetes Service (Amazon EKS)来运行其应用程序。EKS集群成功运行了多个pod。该公司将pod镜像存储在Amazon Elastic Container Registry (Amazon ECR)中。该公司需要为EKS集群配置Pod Identity访问。该公司已经通过使用Pod Identity访问的权限更新了节点IAM角色。哪个解决方案能满足这些要求？ 选项：A. 为EKS集群创建一个IAM OpenID Connect (OIDC)提供商。 B. 确保节点可以访问EKS Auth API。为EKS集群添加并配置EKS Pod Identity Agent附加组件。 C. 创建一个使用API_AND_CONFIG_MAP集群认证模式的EKS访问条目。 D. 为EKS集群中pod使用的Kubernetes服务账户配置AWS Security Token Service (AWS STS)端点。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何为Amazon EKS集群配置Pod Identity访问权限。Pod Identity是AWS提供的一种机制，允许Kubernetes pod安全地访问AWS服务，而无需在pod中硬编码AWS凭证。 **涉及的关键AWS服务和概念：** 1. Amazon EKS - 托管的Kubernetes服务 2. Pod Identity - 允许pod安全访问AWS服务的身份验证机制 3. IAM OIDC Provider - 用于建立EKS集群与AWS IAM之间信任关系的组件 4. Amazon ECR - 容器镜像注册表服务 5. AWS STS - 安全令牌服务，用于临时凭证管理 **正确答案的原因：** 选项A是正确的，因为配置Pod Identity访问的第一步和最关键的步骤是为EKS集群创建IAM OIDC提供商。OIDC提供商建立了EKS集群与AWS IAM之间的信任关系，使得Kubernetes服务账户能够承担IAM角色。这是实现Pod Identity功能的基础架构要求。 **其他选项错误的原因：** - 选项B：虽然EKS Pod Identity Agent是Pod Identity的新实现方式，但题目明确提到已经更新了节点IAM角色权限，这表明使用的是传统的IRSA (IAM Roles for Service Accounts)方式，而不是新的Pod Identity Agent方式。 - 选项C：EKS访问条目主要用于集群访问控制，与Pod Identity访问AWS服务的需求不直接相关。 - 选项D：配置STS端点是在OIDC提供商建立之后的步骤，而且通常是自动配置的，不是主要的配置要求。 **决策标准和最佳实践：** 1. Pod Identity配置遵循先建立信任关系再配置具体权限的原则 2. OIDC提供商是EKS与AWS IAM集成的标准做法 3. 在实际部署中，还需要创建IAM角色、配置服务账户注解等后续步骤 4. 这种方式比在容器中硬编码AWS凭证更安全，符合最小权限原则</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">320</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has multiple AWS accounts in an organization in AWS Organizations that has all features enabled. The company&#x27;s DevOps administrator needs to improve security across all the company&#x27;s AWS accounts. The administrator needs to identify the top users and roles in use across all accounts. Which solution will meet these requirements with the MOST operational efficiency?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new organization trail in AWS CloudTrail. Configure the trail to send log events to Amazon CloudWatch Logs. Create a CloudWatch Contributor Insights rule for the userIdentity.arn log field. View the results in CloudWatch Contributor Insights.
B. Create an unused access analysis for the organization by using AWS Identity and Access Management Access Analyzer. Review the analyzer results and determine if each finding has the intended level of permissions required for the workload.
C. Create a new organization trail in AWS CloudTrail. Create a table in Amazon Athena that uses partition projection. Load the Athena table with CloudTrail data. Query the Athena table to find the top users and roles.
D. Generate a Service access report for each account by using Organizations. From the results, pull the last accessed date and last accessed by account fields to find the top users and roles.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中拥有多个AWS账户，该组织已启用所有功能。公司的DevOps管理员需要提高所有公司AWS账户的安全性。管理员需要识别所有账户中使用最多的顶级用户和角色。哪种解决方案能够以最高的运营效率满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案来识别AWS Organizations中所有账户的顶级用户和角色，重点是要求&quot;最高的运营效率&quot;。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS CloudTrail：API调用日志记录服务 - Amazon CloudWatch Logs：日志管理服务 - CloudWatch Contributor Insights：自动分析和排名工具 - IAM Access Analyzer：权限分析工具 - Amazon Athena：无服务器查询服务 - Organizations Service access report：服务访问报告 **正确答案A的原因：** 1. **组织级CloudTrail**：创建组织级trail可以自动收集所有成员账户的API调用日志 2. **CloudWatch Contributor Insights**：专门设计用于自动识别和排名顶级贡献者（如用户、角色） 3. **自动化程度高**：一旦设置完成，Contributor Insights会自动分析userIdentity.arn字段并提供排名结果 4. **运营效率最高**：无需手动查询或分析，直接在CloudWatch控制台查看结果 **其他选项错误的原因：** - **选项B**：IAM Access Analyzer主要用于分析未使用的权限，而不是识别顶级用户和角色的使用情况 - **选项C**：虽然技术上可行，但需要手动创建Athena表、配置分区投影、编写查询语句，运营复杂度较高 - **选项D**：Service access report主要显示服务的访问情况，不是专门用于识别顶级用户和角色的工具 **决策标准和最佳实践：** 1. **运营效率优先**：选择自动化程度最高、需要最少手动干预的解决方案 2. **工具专业性**：选择专门为特定需求设计的工具（Contributor Insights专门用于识别顶级贡献者） 3. **组织级部署**：利用AWS Organizations的集中管理能力，避免在每个账户单独配置 4. **实时性**：CloudWatch Contributor Insights提供近实时的分析结果，便于持续监控</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">321</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an organization in AWS Organizations with many OUs that contain many AWS accounts. The organization has a dedicated delegated administrator AWS account. The company needs the accounts in one OU to have server-side encryption enforced for all Amazon Elastic Block Store (Amazon EBS) volumes and Amazon Simple Queue Service (Amazon SQS) queues that are created or updated on an AWS CloudFormation stack. Which solution will enforce this policy before a CloudFormation stack operation in the accounts of this OU?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Activate trusted access to CloudFormation StackSets. Create a CloudFormation Hook that enforces server-side encryption on EBS volumes and SQS queues. Deploy the Hook across the accounts in the OU by using StackSets.
B. Set up AWS Config in all the accounts in the OU. Use AWS Systems Manager to deploy AWS Config rules that enforce server-side encryption for EBS volumes and SQS queues across the accounts in the OU.
C. Write an SCP to deny the creation of EBS volumes and SQS queues unless the EBS volumes and SQS queues have server-side encryption. Attach the SCP to the OU.
D. Create an AWS Lambda function in the delegated administrator account that checks whether server-side encryption is enforced for EBS volumes and SQS queues. Create an IAM role to provide the Lambda function access to the accounts in the OU.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中有一个组织，包含许多OU，这些OU包含许多AWS账户。该组织有一个专门的委托管理员AWS账户。公司需要一个OU中的账户对所有在AWS CloudFormation堆栈上创建或更新的Amazon Elastic Block Store (Amazon EBS)卷和Amazon Simple Queue Service (Amazon SQS)队列强制执行服务器端加密。哪种解决方案将在此OU账户的CloudFormation堆栈操作之前强制执行此策略？ 选项： A. 激活CloudFormation StackSets的可信访问。创建一个CloudFormation Hook来强制EBS卷和SQS队列的服务器端加密。使用StackSets在OU的账户中部署Hook。 B. 在OU中的所有账户中设置AWS Config。使用AWS Systems Manager在OU的账户中部署强制EBS卷和SQS队列服务器端加密的AWS Config规则。 C. 编写SCP来拒绝创建EBS卷和SQS队列，除非EBS卷和SQS队列具有服务器端加密。将SCP附加到OU。 D. 在委托管理员账户中创建AWS Lambda函数来检查EBS卷和SQS队列是否强制执行服务器端加密。创建IAM角色为Lambda函数提供访问OU中账户的权限。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在CloudFormation堆栈操作**之前**强制执行EBS卷和SQS队列的服务器端加密策略，这是一个预防性控制（preventive control）而非检测性控制（detective control）。关键词是&quot;before&quot;，说明需要在资源创建前进行拦截和验证。 **涉及的关键AWS服务和概念：** - AWS Organizations和OU管理 - CloudFormation Hooks（预部署验证机制） - CloudFormation StackSets（跨账户部署） - Service Control Policies (SCP) - AWS Config（合规性监控） - 委托管理员账户概念 **正确答案A的原因：** CloudFormation Hooks是专门设计用于在CloudFormation堆栈操作之前进行验证和控制的机制。它可以在资源创建或更新前检查是否满足特定条件（如加密要求），如果不满足则阻止操作继续。StackSets允许从委托管理员账户跨多个账户统一部署这些Hooks，完美满足题目要求的预防性控制需求。 **其他选项错误的原因：** - **选项B（AWS Config）：** Config是检测性控制工具，用于监控资源合规性，但无法在资源创建前阻止不合规的操作，只能在事后发现问题。 - **选项C（SCP）：** 虽然SCP可以预防性地拒绝操作，但它无法基于资源的具体配置（如是否启用加密）进行细粒度控制，只能进行粗粒度的权限控制。 - **选项D（Lambda函数）：** Lambda函数通常用于事后检查，无法直接集成到CloudFormation的预部署验证流程中。 **决策标准和最佳实践：** 1. **时机选择：** 预防性控制优于检测性控制，在问题发生前阻止比事后修复更有效 2. **集中管理：** 使用StackSets实现跨账户的统一策略部署和管理 3. **工具匹配：** 选择专门设计用于特定场景的工具（CloudFormation Hooks用于堆栈操作前验证） 4. **合规性保证：** 确保策略在所有相关账户中一致执行，避免合规性漏洞</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">322</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is running an internal application in an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. The ECS cluster instances can connect to the public internet. The ECS tasks that run on the cluster instances are configured to use images from both private Amazon Elastic Container Registry (Amazon ECR) repositories and a public ECR registry repository. A new security policy requires the company to remove the ECS cluster&#x27;s direct access to the internet. The company must remove any NAT gateways and internet gateways from the VPC that hosts the cluster. A DevOps engineer needs to ensure the ECS cluster can still download images from both the public ECR registry and the private ECR repositories. Images from the public ECR registry must remain up-to-date. New versions of the images must be available to the ECS cluster within 24 hours of publication. Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.) F. Create an Amazon S3 gateway endpoint in the VPC.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CodeBuild project and a new private ECR repository for each image that is downloaded from the public ECR registry. Configure each project to pull the image from the public ECR repository and push the image to the new private ECR repository. Create an Amazon EventBridge rule that invokes the CodeBuild project once every 24 hours. Update each task definition in the ECS cluster to refer to the new private ECR repository.
B. Create a new Amazon ECR pull through cache rule for each image that is downloaded from the public ECR registry. Create an AWS Lambda function that invokes each pull through cache rule. Create an Amazon EventBridge rule that invokes the Lambda function once every 24 hours. Update each task definition in the ECS cluster to refer to the image from the pull through cache.
C. Create a new Amazon ECR pull through cache rule for the public ECR registry. Update each task definition in the ECS cluster to refer to the image from the pull through cache. Ensure each public image has been downloaded through the pull through cache at least once before removing internet access from the VPC.
D. Create an Amazon ECR interface VPC endpoint for the public ECR repositories that are in the VPC.
E. Create an Amazon ECR interface VPC endpoint for the private ECR repositories that are in the VPC.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Amazon EC2上的Amazon Elastic Container Service (Amazon ECS)集群中运行内部应用程序。ECS集群实例可以连接到公共互联网。在集群实例上运行的ECS任务被配置为使用来自私有Amazon Elastic Container Registry (Amazon ECR)存储库和公共ECR注册表存储库的镜像。新的安全策略要求公司移除ECS集群对互联网的直接访问。公司必须从托管集群的VPC中移除任何NAT网关和互联网网关。DevOps工程师需要确保ECS集群仍然可以从公共ECR注册表和私有ECR存储库下载镜像。来自公共ECR注册表的镜像必须保持最新。镜像的新版本必须在发布后24小时内对ECS集群可用。哪种步骤组合能够以最少的运营开销满足这些要求？（选择三个）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在移除ECS集群的互联网访问（包括NAT网关和互联网网关）后，仍能访问公共和私有ECR存储库，并确保公共镜像在24小时内保持更新，同时要求最少的运营开销。 **涉及的关键AWS服务和概念：** - Amazon ECS：容器编排服务 - Amazon ECR：容器镜像注册表服务 - ECR Pull Through Cache：ECR的拉取缓存功能 - VPC Endpoint：VPC终端节点，用于私有网络访问AWS服务 - S3 Gateway Endpoint：S3网关终端节点 **正确答案的原因：** 选项C是正确的，因为： 1. ECR Pull Through Cache可以自动缓存公共ECR镜像到私有ECR中 2. 一旦镜像被缓存，后续访问会自动检查更新，满足24小时更新要求 3. 运营开销最小，只需要预先下载一次镜像 4. 配合VPC endpoint可以在没有互联网访问的情况下正常工作 选项E也是必需的，因为需要VPC endpoint来访问私有ECR存储库。 选项F（S3 Gateway Endpoint）也是必需的，因为ECR底层使用S3存储镜像层。 **其他选项错误的原因：** - 选项A：使用CodeBuild和EventBridge创建复杂的同步机制，运营开销大，不符合&quot;最少运营开销&quot;要求 - 选项B：需要Lambda函数和EventBridge规则来手动触发缓存更新，增加了不必要的复杂性和运营开销 - 选项D：ECR interface VPC endpoint主要用于私有ECR访问，对于公共ECR通过pull through cache已经解决 **决策标准和最佳实践：** 1. 优先选择AWS原生功能（如Pull Through Cache）而不是自建解决方案 2. 最小化运营复杂度，避免不必要的定时任务和Lambda函数 3. 合理使用VPC endpoint确保私有网络中的服务访问 4. 理解ECR Pull Through Cache的自动更新机制，无需手动干预即可保持镜像更新</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">323</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a continuous integration pipeline where the company creates container images by using AWS CodeBuild. The created images are stored in Amazon Elastic Container Registry (Amazon ECR). Checking for and fixing the vulnerabilities in the images takes the company too much time. The company wants to identify the image vulnerabilities quickly and notify the security team of the vulnerabilities. Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Activate Amazon Inspector enhanced scanning for Amazon ECR. Configure the enhanced scanning to use continuous scanning. Set up a topic in Amazon Simple Notification Service (Amazon SNS).
B. Create an Amazon EventBridge rule for Amazon Inspector findings. Set an Amazon Simple Notification Service (Amazon SNS) topic as the rule target.
C. Activate AWS Lambda enhanced scanning for Amazon ECR. Configure the enhanced scanning to use continuous scanning. Set up a topic in Amazon Simple Email Service (Amazon SES).
D. Create a new AWS Lambda function. Invoke the new Lambda function when scan findings are detected.
E. Activate default basic scanning for Amazon ECR for all container images. Configure the default basic scanning to use continuous scanning. Set up a topic in Amazon Simple Notification Service (Amazon SNS).</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个持续集成管道，该公司使用AWS CodeBuild创建容器镜像。创建的镜像存储在Amazon Elastic Container Registry (Amazon ECR)中。检查和修复镜像中的漏洞耗费了公司太多时间。公司希望快速识别镜像漏洞并通知安全团队相关漏洞。以下哪种步骤组合能够以最少的运营开销满足这些要求？（选择两个） 选项：A. 为Amazon ECR激活Amazon Inspector增强扫描。配置增强扫描使用持续扫描。在Amazon Simple Notification Service (Amazon SNS)中设置一个主题。 B. 为Amazon Inspector发现创建Amazon EventBridge规则。设置Amazon Simple Notification Service (Amazon SNS)主题作为规则目标。 C. 为Amazon ECR激活AWS Lambda增强扫描。配置增强扫描使用持续扫描。在Amazon Simple Email Service (Amazon SES)中设置一个主题。 D. 创建一个新的AWS Lambda函数。当检测到扫描发现时调用新的Lambda函数。 E. 为所有容器镜像激活Amazon ECR的默认基础扫描。配置默认基础扫描使用持续扫描。在Amazon Simple Notification Service (Amazon SNS)中设置一个主题。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案来快速识别容器镜像漏洞并通知安全团队，同时要求最少的运营开销。需要选择两个步骤的组合。 **涉及的关键AWS服务和概念：** 1. Amazon ECR - 容器镜像注册表服务 2. Amazon Inspector - 安全漏洞扫描服务，提供增强扫描功能 3. Amazon EventBridge - 事件驱动架构的事件总线服务 4. Amazon SNS - 消息通知服务 5. AWS Lambda - 无服务器计算服务 6. 持续扫描 vs 基础扫描的区别 **正确答案的原因：** 正确答案应该是A和B的组合： - 选项A：Amazon Inspector增强扫描是专门为容器镜像漏洞检测设计的服务，提供更全面和快速的扫描能力。持续扫描确保新推送的镜像能够自动被扫描。 - 选项B：EventBridge可以自动捕获Inspector的扫描结果，并触发SNS通知，实现自动化的通知机制，无需人工干预。 **其他选项错误的原因：** - 选项C：AWS Lambda本身不提供容器扫描功能，这是概念性错误。Lambda是计算服务，不是扫描服务。 - 选项D：创建Lambda函数会增加运营开销，需要编写和维护代码，不符合&quot;最少运营开销&quot;的要求。 - 选项E：基础扫描功能有限，检测能力不如增强扫描全面，且基础扫描不支持持续扫描模式。 **决策标准和最佳实践：** 1. **服务选择**：使用专门的安全扫描服务（Inspector）而不是通用计算服务 2. **自动化程度**：选择能够自动触发和通知的解决方案，减少人工干预 3. **扫描能力**：增强扫描比基础扫描提供更好的漏洞检测能力 4. **运营开销**：避免需要自定义代码开发和维护的解决方案 5. **集成性**：利用AWS原生服务之间的集成能力，如EventBridge与Inspector的集成</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">324</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps administrator is configuring a repository to store a company&#x27;s container images. The administrator needs to configure a lifecycle rule that automatically deletes container images that have a specific tag and that are older than 15 days. Which solution will meet these requirements with the MOST operational efficiency?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a repository in Amazon Elastic Container Registry (Amazon ECR). Add a lifecycle policy to the repository to expire images that have the matching tag after 15 days.
B. Create a repository in AWS CodeArtifact. Add a repository policy to the CodeArtifact repository to expire old assets that have the matching tag after 15 days.
C. Create a bucket in Amazon S3. Add a bucket lifecycle policy to expire old objects that have the matching tag after 15 days.
D. Create an EC2 Image Builder container recipe. Add a build component to expire the container that has the matching tag after 15 days.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps管理员正在配置一个存储库来存储公司的容器镜像。管理员需要配置一个生命周期规则，自动删除具有特定标签且超过15天的容器镜像。哪个解决方案能够以最高的运营效率满足这些要求？ 选项： A. 在Amazon Elastic Container Registry (Amazon ECR)中创建存储库。向存储库添加生命周期策略，使具有匹配标签的镜像在15天后过期。 B. 在AWS CodeArtifact中创建存储库。向CodeArtifact存储库添加存储库策略，使具有匹配标签的旧资产在15天后过期。 C. 在Amazon S3中创建存储桶。添加存储桶生命周期策略，使具有匹配标签的旧对象在15天后过期。 D. 创建EC2 Image Builder容器配方。添加构建组件，使具有匹配标签的容器在15天后过期。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为容器镜像存储配置自动化的生命周期管理，需要根据特定标签和时间条件（15天）自动删除容器镜像，并且要求最高的运营效率。 **涉及的关键AWS服务和概念：** - Amazon ECR：专门的容器镜像注册表服务 - AWS CodeArtifact：软件包和依赖项管理服务 - Amazon S3：对象存储服务 - EC2 Image Builder：自动化镜像构建服务 - 生命周期策略：自动化资源管理的策略机制 **正确答案A的原因：** 1. **服务匹配性**：ECR是AWS专门为容器镜像设计的托管服务，完全符合存储容器镜像的需求 2. **原生功能支持**：ECR提供内置的生命周期策略功能，可以基于标签、镜像年龄等条件自动管理镜像 3. **运营效率最高**：无需额外配置或自定义脚本，通过简单的策略配置即可实现自动化管理 4. **精确控制**：支持基于特定标签和时间的精确过期控制 **其他选项错误的原因：** - **选项B**：CodeArtifact主要用于软件包管理（如Maven、npm包），不是专门为容器镜像设计的服务 - **选项C**：虽然S3可以存储容器镜像文件，但缺乏容器镜像的专门管理功能，且不如ECR高效 - **选项D**：EC2 Image Builder用于构建镜像，不是用于存储和生命周期管理的服务 **决策标准和最佳实践：** 1. **服务专用性原则**：选择专门为特定用途设计的AWS服务 2. **运营效率优先**：优选提供原生自动化功能的解决方案 3. **管理简化**：避免不必要的复杂配置，使用AWS托管服务的内置功能 4. **成本效益**：ECR的生命周期策略可以有效控制存储成本，自动清理不需要的镜像</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">325</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses Amazon Redshift as its data warehouse solution. The company wants to create a dashboard to view changes to the Redshift users and the queries the users perform. Which combination of steps will meet this requirement? (Choose two.)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon CloudWatch log group. Create an AWS CloudTrail trail that writes to the CloudWatch log group.
B. Create a new Amazon S3 bucket. Configure default audit logging on the Redshift cluster. Configure the S3 bucket as the target.
C. Configure the Redshift cluster database audit logging to include user activity logs. Configure Amazon CloudWatch as the target.
D. Create an Amazon CloudWatch dashboard that has a log widget. Configure the widget to display user details from the Redshift logs.
E. Create an AWS Lambda function that uses Amazon Athena to query the Redshift logs. Create an Amazon CloudWatch dashboard that has a custom widget type that uses the Lambda function.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用Amazon Redshift作为其数据仓库解决方案。该公司希望创建一个仪表板来查看Redshift用户的变更以及用户执行的查询。哪种步骤组合将满足这一要求？（选择两个。） 选项：A. 创建一个Amazon CloudWatch日志组。创建一个写入CloudWatch日志组的AWS CloudTrail跟踪。 B. 创建一个新的Amazon S3存储桶。在Redshift集群上配置默认审计日志记录。将S3存储桶配置为目标。 C. 配置Redshift集群数据库审计日志记录以包含用户活动日志。将Amazon CloudWatch配置为目标。 D. 创建一个具有日志小部件的Amazon CloudWatch仪表板。配置小部件以显示来自Redshift日志的用户详细信息。 E. 创建一个使用Amazon Athena查询Redshift日志的AWS Lambda函数。创建一个具有使用Lambda函数的自定义小部件类型的Amazon CloudWatch仪表板。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 题目要求创建仪表板来监控两个方面：1）Redshift用户的变更；2）用户执行的查询。这需要启用审计日志记录并创建可视化仪表板。 **涉及的关键AWS服务和概念：** - Amazon Redshift审计日志记录：用于记录用户活动和查询 - Amazon S3：存储审计日志的目标位置 - Amazon CloudWatch：日志存储和仪表板服务 - AWS CloudTrail：API调用跟踪服务 - Amazon Athena：查询服务 - AWS Lambda：无服务器计算服务 **正确答案的原因：** 题目标注正确答案为B，但这里存在问题。实际上应该选择B和D的组合： - 选项B：正确配置了Redshift审计日志记录，将日志存储到S3，这是获取用户活动和查询信息的正确方法 - 选项D：创建CloudWatch仪表板来显示日志信息，满足了可视化要求 **其他选项错误的原因：** - 选项A：CloudTrail主要跟踪API调用，无法获取Redshift内部的用户查询详情 - 选项C：Redshift不支持直接将审计日志发送到CloudWatch，只能发送到S3 - 选项E：过于复杂，虽然技术上可行，但不是最佳实践 **决策标准和最佳实践：** 1. 使用Redshift原生审计日志功能获取用户活动数据 2. 将审计日志存储在S3中以便长期保存和分析 3. 使用CloudWatch仪表板进行可视化展示 4. 选择简单直接的解决方案而非过度工程化的方案 注：题目给出的正确答案可能不完整，实际应该选择两个选项来满足完整需求。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">326</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an organization in AWS Organizations to manage its 500 AWS accounts. The organization has all features enabled. The AWS accounts are in a single OU. The developers need to use the CostCenter tag key for all resources in the organization&#x27;s member accounts. Some teams do not use the CostCenter tag key to tag their Amazon EC2 instances. The cloud team wrote a script that scans all EC2 instances in the organization&#x27;s member accounts. If the EC2 instances do not have a CostCenter tag key, the script will notify AWS account administrators. To avoid this notification, some developers use the CostCenter tag key with an arbitrary string in the tag value. The cloud team needs to ensure that all EC2 instances in the organization use a CostCenter tag key with the appropriate cost center value. Which solution will meet these requirements? script to scan the tag keys and tag values and notify the administrators when the tag values are not valid. permission boundary in the organization&#x27;s member accounts that restricts the CostCenter tag values to a list of valid cost centers. policy to the OU. Configure an AWS Lambda function that adds an empty CostCenter tag key to an EC2 instance. Create an Amazon EventBridge rule that matches events to the RunInstances API action with the Lambda function as the target.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Create a tag policy that requires the CostCenter tag to be values from a known list of cost centers for all EC2 instances. Attach the policy to the OU. Update the script to scan the tag keys and tag values. Modify the script to update noncompliant resources with a default approved tag value for the CostCenter tag key.
B. Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Attach the policy to the OU. Update the
C. Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Attach the policy to the OU. Create an IAM
D. Create a tag policy that requires the CostCenter tag to be values from a known list of cost centers for all EC2 instances. Attach the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations中的组织来管理其500个AWS账户。该组织已启用所有功能。这些AWS账户位于单个OU中。开发人员需要为组织成员账户中的所有资源使用CostCenter标签键。一些团队没有使用CostCenter标签键来标记他们的Amazon EC2实例。云团队编写了一个脚本，扫描组织成员账户中的所有EC2实例。如果EC2实例没有CostCenter标签键，脚本会通知AWS账户管理员。为了避免此通知，一些开发人员使用CostCenter标签键，但在标签值中使用任意字符串。云团队需要确保组织中的所有EC2实例都使用带有适当成本中心值的CostCenter标签键。哪种解决方案能满足这些要求？ 选项： A. 创建一个SCP，防止创建没有CostCenter标签键的EC2实例。创建一个标签策略，要求所有EC2实例的CostCenter标签必须是已知成本中心列表中的值。将策略附加到OU。更新脚本以扫描标签键和标签值。修改脚本以使用CostCenter标签键的默认批准标签值更新不合规资源。 B. 创建一个SCP，防止创建没有CostCenter标签键的EC2实例。将策略附加到OU。更新脚本以扫描标签键和标签值，并在标签值无效时通知管理员。 C. 创建一个SCP，防止创建没有CostCenter标签键的EC2实例。将策略附加到OU。在组织成员账户中创建权限边界，将CostCenter标签值限制为有效成本中心列表。 D. 创建一个标签策略，要求所有EC2实例的CostCenter标签必须是已知成本中心列表中的值。将策略附加到OU。配置AWS Lambda函数，为EC2实例添加空的CostCenter标签键。创建Amazon EventBridge规则，匹配RunInstances API操作事件，以Lambda函数为目标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求解决标签合规性问题，确保所有EC2实例都有正确的CostCenter标签，且标签值必须来自预定义的有效成本中心列表。需要防止开发人员使用任意字符串作为标签值来绕过检查。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 集中管理多个AWS账户 2. Service Control Policy (SCP) - 组织级别的权限控制策略 3. Tag Policy - 标签合规性策略，用于强制标签标准 4. Organizational Unit (OU) - 组织单元，用于分组管理账户 5. EC2实例标签管理和合规性 **正确答案A的原因：** 选项A提供了完整的解决方案： - SCP防止创建没有CostCenter标签的EC2实例，从源头控制 - Tag Policy确保CostCenter标签值必须来自预定义列表，防止任意值 - 更新脚本不仅检查合规性，还能自动修复不合规资源 - 这种组合提供了预防性控制和补救措施 **其他选项错误的原因：** - 选项B：只有SCP和通知机制，缺少对标签值有效性的强制控制，无法防止任意标签值 - 选项C：权限边界主要用于限制IAM权限，不是标签合规性的最佳工具，且题目描述不完整 - 选项D：缺少SCP的预防性控制，Lambda自动添加空标签键无法解决标签值验证问题 **决策标准和最佳实践：** 1. 采用&quot;预防+检测+修复&quot;的多层防护策略 2. 使用SCP进行预防性控制，在资源创建时就进行限制 3. 使用Tag Policy进行标签标准化和值验证 4. 结合自动化脚本进行持续监控和自动修复 5. 在组织级别统一管理标签策略，确保一致性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">327</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer uses a pipeline in AWS CodePipeline. The pipeline has a build action and a deploy action for a single-page web application that is delivered to an Amazon S3 bucket. Amazon CloudFront serves the web application. The build action creates an artifact for the web application. The DevOps engineer has created an AWS CloudFormation template that defines the S3 bucket and configures the S3 bucket to host the application. The DevOps engineer has configured a CloudFormation deploy action before the S3 action. The CloudFormation deploy action creates the S3 bucket. The DevOps engineer needs to configure the S3 deploy action to use the S3 bucket from the CloudFormation template. Which combination of steps will meet these requirements? (Choose two.)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add an output named BucketName to the CloudFormation template. Set the output&#x27;s value to refer to the S3 bucket from the CloudFormation template. Configure the output value to export to an AWS::SSM::Parameter resource named StackVariables.
B. Add an output named BucketName to the CloudFormation template. Set the output&#x27;s value to refer to the S3 bucket from the CloudFormation template. Set the CloudFormation action&#x27;s namespace to StackVariables in the pipeline.
C. Configure the output artifacts of the CloudFormation action in the pipeline to be an AWS Systems Manager Parameter Store parameter named StackVariables. Name the artifact BucketName.
D. Configure the build artifact from the build action as the input to the CodePipeline S3 deploy action. Configure the deploy action to deploy to the S3 bucket by using the StackVariables.BucketName variable.
E. Configure the build artifact from the build action and the AWS Systems Manager parameter as the inputs to the deploy action. Configure the deploy action to deploy to the S3 bucket by using the StackVariables.BucketName variable.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps工程师在AWS CodePipeline中使用管道。该管道有一个构建操作和一个部署操作，用于将单页Web应用程序交付到Amazon S3存储桶。Amazon CloudFront为Web应用程序提供服务。构建操作为Web应用程序创建一个构件。DevOps工程师已创建了一个AWS CloudFormation模板，该模板定义了S3存储桶并配置S3存储桶来托管应用程序。DevOps工程师在S3操作之前配置了CloudFormation部署操作。CloudFormation部署操作创建S3存储桶。DevOps工程师需要配置S3部署操作以使用CloudFormation模板中的S3存储桶。哪种步骤组合将满足这些要求？（选择两个）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求在CodePipeline中实现CloudFormation部署操作和S3部署操作之间的数据传递，具体是让S3部署操作能够使用CloudFormation模板创建的S3存储桶名称。 **涉及的关键AWS服务和概念：** 1. AWS CodePipeline - CI/CD管道服务 2. AWS CloudFormation - 基础设施即代码服务 3. Amazon S3 - 对象存储服务 4. CodePipeline中的操作间变量传递机制 5. CloudFormation输出(Outputs)和命名空间(Namespace)概念 **正确答案的原因：** 选项B是正确的，因为： 1. 在CloudFormation模板中添加名为BucketName的输出，并将其值设置为引用S3存储桶资源 2. 在管道中设置CloudFormation操作的命名空间为StackVariables 3. 这样就可以通过StackVariables.BucketName的格式在后续的S3部署操作中引用该存储桶名称 4. 这是CodePipeline中标准的操作间变量传递方式 还需要选择选项D，因为： 1. 配置构建构件作为S3部署操作的输入 2. 使用StackVariables.BucketName变量来指定部署目标存储桶 3. 这完成了整个数据流：构建构件→S3部署操作，同时使用CloudFormation输出的存储桶名称 **其他选项错误的原因：** - 选项A：错误地尝试将CloudFormation输出导出到SSM Parameter，这不是CodePipeline中操作间传递变量的正确方式 - 选项C：误解了输出构件的概念，CloudFormation操作的输出不是SSM参数 - 选项E：不必要地引入了SSM参数作为输入，增加了复杂性且不符合标准做法 **决策标准和最佳实践：** 1. 使用CloudFormation输出和命名空间是在CodePipeline中传递动态值的标准方法 2. 保持解决方案简单，避免不必要的外部依赖（如SSM Parameter Store） 3. 遵循CodePipeline的原生变量传递机制，确保管道的可维护性和可靠性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">328</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company used a lift and shift strategy to migrate a workload to AWS. The company has an Auto Scaling group of Amazon EC2 instances. Each EC2 instance runs a web application, a database, and a Redis cache. Users are experiencing large variations in the web application&#x27;s response times. Requests to the web application go to a single EC2 instance that is under significant load. The company wants to separate the application components to improve availability and performance. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a Network Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora Serverless database. Create an Application Load Balancer and an Auto Scaling group for the Redis cache.
B. Create an Application Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora database that has a Multi-AZ deployment. Create a Network Load Balancer and an Auto Scaling group in a single Availability Zone for the Redis cache.
C. Create a Network Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora Serverless database. Create an Amazon ElastiCache (Redis OSS) cluster for the cache. Create a target group that has a DNS target type that contains the ElastiCache (Redis OSS) cluster hostname.
D. Create an Application Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora database that has a Multi-AZ deployment. Create an Amazon ElastiCache (Redis OSS) cluster for the cache.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用lift and shift策略将工作负载迁移到AWS。该公司有一个Amazon EC2实例的Auto Scaling group。每个EC2实例运行一个web应用程序、一个数据库和一个Redis缓存。用户在web应用程序的响应时间上遇到很大的变化。对web应用程序的请求会发送到一个负载很重的单个EC2实例。该公司希望分离应用程序组件以提高可用性和性能。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何将单体架构（monolithic architecture）分解为分布式架构，以解决性能瓶颈和可用性问题。当前问题是所有组件（web应用、数据库、Redis缓存）都在同一个EC2实例上，导致单点故障和性能问题。 **涉及的关键AWS服务和概念：** - Application Load Balancer (ALB)：七层负载均衡器，适合HTTP/HTTPS流量 - Network Load Balancer (NLB)：四层负载均衡器，适合TCP/UDP流量 - Auto Scaling group：自动扩缩容服务 - Amazon Aurora：托管的关系型数据库服务 - Amazon ElastiCache (Redis OSS)：托管的内存缓存服务 **正确答案D的原因：** 1. **Application Load Balancer + Auto Scaling group**：ALB最适合web应用程序的HTTP/HTTPS流量分发，能够进行健康检查和智能路由 2. **Aurora Multi-AZ部署**：提供高可用性和自动故障转移，比Aurora Serverless更适合持续运行的工作负载 3. **ElastiCache (Redis OSS) cluster**：使用托管的Redis服务替代自建Redis，提供更好的可用性、性能和管理便利性 4. **架构合理性**：完全分离了三个组件，每个都使用最适合的AWS托管服务 **其他选项错误的原因：** - **选项A**：使用NLB处理web应用不合适，ALB更适合HTTP流量；为Redis缓存创建ALB和Auto Scaling group是过度设计 - **选项B**：同样为Redis使用NLB和Auto Scaling group是不必要的复杂化，且限制在单AZ降低了可用性 - **选项C**：使用NLB处理web流量不是最佳选择；创建包含ElastiCache hostname的target group在技术上不可行且不必要 **决策标准和最佳实践：** 1. **服务选择原则**：为每种工作负载选择最合适的AWS托管服务 2. **高可用性设计**：使用Multi-AZ部署和跨AZ的服务分布 3. **性能优化**：通过负载均衡和缓存分离提高整体性能 4. **运维简化**：优先选择托管服务减少运维负担 5. **成本效益**：避免过度设计，选择符合实际需求的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">329</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using AWS Organizations and wants to implement a governance strategy with the following requirements: • AWS resource access is restricted to the same two Regions for all accounts. • AWS services are limited to a specific group of authorized services for all accounts. • Authentication is provided by Active Directory. • Access permissions are organized by job function and are identical in each account. Which solution will meet these requirements? Manager (AWS RAM) to share management account roles with permissions for each job function, including AWS IAM Identity Center for authentication in each account.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Establish an organizational unit (OU) with group policies in the management account to restrict Regions and authorized services. Use AWS CloudFormation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account.
B. Establish a permission boundary in the management account to restrict Regions and authorized services. Use AWS CloudFormation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account.
C. Establish a service control policy in the management account to restrict Regions and authorized services. Use AWS Resource Access
D. Establish a service control policy in the management account to restrict Regions and authorized services. Use AWS CloudFormation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication in each account.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS Organizations，希望实施一个治理策略，具有以下要求：• 所有账户的AWS资源访问仅限于相同的两个Region • 所有账户的AWS服务仅限于特定的授权服务组 • 通过Active Directory提供身份验证 • 访问权限按工作职能组织，并在每个账户中保持一致。哪个解决方案能满足这些要求？ 选项： A. 在管理账户中建立organizational unit (OU)和组策略来限制Region和授权服务。使用AWS CloudFormation StackSets为每个工作职能配置角色和权限，包括在每个账户中为IAM identity provider身份验证配置IAM信任策略。 B. 在管理账户中建立permission boundary来限制Region和授权服务。使用AWS CloudFormation StackSets为每个工作职能配置角色和权限，包括在每个账户中为IAM identity provider身份验证配置IAM信任策略。 C. 在管理账户中建立service control policy来限制Region和授权服务。使用AWS Resource Access Manager (AWS RAM)共享管理账户角色，为每个工作职能分配权限，包括在每个账户中使用AWS IAM Identity Center进行身份验证。 D. 在管理账户中建立service control policy来限制Region和授权服务。使用AWS CloudFormation StackSets为每个工作职能配置角色和权限，包括在每个账户中为IAM identity provider身份验证配置IAM信任策略。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在AWS Organizations环境下实施统一治理策略的最佳实践，需要同时满足区域限制、服务限制、Active Directory身份验证和跨账户权限一致性四个要求。 **涉及的关键AWS服务和概念：** 1. **Service Control Policy (SCP)** - AWS Organizations的核心治理工具，用于在组织层面限制权限 2. **AWS CloudFormation StackSets** - 跨多个账户和区域部署CloudFormation模板的服务 3. **Permission Boundary** - IAM权限边界，用于限制单个账户内的最大权限 4. **AWS RAM** - 资源访问管理器，用于跨账户共享资源 5. **IAM Identity Provider** - 用于集成外部身份提供商（如Active Directory） **正确答案D的原因：** 1. **SCP是正确的区域和服务限制工具** - SCP在组织层面工作，可以有效限制所有成员账户只能在指定区域使用授权服务 2. **CloudFormation StackSets确保跨账户一致性** - 能够在所有账户中部署相同的IAM角色和权限配置，保证工作职能权限的一致性 3. **IAM trust policy支持Active Directory集成** - 通过IAM identity provider和信任策略可以实现与Active Directory的联合身份验证 **其他选项错误的原因：** - **选项A错误** - &quot;组策略&quot;不是AWS Organizations的概念，AWS中没有这个术语用于限制区域和服务 - **选项B错误** - Permission Boundary只能在单个账户内工作，无法在组织层面限制所有账户的区域和服务访问 - **选项C错误** - AWS RAM主要用于资源共享而非角色权限管理，且IAM Identity Center虽然可以提供SSO，但题目明确要求使用Active Directory身份验证 **决策标准和最佳实践：** 1. **组织级别的限制使用SCP** - 这是AWS Organizations治理的最佳实践 2. **跨账户一致性部署使用StackSets** - 确保所有账户具有相同的权限配置 3. **外部身份集成使用IAM Identity Provider** - 标准的企业级身份联合解决方案 4. **分层治理模型** - SCP提供护栏限制，IAM角色提供具体权限，实现了安全的分层权限管理</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">330</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company detects unusual login attempts in many of its AWS accounts. A DevOps engineer must implement a solution that sends a notification to the company&#x27;s security team when multiple failed login attempts occur. The DevOps engineer has already created an Amazon Simple Notification Service (Amazon SNS) topic and has subscribed the security team to the SNS topic. Which solution will provide the notification with the LEAST operational effort? second EventBridge rule to detect when the query fails and to send a message to the SNS topic.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure AWS CloudTrail to send management events to an Amazon CloudWatch Logs log group. Create a CloudWatch Logs metric filter to match failed ConsoleLogin events. Create a CloudWatch alarm that is based on the metric filter. Configure an alarm action to send messages to the SNS topic.
B. Configure AWS CloudTrail to send management events to an Amazon S3 bucket. Create an Amazon Athena query that returns a failure if the query finds failed logins in the logs in the S3 bucket. Create an Amazon EventBridge rule to periodically run the query. Create a
C. Configure AWS CloudTrail to send data events to an Amazon CloudWatch Logs log group. Create a CloudWatch logs metric filter to match failed ConsoleLogin events. Create a CloudWatch alarm that is based on the metric filter. Configure an alarm action to send messages to the SNS topic.
D. Configure AWS CloudTrail to send data events to an Amazon S3 bucket. Configure an Amazon S3 event notification for the s3:ObjectCreated event type. Filter the event type by ConsoleLogin failed events. Configure the event notification to forward to the SNS topic.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在其多个AWS账户中检测到异常登录尝试。DevOps工程师必须实施一个解决方案，当发生多次登录失败尝试时向公司安全团队发送通知。DevOps工程师已经创建了一个Amazon Simple Notification Service (Amazon SNS) topic，并让安全团队订阅了该SNS topic。哪个解决方案能以最少的运维工作量提供通知？ 选项： A. 配置AWS CloudTrail将管理事件发送到Amazon CloudWatch Logs日志组。创建CloudWatch Logs指标过滤器来匹配失败的ConsoleLogin事件。基于指标过滤器创建CloudWatch告警。配置告警操作向SNS topic发送消息。 B. 配置AWS CloudTrail将管理事件发送到Amazon S3存储桶。创建Amazon Athena查询，如果在S3存储桶的日志中发现登录失败则返回失败。创建Amazon EventBridge规则定期运行查询。创建第二个EventBridge规则检测查询失败并向SNS topic发送消息。 C. 配置AWS CloudTrail将数据事件发送到Amazon CloudWatch Logs日志组。创建CloudWatch logs指标过滤器来匹配失败的ConsoleLogin事件。基于指标过滤器创建CloudWatch告警。配置告警操作向SNS topic发送消息。 D. 配置AWS CloudTrail将数据事件发送到Amazon S3存储桶。为s3:ObjectCreated事件类型配置Amazon S3事件通知。通过ConsoleLogin失败事件过滤事件类型。配置事件通知转发到SNS topic。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要实现一个自动化监控解决方案，检测多次AWS账户登录失败尝试，并通过已有的SNS topic通知安全团队，要求运维工作量最少。 **涉及的关键AWS服务和概念：** - AWS CloudTrail：记录AWS API调用和用户活动 - Amazon CloudWatch：监控和告警服务 - CloudWatch Logs：日志管理服务 - CloudWatch Metrics和Alarms：指标监控和告警 - Amazon SNS：消息通知服务 - 管理事件vs数据事件：CloudTrail中不同类型的事件记录 **正确答案A的原因：** 1. **事件类型正确**：登录尝试属于管理事件（management events），不是数据事件 2. **架构简洁高效**：CloudTrail → CloudWatch Logs → Metric Filter → CloudWatch Alarm → SNS，流程直接且自动化 3. **实时性好**：CloudWatch Logs可以实时接收CloudTrail事件，告警响应迅速 4. **运维工作量最少**：一旦配置完成，整个流程完全自动化，无需人工干预 5. **成本效益高**：避免了复杂的查询和额外的服务调用 **其他选项错误的原因：** - **选项B**：使用Athena查询和EventBridge定期执行增加了复杂性和延迟，运维工作量大，且需要管理查询调度 - **选项C**：错误使用了数据事件而非管理事件，ConsoleLogin属于管理事件范畴 - **选项D**：同样错误使用数据事件，且S3事件通知无法直接过滤CloudTrail日志内容中的特定登录失败事件 **决策标准和最佳实践：** 1. **选择正确的事件类型**：理解管理事件和数据事件的区别，登录活动属于管理事件 2. **优先考虑原生集成**：CloudWatch与CloudTrail的原生集成提供最佳性能和可靠性 3. **最小化架构复杂度**：避免不必要的中间步骤和额外服务 4. **实时监控优于批处理**：对于安全事件，实时告警比定期查询更重要 5. **自动化优先**：选择能够完全自动化运行的解决方案，减少人工运维需求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">331</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has deployed a new REST API by using Amazon API Gateway. The company uses the API to access confidential data. The API must be accessed from only specific VPCs in the company. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create and attach a resource policy to the API Gateway API. Configure the resource policy to allow only the specific VPC IDs.
B. Add a security group to the API Gateway API. Configure the inbound rules to allow only the specific VPC IP address ranges.
C. Create and attach an IAM role to the API Gateway API. Configure the IAM role to allow only the specific VPC IDs.
D. Add an ACL to the API Gateway API. Configure the outbound rules to allow only the specific VPC IP address ranges.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用Amazon API Gateway部署了一个新的REST API。该公司使用这个API来访问机密数据。这个API必须只能从公司内的特定VPC中访问。哪个解决方案能满足这些要求？ 选项：A. 为API Gateway API创建并附加一个资源策略。配置资源策略只允许特定的VPC ID。 B. 为API Gateway API添加安全组。配置入站规则只允许特定的VPC IP地址范围。 C. 为API Gateway API创建并附加一个IAM角色。配置IAM角色只允许特定的VPC ID。 D. 为API Gateway API添加ACL。配置出站规则只允许特定的VPC IP地址范围。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何限制API Gateway只能从特定VPC访问，这是一个网络访问控制和安全隔离的问题。需要确保机密数据只能通过指定的VPC进行访问。 **涉及的关键AWS服务和概念：** - Amazon API Gateway：AWS的托管API服务 - VPC（Virtual Private Cloud）：AWS的虚拟私有云 - Resource Policy：资源策略，用于控制对AWS资源的访问 - IAM角色和安全组：AWS的身份和访问管理组件 **正确答案A的原因：** API Gateway Resource Policy是控制API访问的正确机制。通过资源策略可以： - 基于源VPC ID进行访问控制 - 使用aws:sourceVpc条件键来限制只允许特定VPC的请求 - 在API Gateway层面直接实施网络级别的访问控制 - 提供细粒度的访问控制，支持VPC级别的限制 **其他选项错误的原因：** - 选项B错误：API Gateway是托管服务，不支持直接附加安全组。安全组主要用于EC2实例等计算资源的网络访问控制。 - 选项C错误：IAM角色主要用于身份验证和授权，而不是基于网络位置（VPC）的访问控制。IAM角色无法直接限制VPC级别的访问。 - 选项D错误：API Gateway不支持传统的网络ACL配置，且ACL的出站规则不适用于控制对API的入站访问请求。 **决策标准和最佳实践：** 1. 对于API Gateway的网络访问控制，应优先使用Resource Policy 2. Resource Policy支持基于VPC、IP地址、IAM用户/角色等多种条件的访问控制 3. 对于机密数据的API，建议结合使用VPC Endpoint、Resource Policy和IAM认证实现多层安全防护 4. 网络隔离应该在服务层面实现，而不是依赖于基础设施组件（如安全组）</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">332</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs a website by using an Amazon Elastic Container Service (Amazon ECS) service that is connected to an Application Load Balancer (ALB). The service was in a steady state with tasks responding to requests successfully. A DevOps engineer updated the task definition with a new container image and deployed the new task definition to the service. The DevOps engineer noticed that the service is frequently stopping and starting new tasks because the ALB health checks are failing. What should the DevOps engineer do to troubleshoot the failed deployment? B (100%) Get IT Certification Unlock free, top-quality video courses on ExamTopics with a simple registration. Elevate your learning journey with our expertly curated content. Register now to access a diverse range of educational resources designed for success with ExamTopics!</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Ensure that a security group associated with the service allows traffic from the ALB.
B. Increase the ALB health check grace period for the service.
C. Increase the service minimum healthy percent setting.
D. Decrease the ALB health check interval.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用连接到Application Load Balancer (ALB)的Amazon Elastic Container Service (Amazon ECS)服务来运行网站。该服务之前处于稳定状态，任务能够成功响应请求。DevOps工程师使用新的容器镜像更新了任务定义，并将新的任务定义部署到服务中。DevOps工程师注意到由于ALB健康检查失败，服务频繁地停止和启动新任务。DevOps工程师应该如何排查这次失败的部署？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是ECS服务部署后ALB健康检查失败的故障排查能力。关键信息是服务之前运行正常，但在更新容器镜像后开始出现健康检查失败，导致任务频繁重启。 **涉及的关键AWS服务和概念：** - Amazon ECS：容器编排服务，管理Docker容器的运行 - Application Load Balancer (ALB)：七层负载均衡器，提供健康检查功能 - 安全组：AWS的虚拟防火墙，控制入站和出站流量 - ECS任务定义：定义容器配置的蓝图 - ALB健康检查：确保后端目标健康状态的机制 **正确答案A的原因：** 当更新容器镜像后健康检查开始失败，最可能的原因是网络连接问题。新的容器镜像可能使用了不同的端口或网络配置，导致ALB无法通过现有的安全组规则访问ECS任务。安全组必须允许来自ALB的流量才能进行健康检查。这是部署新镜像后最常见的问题。 **其他选项错误的原因：** - 选项B（增加健康检查宽限期）：如果安全组阻止了流量，增加宽限期也无法解决连接问题 - 选项C（增加最小健康百分比）：这只会影响部署策略，不能解决健康检查失败的根本原因 - 选项D（减少健康检查间隔）：更频繁的检查只会加剧问题，不能解决连接失败 **决策标准和最佳实践：** 1. 故障排查应该从最基础的网络连接开始 2. 部署新镜像时要确保网络配置的一致性 3. 安全组规则应该允许ALB到ECS任务的必要端口访问 4. 在更新任务定义时，要检查端口映射和安全组配置的匹配性 5. 遵循&quot;先检查基础设施，再调整参数&quot;的排查原则</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">333</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company that uses electronic patient health records runs a fleet of Amazon EC2 instances with an Amazon Linux operating system. The company must continuously ensure that the EC2 instances are running operating system patches and application patches that are in compliance with current privacy regulations. The company uses a custom repository to store application patches. A DevOps engineer needs to automate the deployment of operating system patches and application patches. The DevOps engineer wants to use both the default operating system patch repository and the custom patch repository. Which solution will meet these requirements with the LEAST effort? custom repository. Run the AWS-RunPatchBaseline document by using the Run command to verify and install patches. Use the BaselineOverride API to configure the default patch baseline and the custom patch baseline.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Systems Manager to create a new custom patch baseline that includes the default operating system repository and the custom repository. Run the AWS-RunPatchBaseline document by using the Run command to verify and install patches. Use the BaselineOverride API to configure the new custom patch baseline.
B. Use AWS Direct Connect to integrate the custom repository with the EC2 instances. Use Amazon EventBridge events to deploy the patches.
C. Use the yum-config-manager command to add the custom repository to the /etc/yum.repos.d configuration. Run the yum-config-manager-enable command to activate the new repository.
D. Use AWS Systems Manager to create a patch baseline for the default operating system repository and a second patch baseline for the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家使用电子患者健康记录的公司运行着一组运行Amazon Linux操作系统的Amazon EC2实例。该公司必须持续确保EC2实例运行的操作系统补丁和应用程序补丁符合当前隐私法规的合规要求。该公司使用自定义repository来存储应用程序补丁。DevOps工程师需要自动化部署操作系统补丁和应用程序补丁。DevOps工程师希望同时使用默认的操作系统补丁repository和自定义补丁repository。哪种解决方案能够以最少的工作量满足这些要求？ 选项： A. 使用AWS Systems Manager创建一个新的自定义patch baseline，包含默认操作系统repository和自定义repository。使用Run command运行AWS-RunPatchBaseline文档来验证和安装补丁。使用BaselineOverride API来配置新的自定义patch baseline。 B. 使用AWS Direct Connect将自定义repository与EC2实例集成。使用Amazon EventBridge事件来部署补丁。 C. 使用yum-config-manager命令将自定义repository添加到/etc/yum.repos.d配置中。运行yum-config-manager-enable命令来激活新的repository。 D. 使用AWS Systems Manager为默认操作系统repository创建一个patch baseline，为自定义repository创建第二个patch baseline。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个能够自动化部署操作系统补丁和应用程序补丁的解决方案，需要同时支持默认的操作系统repository和自定义repository，并且要求以最少的工作量实现。 **涉及的关键AWS服务和概念：** - AWS Systems Manager Patch Manager：用于自动化补丁管理的服务 - Patch Baseline：定义哪些补丁应该被安装的规则集合 - AWS-RunPatchBaseline文档：Systems Manager中用于执行补丁安装的预定义文档 - Run Command：Systems Manager的功能，用于在EC2实例上远程执行命令 - BaselineOverride API：用于覆盖默认patch baseline配置的API **正确答案A的原因：** 1. **统一管理**：创建单个自定义patch baseline同时包含默认和自定义repository，实现统一管理 2. **自动化程度高**：使用AWS-RunPatchBaseline文档和Run Command实现完全自动化的补丁部署 3. **工作量最少**：通过AWS托管服务实现，无需手动配置每个实例 4. **灵活性强**：BaselineOverride API提供了灵活的配置选项 5. **合规性支持**：Systems Manager提供详细的补丁安装报告，支持合规性审计 **其他选项错误的原因：** - **选项B**：AWS Direct Connect主要用于网络连接，不是补丁管理的合适工具；EventBridge虽然可以触发事件，但不提供补丁管理的核心功能，实现复杂度高 - **选项C**：这是手动配置方法，需要在每个实例上单独配置，工作量大，不符合&quot;最少工作量&quot;的要求，且缺乏自动化和集中管理能力 - **选项D**：题目描述不完整，但从逻辑上看，创建两个独立的patch baseline会增加管理复杂性，不如单个统一baseline高效 **决策标准和最佳实践：** 1. **自动化优先**：选择能够最大程度自动化的解决方案 2. **集中管理**：使用AWS托管服务实现集中化的补丁管理 3. **最少运维工作量**：避免需要在每个实例上手动配置的方案 4. **合规性考虑**：选择能够提供审计跟踪和报告功能的解决方案 5. **可扩展性**：解决方案应该能够轻松扩展到更多实例</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">334</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an organization in AWS Organizations to manage multiple AWS accounts. The company has enabled all features for the organization. The company configured the organization as a hierarchy of OUs under the root OU. The company recently registered all its OUs and enrolled all its AWS accounts in AWS Control Tower. The company needs to customize the AWS Control Tower managed AWS Config configuration recorder in each of the company&#x27;s AWS accounts. The company needs to apply the customizations to both the existing AWS accounts and to any new AWS accounts that the company enrolls in AWS Control Tower in the future. Which combination of steps will meet these requirements? (Choose three.) F. Configure an Amazon EventBridge rule in the AWS Control Tower management account to invoke an AWS Lambda function when an AWS account is updated or enrolled in AWS Control Tower or when the landing zone is updated. Re-register each Organizations OU in the organization.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new AWS account. Create an AWS Lambda function in the new account to apply the customizations to the AWS Config configuration recorder in each AWS account in the organization.
B. Create a new AWS account as an AWS Config delegated administrator. Create an AWS Lambda function in the delegated administrator account to apply the customizations to the AWS Config configuration recorder in the delegated administrator account.
C. Configure an Amazon EventBridge rule in the AWS Control Tower management account to invoke an AWS Lambda function when the Organizations OU is registered or reregistered. Re-register the root Organizations OU.
D. Configure the AWSControlTowerExecution IAM role in each AWS account in the organization to be assumable by an AWS Lambda function. Configure the Lambda function to assume the AWSControlTowerExecution IAM role.
E. Create an IAM role in the AWS Control Tower management account that an AWS Lambda function can assume. Grant the IAM role permission to assume the AWSControlTowerExecution IAM role in any account in the organization. Configure the Lambda function to use the new IAM role.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中使用组织来管理多个AWS账户。该公司已为组织启用了所有功能。公司将组织配置为根OU下的OU层次结构。公司最近注册了所有OU并将所有AWS账户注册到AWS Control Tower中。公司需要在每个AWS账户中自定义AWS Control Tower管理的AWS Config配置记录器。公司需要将自定义应用到现有的AWS账户以及将来注册到AWS Control Tower的任何新AWS账户。哪种步骤组合将满足这些要求？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Control Tower环境中自定义AWS Config配置记录器，需要同时覆盖现有账户和未来新增账户，并且需要选择三个正确的步骤。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Control Tower：Landing Zone管理和治理服务 - AWS Config：资源配置监控服务 - Amazon EventBridge：事件驱动服务 - AWS Lambda：无服务器计算服务 - IAM角色：权限管理，特别是AWSControlTowerExecution角色 **正确答案分析：** 根据题目要求需要选择三个答案，但这里只给出了C作为正确答案。让我分析各选项： **选项C正确的原因：** - 在AWS Control Tower管理账户中配置EventBridge规则来监听OU注册/重新注册事件 - 通过重新注册根OU可以触发对所有现有账户的更新 - 这种方法可以确保未来新账户也会自动应用自定义配置 **其他选项分析：** - **选项A：** 创建新账户来管理配置不是最佳实践，应该使用现有的管理结构 - **选项B：** 创建Config委托管理员账户过于复杂，且只处理委托管理员账户本身 - **选项D：** 配置AWSControlTowerExecution角色是必要的权限设置，应该也是正确答案之一 - **选项E：** 在管理账户中创建IAM角色来跨账户操作是标准做法，也应该是正确答案 - **选项F：** 监听账户更新和注册事件也是合理的自动化方案 **决策标准和最佳实践：** 1. 使用事件驱动架构实现自动化 2. 利用Control Tower的原生权限模型（AWSControlTowerExecution角色） 3. 在管理账户中集中管理跨账户操作 4. 确保解决方案覆盖现有和未来账户 **注意：** 题目要求选择三个答案，但只提供了C作为正确答案，完整的正确答案组合很可能是C、D、E，因为这三个选项共同构成了完整的解决方案：事件触发机制、权限配置和跨账户访问能力。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">335</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs an application in an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run Docker containers that make requests to a MySQL database that runs on separate EC2 instances. A DevOps engineer needs to update the application to use a serverless architecture. Which solution will meet this requirement with the FEWEST changes?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Replace the containers that run on EC2 instances and the ALB with AWS Lambda functions. Replace the MySQL database with an Amazon Aurora Serverless v2 database that is compatible with MySQL.
B. Replace the containers that run on EC2 instances with AWS Fargate. Replace the MySQL database with an Amazon Aurora Serverless v2 database that is compatible with MySQL.
C. Replace the containers that run on EC2 instances and the ALB with AWS Lambda functions. Replace the MySQL database with Amazon DynamoDB tables.
D. Replace the containers that run on EC2 instances with AWS Fargate. Replace the MySQL database with Amazon DynamoDB tables.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Application Load Balancer (ALB)后面的Amazon EC2实例Auto Scaling组中运行应用程序。EC2实例运行Docker容器，这些容器向运行在独立EC2实例上的MySQL数据库发出请求。DevOps工程师需要更新应用程序以使用serverless架构。哪个解决方案能够以最少的更改满足这个要求？ 选项： A. 用AWS Lambda函数替换运行在EC2实例上的容器和ALB。用与MySQL兼容的Amazon Aurora Serverless v2数据库替换MySQL数据库。 B. 用AWS Fargate替换运行在EC2实例上的容器。用与MySQL兼容的Amazon Aurora Serverless v2数据库替换MySQL数据库。 C. 用AWS Lambda函数替换运行在EC2实例上的容器和ALB。用Amazon DynamoDB表替换MySQL数据库。 D. 用AWS Fargate替换运行在EC2实例上的容器。用Amazon DynamoDB表替换MySQL数据库。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 题目要求将现有的基于EC2的容器化应用迁移到serverless架构，关键约束是&quot;最少的更改&quot;。 **涉及的关键AWS服务和概念：** - AWS Fargate：serverless容器计算服务，无需管理底层EC2实例 - AWS Lambda：事件驱动的serverless计算服务 - Amazon Aurora Serverless v2：按需自动扩缩容的关系型数据库 - Amazon DynamoDB：NoSQL数据库服务 - 容器化应用迁移策略 **正确答案B的原因：** 1. **容器兼容性**：现有应用已经容器化，Fargate可以直接运行Docker容器，无需重构应用代码 2. **数据库兼容性**：Aurora Serverless v2与MySQL完全兼容，应用的数据库连接代码和SQL查询无需修改 3. **架构连续性**：保持了容器化的应用架构，只是将计算层从EC2迁移到Fargate 4. **负载均衡**：Fargate仍可与ALB配合使用，网络架构变化最小 **其他选项错误的原因：** - **选项A错误**：Lambda需要将容器化应用重构为函数，代码改动大；且需要移除ALB改用API Gateway等，架构变化显著 - **选项C错误**：同样需要重构为Lambda函数，且DynamoDB是NoSQL数据库，需要重写所有SQL查询和数据模型 - **选项D错误**：虽然Fargate迁移简单，但DynamoDB需要大量应用代码修改来适应NoSQL模式 **决策标准和最佳实践：** 1. **最小化代码更改**：优先选择与现有技术栈兼容的serverless服务 2. **渐进式迁移**：先迁移基础设施层（EC2到Fargate），再考虑应用层优化 3. **数据库兼容性**：关系型应用迁移时，优先考虑兼容的serverless数据库服务 4. **容器优先策略**：对于已容器化的应用，Fargate通常比Lambda更适合作为第一步迁移目标</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">336</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an organization in AWS Organizations to manage 10 AWS accounts. All features are enabled, and trusted access for AWS CloudFormation is enabled. A DevOps engineer needs to use CloudFormation to deploy an IAM role to the Organizations management account and all member accounts in the organization. Which solution will meet these requirements with the LEAST operational overhead?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CloudFormation StackSet that has service-managed permissions. Set the root OU as a deployment target.
B. Create a CloudFormation StackSet that has service-managed permissions. Set the root OU as a deployment target. Deploy a separate CloudFormation stack in the Organizations management account.
C. Create a CloudFormation StackSet that has self-managed permissions. Set the root OU as a deployment target.
D. Create a CloudFormation StackSet that has self-managed permissions. Set the root OU as a deployment target. Deploy a separate CloudFormation stack in the Organizations management account.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations中的组织来管理10个AWS账户。所有功能都已启用，并且已启用AWS CloudFormation的可信访问。DevOps工程师需要使用CloudFormation将IAM角色部署到Organizations管理账户和组织中的所有成员账户。哪种解决方案能够以最少的运营开销满足这些要求？ 选项： A. 创建具有服务管理权限的CloudFormation StackSet。将根OU设置为部署目标。 B. 创建具有服务管理权限的CloudFormation StackSet。将根OU设置为部署目标。在Organizations管理账户中部署单独的CloudFormation堆栈。 C. 创建具有自管理权限的CloudFormation StackSet。将根OU设置为部署目标。 D. 创建具有自管理权限的CloudFormation StackSet。将根OU设置为部署目标。在Organizations管理账户中部署单独的CloudFormation堆栈。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要将IAM角色同时部署到Organizations管理账户和所有成员账户，要求运营开销最小。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. CloudFormation StackSet - 跨多个账户和区域部署资源的服务 3. 服务管理权限 vs 自管理权限的区别 4. 根OU（组织单元）- 包含管理账户和所有成员账户 **正确答案A的原因：** 1. **服务管理权限**：由于已启用CloudFormation的可信访问，StackSet可以自动获得跨账户部署的权限，无需手动配置IAM角色 2. **根OU作为目标**：根OU包含管理账户和所有成员账户，一次部署即可覆盖所有目标账户 3. **运营开销最小**：只需创建一个StackSet，自动处理权限管理，无需额外操作 **其他选项错误的原因：** - **选项B**：不必要地增加了单独的CloudFormation堆栈部署，增加了运营复杂性。当StackSet目标为根OU时，已经包含管理账户 - **选项C和D**：使用自管理权限需要手动在每个目标账户中预先创建和配置IAM角色，大大增加了运营开销和复杂性 **决策标准和最佳实践：** 1. **权限模式选择**：当Organizations启用可信访问时，优先选择服务管理权限以减少手动配置 2. **目标范围**：根OU自动包含所有账户（管理账户+成员账户），避免重复部署 3. **运营效率**：选择能够一次性完成所有部署需求的方案，避免多步骤操作</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">337</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs an application that stores artifacts in an Amazon S3 bucket. The application has a large user base. The application writes a high volume of objects to the S3 bucket. The company has enabled event notifications for the S3 bucket. When the application writes an object to the S3 bucket, several processing tasks need to be performed simultaneously. The company&#x27;s DevOps team needs to create an AWS Step Functions workflow to orchestrate the processing tasks. Which combination of steps should the DevOps team take to meet these requirements with the LEAST operational overhead? (Choose two.)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a Standard workflow that contains a parallel state that defines the processing tasks. Create an Asynchronous Express workflow that contains a parallel state that defines the processing tasks.
B. Create a Synchronous Express workflow that contains a map state that defines the processing tasks.
C. Create an Amazon EventBridge rule to match when a new S3 object is created. Configure the EventBridge rule to invoke an AWS Lambda function. Configure the Lambda function to start the processing workflow.
D. Create an Amazon EventBridge rule to match when a new S3 object is created. Configure the EventBridge rule to start the processing workflow.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司运行一个应用程序，该应用程序将工件存储在Amazon S3存储桶中。该应用程序拥有大量用户群体。应用程序向S3存储桶写入大量对象。公司已为S3存储桶启用了事件通知。当应用程序向S3存储桶写入对象时，需要同时执行多个处理任务。公司的DevOps团队需要创建一个AWS Step Functions工作流来编排这些处理任务。DevOps团队应该采取哪种步骤组合来以最少的运营开销满足这些要求？（选择两个。） 选项：A. 创建一个包含定义处理任务的并行状态的Standard工作流。创建一个包含定义处理任务的并行状态的Asynchronous Express工作流。 B. 创建一个包含定义处理任务的映射状态的Synchronous Express工作流。 C. 创建一个Amazon EventBridge规则来匹配新S3对象创建时的事件。配置EventBridge规则调用AWS Lambda函数。配置Lambda函数启动处理工作流。 D. 创建一个Amazon EventBridge规则来匹配新S3对象创建时的事件。配置EventBridge规则启动处理工作流。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个解决方案，当S3存储桶中有新对象创建时，能够触发Step Functions工作流来同时执行多个处理任务，并且要求最少的运营开销。 **涉及的关键AWS服务和概念：** 1. Amazon S3事件通知机制 2. AWS Step Functions的工作流类型（Standard vs Express） 3. Step Functions的状态类型（Parallel state vs Map state） 4. Amazon EventBridge事件路由 5. AWS Lambda函数作为中间层 **正确答案的原因：** 题目标注正确答案是A，但这个答案存在问题。实际上正确的组合应该是A（选择Standard工作流）和D（直接通过EventBridge触发）。 - Standard工作流适合处理复杂的业务逻辑，支持所有Step Functions功能 - Parallel state能够同时执行多个处理任务，满足并行处理需求 - EventBridge直接触发工作流减少了中间层，降低运营开销 **其他选项错误的原因：** - Express工作流虽然成本更低，但在功能上有限制，且对于高频触发可能不是最佳选择 - Synchronous Express工作流不适合长时间运行的任务 - Map state主要用于处理数组数据的迭代，不如Parallel state适合同时执行不同类型的任务 - 选项C增加了Lambda函数作为中间层，增加了复杂性和运营开销 **决策标准和最佳实践：** 1. 选择合适的工作流类型：Standard适合复杂长时间任务，Express适合高频短时间任务 2. 状态选择：Parallel state用于同时执行不同任务，Map state用于批量处理相似任务 3. 最小化架构复杂性：直接集成优于通过中间服务 4. 考虑成本和性能平衡：避免不必要的服务调用链</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">338</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps team supports an application that runs in an Amazon Elastic Container Service (Amazon ECS) cluster behind an Application Load Balancer (ALB). Currently, the DevOps team uses AWS CodeDeploy to deploy the application by using a blue/green all-at-once strategy. Recently, the DevOps team had to roll back a deployment when a new version of the application dramatically increased response times for requests. The DevOps team needs to use a deployment strategy that will allow the team to monitor a new version of the application before the team shifts all traffic to the new version. If a new version of the application increases response times, the deployment should be rolled back as quickly as possible. Which combination of steps will meet these requirements? (Choose two.) higher than the desired value. Associate the alarm with the CodeDeploy deployment group. Modify the deployment group to roll back when alarm thresholds are met. metric is higher than the desired value. Associate the alarm with the CodeDeploy deployment group. Modify the deployment group to roll back when alarm thresholds are met.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the CodeDeploy deployment to use the CodeDeployDefault.ECSCanary10Percent5Minutes configuration.
B. Modify the CodeDeploy deployment to use the CodeDeployDefault.ECSLinear10PercentEvery3Minutes configuration.
C. Create an Amazon CloudWatch alarm to monitor the UnHealthyHostCount metric for the ALB. Set the alarm to activate if the metric is higher than the desired value. Associate the alarm with the CodeDeploy deployment group. Modify the deployment group to roll back when a deployment fails.
D. Create an Amazon CloudWatch alarm to monitor the TargetResponseTime metric for the ALB. Set the alarm to activate if the metric is
E. Create an Amazon CloudWatch alarm to monitor the TargetConnectionErrorCount metric for the ALB. Set the alarm to activate if the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps团队支持运行在Amazon Elastic Container Service (Amazon ECS)集群中的应用程序，该集群位于Application Load Balancer (ALB)后面。目前，DevOps团队使用AWS CodeDeploy通过蓝/绿一次性全部部署策略来部署应用程序。最近，当应用程序的新版本显著增加了请求响应时间时，DevOps团队不得不回滚部署。DevOps团队需要使用一种部署策略，允许团队在将所有流量切换到新版本之前监控应用程序的新版本。如果应用程序的新版本增加了响应时间，部署应该尽快回滚。哪种步骤组合将满足这些要求？（选择两个） 选项： A. 修改CodeDeploy部署以使用CodeDeployDefault.ECSCanary10Percent5Minutes配置 B. 修改CodeDeploy部署以使用CodeDeployDefault.ECSLinear10PercentEvery3Minutes配置 C. 创建Amazon CloudWatch告警来监控ALB的UnHealthyHostCount指标。如果指标高于期望值，设置告警激活。将告警与CodeDeploy部署组关联。修改部署组在部署失败时回滚 D. 创建Amazon CloudWatch告警来监控ALB的TargetResponseTime指标。如果指标高于期望值，设置告警激活 E. 创建Amazon CloudWatch告警来监控ALB的TargetConnectionErrorCount指标。如果指标高于期望值，设置告警激活</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求选择一种部署策略，能够在将所有流量切换到新版本之前监控应用程序，并在响应时间增加时快速回滚。关键需求是：1）渐进式部署而非一次性全部部署；2）能够监控和自动回滚；3）针对响应时间问题的快速检测。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：支持ECS的蓝/绿部署，包括Canary和Linear部署策略 - Amazon ECS：容器服务，支持渐进式流量切换 - Application Load Balancer (ALB)：提供流量分发和健康检查 - Amazon CloudWatch：提供监控指标和告警功能 - 部署策略：Canary（金丝雀）vs Linear（线性）部署模式 **正确答案的原因：** 选项A（CodeDeployDefault.ECSCanary10Percent5Minutes）是正确的，因为： 1. Canary部署策略先将10%流量切换到新版本，等待5分钟观察期 2. 这允许团队在全量切换前监控新版本的表现 3. 如果在观察期内发现问题，可以快速回滚，只影响少量流量 4. 满足&quot;监控新版本后再全量切换&quot;的核心需求 题目显示还需要选择第二个答案，应该是关于CloudWatch告警监控TargetResponseTime指标的选项，用于自动检测响应时间问题并触发回滚。 **其他选项错误的原因：** - 选项B（Linear部署）：虽然也是渐进式部署，但线性部署会持续增加流量比例，没有观察等待期，不如Canary策略适合这种需要监控验证的场景 - 选项C（UnHealthyHostCount）：监控的是不健康主机数量，不能直接反映响应时间问题 - 选项E（TargetConnectionErrorCount）：监控连接错误，同样不能反映响应时间性能问题 **决策标准和最佳实践：** 1. 对于需要验证新版本性能的场景，Canary部署优于Linear部署 2. 监控指标应该与具体问题相关（响应时间问题用TargetResponseTime指标） 3. 自动回滚机制应该结合CloudWatch告警和CodeDeploy部署组配置 4. 渐进式部署策略能够最小化故障影响范围，提供快速回滚能力</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">339</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A security team must record the configuration of AWS resources, detect issues, and send notifications for findings. The main workload in the AWS account consists of an Amazon EC2 Auto Scaling group that scales in and out several times during the day. The team wants to be notified within 2 days if any Amazon EC2 security group allows traffic on port 22 for 0.0.0.0/0. The team also needs a snapshot of the configuration of the AWS resources to be taken routinely. The security team has already created and subscribed to an Amazon Simple Notification Service (Amazon SNS) topic. Which solution meets these requirements? Configure AWS Config to use the SNS topic as the target for notifications. Configure managed rule. Configure AWS Config to use the SNS topic as the target for notifications. Configure AWS Config to use the SNS topic as the target for notifications. to schedule the Lambda function to run once a day.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure AWS Config to use periodic recording for the AWS account. Deploy the vpc-sg-port-restriction-check AWS Config managed rule.
B. Configure AWS Config to use configuration change recording for the AWS account. Deploy the vpc-sg-open-only-to-authorized-ports AWS
C. Configure AWS Config to use configuration change recording for the AWS account. Deploy the ssh-restricted AWS Config managed rule.
D. Create an AWS Lambda function to evaluate security groups and publish a message to the SNS topic. Use an Amazon EventBridge rule</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个安全团队必须记录AWS资源的配置，检测问题，并为发现的问题发送通知。AWS账户中的主要工作负载包含一个Amazon EC2 Auto Scaling组，该组在一天中会多次进行扩展和收缩。团队希望在任何Amazon EC2安全组允许0.0.0.0/0在端口22上的流量时，能在2天内收到通知。团队还需要定期获取AWS资源配置的快照。安全团队已经创建并订阅了一个Amazon Simple Notification Service (Amazon SNS)主题。哪个解决方案满足这些要求？ 选项： A. 为AWS账户配置AWS Config使用周期性记录。部署vpc-sg-port-restriction-check AWS Config托管规则。 B. 为AWS账户配置AWS Config使用配置变更记录。部署vpc-sg-open-only-to-authorized-ports AWS Config托管规则。 C. 为AWS账户配置AWS Config使用配置变更记录。部署ssh-restricted AWS Config托管规则。 D. 创建AWS Lambda函数来评估安全组并向SNS主题发布消息。使用Amazon EventBridge规则来调度Lambda函数每天运行一次。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 记录AWS资源配置并检测问题 2. 检测安全组是否允许0.0.0.0/0在端口22上的流量 3. 在2天内发送通知 4. 定期获取资源配置快照 5. 与现有SNS主题集成 **涉及的关键AWS服务和概念：** - AWS Config：用于监控和记录AWS资源配置变更的服务 - AWS Config托管规则：预定义的合规性检查规则 - Amazon SNS：消息通知服务 - 周期性记录 vs 配置变更记录：两种不同的Config记录模式 **正确答案A的原因：** 1. **周期性记录适合需求**：由于要求在2天内通知，而不是实时通知，周期性记录更经济高效 2. **vpc-sg-port-restriction-check规则准确**：这个托管规则专门检查安全组是否限制了特定端口的访问，完全符合检测端口22对0.0.0.0/0开放的需求 3. **满足快照需求**：AWS Config自动提供资源配置的历史快照 4. **成本效益**：对于频繁变化的Auto Scaling环境，周期性记录比持续监控更经济 **其他选项错误的原因：** - **选项B**：vpc-sg-open-only-to-authorized-ports规则过于宽泛，不是专门针对端口22的检查 - **选项C**：ssh-restricted规则虽然相关，但不如vpc-sg-port-restriction-check精确；且使用配置变更记录在频繁扩缩容环境中成本较高 - **选项D**：自定义Lambda解决方案增加了复杂性和维护成本，而AWS Config托管规则已经提供了现成的解决方案 **决策标准和最佳实践：** 1. **选择合适的记录模式**：对于不需要实时响应的合规性检查，周期性记录更经济 2. **优先使用托管规则**：AWS Config托管规则经过充分测试，比自定义解决方案更可靠 3. **考虑成本因素**：在Auto Scaling环境中，配置变更记录会产生大量事件，增加成本 4. **规则精确性**：选择最符合具体检查需求的规则，避免过于宽泛或狭窄的规则</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">340</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has proprietary data available by using an Amazon CloudFront distribution. The company needs to ensure that the distribution is accessible by only users from the corporate office that have a known set of IP address ranges. An AWS WAF web ACL is associated with the distribution and has a default action set to Count. Which solution will meet these requirements with the LEAST operational overhead? Block. Associate the web ACL with the CloudFront distribution. Add a rule that allows traffic based on the new rule group. set to Allow. Associate the web ACL with the CloudFront distribution. Add a rule that allows traffic from the IP address set.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new regex pattern set. Add the regex pattern set to a new rule group. Create a new web ACL that has a default action set to
B. Create an AWS WAF IP address set that matches the corporate office IP address range. Create a new web ACL that has a default action
C. Create a new regex pattern set. Add the regex pattern set to a new rule group. Set the default action on the existing web ACL to Allow. Add a rule that has priority 0 that allows traffic based on the regex pattern set.
D. Create a WAF IP address set that matches the corporate office IP address range. Set the default action on the existing web ACL to Block. Add a rule that has priority 0 that allows traffic from the IP address set.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司通过Amazon CloudFront分发有专有数据。该公司需要确保该分发仅能被来自公司办公室的用户访问，这些用户具有已知的IP地址范围集合。一个AWS WAF web ACL已与该分发关联，并且默认操作设置为Count。哪种解决方案能以最少的运营开销满足这些要求？ 选项： A. 创建一个新的regex pattern set。将regex pattern set添加到新的rule group中。创建一个默认操作设置为Block的新web ACL。将web ACL与CloudFront分发关联。添加一个基于新rule group允许流量的规则。 B. 创建一个匹配公司办公室IP地址范围的AWS WAF IP address set。创建一个默认操作设置为Allow的新web ACL。将web ACL与CloudFront分发关联。添加一个允许来自IP address set流量的规则。 C. 创建一个新的regex pattern set。将regex pattern set添加到新的rule group中。将现有web ACL的默认操作设置为Allow。添加一个优先级为0的规则，该规则基于regex pattern set允许流量。 D. 创建一个匹配公司办公室IP地址范围的WAF IP address set。将现有web ACL的默认操作设置为Block。添加一个优先级为0的规则，该规则允许来自IP address set的流量。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现IP白名单访问控制，只允许来自公司办公室特定IP地址范围的用户访问CloudFront分发，同时要求最少的运营开销。 **涉及的关键AWS服务和概念：** - Amazon CloudFront：内容分发网络服务 - AWS WAF：Web应用防火墙，用于保护Web应用免受常见攻击 - WAF Web ACL：访问控制列表，定义允许或阻止的流量规则 - IP Address Set：WAF中用于匹配IP地址范围的组件 - Rule Priority：规则优先级，数字越小优先级越高 **正确答案D的原因：** 1. **使用IP Address Set**：这是处理IP地址范围匹配的正确WAF组件，直接针对问题需求 2. **修改现有WAF**：利用已存在的web ACL，避免重新创建和重新关联的开销 3. **正确的安全逻辑**：默认Block + 优先级0的Allow规则，确保只有白名单IP能访问 4. **最少运营开销**：不需要创建新的web ACL，不需要重新关联CloudFront **其他选项错误的原因：** - **选项A**：使用regex pattern set处理IP地址是错误的技术选择，regex用于字符串模式匹配而非IP地址范围 - **选项B**：创建新的web ACL增加了不必要的运营开销，且默认Allow + Allow规则的逻辑配置不当 - **选项C**：同样错误使用regex pattern set，且默认Allow会允许所有流量，违背安全要求 **决策标准和最佳实践：** 1. **选择合适的WAF组件**：IP地址控制应使用IP Address Set而非regex pattern 2. **安全优先原则**：采用&quot;默认拒绝，明确允许&quot;的安全模型 3. **运营效率**：优先利用现有资源，避免不必要的重复配置 4. **规则优先级管理**：使用priority 0确保白名单规则优先执行</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">341</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs several applications in the same AWS account. The applications send logs to Amazon CloudWatch. A data analytics team needs to collect performance metrics and custom metrics from the applications. The analytics team needs to transform the metrics data before storing the data in an Amazon S3 bucket. The analytics team must automatically collect any new metrics that are added to the CloudWatch namespace. Which solution will meet these requirements with the LEAST operational overhead? stream. Configure the Firehose delivery stream to invoke an AWS Lambda function to transform the data. Configure the delivery stream to send the transformed data to the S3 bucket.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a CloudWatch metric stream to include metrics from the application and the CloudWatch namespace. Configure the metric stream to deliver the metrics to an Amazon Data Firehose delivery stream. Configure the Firehose delivery stream to invoke an AWS Lambda function to transform the data. Configure the delivery stream to send the transformed data to the S3 bucket.
B. Configure a CloudWatch metrics stream to include all the metrics and to deliver the metrics to an Amazon Data Firehose delivery
C. Configure metric filters for the CloudWatch logs to create custom metrics. Configure a CloudWatch metric stream to deliver the application metrics to the S3 bucket.
D. Configure subscription filters on the application log groups to target an Amazon Data Firehose delivery stream. Configure the Firehose delivery stream to invoke an AWS Lambda function to transform the data. Configure the delivery stream to send the transformed data to the S3 bucket.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在同一个AWS账户中运行多个应用程序。这些应用程序将日志发送到Amazon CloudWatch。数据分析团队需要从应用程序中收集性能指标和自定义指标。分析团队需要在将指标数据存储到Amazon S3存储桶之前对数据进行转换。分析团队必须自动收集添加到CloudWatch命名空间的任何新指标。哪种解决方案能够以最少的运营开销满足这些要求？ 选项： A. 配置CloudWatch metric stream以包含来自应用程序和CloudWatch命名空间的指标。配置metric stream将指标传递到Amazon Data Firehose delivery stream。配置Firehose delivery stream调用AWS Lambda函数来转换数据。配置delivery stream将转换后的数据发送到S3存储桶。 B. 配置CloudWatch metrics stream以包含所有指标并将指标传递到Amazon Data Firehose delivery stream。 C. 为CloudWatch日志配置metric filters以创建自定义指标。配置CloudWatch metric stream将应用程序指标传递到S3存储桶。 D. 在应用程序日志组上配置subscription filters以目标Amazon Data Firehose delivery stream。配置Firehose delivery stream调用AWS Lambda函数来转换数据。配置delivery stream将转换后的数据发送到S3存储桶。 正确答案：B</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 收集CloudWatch中的性能指标和自定义指标 - 在存储到S3之前转换指标数据 - 自动收集添加到CloudWatch命名空间的新指标 - 最少的运营开销 **涉及的关键AWS服务和概念：** - CloudWatch Metric Streams：用于实时流式传输CloudWatch指标数据 - Amazon Data Firehose：用于数据传输和转换的托管服务 - AWS Lambda：用于数据转换的无服务器计算服务 - Amazon S3：目标存储服务 - CloudWatch Logs和Metric Filters：日志处理和指标创建 **正确答案B的原因：** 虽然选项B在题目中显示不完整，但从上下文可以推断它应该包含完整的解决方案：配置CloudWatch metrics stream包含所有指标，传递到Firehose进行转换，然后存储到S3。这个方案： - 使用Metric Streams自动捕获所有现有和新增的指标 - 通过Firehose提供内置的数据转换能力 - 完全托管的服务，运营开销最小 - 自动处理新添加的指标，无需手动配置 **其他选项错误的原因：** - 选项A：功能正确但描述冗余，且可能不如选项B简洁 - 选项C：需要手动配置metric filters，无法自动收集新指标，运营开销大 - 选项D：使用subscription filters处理日志而非直接处理指标，不符合需求，且需要更多手动配置 **决策标准和最佳实践：** - 优先选择托管服务以减少运营开销 - 使用CloudWatch Metric Streams可以自动包含命名空间中的所有指标 - Firehose提供内置转换功能，比自定义Lambda更简单 - 避免需要持续手动维护的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">342</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an HPC platform to run analysis jobs for data. The company uses AWS CodeBuild to create container images and store the images on Amazon Elastic Container Registry (Amazon ECR). The images are then deployed on Amazon Elastic Kubernetes Service (Amazon EKS). To maintain compliance, the company needs to ensure that the images are signed before the images are deployed on Amazon EKS. The signing keys must be rotated periodically and must be managed automatically. The company needs to track who generates the signatures. Which solution will meet these requirements with the LEAST operational effort?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use CodeBuild to retrieve the image that was previously pushed to Amazon ECR. Use AWS Signer to sign the image. Use AWS CloudTrail to track who generates the signatures.
B. Use AWS Lambda to retrieve the image that was previously pushed to Amazon ECR. Use a Lambda function to sign the image. Use Amazon CloudWatch to track who generates the signatures.
C. Use AWS Lambda to retrieve the image that was previously pushed to Amazon ECR. Use AWS Signer to sign the image. Use Amazon CloudWatch to track who generates the signatures.
D. Use CodeBuild to build the image. Sign the image by using AWS Signer before pushing the image to Amazon ECR. Use AWS CloudTrail to track who generates the signatures.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用HPC平台来运行数据分析作业。该公司使用AWS CodeBuild创建容器镜像并将镜像存储在Amazon Elastic Container Registry (Amazon ECR)上。然后将镜像部署在Amazon Elastic Kubernetes Service (Amazon EKS)上。为了保持合规性，公司需要确保镜像在部署到Amazon EKS之前已经签名。签名密钥必须定期轮换并且必须自动管理。公司需要跟踪谁生成了签名。哪个解决方案能以最少的运维工作量满足这些要求？ 选项： A. 使用CodeBuild检索之前推送到Amazon ECR的镜像。使用AWS Signer对镜像进行签名。使用AWS CloudTrail跟踪谁生成了签名。 B. 使用AWS Lambda检索之前推送到Amazon ECR的镜像。使用Lambda函数对镜像进行签名。使用Amazon CloudWatch跟踪谁生成了签名。 C. 使用AWS Lambda检索之前推送到Amazon ECR的镜像。使用AWS Signer对镜像进行签名。使用Amazon CloudWatch跟踪谁生成了签名。 D. 使用CodeBuild构建镜像。在将镜像推送到Amazon ECR之前使用AWS Signer对镜像进行签名。使用AWS CloudTrail跟踪谁生成了签名。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是容器镜像签名的最佳实践，要求满足以下条件：1）镜像必须在部署前签名；2）签名密钥需要自动管理和定期轮换；3）需要跟踪签名生成者；4）运维工作量最少。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：持续集成服务，用于构建和测试代码 - Amazon ECR：托管的Docker容器注册表服务 - AWS Signer：代码签名服务，提供自动密钥管理和轮换 - AWS CloudTrail：API调用审计和跟踪服务 - AWS Lambda：无服务器计算服务 - 容器镜像签名：确保镜像完整性和来源可信的安全实践 **正确答案D的原因：** 1. **工作流程最优化**：在CodeBuild构建过程中直接签名，避免了额外的检索步骤，符合CI/CD最佳实践 2. **AWS Signer的优势**：提供托管的签名服务，自动处理密钥轮换和管理，满足自动化要求 3. **CloudTrail审计**：能够准确跟踪AWS Signer的API调用，记录签名操作的执行者 4. **运维工作量最少**：整个流程集成在现有的构建管道中，无需额外的基础设施 **其他选项错误的原因：** - **选项A**：需要先推送未签名镜像再检索签名，增加了不必要的步骤和安全风险 - **选项B**：使用Lambda自定义签名功能需要自己管理密钥轮换，违背了自动管理要求；CloudWatch无法有效跟踪签名生成者 - **选项C**：虽然使用了AWS Signer，但Lambda检索已推送镜像的方式增加了复杂性；CloudWatch不是审计跟踪的最佳选择 **决策标准和最佳实践：** 1. **左移安全**：在构建阶段就进行签名，而不是事后处理 2. **托管服务优先**：使用AWS Signer而非自建签名方案 3. **审计完整性**：使用CloudTrail进行完整的API调用跟踪 4. **流程简化**：集成到现有CI/CD管道中，避免额外的架构复杂性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">343</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an AWS CodeArtifact repository to store Python packages that the company developed internally. A DevOps engineer needs to use AWS CodeDeploy to deploy an application to an Amazon EC2 instance. The application uses a Python package that is stored in the CodeArtifact repository. A BeforeInstall lifecycle event hook will install the package. The DevOps engineer needs to grant the EC2 instance access to the CodeArtifact repository. Which solution will meet this requirement?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a service-linked role for CodeArtifact. Associate the role with the EC2 instance. Use the aws codeartifact get-authorization-token CLI command on the instance.
B. Configure a resource-based policy for the CodeArtifact repository that allows the ReadFromRepository action for the EC2 instance principal.
C. Configure ACLs on the CodeArtifact repository to allow the EC2 instance to access the Python package.
D. Create an instance profile that contains an IAM role that has access to CodeArtifact. Associate the instance profile with the EC2 instance. Use the aws codeartifact login CLI command on the instance.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS CodeArtifact存储库来存储公司内部开发的Python包。DevOps工程师需要使用AWS CodeDeploy将应用程序部署到Amazon EC2实例。该应用程序使用存储在CodeArtifact存储库中的Python包。BeforeInstall生命周期事件钩子将安装该包。DevOps工程师需要授予EC2实例访问CodeArtifact存储库的权限。哪种解决方案能满足这个要求？ 选项： A. 为CodeArtifact创建服务链接角色。将该角色与EC2实例关联。在实例上使用aws codeartifact get-authorization-token CLI命令。 B. 为CodeArtifact存储库配置基于资源的策略，允许EC2实例主体执行ReadFromRepository操作。 C. 在CodeArtifact存储库上配置ACL，允许EC2实例访问Python包。 D. 创建包含具有CodeArtifact访问权限的IAM角色的实例配置文件。将实例配置文件与EC2实例关联。在实例上使用aws codeartifact login CLI命令。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为EC2实例配置访问CodeArtifact存储库的权限，以便在CodeDeploy部署过程中能够下载和安装Python包。 **涉及的关键AWS服务和概念：** - AWS CodeArtifact：用于存储和管理软件包的托管服务 - AWS CodeDeploy：应用程序部署服务 - EC2实例配置文件（Instance Profile）：允许EC2实例承担IAM角色的机制 - IAM角色和权限管理 - CodeArtifact身份验证机制 **正确答案D的原因：** 1. **标准的EC2权限模式**：使用实例配置文件是为EC2实例分配权限的标准和推荐方式 2. **正确的身份验证流程**：`aws codeartifact login`命令会自动获取授权令牌并配置pip等包管理器 3. **完整的解决方案**：既提供了权限（IAM角色），又提供了访问机制（实例配置文件和login命令） 4. **安全性**：避免了在实例上存储长期凭证，使用临时凭证 **其他选项错误的原因：** - **选项A**：服务链接角色是AWS服务代表用户执行操作时使用的，不适用于EC2实例访问CodeArtifact的场景；且`get-authorization-token`只获取令牌但不配置包管理器 - **选项B**：CodeArtifact主要使用基于身份的策略而非资源策略；且单独的资源策略无法解决EC2实例的身份验证问题 - **选项C**：CodeArtifact不使用传统的ACL机制，而是使用IAM策略进行访问控制 **决策标准和最佳实践：** 1. 对于EC2实例访问AWS服务，始终使用实例配置文件和IAM角色 2. 使用CodeArtifact时，应使用`aws codeartifact login`进行身份验证和配置 3. 遵循最小权限原则，只授予必要的CodeArtifact权限 4. 避免在实例上硬编码凭证，使用临时凭证机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">344</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a file-reading application that saves files to a database that runs on Amazon EC2 instances. Regulations require the company to delete files from EC2 instances every day at a specific time. The company must delete database records that are older than 60 days. The database record deletion must occur after the file deletions. The company has created scripts to delete files and database records. The company needs to receive an email notification for any failure of the deletion scripts. Which solution will meet these requirements with the LEAST development effort? statement inside the Automation document as the last step to check for errors. Use Amazon Simple Email Service (Amazon SES) to send failure notifications as email messages to the company.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Systems Manager State Manager to automatically invoke a Systems Manager Automation document at the specified time each day. Configure the Automation document to use a run command to run the deletion scripts in sequential order. Create an Amazon EventBridge rule to use Amazon Simple Notification Service (Amazon SNS) to send failure notifications to the company.
B. Use AWS Systems Manager State Manager to automatically invoke a Systems Manager Automation document at the specified time each day. Configure the Automation document to use a run command to run the deletion scripts in sequential order. Create a conditional
C. Create an Amazon EventBridge rule that invokes an AWS Lambda function at the specified time. Add the necessary permissions for the invocation to the Lambda function&#x27;s resource-based policy. Configure the Lambda function to run the deletion scripts in sequential order. Configure the Lambda function to use Amazon Simple Notification Service (Amazon SNS) to send failure notifications to the company.
D. Create an Amazon EventBridge rule that invokes an AWS Lambda function at the specified time. Add the necessary permissions for the invocation to the Lambda function&#x27;s resource-based policy. Configure the Lambda function to run the deletion scripts in sequential order. Configure the Lambda function to use Amazon Simple Email Service (Amazon SES) to send failure notifications as email messages to the company.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个文件读取应用程序，将文件保存到运行在Amazon EC2实例上的数据库中。法规要求公司每天在特定时间从EC2实例删除文件。公司必须删除超过60天的数据库记录。数据库记录删除必须在文件删除之后进行。公司已经创建了删除文件和数据库记录的脚本。公司需要在删除脚本失败时收到电子邮件通知。哪种解决方案能以最少的开发工作量满足这些要求？ 选项： A. 使用AWS Systems Manager State Manager每天在指定时间自动调用Systems Manager Automation文档。配置Automation文档使用run command按顺序运行删除脚本。创建Amazon EventBridge规则使用Amazon Simple Notification Service (Amazon SNS)向公司发送失败通知。 B. 使用AWS Systems Manager State Manager每天在指定时间自动调用Systems Manager Automation文档。配置Automation文档使用run command按顺序运行删除脚本。在Automation文档中创建条件语句作为最后一步检查错误。使用Amazon Simple Email Service (Amazon SES)向公司发送失败通知邮件。 C. 创建Amazon EventBridge规则在指定时间调用AWS Lambda函数。为Lambda函数的基于资源的策略添加必要的调用权限。配置Lambda函数按顺序运行删除脚本。配置Lambda函数使用Amazon Simple Notification Service (Amazon SNS)向公司发送失败通知。 D. 创建Amazon EventBridge规则在指定时间调用AWS Lambda函数。为Lambda函数的基于资源的策略添加必要的调用权限。配置Lambda函数按顺序运行删除脚本。配置Lambda函数使用Amazon Simple Email Service (Amazon SES)向公司发送失败通知邮件。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 每天定时删除EC2实例上的文件 2. 删除超过60天的数据库记录（必须在文件删除后执行） 3. 脚本执行失败时发送邮件通知 4. 要求最少的开发工作量 **涉及的关键AWS服务和概念：** - AWS Systems Manager State Manager：用于自动化运维任务的调度和执行 - Systems Manager Automation：提供预定义的自动化工作流 - Amazon EventBridge：事件驱动的服务，可以触发其他AWS服务 - AWS Lambda：无服务器计算服务 - Amazon SNS：简单通知服务，支持多种通知方式 - Amazon SES：简单邮件服务，专门用于发送邮件 **正确答案A的原因：** 1. **最少开发工作量**：Systems Manager State Manager和Automation提供了现成的自动化框架，无需编写复杂的调度和执行逻辑 2. **原生集成**：State Manager可以直接调度Automation文档，Automation文档可以使用run command执行现有脚本 3. **错误处理**：EventBridge可以监听Systems Manager的执行状态事件，自动触发SNS通知 4. **顺序执行**：Automation文档天然支持步骤的顺序执行 5. **可靠性**：Systems Manager是专为运维自动化设计的托管服务 **其他选项错误的原因：** - **选项B**：虽然使用了正确的调度服务，但在Automation文档中添加条件语句和SES集成需要更多的开发工作，且SES需要额外的配置和验证 - **选项C和D**：使用Lambda需要编写函数代码来处理脚本执行、错误处理、顺序控制等逻辑，开发工作量较大。Lambda更适合轻量级的处理，而不是运行系统级的删除脚本 **决策标准和最佳实践：** 1. **服务选择**：对于定期的系统运维任务，优先选择Systems Manager而不是Lambda 2. **开发效率**：利用AWS托管服务的现有功能，减少自定义开发 3. **通知机制**：SNS比SES更适合系统通知，因为它支持多种通知方式且配置更简单 4. **事件驱动**：使用EventBridge监听服务状态变化是AWS推荐的最佳实践</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">345</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an organization in AWS Organizations that has all features enabled to manage its AWS accounts. Amazon EC2 instances run in the AWS accounts. The company requires that all current EC2 instances must use Instance Metadata Service Version 2 (IMDSv2). The company needs to block AWS API calls that originate from EC2 instances that do not use IMDSv2. Which solution will meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new SCP statement that denies the ec2:RunInstances action when the ec2:MetadataHttpTokens condition key is not equal to the value of required. Attach the SCP to the root of the organization.
B. Create a new SCP statement that denies the ec2:RunInstances action when the ec2:MetadataHttpPutResponseHopLimit condition key value is greater than two. Attach the SCP to the root of the organization.
C. Create a new SCP statement that denies &quot;*&quot; when the ec2:RoleDelivery condition key value is less than two. Attach the SCP to the root of the organization.
D. Create a new SCP statement that denies &quot;*&quot; when the ec2:MetadataHttpTokens condition key value is not equal to required. Attach the SCP to the root of the organization.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations中启用了所有功能的组织来管理其AWS账户。Amazon EC2实例在这些AWS账户中运行。公司要求所有当前的EC2实例必须使用Instance Metadata Service Version 2 (IMDSv2)。公司需要阻止来自未使用IMDSv2的EC2实例发起的AWS API调用。哪个解决方案能满足这些要求？ 选项： A. 创建一个新的SCP语句，当ec2:MetadataHttpTokens条件键不等于required值时拒绝ec2:RunInstances操作。将SCP附加到组织的根部。 B. 创建一个新的SCP语句，当ec2:MetadataHttpPutResponseHopLimit条件键值大于2时拒绝ec2:RunInstances操作。将SCP附加到组织的根部。 C. 创建一个新的SCP语句，当ec2:RoleDelivery条件键值小于2时拒绝&quot;*&quot;操作。将SCP附加到组织的根部。 D. 创建一个新的SCP语句，当ec2:MetadataHttpTokens条件键值不等于required时拒绝&quot;*&quot;操作。将SCP附加到组织的根部。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现两个目标：1）确保所有EC2实例使用IMDSv2；2）阻止来自未使用IMDSv2的实例的AWS API调用。关键是要理解题目要求的是预防性措施，防止创建不符合要求的实例。 **涉及的关键AWS服务和概念：** - AWS Organizations和Service Control Policies (SCP)：组织级别的权限控制 - EC2 Instance Metadata Service v2 (IMDSv2)：更安全的元数据服务版本 - IAM条件键：ec2:MetadataHttpTokens用于控制IMDSv2要求 - SCP策略语法和条件逻辑 **正确答案A的原因：** 选项A通过SCP在实例创建时进行控制，当ec2:MetadataHttpTokens不等于&quot;required&quot;时拒绝ec2:RunInstances操作。这是最有效的预防措施，从源头确保只能创建使用IMDSv2的实例。ec2:MetadataHttpTokens是控制IMDSv2的正确条件键，&quot;required&quot;值确保强制使用IMDSv2。 **其他选项错误的原因：** - 选项B：ec2:MetadataHttpPutResponseHopLimit条件键控制的是跳数限制，不是IMDSv2的核心要求 - 选项C：ec2:RoleDelivery不是有效的条件键，且逻辑不正确 - 选项D：虽然使用了正确的条件键，但拒绝&quot;*&quot;（所有操作）过于宽泛，会影响正常的API调用，而题目要求是防止创建不合规实例 **决策标准和最佳实践：** 1. 使用SCP进行组织级别的预防性控制比事后补救更有效 2. 选择精确的IAM条件键（ec2:MetadataHttpTokens）而非相关但非核心的条件 3. 策略应该具体针对问题（ec2:RunInstances）而非过度宽泛（&quot;*&quot;） 4. 在组织根部署SCP确保所有账户都受到约束</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">346</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps team supports an application that runs on a large number of Amazon EC2 instances in an Auto Scaling group. The DevOps team uses AWS CloudFormation to deploy the EC2 instances. The application recently experienced an issue. A single instance returned errors to a large percentage of requests. The EC2 instance responded as healthy to both Amazon EC2 and Elastic Load Balancing health checks. The DevOps team collects application logs in Amazon CloudWatch by using the embedded metric format. The DevOps team needs to receive an alert if any EC2 instance is responsible for more than half of all errors. Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CloudWatch Contributor Insights rule that groups logs from the CloudWatch application logs based on instance ID and errors.
B. Create a resource group in AWS Resource Groups. Use the CloudFormation stack to group the resources for the application. Add the application to CloudWatch Application Insights. Use the resource group to identify the application.
C. Create a metric filter for the application logs to count the occurrence of the term &quot;Error.&quot; Create a CloudWatch alarm that uses the METRIC_COUNT function to determine whether errors have occurred. Configure the CloudWatch alarm to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic to notify the DevOps team.
D. Create a CloudWatch alarm that uses the INSIGHT_RULE_METRIC function to determine whether a specific instance is responsible for more than half of all errors reported by EC2 instances. Configure the CloudWatch alarm to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic to notify the DevOps team.
E. Create a CloudWatch subscription filter for the application logs that filters for errors and invokes an AWS Lambda function. Configure the Lambda function to send the instance ID and error in a notification to an Amazon Simple Notification Service (Amazon SNS) topic to notify the DevOps team.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps团队支持运行在Auto Scaling组中大量Amazon EC2实例上的应用程序。DevOps团队使用AWS CloudFormation来部署EC2实例。应用程序最近遇到了一个问题：单个实例对大部分请求返回错误。该EC2实例对Amazon EC2和Elastic Load Balancing健康检查的响应都是健康的。DevOps团队使用嵌入式指标格式在Amazon CloudWatch中收集应用程序日志。DevOps团队需要在任何EC2实例负责超过一半的所有错误时收到警报。哪种步骤组合能够以最少的运营开销满足这些要求？（选择两个） 选项： A. 创建一个CloudWatch Contributor Insights规则，根据实例ID和错误对CloudWatch应用程序日志进行分组。 B. 在AWS Resource Groups中创建资源组。使用CloudFormation堆栈对应用程序资源进行分组。将应用程序添加到CloudWatch Application Insights。使用资源组来识别应用程序。 C. 为应用程序日志创建指标过滤器来计算&quot;Error&quot;术语的出现次数。创建使用METRIC_COUNT函数的CloudWatch警报来确定是否发生了错误。配置CloudWatch警报向Amazon Simple Notification Service (Amazon SNS)主题发送通知以通知DevOps团队。 D. 创建使用INSIGHT_RULE_METRIC函数的CloudWatch警报来确定特定实例是否负责EC2实例报告的超过一半的所有错误。配置CloudWatch警报向Amazon Simple Notification Service (Amazon SNS)主题发送通知以通知DevOps团队。 E. 为应用程序日志创建CloudWatch订阅过滤器，过滤错误并调用AWS Lambda函数。配置Lambda函数在通知中向Amazon Simple Notification Service (Amazon SNS)主题发送实例ID和错误以通知DevOps团队。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要监控并警报当单个EC2实例产生超过总错误数一半时的情况，要求最少的运营开销。关键是要能够按实例ID分组统计错误，并比较各实例的错误占比。 **涉及的关键AWS服务和概念：** - CloudWatch Contributor Insights：用于分析日志数据中的顶级贡献者 - CloudWatch嵌入式指标格式：结构化日志格式 - CloudWatch警报和指标过滤器：监控和通知机制 - INSIGHT_RULE_METRIC函数：基于Contributor Insights规则创建指标 - Auto Scaling组和EC2实例监控 **正确答案的原因：** 选项A是正确的，因为CloudWatch Contributor Insights专门设计用于识别日志数据中的顶级贡献者。它可以： - 自动按实例ID分组错误日志 - 计算每个实例的错误贡献比例 - 识别产生异常高错误率的实例 - 提供低运营开销的解决方案 题目显示正确答案只有A，这意味着还需要选择第二个正确答案。根据逻辑，应该是选项D，因为INSIGHT_RULE_METRIC函数可以基于Contributor Insights规则创建CloudWatch警报。 **其他选项错误的原因：** - 选项B：CloudWatch Application Insights主要用于应用程序性能监控，不是专门用于按实例分析错误分布的最佳工具 - 选项C：METRIC_COUNT只能统计总错误数，无法按实例分组比较各实例的错误占比 - 选项E：Lambda函数方案增加了不必要的复杂性和运营开销，需要自定义代码来实现统计逻辑 **决策标准和最佳实践：** 1. **最少运营开销**：选择AWS托管服务而非自定义解决方案 2. **功能匹配度**：Contributor Insights专门用于识别异常贡献者 3. **自动化程度**：避免需要手动编码和维护的方案 4. **可扩展性**：解决方案应能处理大量EC2实例的场景 5. **实时性**：能够及时检测和警报异常情况</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">347</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using AWS CloudFormation to perform deployments of its application environment. A deployment failed during a recent update to the existing CloudFormation stack. A DevOps engineer discovered that some resources in the stack were manually modified. The DevOps engineer needs a solution that detects manual modification of resources and sends an alert to the DevOps lead. Which solution will meet these requirements with the LEAST operational effort?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps lead to the topic by using an email address. Create an AWS Config managed rule that has the CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK identifier. Create an Amazon EventBridge rule that is invoked on the NON_COMPLIANT resources status. Set the SNS topic as the rule target.
B. Tag all CloudFormation resources with a specific tag. Create an AWS Config custom rule by using the AWS Config Rules Development Kit Library (RDKlib) that checks all resource changes that have the specific tag. Configure the custom rule to mark all the tagged resource changes as NON_COMPLIANT when the change is not performed by CloudFormation. Create an Amazon EventBridge rule that is invoked on the NON_COMPLIANT resources status. Create an AWS Lambda function that sends an email message to the DevOps lead. Set the Lambda function as the rule target.
C. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps lead to the topic by using an email address. Create an AWS Config managed rule that has the CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK identifier. Create an Amazon EventBridge rule that is invoked on the COMPLIANT resources status. Set the SNS topic as the rule target.
D. Create an AWS Config managed rule that has the CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK identifier. Create an Amazon EventBridge rule that is invoked on the NON_COMPLIANT resources status. Create an AWS Lambda function that sends an email message to the DevOps lead. Set the Lambda function as the rule target.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS CloudFormation来执行其应用程序环境的部署。在最近对现有CloudFormation堆栈进行更新时，部署失败了。一名DevOps工程师发现堆栈中的一些资源被手动修改了。DevOps工程师需要一个解决方案来检测资源的手动修改并向DevOps负责人发送警报。哪个解决方案能以最少的运营工作量满足这些要求？ 选项： A. 创建一个Amazon Simple Notification Service (Amazon SNS) 主题。通过电子邮件地址让DevOps负责人订阅该主题。创建一个具有CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK标识符的AWS Config托管规则。创建一个Amazon EventBridge规则，当资源状态为NON_COMPLIANT时触发。将SNS主题设置为规则目标。 B. 为所有CloudFormation资源标记特定标签。使用AWS Config Rules Development Kit Library (RDKlib)创建AWS Config自定义规则，检查所有具有特定标签的资源更改。配置自定义规则，当更改不是由CloudFormation执行时，将所有标记的资源更改标记为NON_COMPLIANT。创建一个Amazon EventBridge规则，当资源状态为NON_COMPLIANT时触发。创建一个AWS Lambda函数向DevOps负责人发送电子邮件。将Lambda函数设置为规则目标。 C. 创建一个Amazon Simple Notification Service (Amazon SNS) 主题。通过电子邮件地址让DevOps负责人订阅该主题。创建一个具有CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK标识符的AWS Config托管规则。创建一个Amazon EventBridge规则，当资源状态为COMPLIANT时触发。将SNS主题设置为规则目标。 D. 创建一个具有CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK标识符的AWS Config托管规则。创建一个Amazon EventBridge规则，当资源状态为NON_COMPLIANT时触发。创建一个AWS Lambda函数向DevOps负责人发送电子邮件。将Lambda函数设置为规则目标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个解决方案来检测CloudFormation堆栈资源的手动修改（即堆栈漂移），并在检测到时向DevOps负责人发送警报，同时要求运营工作量最少。 **涉及的关键AWS服务和概念：** 1. **AWS Config** - 用于监控和评估AWS资源配置的服务 2. **CloudFormation Stack Drift Detection** - 检测堆栈资源是否偏离了模板定义的配置 3. **Amazon EventBridge** - 事件驱动架构的核心服务，用于路由事件 4. **Amazon SNS** - 简单通知服务，用于发送消息和通知 5. **AWS Lambda** - 无服务器计算服务 6. **CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK** - AWS Config的托管规则标识符 **正确答案A的原因：** 1. 使用AWS Config托管规则CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK，这是专门用于检测CloudFormation堆栈漂移的现成规则 2. 通过EventBridge监听NON_COMPLIANT状态，当检测到堆栈漂移时触发 3. 直接使用SNS发送邮件通知，无需额外的Lambda函数 4. 架构简单，运营开销最小，完全使用托管服务 **其他选项错误的原因：** - **选项B错误：** 需要创建自定义规则和使用RDKlib，增加了开发和维护复杂性，不符合&quot;最少运营工作量&quot;的要求 - **选项C错误：** 监听COMPLIANT状态而不是NON_COMPLIANT状态，这意味着在资源合规时发送警报，逻辑完全错误 - **选项D错误：** 虽然逻辑正确，但需要额外创建和维护Lambda函数，而SNS可以直接发送邮件，增加了不必要的复杂性 **决策标准和最佳实践：** 1. **最小运营开销原则** - 优先选择托管服务而非自定义解决方案 2. **使用专用工具** - AWS Config的CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK规则专门为此场景设计 3. **简化架构** - 能用SNS直接发送通知就不需要Lambda函数 4. **事件驱动设计** - 使用EventBridge实现松耦合的事件驱动架构 5. **监控合规性** - 通过NON_COMPLIANT状态准确识别问题资源</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">348</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer deployed multiple AWS accounts by using AWS Control Tower to support different business, technical, and administrative units in a company. A security team needs the DevOps engineer to automate AWS Control Tower guardrails for the company. The guardrails must be applied to all accounts in an OU of the company&#x27;s organization in AWS Organizations. The security team needs a solution that has version control and can be reviewed and rolled back if necessary. The security team will maintain the management of the solution in its OU. The security team wants to limit the type of guardrails that are allowed and allow only new guardrails that are approved by the security team. Which solution will meet these requirements with the MOST operational efficiency? CodePipeline pipeline in the security team&#x27;s account. Advise the security team to invoke the pipeline and provide these parameters when starting the pipeline.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in an AWS CodeCommit repository. Create an AWS::ControlTower::EnableControl logical resource in the template for each OU in the organization. Configure an AWS CodeBuild project that an Amazon EventBridge rule will invoke for the security team&#x27;s AWS CodeCommit changes.
B. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in an AWS CodeCommit repository. Create an AWS::ControlTower::EnableControl logical resource in the template for each account in the organization. Configure an AWS
C. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in an AWS CodeCommit repository. Create an AWS::ControlTower::EnableControl logical resource in the template for each OU in the organization. Configure an AWS CodePipeline pipeline in the security team&#x27;s account that an Amazon EventBridge rule will invoke for the security team&#x27;s CodeCommit changes.
D. Configure an AWS CodePipeline pipeline in the security team&#x27;s account that an Amazon EventBridge rule will invoke for PutObject events to an Amazon S3 bucket. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in the S3 bucket. Create an AWS::ControlTower::EnableControl logical resource in the template for each OU in the organization.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师使用AWS Control Tower部署了多个AWS账户，以支持公司中不同的业务、技术和管理单元。安全团队需要DevOps工程师为公司自动化AWS Control Tower guardrails。这些guardrails必须应用到AWS Organizations组织中公司OU的所有账户。安全团队需要一个具有版本控制功能的解决方案，可以进行审查并在必要时回滚。安全团队将在其OU中维护解决方案的管理。安全团队希望限制允许的guardrails类型，只允许经安全团队批准的新guardrails。哪个解决方案能以最高的运营效率满足这些要求？ 选项： A. 创建与guardrail对应的单独AWS CloudFormation模板。将模板存储在AWS CodeCommit存储库中。在模板中为组织中的每个OU创建AWS::ControlTower::EnableControl逻辑资源。配置AWS CodeBuild项目，由Amazon EventBridge规则为安全团队的AWS CodeCommit更改调用。 B. 创建与guardrail对应的单独AWS CloudFormation模板。将模板存储在AWS CodeCommit存储库中。在模板中为组织中的每个账户创建AWS::ControlTower::EnableControl逻辑资源。配置AWS... C. 创建与guardrail对应的单独AWS CloudFormation模板。将模板存储在AWS CodeCommit存储库中。在模板中为组织中的每个OU创建AWS::ControlTower::EnableControl逻辑资源。在安全团队账户中配置AWS CodePipeline管道，由Amazon EventBridge规则为安全团队的CodeCommit更改调用。 D. 在安全团队账户中配置AWS CodePipeline管道，由Amazon EventBridge规则为Amazon S3存储桶的PutObject事件调用。创建与guardrail对应的单独AWS CloudFormation模板。将模板存储在S3存储桶中。在模板中为组织中的每个OU创建AWS::ControlTower::EnableControl逻辑资源。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动化AWS Control Tower guardrails的解决方案，需要满足以下关键需求：1）应用到OU级别的所有账户；2）具备版本控制功能；3）支持审查和回滚；4）由安全团队管理和控制；5）限制guardrails类型并需要安全团队批准；6）追求最高运营效率。 **涉及的关键AWS服务和概念：** - AWS Control Tower：提供多账户治理和guardrails管理 - AWS::ControlTower::EnableControl：CloudFormation资源类型，用于启用Control Tower guardrails - AWS CodeCommit：Git版本控制服务，提供代码版本管理 - AWS CodePipeline：持续集成/持续部署服务 - AWS CodeBuild：构建服务 - Amazon EventBridge：事件驱动架构服务 - AWS Organizations OU：组织单元概念 **正确答案C的原因：** 选项C完美满足所有要求：1）使用CodeCommit提供Git版本控制，支持审查和回滚；2）针对OU级别配置AWS::ControlTower::EnableControl，符合题目要求；3）使用CodePipeline提供完整的CI/CD流程，包括构建、测试、部署等阶段，运营效率最高；4）通过EventBridge自动触发，实现事件驱动的自动化；5）安全团队通过CodeCommit的分支保护和合并请求机制控制guardrails的批准流程。 **其他选项错误的原因：** - 选项A：使用CodeBuild而非CodePipeline，缺少完整的CI/CD流程管理，无法提供pipeline的审批、测试等高级功能，运营效率较低 - 选项B：描述不完整，且针对每个账户而非OU配置，不符合题目要求 - 选项D：使用S3存储模板而非CodeCommit，缺乏Git版本控制功能，无法提供分支管理、合并请求等版本控制特性 **决策标准和最佳实践：** 在选择自动化部署方案时应考虑：1）版本控制能力：CodeCommit &gt; S3存储；2）CI/CD完整性：CodePipeline &gt; CodeBuild单独使用；3）治理粒度：OU级别 &gt; 账户级别，更易管理；4）审批流程：Git工作流提供更好的代码审查机制；5）运营效率：完整的pipeline提供更好的可视化、监控和管理能力。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">349</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs a web application on Amazon Elastic Kubernetes Service (Amazon EKS). The company uses Amazon CloudFront to distribute the application. The company recently enabled AWS WAF. The company set up Amazon CloudWatch Logs to send logs to an aws-waf-logs log group. The company wants a DevOps engineer to receive alerts if there are sudden changes in blocked traffic. The company does not want to receive alerts for other changes in AWS WAF log behavior. The company will tune AWS WAF rules over time. The DevOps engineer is currently subscribed to an Amazon Simple Notification Service (Amazon SNS) topic in the environment. Which solution will meet these requirements? alarm that activates when the sum of blocked requests in the custom metric during a period of 1 hour is greater than a static estimate for the acceptable number of blocked requests in 1 hour. Configure the alarm to notify the SNS topic to alert the DevOps engineer.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CloudWatch Logs metrics filter for blocked requests on the AWS WAF log group to create a custom metric. Create a CloudWatch alarm by using CloudWatch anomaly detection and the published custom metric. Configure the alarm to notify the SNS topic to alert the DevOps engineer.
B. Create a CloudWatch anomaly detector for the log group. Create a CloudWatch alarm by using metrics that the CloudWatch anomaly detector publishes. Use the high setting for the LogAnomalyPriority metric. Configure the alarm to go into alarm state if a static threshold of one anomaly is detected. Configure the alarm to notify the SNS topic to alert the DevOps engineer.
C. Create a CloudWatch metrics filter for counted requests on the AWS WAF log group to create a custom metric. Create a CloudWatch
D. Create a CloudWatch anomaly detector for the log group. Create a CloudWatch alarm by using metrics that the CloudWatch anomaly detector publishes. Use the medium setting for the LogAnomalyPriority metric. Configure the alarm to go into alarm state if a sum of anomalies over 1 hour is greater than an expected value. Configure the alarm to notify the SNS topic to alert the DevOps engineer.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Amazon Elastic Kubernetes Service (Amazon EKS)上运行Web应用程序。该公司使用Amazon CloudFront来分发应用程序。该公司最近启用了AWS WAF。该公司设置了Amazon CloudWatch Logs将日志发送到aws-waf-logs日志组。该公司希望DevOps工程师在被阻止的流量发生突然变化时收到警报。该公司不希望因AWS WAF日志行为的其他变化而收到警报。该公司将随时间调整AWS WAF规则。DevOps工程师目前已订阅环境中的Amazon Simple Notification Service (Amazon SNS)主题。哪种解决方案能满足这些要求？ 选项： A. 为AWS WAF日志组上的被阻止请求创建CloudWatch Logs指标过滤器以创建自定义指标。使用CloudWatch异常检测和发布的自定义指标创建CloudWatch警报。配置警报通知SNS主题以警告DevOps工程师。 B. 为日志组创建CloudWatch异常检测器。使用CloudWatch异常检测器发布的指标创建CloudWatch警报。为LogAnomalyPriority指标使用高设置。配置警报在检测到一个异常的静态阈值时进入警报状态。配置警报通知SNS主题以警告DevOps工程师。 C. 为AWS WAF日志组上的计数请求创建CloudWatch指标过滤器以创建自定义指标。创建CloudWatch警报，当1小时内自定义指标中被阻止请求的总和大于1小时内可接受被阻止请求数量的静态估计时激活。配置警报通知SNS主题以警告DevOps工程师。 D. 为日志组创建CloudWatch异常检测器。使用CloudWatch异常检测器发布的指标创建CloudWatch警报。为LogAnomalyPriority指标使用中等设置。配置警报在1小时内异常总和大于预期值时进入警报状态。配置警报通知SNS主题以警告DevOps工程师。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 该问题要求为AWS WAF被阻止流量的突然变化设置警报系统，关键需求包括：1）仅针对被阻止流量的突然变化发送警报；2）不对其他AWS WAF日志行为变化发送警报；3）考虑到WAF规则会随时间调整；4）通过现有SNS主题通知DevOps工程师。 **涉及的关键AWS服务和概念：** - AWS WAF：Web应用防火墙，用于过滤和阻止恶意流量 - CloudWatch Logs：日志存储和管理服务 - CloudWatch Metrics Filter：从日志中提取特定模式并创建自定义指标 - CloudWatch Anomaly Detection：基于机器学习的异常检测功能 - CloudWatch Alarms：基于指标阈值或异常的警报系统 - Amazon SNS：通知服务 **正确答案A的原因：** 选项A是最佳解决方案，因为：1）使用指标过滤器专门针对&quot;被阻止请求&quot;创建自定义指标，精确匹配需求；2）CloudWatch异常检测能够自动学习正常模式并识别突然变化，适应WAF规则的调整；3）异常检测比静态阈值更智能，能够区分正常的规则调整和真正的异常流量变化；4）直接集成SNS通知机制。 **其他选项错误的原因：** 选项B错误：使用日志组级别的异常检测器过于宽泛，会检测所有日志行为变化，不符合&quot;仅针对被阻止流量变化&quot;的要求。选项C错误：使用静态阈值无法适应WAF规则调整带来的正常变化，且题目中选项C似乎不完整。选项D错误：同样使用日志组级别检测，范围过宽，且&quot;异常总和大于预期值&quot;的配置逻辑不够精确。 **决策标准和最佳实践：** 1）精确性原则：使用指标过滤器精确提取被阻止请求指标；2）智能化原则：采用异常检测而非静态阈值，提高准确性；3）适应性原则：异常检测能够学习和适应WAF规则变化；4）集成性原则：充分利用现有SNS基础设施进行通知。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">350</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A video platform company is migrating its video catalog to AWS. The company will host MP4 video files in an Amazon S3 bucket. The company will use Amazon CloudFront and Amazon EC2 instances to serve the video files. Users first connect to a frontend application that redirects to a video URL. The video URL contains an authorization token in CloudFront. The cache is activated on the CloudFront distribution. Authorization token check activity needs to be logged in Amazon CloudWatch. The company wants to prevent direct access to video files on CloudFront and Amazon S3 and wants to implement checks of the authorization token that the frontend application provides. The company also wants to perform regular rolling updates of the code that checks the authorization token signature. Which solution will meet these requirements with the LEAST operational effort?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Implement an authorization token check in Lambda@Edge as a trigger on the CloudFront distribution. Enable CloudWatch logging for the Lambda@Edge function. Attach the Lambda@Edge function to the CloudFront distribution. Implement CloudFront continuous deployment to perform updates.
B. Implement an authorization token check in CloudFront Functions. Enable CloudWatch logging for the CloudFront function. Attach the CloudFront function to the CloudFront distribution. Implement CloudFront continuous deployment to perform updates.
C. Implement an authorization token check in the application code that is installed on the EC2 instances. Install the CloudWatch agent on the EC2 instances. Configure the application to log to the CloudWatch agent. Implement a second CloudFront distribution. Migrate the traffic from the first CloudFront distribution by using Amazon Route 53 weighted routing.
D. Implement an authorization token check in CloudFront Functions. Enable CloudWatch logging for the CloudFront function. Attach the CloudFront function to the CloudFront distribution. Implement a second CloudFront distribution. Migrate the traffic from the first CloudFront distribution by using Amazon Route 53 weighted routing.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家视频平台公司正在将其视频目录迁移到AWS。该公司将在Amazon S3存储桶中托管MP4视频文件。公司将使用Amazon CloudFront和Amazon EC2实例来提供视频文件服务。用户首先连接到前端应用程序，该应用程序重定向到视频URL。视频URL在CloudFront中包含授权令牌。CloudFront分发上启用了缓存。授权令牌检查活动需要记录在Amazon CloudWatch中。公司希望防止直接访问CloudFront和Amazon S3上的视频文件，并希望实现对前端应用程序提供的授权令牌的检查。公司还希望对检查授权令牌签名的代码执行定期滚动更新。哪种解决方案能以最少的运营工作量满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 防止直接访问CloudFront和S3上的视频文件 2. 实现授权令牌检查功能 3. 将授权检查活动记录到CloudWatch 4. 支持代码的定期滚动更新 5. 以最少的运营工作量实现上述功能 **涉及的关键AWS服务和概念：** - CloudFront Functions：轻量级JavaScript函数，在边缘位置执行 - Lambda@Edge：在CloudFront边缘位置运行的Lambda函数 - CloudFront continuous deployment：CloudFront的持续部署功能 - CloudWatch：监控和日志记录服务 - S3：对象存储服务 **正确答案B的原因：** 1. **CloudFront Functions最适合此场景**：专门设计用于简单的请求/响应处理，如授权令牌验证 2. **原生CloudWatch集成**：可以直接启用CloudWatch日志记录 3. **CloudFront continuous deployment**：提供了最简单的滚动更新机制，支持蓝绿部署 4. **运营开销最小**：无需管理额外的基础设施或复杂的流量迁移策略 5. **成本效益高**：CloudFront Functions比Lambda@Edge更便宜，执行时间更短 **其他选项错误的原因：** - **选项A**：Lambda@Edge虽然功能强大，但对于简单的令牌验证来说过于复杂，运营开销更大，成本更高 - **选项C**：在EC2实例上实现检查无法防止直接访问CloudFront，且需要管理CloudWatch agent，运营复杂度高 - **选项D**：使用Route 53加权路由和第二个CloudFront分发进行更新过于复杂，运营开销大，而CloudFront continuous deployment已经提供了更简单的解决方案 **决策标准和最佳实践：** 1. **选择合适的计算服务**：对于简单的边缘计算任务，CloudFront Functions优于Lambda@Edge 2. **最小化运营复杂度**：利用AWS原生功能（如continuous deployment）而不是自建解决方案 3. **成本优化**：选择最经济的服务来满足功能需求 4. **安全最佳实践**：在边缘进行授权检查，防止未授权访问到达源服务器</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">351</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses an organization in AWS Organizations to manage multiple AWS accounts in a hierarchical structure. An SCP that is associated with the organization root allows IAM users to be created. A DevOps team must be able to create IAM users with any level of permissions. Developers must also be able to create IAM users. However, developers must not be able to grant new IAM users excessive permissions. The developers have the CreateAndManageUsers role in each account. The DevOps team must be able to prevent other users from creating IAM users. Which combination of steps will meet these requirements? (Choose two.) who have the PermissionBoundaries policy to create new IAM users.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an SCP in the organization to deny users the ability to create and modify IAM users. Attach the SCP to the root of the organization. Attach the CreateAndManageUsers role to developers.
B. Create an SCP in the organization to grant users that have the DeveloperBoundary policy attached the ability to create new IAM users and to modify IAM users. Configure the SCP to require users to attach the PermissionBoundaries policy to any new IAM user. Attach the SCP to the root of the organization.
C. Create an IAM permissions policy named PermissionBoundaries within each account. Configure the PermissionBoundaries policy to specify the maximum permissions that a developer can grant to a new IAM user.
D. Create an IAM permissions policy named PermissionBoundaries within each account. Configure PermissionBoundaries to allow users
E. Create an IAM permissions policy named DeveloperBoundary within each account. Configure the DeveloperBoundary policy to allow developers to create IAM users and to assign policies to IAM users only if the developer includes the PermissionBoundaries policy as the permissions boundary. Attach the DeveloperBoundary policy to the CreateAndManageUsers role within each account.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations中的组织来管理分层结构中的多个AWS账户。与组织根关联的SCP允许创建IAM用户。DevOps团队必须能够创建具有任何级别权限的IAM用户。开发人员也必须能够创建IAM用户。但是，开发人员不得向新IAM用户授予过度权限。开发人员在每个账户中都有CreateAndManageUsers角色。DevOps团队必须能够阻止其他用户创建IAM用户。哪种步骤组合将满足这些要求？（选择两个） 选项： A. 在组织中创建SCP以拒绝用户创建和修改IAM用户的能力。将SCP附加到组织的根。将CreateAndManageUsers角色附加给开发人员。 B. 在组织中创建SCP以授予附加了DeveloperBoundary策略的用户创建新IAM用户和修改IAM用户的能力。配置SCP要求用户将PermissionBoundaries策略附加到任何新IAM用户。将SCP附加到组织的根。 C. 在每个账户内创建名为PermissionBoundaries的IAM权限策略。配置PermissionBoundaries策略以指定开发人员可以授予新IAM用户的最大权限。 D. 在每个账户内创建名为PermissionBoundaries的IAM权限策略。配置PermissionBoundaries以允许具有PermissionBoundaries策略的用户创建新IAM用户。 E. 在每个账户内创建名为DeveloperBoundary的IAM权限策略。配置DeveloperBoundary策略以允许开发人员创建IAM用户并仅在开发人员包含PermissionBoundaries策略作为权限边界时向IAM用户分配策略。将DeveloperBoundary策略附加到每个账户内的CreateAndManageUsers角色。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个权限控制方案，满足以下需求： 1. DevOps团队可以创建任何权限级别的IAM用户 2. 开发人员可以创建IAM用户，但不能授予过度权限 3. DevOps团队能够阻止其他用户创建IAM用户 4. 需要选择两个正确的步骤组合 **涉及的关键AWS服务和概念：** - AWS Organizations和Service Control Policies (SCP)：组织级别的权限控制 - IAM权限边界(Permission Boundaries)：限制用户或角色的最大权限 - IAM角色和策略：账户级别的权限管理 - 权限继承和优先级：SCP &gt; IAM策略 &gt; 权限边界 **正确答案的原因：** 题目显示正确答案是A，但从技术角度分析，这个答案存在问题： - 选项A创建了拒绝所有用户创建IAM用户的SCP，这会阻止包括DevOps团队在内的所有用户创建IAM用户 - 这与&quot;DevOps团队必须能够创建具有任何级别权限的IAM用户&quot;的要求相矛盾 - SCP的拒绝规则优先级最高，无法通过IAM策略覆盖 **更合理的解决方案应该是C+E的组合：** - 选项C：创建PermissionBoundaries策略定义开发人员可授予的最大权限 - 选项E：创建DeveloperBoundary策略，要求开发人员在创建用户时必须附加权限边界 **其他选项错误的原因：** - 选项B：SCP不能用于授予权限，只能限制权限 - 选项D：描述不完整，缺少关键的权限控制逻辑 **决策标准和最佳实践：** 1. 使用权限边界限制开发人员创建用户的最大权限 2. 通过IAM策略条件要求强制使用权限边界 3. 为不同角色设计分层的权限控制机制 4. SCP主要用于组织级别的权限限制，不用于权限授予</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">352</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has deployed a landing zone that has a well-defined AWS Organizations structure and an SCP. The company&#x27;s development team can create their AWS resources only by using AWS CloudFormation and the AWS Cloud Development Kit (AWS CDK). A DevOps engineer notices that Amazon Simple Queue Service (Amazon SQS) queues that are deployed in different CloudFormation stacks have different configurations. The DevOps engineer also notices that the application cost allocation tag is not always set. The DevOps engineer needs a solution that will enforce tagging and promote the reuse of code. The DevOps engineer needs to avoid different configurations for the deployed SQS queues. What should the DevOps engineer do to meet these requirements?</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Organizations tag policy to enforce the cost allocation tag in CloudFormation stacks. Instruct the development team to use CloudFormation to define SQS queues. Instruct the development team to deploy the SQS queues by using CloudFormation StackSets.
B. Update the SCP to enforce the cost allocation tag in CloudFormation stacks. Instruct the development team to use CloudFormation modules to define SQS queues. Instruct the development team to deploy the SQS queues by using CloudFormation stacks.
C. Use AWS CDK tagging to enforce the cost allocation tag in CloudFormation StackSets. Instruct the development team to use the AWS CDK to define SQS queues. Instruct the development team to deploy the SQS queues by using CDK stacks.
D. Use AWS CDK tagging to enforce the cost allocation tag in CloudFormation stacks. Instruct the development team to use the AWS CDK to define SQS queues. Instruct the development team to deploy the SQS queues by using CDK feature flags.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司部署了一个具有明确定义的AWS Organizations结构和SCP的landing zone。公司的开发团队只能通过使用AWS CloudFormation和AWS Cloud Development Kit (AWS CDK)来创建他们的AWS资源。一名DevOps工程师注意到在不同CloudFormation堆栈中部署的Amazon Simple Queue Service (Amazon SQS)队列具有不同的配置。DevOps工程师还注意到应用程序成本分配标签并不总是被设置。DevOps工程师需要一个解决方案来强制执行标签并促进代码重用。DevOps工程师需要避免已部署的SQS队列出现不同配置。DevOps工程师应该怎么做来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求解决三个主要问题：1）强制执行成本分配标签；2）促进代码重用；3）避免SQS队列配置不一致。需要找到一个统一的解决方案来标准化资源部署。 **涉及的关键AWS服务和概念：** - AWS Organizations和SCP（Service Control Policy）：用于组织级别的权限控制 - AWS CloudFormation：基础设施即代码服务 - AWS CDK：更高级的基础设施即代码工具 - CloudFormation StackSets：跨账户和区域部署堆栈的服务 - Amazon SQS：消息队列服务 - 标签策略和成本分配 **正确答案C的原因：** 1. **CDK tagging功能**：AWS CDK提供了强大的标签管理功能，可以在应用级别统一设置标签，确保所有资源都包含必需的成本分配标签 2. **CloudFormation StackSets**：能够跨多个账户和区域标准化部署，确保SQS队列配置的一致性 3. **代码重用**：CDK允许创建可重用的构造（constructs），促进代码标准化和重用 4. **配置一致性**：通过StackSets的集中管理，可以确保所有SQS队列使用相同的配置模板 **其他选项错误的原因：** - **选项A**：Organizations标签策略主要用于合规检查而非强制执行，且普通CloudFormation stacks无法解决跨环境一致性问题 - **选项B**：SCP主要用于权限控制而非标签强制执行，CloudFormation modules虽然支持重用但不如CDK灵活 - **选项D**：CDK feature flags主要用于控制CDK功能特性，不是部署机制，无法解决跨账户部署和配置一致性问题 **决策标准和最佳实践：** 1. **标签治理**：使用CDK的内置标签功能比依赖外部策略更可靠 2. **基础设施标准化**：StackSets是实现跨账户资源标准化的最佳选择 3. **代码重用性**：CDK constructs提供了比传统CloudFormation更好的代码重用机制 4. **运维效率**：集中化的部署和管理减少了配置漂移的风险</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">353</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps team manages a company&#x27;s AWS account. The company wants to ensure that specific AWS resource configuration changes are automatically reverted. Which solution will meet this requirement? changes need to be reverted. Get IT Certification Unlock free, top-quality video courses on ExamTopics with a simple registration. Elevate your learning journey with our expertly curated content. Register now to access a diverse range of educational resources designed for learners and professionals.</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Config rules to detect changes in resource configurations. Configure remediation action that uses AWS Systems Manager Automation documents to revert the configuration changes.
B. Use Amazon CloudWatch alarms to monitor resource metrics. When an alarm is activated, use an Amazon Simple Notification Service (Amazon SNS) topic to notify an administrator to manually revert the configuration changes.
C. Use AWS CloudFormation to create a stack that deploys the necessary configuration changes. Update the stack when configuration
D. Use AWS Trusted Advisor to check for noncompliant configurations. Manually apply necessary changes based on Trusted Advisor recommendations.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps团队管理公司的AWS账户。公司希望确保特定的AWS资源配置更改能够自动回滚。哪个解决方案能满足这个要求？ 选项： A. 使用AWS Config规则来检测资源配置的更改。配置修复操作，使用AWS Systems Manager自动化文档来回滚配置更改。 B. 使用Amazon CloudWatch告警来监控资源指标。当告警被触发时，使用Amazon Simple Notification Service (Amazon SNS)主题通知管理员手动回滚配置更改。 C. 使用AWS CloudFormation创建一个堆栈来部署必要的配置更改。当配置需要更新时更新堆栈。 D. 使用AWS Trusted Advisor检查不合规的配置。根据Trusted Advisor建议手动应用必要的更改。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个能够自动检测并回滚特定AWS资源配置更改的解决方案。关键词是&quot;自动回滚&quot;(automatically reverted)，这意味着需要一个无需人工干预的自动化解决方案。 **涉及的关键AWS服务和概念：** - AWS Config：配置管理和合规性监控服务 - AWS Systems Manager Automation：自动化运维任务执行 - Amazon CloudWatch：监控和告警服务 - AWS CloudFormation：基础设施即代码服务 - AWS Trusted Advisor：最佳实践建议服务 **正确答案A的原因：** AWS Config规则可以持续监控资源配置变化，当检测到不符合预定义规则的配置更改时，可以触发自动修复操作。结合AWS Systems Manager Automation文档，可以实现完全自动化的配置回滚，无需人工干预。这完美满足了&quot;自动回滚&quot;的要求。 **其他选项错误的原因：** - 选项B：虽然CloudWatch可以监控，但需要通过SNS通知管理员&quot;手动&quot;回滚，不符合自动化要求 - 选项C：CloudFormation主要用于基础设施部署和管理，而不是检测和回滚意外的配置更改 - 选项D：Trusted Advisor提供建议但需要&quot;手动&quot;应用更改，同样不满足自动化要求 **决策标准和最佳实践：** 1. 自动化优先：选择能够实现端到端自动化的解决方案 2. 实时监控：需要能够实时检测配置变化的服务 3. 预防性合规：使用Config规则主动维护配置合规性 4. 减少人工干预：避免依赖手动操作的解决方案，提高运维效率和可靠性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>

</body>
</html>
