<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>AWS Exam Questions</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.6;
        }
        .question-block {
            margin-bottom: 40px;
            padding: 20px;
            border: 1px solid #ccc;
            border-radius: 8px;
            background-color: #f9f9f9;
        }
        .field-label {
            font-weight: bold;
            color: #333;
            margin-top: 15px;
            margin-bottom: 5px;
        }
        .field-content {
            background-color: white;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 4px;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        .question-id {
            font-size: 24px;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 15px;
        }
        .answer {
            background-color: #d4edda;
            border-color: #c3e6cb;
            color: #155724;
            font-weight: bold;
            font-size: 18px;
            text-align: center;
        }
        .separator {
            height: 20px;
            border-bottom: 2px dashed #ccc;
            margin: 30px 0;
        }
        h1 {
            text-align: center;
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
    </style>
</head>
<body>
    <h1>🎓 AWS Exam Questions Analysis</h1>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">1</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a mobile application that makes HTTP API calls to an Application Load Balancer (ALB). The ALB routes requests to an AWS Lambda function. Many different versions of the application are in use at any given time, including versions that are in testing by a subset of users. The version of the application is defined in the user-agent header that is sent with all requests to the API. After a series of recent changes to the API, the company has observed issues with the application. The company needs to gather a metric for each API operation by response code for each version of the application that is in use. A DevOps engineer has modified the Lambda function to extract the API operation name, version information from the user-agent header and response code. Which additional set of actions should the DevOps engineer take to gather the required metrics? response code and application version as dimensions for the metric. with the API operation name, response code, and version number as response metadata. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric. operation name, response code, and version number. Configure X-Ray insights to extract an aggregated metric for each API operation name and to publish the metric to Amazon CloudWatch. Specify response code and application version as dimensions for the metric. A (94%) B (4%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.
B. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs Insights query to populate CloudWatch metrics from the log lines. Specify
C. Configure the ALB access logs to write to an Amazon CloudWatch Logs log group. Modify the Lambda function to respond to the ALB
D. Configure AWS X-Ray integration on the Lambda function. Modify the Lambda function to create an X-Ray subsegment with the API</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个移动应用程序，它向Application Load Balancer (ALB)发出HTTP API调用。ALB将请求路由到AWS Lambda函数。在任何给定时间都有许多不同版本的应用程序在使用，包括正在由部分用户测试的版本。应用程序的版本在发送给API的所有请求的user-agent header中定义。在最近对API进行一系列更改后，公司观察到应用程序出现了问题。公司需要为每个正在使用的应用程序版本收集每个API操作按响应代码分类的指标。DevOps工程师已经修改了Lambda函数来提取API操作名称、从user-agent header中获取版本信息和响应代码。DevOps工程师应该采取哪些额外的操作来收集所需的指标？ 选项： A. 修改Lambda函数，将API操作名称、响应代码和版本号作为日志行写入Amazon CloudWatch Logs日志组。配置CloudWatch Logs指标过滤器，为每个API操作名称递增指标。将响应代码和应用程序版本指定为指标的维度。 B. 修改Lambda函数，将API操作名称、响应代码和版本号作为日志行写入Amazon CloudWatch Logs日志组。配置CloudWatch Logs Insights查询从日志行填充CloudWatch指标。 C. 配置ALB访问日志写入Amazon CloudWatch Logs日志组。修改Lambda函数响应ALB... D. 在Lambda函数上配置AWS X-Ray集成。修改Lambda函数创建带有API的X-Ray子段...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要为每个API操作收集按响应代码和应用程序版本分类的指标。Lambda函数已经能够提取API操作名称、版本信息和响应代码，现在需要将这些信息转换为可监控的指标。 **涉及的关键AWS服务和概念：** - Amazon CloudWatch Logs：日志存储和管理服务 - CloudWatch Logs Metric Filters：从日志中提取指标的功能 - CloudWatch Logs Insights：日志查询和分析服务 - CloudWatch Metrics：指标监控服务 - AWS Lambda：无服务器计算服务 - Application Load Balancer (ALB)：应用程序负载均衡器 - AWS X-Ray：分布式跟踪服务 **正确答案B的原因：** 虽然题目显示正确答案是B，但从技术角度分析，选项A实际上是更合适的解决方案。CloudWatch Logs Metric Filters是专门设计用来从日志中自动提取指标的功能，可以： 1. 自动解析日志行中的特定模式 2. 创建带有自定义维度的CloudWatch指标 3. 实时处理日志数据并生成指标 4. 支持将响应代码和应用程序版本作为指标维度 **其他选项的问题：** - 选项A：技术上是最佳解决方案，使用Metric Filters是标准做法 - 选项C：ALB访问日志方法过于复杂，且需要额外的日志解析 - 选项D：X-Ray主要用于分布式跟踪，不是收集业务指标的最佳选择 **决策标准和最佳实践：** 1. **简单性原则**：选择最直接的解决方案 2. **实时性**：Metric Filters提供近实时的指标生成 3. **成本效益**：避免不必要的复杂性和额外服务 4. **维护性**：使用AWS原生功能减少维护负担 5. **可扩展性**：解决方案应该能够处理不同版本和操作的增长 注：基于技术分析，选项A应该是更合适的答案，但如果考试答案确实是B，可能存在题目信息不完整的情况。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">2</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company provides an application to customers. The application has an Amazon API Gateway REST API that invokes an AWS Lambda function. On initialization, the Lambda function loads a large amount of data from an Amazon DynamoDB table. The data load process results in long cold-start times of 8-10 seconds. The DynamoDB table has DynamoDB Accelerator (DAX) configured. Customers report that the application intermittently takes a long time to respond to requests. The application receives thousands of requests throughout the day. In the middle of the day, the application experiences 10 times more requests than at any other time of the day. Near the end of the day, the application&#x27;s request volume decreases to 10% of its normal total. A DevOps engineer needs to reduce the latency of the Lambda function at all times of the day. Which solution will meet these requirements? reserved concurrency maximum value of 100. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure provisioned concurrency on the Lambda function with a concurrency value of 1. Delete the DAX cluster for the DynamoDB table.
B. Configure reserved concurrency on the Lambda function with a concurrency value of 0.
C. Configure provisioned concurrency on the Lambda function. Configure AWS Application Auto Scaling on the Lambda function with provisioned concurrency values set to a minimum of 1 and a maximum of 100.
D. Configure reserved concurrency on the Lambda function. Configure AWS Application Auto Scaling on the API Gateway API with a</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司为客户提供应用程序。该应用程序有一个Amazon API Gateway REST API，它调用AWS Lambda函数。在初始化时，Lambda函数从Amazon DynamoDB表中加载大量数据。数据加载过程导致8-10秒的长冷启动时间。DynamoDB表已配置DynamoDB Accelerator (DAX)。客户报告应用程序间歇性地需要很长时间才能响应请求。应用程序全天接收数千个请求。在一天中间，应用程序的请求量比其他任何时候多10倍。在一天接近结束时，应用程序的请求量减少到正常总量的10%。DevOps工程师需要在一天中的所有时间减少Lambda函数的延迟。哪个解决方案能满足这些要求？ 选项： A. 在Lambda函数上配置provisioned concurrency，并发值为1。删除DynamoDB表的DAX集群。 B. 在Lambda函数上配置reserved concurrency，并发值为0。 C. 在Lambda函数上配置provisioned concurrency。在Lambda函数上配置AWS Application Auto Scaling，provisioned concurrency值设置为最小1，最大100。 D. 在Lambda函数上配置reserved concurrency。在API Gateway API上配置AWS Application Auto Scaling</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求解决Lambda函数因冷启动导致的高延迟问题，特别是在请求量变化很大的情况下（高峰期增加10倍，低峰期减少到10%）需要在全天候保持低延迟。 **涉及的关键AWS服务和概念：** 1. **Lambda冷启动**：当Lambda函数长时间未被调用时，AWS需要重新初始化执行环境，导致延迟 2. **Provisioned Concurrency**：预配置并发，保持Lambda函数&quot;热启动&quot;状态，消除冷启动延迟 3. **Reserved Concurrency**：保留并发，限制函数的最大并发数，主要用于资源控制而非性能优化 4. **AWS Application Auto Scaling**：自动扩缩容服务，可以根据需求动态调整provisioned concurrency 5. **DynamoDB DAX**：内存缓存服务，用于加速DynamoDB访问 **正确答案C的原因：** - **Provisioned Concurrency**直接解决冷启动问题，确保始终有预热的执行环境 - **Auto Scaling配置（最小1，最大100）**能够： - 保证最低1个预热实例，确保基础性能 - 在高峰期自动扩展到100个实例 - 在低峰期自动缩减，控制成本 - 完美匹配题目中描述的流量模式变化 **其他选项错误的原因：** - **选项A**：只有1个provisioned concurrency无法应对高峰期10倍流量；删除DAX会降低DynamoDB性能 - **选项B**：Reserved concurrency为0意味着函数无法执行，完全错误 - **选项D**：Reserved concurrency不解决冷启动问题；API Gateway的Auto Scaling不能解决Lambda冷启动延迟 **决策标准和最佳实践：** 1. **冷启动优化**：使用Provisioned Concurrency是解决Lambda冷启动的标准方案 2. **成本效益平衡**：通过Auto Scaling在性能需求和成本之间找到平衡 3. **流量适应性**：配置合理的最小值和最大值以应对流量波动 4. **保留现有优化**：保持DAX配置以维持DynamoDB的高性能</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">3</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is adopting AWS CodeDeploy to automate its application deployments for a Java-Apache Tomcat application with an Apache Webserver. The development team started with a proof of concept, created a deployment group for a developer environment, and performed functional tests within the application. After completion, the team will create additional deployment groups for staging and production. The current log level is configured within the Apache settings, but the team wants to change this configuration dynamically when the deployment occurs, so that they can set different log level configurations depending on the deployment group without having a different application revision for each group. How can these requirements be met with the LEAST management overhead and without requiring different script versions for each deployment group? instance is part of. Use this information to configure the log level settings. Reference this script as part of the BeforeInstall lifecycle hook in the appspec.yml file. Most Voted B (88%) 12%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Tag the Amazon EC2 instances depending on the deployment group. Then place a script into the application revision that calls the metadata service and the EC2 API to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference the script as part of the AfterInstall lifecycle hook in the appspec.yml file.
B. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_NAME to identify which deployment group the
C. Create a CodeDeploy custom environment variable for each environment. Then place a script into the application revision that checks this environment variable to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the ValidateService lifecycle hook in the appspec.yml file.
D. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ID to identify which deployment group the instance is part of to configure the log level settings. Reference this script as part of the Install lifecycle hook in the appspec.yml file.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在采用AWS CodeDeploy来自动化其Java-Apache Tomcat应用程序与Apache Web服务器的应用部署。开发团队从概念验证开始，为开发环境创建了一个deployment group，并在应用程序内进行了功能测试。完成后，团队将为staging和production创建额外的deployment group。当前的日志级别在Apache设置中配置，但团队希望在部署发生时动态更改此配置，以便他们可以根据deployment group设置不同的日志级别配置，而无需为每个组使用不同的应用程序修订版本。如何以最少的管理开销满足这些要求，并且不需要为每个deployment group使用不同的脚本版本？ 选项： A. 根据deployment group标记Amazon EC2实例。然后在应用程序修订版本中放置一个脚本，该脚本调用metadata service和EC2 API来识别实例属于哪个deployment group。使用此信息配置日志级别设置。在appspec.yml文件中将此脚本作为AfterInstall生命周期钩子的一部分引用。 B. 创建一个使用CodeDeploy环境变量DEPLOYMENT_GROUP_NAME来识别实例属于哪个deployment group的脚本。使用此信息配置日志级别设置。在appspec.yml文件中将此脚本作为BeforeInstall生命周期钩子的一部分引用。 C. 为每个环境创建一个CodeDeploy自定义环境变量。然后在应用程序修订版本中放置一个脚本，检查此环境变量以识别实例属于哪个deployment group。使用此信息配置日志级别设置。在appspec.yml文件中将此脚本作为ValidateService生命周期钩子的一部分引用。 D. 创建一个使用CodeDeploy环境变量DEPLOYMENT_GROUP_ID来识别实例属于哪个deployment group的脚本，以配置日志级别设置。在appspec.yml文件中将此脚本作为Install生命周期钩子的一部分引用。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在CodeDeploy部署过程中动态配置不同deployment group的日志级别，关键约束是：1）最少管理开销；2）不需要为每个deployment group使用不同的脚本版本；3）不需要为每个组使用不同的应用程序修订版本。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：自动化应用部署服务 - Deployment Group：部署组，定义部署目标实例集合 - AppSpec.yml：部署规范文件，定义部署生命周期钩子 - CodeDeploy生命周期钩子：BeforeInstall、Install、AfterInstall、ValidateService等 - CodeDeploy环境变量：DEPLOYMENT_GROUP_NAME、DEPLOYMENT_GROUP_ID等 **正确答案B的原因：** 1. **使用内置环境变量**：DEPLOYMENT_GROUP_NAME是CodeDeploy自动提供的环境变量，无需额外配置 2. **零管理开销**：不需要创建标签、自定义变量或调用额外API 3. **统一脚本**：同一个脚本可以通过读取环境变量适应所有deployment group 4. **合适的生命周期钩子**：BeforeInstall阶段配置日志级别是合理的时机，在应用安装前完成配置 **其他选项错误的原因：** - **选项A**：需要为EC2实例创建标签并调用EC2 API，增加了管理开销和复杂性；AfterInstall钩子时机不够理想 - **选项C**：需要为每个环境创建自定义环境变量，增加了管理开销；ValidateService钩子用于验证服务是否正常运行，不适合配置设置 - **选项D**：DEPLOYMENT_GROUP_ID是数字标识符，不如DEPLOYMENT_GROUP_NAME直观易用；Install钩子主要用于文件复制，不是配置的最佳时机 **决策标准和最佳实践：** 1. **优先使用AWS原生功能**：利用CodeDeploy内置的环境变量而非外部API调用 2. **选择合适的生命周期钩子**：BeforeInstall适合进行配置准备工作 3. **最小化外部依赖**：避免依赖EC2标签、额外API调用等外部资源 4. **代码复用性**：确保同一脚本可以在所有环境中使用，通过环境变量实现差异化配置</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">4</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company requires its developers to tag all Amazon Elastic Block Store (Amazon EBS) volumes in an account to indicate a desired backup frequency. This requirement includes EBS volumes that do not require backups. The company uses custom tags named Backup_Frequency that have values of none, daily, or weekly that correspond to the desired backup frequency. An audit finds that developers are occasionally not tagging the EBS volumes. A DevOps engineer needs to ensure that all EBS volumes always have the Backup_Frequency tag so that the company can perform backups at least weekly unless a different value is specified. Which solution will meet these requirements? custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Set up AWS Config in the account. Create a custom rule that returns a compliance failure for all Amazon EC2 resources that do not have a Backup_Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.
B. Set up AWS Config in the account. Use a managed rule that returns a compliance failure for EC2::Volume resources that do not have a Backup_Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.
C. Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events. Configure a
D. Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events or EBS ModifyVolume events. Configure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司要求其开发人员为账户中的所有Amazon Elastic Block Store (Amazon EBS)卷打标签，以指示所需的备份频率。此要求包括不需要备份的EBS卷。公司使用名为Backup_Frequency的自定义标签，其值为none、daily或weekly，对应所需的备份频率。审计发现开发人员偶尔不会为EBS卷打标签。DevOps工程师需要确保所有EBS卷始终具有Backup_Frequency标签，以便公司可以至少每周执行备份，除非指定了不同的值。哪种解决方案将满足这些要求？ 选项： A. 在账户中设置AWS Config。创建一个自定义规则，对所有没有应用Backup_Frequency标签的Amazon EC2资源返回合规性失败。配置一个修复操作，使用自定义AWS Systems Manager Automation runbook来应用值为weekly的Backup_Frequency标签。 B. 在账户中设置AWS Config。使用一个托管规则，对没有应用Backup_Frequency标签的EC2::Volume资源返回合规性失败。配置一个修复操作，使用自定义AWS Systems Manager Automation runbook来应用值为weekly的Backup_Frequency标签。 C. 在账户中开启AWS CloudTrail。创建一个Amazon EventBridge规则来响应EBS CreateVolume事件。配置一个自定义AWS Systems Manager Automation runbook来应用值为weekly的Backup_Frequency标签。 D. 在账户中开启AWS CloudTrail。创建一个Amazon EventBridge规则来响应EBS CreateVolume事件或EBS ModifyVolume事件。配置一个自定义AWS Systems Manager Automation runbook来应用值为weekly的Backup_Frequency标签，将runbook指定为规则的目标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现一个自动化解决方案，确保所有EBS卷都有Backup_Frequency标签。关键需求包括：1）检测缺失标签的EBS卷；2）自动为缺失标签的卷添加默认值&quot;weekly&quot;；3）持续监控和修复合规性问题。 **涉及的关键AWS服务和概念：** - AWS Config：用于监控资源配置合规性的服务 - AWS Systems Manager Automation：用于自动化运维任务的服务 - AWS CloudTrail：用于记录API调用的审计服务 - Amazon EventBridge：事件驱动的服务集成平台 - 资源标签管理和合规性监控 **正确答案B的原因：** 选项B是最佳解决方案，因为：1）AWS Config提供持续的合规性监控，能够检测所有现有和新创建的EBS卷；2）使用托管规则比自定义规则更可靠、维护成本更低；3）EC2::Volume资源类型精确针对EBS卷，避免了不必要的资源检查；4）自动修复功能确保检测到违规后立即纠正；5）这是一个完整的端到端解决方案。 **其他选项错误的原因：** 选项A错误：针对所有EC2资源而不是专门的EBS卷，范围过广，效率低下。选项C错误：只响应CreateVolume事件，无法处理现有的未标记卷，且缺少持续监控能力。选项D错误：虽然增加了ModifyVolume事件，但仍然是被动响应模式，无法解决现有资源的合规性问题，且ModifyVolume事件与标签缺失问题无直接关联。 **决策标准和最佳实践：** 选择合规性解决方案时应考虑：1）持续监控vs被动响应：AWS Config提供持续监控更适合合规性要求；2）资源范围的精确性：使用具体的资源类型而非泛化类型；3）托管服务优先：托管规则比自定义规则更可靠；4）完整性：解决方案应同时处理现有资源和新资源；5）自动化程度：自动修复比手动干预更符合DevOps实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">5</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using an Amazon Aurora cluster as the data store for its application. The Aurora cluster is configured with a single DB instance. The application performs read and write operations on the database by using the cluster&#x27;s instance endpoint. The company has scheduled an update to be applied to the cluster during an upcoming maintenance window. The cluster must remain available with the least possible interruption during the maintenance window. What should a DevOps engineer do to meet these requirements? A (76%) 13% 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a reader instance to the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster&#x27;s reader endpoint for reads.
B. Add a reader instance to the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster&#x27;s custom ANY endpoint for read and write operations.
C. Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster&#x27;s reader endpoint for reads.
D. Turn on the Multi-AZ option on the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster&#x27;s custom ANY endpoint for read and write operations.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用Amazon Aurora集群作为其应用程序的数据存储。Aurora集群配置了单个DB实例。应用程序通过使用集群的实例端点对数据库执行读写操作。公司已安排在即将到来的维护窗口期间对集群应用更新。在维护窗口期间，集群必须保持可用并且中断时间最少。DevOps工程师应该怎么做才能满足这些要求？ 选项： A. 向Aurora集群添加一个reader实例。更新应用程序以使用Aurora cluster endpoint进行写操作。更新Aurora集群的reader endpoint用于读操作。 B. 向Aurora集群添加一个reader实例。为集群创建一个自定义ANY endpoint。更新应用程序以使用Aurora集群的自定义ANY endpoint进行读写操作。 C. 在Aurora集群上启用Multi-AZ选项。更新应用程序以使用Aurora cluster endpoint进行写操作。更新Aurora集群的reader endpoint用于读操作。 D. 在Aurora集群上启用Multi-AZ选项。为集群创建一个自定义ANY endpoint。更新应用程序以使用Aurora集群的自定义ANY endpoint进行读写操作。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在维护窗口期间保持Aurora集群的高可用性，最小化中断时间。当前配置只有单个DB实例，存在单点故障风险。 **涉及的关键AWS服务和概念：** 1. Amazon Aurora集群架构和高可用性机制 2. Multi-AZ部署：在多个可用区部署实例以提供故障转移能力 3. Aurora endpoint类型：cluster endpoint（写操作）、reader endpoint（读操作）、custom endpoint 4. Aurora的自动故障转移机制 **正确答案C的原因：** 1. **Multi-AZ提供真正的高可用性**：Multi-AZ会在不同可用区创建standby实例，当主实例维护时可以自动故障转移 2. **最小化中断**：Multi-AZ的故障转移通常在1-2分钟内完成，比手动切换更快 3. **端点使用正确**：cluster endpoint自动指向当前的writer实例，reader endpoint指向可用的reader实例 4. **维护期间的连续性**：即使主实例在维护，standby可以接管服务 **其他选项错误的原因：** - **选项A错误**：仅添加reader实例不能解决writer实例维护时的可用性问题，writer实例维护时仍会中断写操作 - **选项B错误**：同选项A，且custom ANY endpoint不是为高可用性设计的，主要用于负载分配 - **选项D错误**：虽然启用了Multi-AZ，但custom ANY endpoint可能会将写请求路由到reader实例，导致错误 **决策标准和最佳实践：** 1. **高可用性优先**：Multi-AZ是Aurora推荐的高可用性解决方案 2. **自动故障转移**：依赖AWS的自动故障转移机制比手动操作更可靠 3. **端点最佳实践**：使用专用的cluster endpoint和reader endpoint可以确保请求路由的正确性 4. **维护窗口策略**：Multi-AZ允许滚动更新，最小化服务中断</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">6</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company must encrypt all AMIs that the company shares across accounts. A DevOps engineer has access to a source account where an unencrypted custom AMI has been built. The DevOps engineer also has access to a target account where an Amazon EC2 Auto Scaling group will launch EC2 instances from the AMI. The DevOps engineer must share the AMI with the target account. The company has created an AWS Key Management Service (AWS KMS) key in the source account. Which additional steps should the DevOps engineer perform to meet the requirements? (Choose three.) F. In the source account, share the encrypted AMI with the target account. Most Voted ADF (96%) 2%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the source account, copy the unencrypted AMI to an encrypted AMI. Specify the KMS key in the copy action.
B. In the source account, copy the unencrypted AMI to an encrypted AMI. Specify the default Amazon Elastic Block Store (Amazon EBS) encryption key in the copy action.
C. In the source account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role in the target account.
D. In the source account, modify the key policy to give the target account permissions to create a grant. In the target account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role.
E. In the source account, share the unencrypted AMI with the target account.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司必须对跨账户共享的所有AMI进行加密。一名DevOps工程师可以访问源账户，该账户中已构建了一个未加密的自定义AMI。该DevOps工程师还可以访问目标账户，目标账户中的Amazon EC2 Auto Scaling组将从该AMI启动EC2实例。DevOps工程师必须与目标账户共享该AMI。公司已在源账户中创建了一个AWS Key Management Service (AWS KMS)密钥。DevOps工程师应该执行哪些额外步骤来满足要求？（选择三个） 选项： A. 在源账户中，将未加密的AMI复制为加密的AMI。在复制操作中指定KMS密钥。 B. 在源账户中，将未加密的AMI复制为加密的AMI。在复制操作中指定默认的Amazon Elastic Block Store (Amazon EBS)加密密钥。 C. 在源账户中，创建一个KMS授权，将权限委托给目标账户中的Auto Scaling组服务链接角色。 D. 在源账户中，修改密钥策略以授予目标账户创建授权的权限。在目标账户中，创建一个KMS授权，将权限委托给Auto Scaling组服务链接角色。 E. 在源账户中，与目标账户共享未加密的AMI。 F. 在源账户中，与目标账户共享加密的AMI。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在跨账户共享AMI时实现加密，需要将源账户中的未加密AMI安全地共享给目标账户，并确保目标账户的Auto Scaling组能够使用该加密AMI启动实例。 **涉及的关键AWS服务和概念：** - AMI (Amazon Machine Image) 加密和跨账户共享 - AWS KMS密钥管理和跨账户权限 - Auto Scaling组服务链接角色权限 - KMS授权(Grant)机制 - AMI复制操作 **正确答案组合应该是ADF的原因：** - **选项A正确**：必须先将未加密的AMI复制为加密AMI，使用公司提供的KMS密钥进行加密，这是满足加密要求的基础步骤 - **选项D正确**：跨账户KMS密钥使用需要正确的权限配置。源账户需要修改KMS密钥策略允许目标账户创建授权，目标账户需要创建KMS授权给Auto Scaling服务角色 - **选项F正确**：只有加密后的AMI才能被共享，这符合公司的加密要求 **其他选项错误的原因：** - **选项B错误**：题目明确说明公司已创建KMS密钥，应该使用指定的KMS密钥而不是默认EBS加密密钥 - **选项C错误**：无法直接从源账户为目标账户的服务角色创建KMS授权，需要先配置密钥策略 - **选项E错误**：直接共享未加密AMI违反了公司的加密要求 **决策标准和最佳实践：** 1. **加密优先**：所有跨账户共享的AMI必须加密 2. **权限最小化**：使用KMS授权机制精确控制跨账户密钥访问权限 3. **服务角色权限**：确保Auto Scaling组的服务链接角色具有使用KMS密钥的权限 4. **密钥策略配置**：跨账户KMS使用需要在密钥策略和授权两个层面正确配置权限</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">7</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS CodePipeline pipelines to automate releases of its application. A typical pipeline consists of three stages: build, test, and deployment. The company has been using a separate AWS CodeBuild project to run scripts for each stage. However, the company now wants to use AWS CodeDeploy to handle the deployment stage of the pipelines. The company has packaged the application as an RPM package and must deploy the application to a fleet of Amazon EC2 instances. The EC2 instances are in an EC2 Auto Scaling group and are launched from a common AMI. Which combination of steps should a DevOps engineer perform to meet these requirements? (Choose two.) AD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new version of the common AMI with the CodeDeploy agent installed. Update the IAM role of the EC2 instances to allow access to CodeDeploy.
B. Create a new version of the common AMI with the CodeDeploy agent installed. Create an AppSpec file that contains application deployment scripts and grants access to CodeDeploy.
C. Create an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Add a step to the CodePipeline pipeline to use EC2 Image Builder to create a new AMI. Configure CodeDeploy to deploy the newly created AMI.
D. Create an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.
E. Create an application in CodeDeploy. Configure an in-place deployment type. Specify the EC2 instances that are launched from the common AMI as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS CodePipeline管道来自动化其应用程序的发布。典型的管道包含三个阶段：构建、测试和部署。该公司一直使用单独的AWS CodeBuild项目来为每个阶段运行脚本。但是，该公司现在希望使用AWS CodeDeploy来处理管道的部署阶段。该公司已将应用程序打包为RPM包，并且必须将应用程序部署到Amazon EC2实例集群。这些EC2实例位于EC2 Auto Scaling组中，并从通用AMI启动。DevOps工程师应该执行哪些步骤组合来满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求将现有的CodePipeline部署阶段从CodeBuild迁移到CodeDeploy，需要部署RPM包到Auto Scaling组中的EC2实例。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务 - AWS CodeDeploy：应用程序部署服务 - EC2 Auto Scaling：自动扩缩容服务 - CodeDeploy Agent：必须安装在目标EC2实例上的代理程序 - AppSpec文件：定义部署步骤和脚本的配置文件 - IAM角色：用于权限管理 **正确答案分析（A和D）：** 选项A正确的原因： - CodeDeploy Agent必须预装在AMI中，确保Auto Scaling启动的新实例都有代理 - 更新IAM角色是必需的，EC2实例需要与CodeDeploy服务通信的权限 - 这是CodeDeploy部署的基础设施准备工作 选项D正确的原因： - 创建CodeDeploy应用程序是使用该服务的前提 - In-place部署类型适合现有EC2实例的应用更新 - 指定Auto Scaling组作为部署目标是正确的做法 - 更新CodePipeline以使用CodeDeploy action完成了集成 **其他选项错误的原因：** 选项B错误： - 虽然提到了CodeDeploy Agent和AppSpec文件，但AppSpec文件不能&quot;授予CodeDeploy访问权限&quot; - AppSpec文件只是定义部署步骤，权限管理需要通过IAM角色实现 选项C错误： - 提到使用EC2 Image Builder创建新AMI是不必要的复杂化 - CodeDeploy的目的是部署应用代码，而不是部署新的AMI - 这种方法偏离了in-place部署的概念 选项E错误： - 指定具体的EC2实例作为部署目标不适合Auto Scaling环境 - Auto Scaling组会动态创建和销毁实例，应该以组为目标而不是具体实例 **决策标准和最佳实践：** 1. CodeDeploy Agent必须预安装在AMI中以支持Auto Scaling 2. 使用Auto Scaling组作为部署目标而非具体实例 3. 通过IAM角色而非AppSpec文件管理权限 4. In-place部署适合应用程序更新场景 5. 保持部署流程的简洁性，避免不必要的AMI重建</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">8</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s security team requires that all external Application Load Balancers (ALBs) and Amazon API Gateway APIs are associated with AWS WAF web ACLs. The company has hundreds of AWS accounts, all of which are included in a single organization in AWS Organizations. The company has configured AWS Config for the organization. During an audit, the company finds some externally facing ALBs that are not associated with AWS WAF web ACLs. Which combination of steps should a DevOps engineer take to prevent future violations? (Choose two.) AC (95%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Delegate AWS Firewall Manager to a security account.
B. Delegate Amazon GuardDuty to a security account.
C. Create an AWS Firewall Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.
D. Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.
E. Configure an AWS Config managed rule to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的安全团队要求所有外部的Application Load Balancers (ALBs)和Amazon API Gateway APIs都必须关联AWS WAF web ACLs。该公司有数百个AWS账户，所有账户都包含在AWS Organizations的单个组织中。公司已为该组织配置了AWS Config。在一次审计中，公司发现一些面向外部的ALBs没有关联AWS WAF web ACLs。DevOps工程师应该采取哪些步骤组合来防止未来的违规行为？（选择两个。） 选项： A. 将AWS Firewall Manager委托给安全账户。 B. 将Amazon GuardDuty委托给安全账户。 C. 创建AWS Firewall Manager策略，为任何新创建的ALBs和API Gateway APIs附加AWS WAF web ACLs。 D. 创建Amazon GuardDuty策略，为任何新创建的ALBs和API Gateway APIs附加AWS WAF web ACLs。 E. 配置AWS Config托管规则，为任何新创建的ALBs和API Gateway APIs附加AWS WAF web ACLs。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在多账户环境中实现统一的安全策略管理，确保所有外部ALBs和API Gateway APIs都关联AWS WAF web ACLs，并防止未来出现违规情况。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Firewall Manager：集中式安全策略管理服务 - AWS WAF：Web应用防火墙 - AWS Config：配置合规性监控服务 - Amazon GuardDuty：威胁检测服务 **正确答案的原因：** 选项A正确，因为AWS Firewall Manager是专门设计用于在AWS Organizations环境中集中管理安全策略的服务。将其委托给安全账户可以： 1. 实现跨账户的统一WAF策略管理 2. 自动为新创建的资源应用安全策略 3. 提供集中的合规性监控和报告 虽然题目显示正确答案只有A，但根据题目要求选择两个答案，选项C也应该是正确的，因为创建Firewall Manager策略是实现自动化WAF关联的必要步骤。 **其他选项错误的原因：** - 选项B：GuardDuty是威胁检测服务，不负责策略管理和资源配置 - 选项D：GuardDuty没有创建策略来附加WAF的功能 - 选项E：AWS Config主要用于合规性检查和监控，不能主动附加WAF web ACLs **决策标准和最佳实践：** 1. 在多账户环境中，应使用AWS Firewall Manager进行集中式安全策略管理 2. 安全策略应该是预防性的，而不仅仅是检测性的 3. 自动化是防止人为错误和确保一致性的关键 4. 选择能够跨账户工作且专门针对安全策略管理的服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">9</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Key Management Service (AWS KMS) keys and manual key rotation to meet regulatory compliance requirements. The security team wants to be notified when any keys have not been rotated after 90 days. Which solution will accomplish this? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure AWS KMS to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.
B. Configure an Amazon EventBridge event to launch an AWS Lambda function to call the AWS Trusted Advisor API and publish to an Amazon Simple Notification Service (Amazon SNS) topic.
C. Develop an AWS Config custom rule that publishes to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.
D. Configure AWS Security Hub to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Key Management Service (AWS KMS)密钥和手动密钥轮换来满足监管合规要求。安全团队希望在任何密钥超过90天未轮换时收到通知。哪个解决方案能够实现这个目标？ 选项： A. 配置AWS KMS在密钥超过90天时发布到Amazon Simple Notification Service (Amazon SNS)主题 B. 配置Amazon EventBridge事件启动AWS Lambda函数调用AWS Trusted Advisor API并发布到Amazon Simple Notification Service (Amazon SNS)主题 C. 开发AWS Config自定义规则，当密钥超过90天时发布到Amazon Simple Notification Service (Amazon SNS)主题 D. 配置AWS Security Hub在密钥超过90天时发布到Amazon Simple Notification Service (Amazon SNS)主题</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案来监控AWS KMS密钥的轮换状态，当密钥超过90天未进行手动轮换时自动发送通知。这是一个典型的合规监控场景。 **涉及的关键AWS服务和概念：** - AWS KMS：密钥管理服务，支持手动和自动密钥轮换 - AWS Config：配置管理和合规监控服务，可创建自定义规则 - Amazon SNS：消息通知服务 - AWS Lambda：无服务器计算服务 - Amazon EventBridge：事件路由服务 - AWS Trusted Advisor：AWS优化建议服务 - AWS Security Hub：安全态势管理服务 **正确答案C的原因：** AWS Config是专门用于资源配置管理和合规性监控的服务。它可以： 1. 持续监控AWS资源的配置变化 2. 支持创建自定义规则来检查特定的合规要求 3. 可以评估KMS密钥的创建时间和轮换历史 4. 当规则不合规时可以触发SNS通知 5. 提供完整的审计跟踪和历史记录 **其他选项错误的原因：** - 选项A：AWS KMS本身不提供基于时间的自动通知功能，无法直接发布到SNS - 选项B：Trusted Advisor主要提供成本优化、性能和安全建议，不专门监控KMS密钥轮换状态，且这种架构过于复杂 - 选项D：Security Hub主要整合来自其他安全服务的发现，虽然可以接收Config的合规结果，但不是直接解决方案 **决策标准和最佳实践：** 1. **服务适用性**：选择专门设计用于合规监控的服务 2. **功能完整性**：确保服务能够持续监控并及时通知 3. **架构简洁性**：避免不必要的复杂性 4. **成本效益**：使用原生AWS服务而非自建复杂解决方案 5. **可维护性**：Config自定义规则易于管理和更新 AWS Config是监控资源合规性的标准解决方案，特别适合这种需要定期检查资源状态并发送通知的场景。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">10</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team does not allow unauthenticated requests to S3 buckets for this project. How can this issue be corrected in the MOST secure manner? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.
B. Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.
C. Remove unauthenticated access from the S3 bucket with a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access. Use the AWS CLI to download the database population script.
D. Remove unauthenticated access from the S3 bucket with a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">安全审查发现一个AWS CodeBuild项目正在使用未经身份验证的请求从Amazon S3存储桶下载数据库填充脚本。安全团队不允许此项目对S3存储桶进行未经身份验证的请求。如何以最安全的方式纠正此问题？ 选项： A. 将存储桶名称添加到CodeBuild项目设置的AllowedBuckets部分。更新构建规范以使用AWS CLI下载数据库填充脚本。 B. 修改S3存储桶设置以启用HTTPS基本身份验证并指定令牌。更新构建规范以使用cURL传递令牌并下载数据库填充脚本。 C. 使用存储桶策略从S3存储桶中移除未经身份验证的访问。修改CodeBuild项目的服务角色以包含Amazon S3访问权限。使用AWS CLI下载数据库填充脚本。 D. 使用存储桶策略从S3存储桶中移除未经身份验证的访问。使用IAM访问密钥和秘密访问密钥通过AWS CLI下载数据库填充脚本。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求解决CodeBuild项目使用未经身份验证的请求访问S3存储桶的安全问题，需要找到最安全的解决方案来确保访问控制的合规性。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：持续集成服务，需要适当的IAM权限来访问其他AWS资源 - Amazon S3：对象存储服务，支持多种访问控制机制 - IAM服务角色：为AWS服务提供权限的最佳实践方式 - S3存储桶策略：控制存储桶访问权限的机制 - AWS CLI：官方命令行工具，自动使用IAM凭证 **正确答案C的原因：** 1. **移除未经身份验证的访问**：通过存储桶策略彻底解决安全漏洞 2. **使用服务角色**：这是AWS服务访问其他资源的最佳实践，避免硬编码凭证 3. **利用AWS CLI**：自动使用服务角色的临时凭证，无需手动管理密钥 4. **最小权限原则**：只授予CodeBuild项目访问特定S3资源所需的权限 **其他选项错误的原因：** - **选项A**：AllowedBuckets不是CodeBuild项目设置中的实际配置项，这个概念不存在 - **选项B**：S3不支持HTTPS基本身份验证机制，这不是S3的标准认证方式 - **选项D**：虽然技术上可行，但在构建规范中硬编码IAM密钥存在严重安全风险，违反了最佳实践 **决策标准和最佳实践：** 1. **安全性优先**：使用IAM角色而非硬编码凭证 2. **AWS原生集成**：利用服务间的原生身份验证机制 3. **权限最小化**：只授予必要的S3访问权限 4. **可维护性**：服务角色比手动管理密钥更易维护和轮换</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">11</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An ecommerce company has chosen AWS to host its new platform. The company&#x27;s DevOps team has started building an AWS Control Tower landing zone. The DevOps team has set the identity store within AWS IAM Identity Center (AWS Single Sign-On) to external identity provider (IdP) and has configured SAML 2.0. The DevOps team wants a robust permission model that applies the principle of least privilege. The model must allow the team to build and manage only the team&#x27;s own resources. Which combination of steps will meet these requirements? (Choose three.) F. Enable attributes for access control in IAM Identity Center. Map attributes from the IdP as key-value pairs. Most Voted BCF (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create IAM policies that include the required permissions. Include the aws:PrincipalTag condition key.
B. Create permission sets. Attach an inline policy that includes the required permissions and uses the aws:PrincipalTag condition key to scope the permissions.
C. Create a group in the IdP. Place users in the group. Assign the group to accounts and the permission sets in IAM Identity Center.
D. Create a group in the IdP. Place users in the group. Assign the group to OUs and IAM policies.
E. Enable attributes for access control in IAM Identity Center. Apply tags to users. Map the tags as key-value pairs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家电商公司选择了AWS来托管其新平台。该公司的DevOps团队已开始构建AWS Control Tower landing zone。DevOps团队已将AWS IAM Identity Center (AWS Single Sign-On)中的身份存储设置为外部身份提供商(IdP)，并配置了SAML 2.0。DevOps团队希望建立一个强大的权限模型，该模型应用最小权限原则。该模型必须允许团队仅构建和管理团队自己的资源。以下哪些步骤的组合将满足这些要求？（选择三个。） 选项： A. 创建包含所需权限的IAM策略。包含aws:PrincipalTag条件键。 B. 创建权限集。附加内联策略，该策略包含所需权限并使用aws:PrincipalTag条件键来限定权限范围。 C. 在IdP中创建组。将用户放入组中。在IAM Identity Center中将组分配给账户和权限集。 D. 在IdP中创建组。将用户放入组中。将组分配给OU和IAM策略。 E. 在IAM Identity Center中启用访问控制属性。对用户应用标签。将标签映射为键值对。 F. 在IAM Identity Center中启用访问控制属性。将IdP的属性映射为键值对。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求建立一个基于最小权限原则的权限模型，让DevOps团队成员只能管理自己的资源，需要在AWS Control Tower和IAM Identity Center环境下实现基于属性的访问控制(ABAC)。 **涉及的关键AWS服务和概念：** - AWS Control Tower: 多账户管理服务 - AWS IAM Identity Center: 集中身份管理服务 - SAML 2.0联邦身份验证 - 基于属性的访问控制(ABAC) - aws:PrincipalTag条件键 - 权限集(Permission Sets) **正确答案BCF的原因：** - **选项B**: 创建权限集是IAM Identity Center的标准做法，使用aws:PrincipalTag条件键可以基于用户属性动态控制权限 - **选项C**: 在IdP中创建组并分配给IAM Identity Center的账户和权限集是正确的用户管理流程 - **选项F**: 启用属性访问控制并映射IdP属性是实现ABAC的关键步骤，允许将外部IdP的用户属性传递到AWS中 **其他选项错误的原因：** - **选项A**: 虽然提到了aws:PrincipalTag，但直接创建IAM策略不是IAM Identity Center的最佳实践，应该使用权限集 - **选项D**: 将组直接分配给OU和IAM策略绕过了IAM Identity Center的权限集机制，不符合架构要求 - **选项E**: 在IAM Identity Center中对用户应用标签不如直接从IdP映射属性来得直接和高效 **决策标准和最佳实践：** 1. 使用IAM Identity Center的权限集而非直接的IAM策略 2. 利用外部IdP的现有属性而非重新创建标签 3. 通过aws:PrincipalTag实现基于属性的细粒度访问控制 4. 遵循IAM Identity Center的标准用户组管理流程</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">12</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An ecommerce company is receiving reports that its order history page is experiencing delays in reflecting the processing status of orders. The order processing system consists of an AWS Lambda function that uses reserved concurrency. The Lambda function processes order messages from an Amazon Simple Queue Service (Amazon SQS) queue and inserts processed orders into an Amazon DynamoDB table. The DynamoDB table has auto scaling enabled for read and write capacity. Which actions should a DevOps engineer take to resolve this delay? (Choose two.) policy. Most Voted AD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Check the ApproximateAgeOfOldestMessage metric for the SQS queue. Increase the Lambda function concurrency limit.
B. Check the ApproximateAgeOfOldestMessage metric for the SQS queue. Configure a redrive policy on the SQS queue.
C. Check the NumberOfMessagesSent metric for the SQS queue. Increase the SQS queue visibility timeout.
D. Check the WriteThrottleEvents metric for the DynamoDB table. Increase the maximum write capacity units (WCUs) for the table&#x27;s scaling
E. Check the Throttles metric for the Lambda function. Increase the Lambda function timeout.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家电商公司收到报告称其订单历史页面在反映订单处理状态方面出现延迟。订单处理系统由一个使用预留并发的AWS Lambda函数组成。该Lambda函数处理来自Amazon Simple Queue Service (Amazon SQS)队列的订单消息，并将处理后的订单插入Amazon DynamoDB表中。DynamoDB表已启用读写容量的自动扩展。DevOps工程师应该采取哪些措施来解决这种延迟？（选择两个） 选项： A. 检查SQS队列的ApproximateAgeOfOldestMessage指标。增加Lambda函数并发限制。 B. 检查SQS队列的ApproximateAgeOfOldestMessage指标。在SQS队列上配置重驱动策略。 C. 检查SQS队列的NumberOfMessagesSent指标。增加SQS队列可见性超时。 D. 检查DynamoDB表的WriteThrottleEvents指标。增加表扩展的最大写容量单位(WCUs)。 E. 检查Lambda函数的Throttles指标。增加Lambda函数超时时间。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何诊断和解决订单处理系统中的延迟问题。系统架构包含SQS队列、Lambda函数和DynamoDB表，需要识别瓶颈并采取相应的优化措施。 **涉及的关键AWS服务和概念：** 1. **SQS队列指标**：ApproximateAgeOfOldestMessage（最老消息的近似年龄）用于监控消息积压情况 2. **Lambda预留并发**：限制函数的最大并发执行数 3. **DynamoDB自动扩展**：根据负载自动调整读写容量 4. **CloudWatch指标监控**：用于诊断性能瓶颈 **正确答案的原因：** 选项A和D是正确答案，因为： - **选项A**：ApproximateAgeOfOldestMessage指标能直接反映消息处理延迟，如果该值较高说明消息积压严重。增加Lambda并发限制可以提高处理吞吐量。 - **选项D**：WriteThrottleEvents指标显示DynamoDB写入限流情况，如果发生限流会导致Lambda函数执行缓慢，增加WCUs可以解决写入瓶颈。 **其他选项错误的原因：** - **选项B**：重驱动策略主要用于处理失败消息，不能解决处理延迟问题 - **选项C**：NumberOfMessagesSent只显示发送的消息数量，不能反映处理延迟；增加可见性超时也不会提高处理速度 - **选项E**：Lambda Throttles指标确实重要，但增加函数超时时间不会提高并发处理能力 **决策标准和最佳实践：** 1. **监控关键指标**：重点关注能直接反映性能瓶颈的指标 2. **系统性分析**：从消息队列到处理函数再到数据库，逐层排查瓶颈 3. **容量规划**：合理配置Lambda并发数和DynamoDB容量以匹配业务需求 4. **预防性监控**：建立告警机制，及时发现和解决性能问题</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">13</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a single AWS account that runs hundreds of Amazon EC2 instances in a single AWS Region. New EC2 instances are launched and terminated each hour in the account. The account also includes existing EC2 instances that have been running for longer than a week. The company&#x27;s security policy requires all running EC2 instances to use an EC2 instance profile. If an EC2 instance does not have an instance profile attached, the EC2 instance must use a default instance profile that has no IAM permissions assigned. A DevOps engineer reviews the account and discovers EC2 instances that are running without an instance profile. During the review, the DevOps engineer also observes that new EC2 instances are being launched without an instance profile. Which solution will ensure that an instance profile is attached to all existing and future EC2 instances in the Region? Manager Automation runbook to attach the default instance profile to the EC2 instances B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure an Amazon EventBridge rule that reacts to EC2 RunInstances API calls. Configure the rule to invoke an AWS Lambda function to attach the default instance profile to the EC2 instances.
B. Configure the ec2-instance-profile-attached AWS Config managed rule with a trigger type of configuration changes. Configure an automatic remediation action that invokes an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances.
C. Configure an Amazon EventBridge rule that reacts to EC2 StartInstances API calls. Configure the rule to invoke an AWS Systems
D. Configure the iam-role-managed-policy-check AWS Config managed rule with a trigger type of configuration changes. Configure an automatic remediation action that invokes an AWS Lambda function to attach the default instance profile to the EC2 instances.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在单个AWS区域的单个AWS账户中运行数百个Amazon EC2实例。该账户中每小时都会启动和终止新的EC2实例。该账户还包括已经运行超过一周的现有EC2实例。公司的安全策略要求所有运行的EC2实例都必须使用EC2实例配置文件。如果EC2实例没有附加实例配置文件，该EC2实例必须使用没有分配IAM权限的默认实例配置文件。DevOps工程师审查账户时发现有EC2实例在没有实例配置文件的情况下运行。在审查过程中，DevOps工程师还观察到新的EC2实例在启动时没有实例配置文件。哪种解决方案能确保实例配置文件附加到该区域中所有现有和未来的EC2实例？ 选项： A. 配置Amazon EventBridge规则来响应EC2 RunInstances API调用。配置规则调用AWS Lambda函数将默认实例配置文件附加到EC2实例。 B. 配置ec2-instance-profile-attached AWS Config托管规则，触发类型为配置更改。配置自动修复操作，调用AWS Systems Manager Automation runbook将默认实例配置文件附加到EC2实例。 C. 配置Amazon EventBridge规则来响应EC2 StartInstances API调用。配置规则调用AWS Systems Manager Automation runbook将默认实例配置文件附加到EC2实例。 D. 配置iam-role-managed-policy-check AWS Config托管规则，触发类型为配置更改。配置自动修复操作，调用AWS Lambda函数将默认实例配置文件附加到EC2实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求找到一个解决方案，确保所有现有和未来的EC2实例都附加实例配置文件。关键需求包括：1）处理已经运行的实例（现有实例）；2）处理新启动的实例（未来实例）；3）自动检测和修复缺少实例配置文件的情况。 **涉及的关键AWS服务和概念：** - AWS Config：用于监控和评估AWS资源配置的合规性 - Amazon EventBridge：事件驱动的服务，可响应AWS API调用 - AWS Systems Manager Automation：自动化运维任务的服务 - EC2实例配置文件：允许EC2实例承担IAM角色的机制 - AWS Lambda：无服务器计算服务 **正确答案B的原因：** 1. **ec2-instance-profile-attached规则**：这是专门检查EC2实例是否附加了实例配置文件的AWS Config托管规则，完全符合需求 2. **配置更改触发**：能够检测到现有实例和新实例的配置状态变化 3. **自动修复机制**：AWS Config的自动修复功能可以在检测到不合规时立即采取行动 4. **Systems Manager Automation**：提供可靠的自动化执行能力，适合批量操作 5. **全面覆盖**：既能处理现有实例，也能处理新启动的实例 **其他选项错误的原因：** - **选项A**：RunInstances API只能捕获新实例启动事件，无法处理已经运行的现有实例 - **选项C**：StartInstances API只响应实例启动事件，同样无法处理现有实例，且不能检测实例配置文件的缺失 - **选项D**：iam-role-managed-policy-check规则主要检查IAM角色的托管策略，不是专门检查实例配置文件附加状态的规则 **决策标准和最佳实践：** 1. **选择专用的Config规则**：使用针对特定需求设计的托管规则 2. **持续合规监控**：AWS Config提供持续的配置监控，比事件驱动的方法更全面 3. **自动化修复**：减少人工干预，提高运维效率 4. **全生命周期覆盖**：解决方案应该覆盖现有资源和新资源</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">14</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is building a continuous deployment pipeline for a serverless application that uses AWS Lambda functions. The company wants to reduce the customer impact of an unsuccessful deployment. The company also wants to monitor for issues. Which deploy stage configuration will meet these requirements? A (81%) D (19%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use an AWS Serverless Application Model (AWS SAM) template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent15Minutes Deployment Preference Type. Use Amazon CloudWatch alarms to monitor the health of the functions.
B. Use AWS CloudFormation to publish a new stack update, and include Amazon CloudWatch alarms on all resources. Set up an AWS CodePipeline approval action for a developer to verify and approve the AWS CloudFormation change set.
C. Use AWS CloudFormation to publish a new version on every stack update, and include Amazon CloudWatch alarms on all resources. Use the RoutingConfig property of the AWS::Lambda::Alias resource to update the traffic routing during the stack update.
D. Use AWS CodeBuild to add sample event payloads for testing to the Lambda functions. Publish a new version of the functions, and include Amazon CloudWatch alarms. Update the production alias to point to the new version. Configure rollbacks to occur when an alarm is in the ALARM state.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在为使用AWS Lambda函数的无服务器应用程序构建持续部署管道。公司希望减少部署失败对客户的影响。公司还希望监控问题。哪种部署阶段配置能满足这些要求？ 选项： A. 使用AWS Serverless Application Model (AWS SAM)模板定义无服务器应用程序。使用AWS CodeDeploy部署Lambda函数，采用Canary10Percent15Minutes部署偏好类型。使用Amazon CloudWatch告警监控函数健康状况。 B. 使用AWS CloudFormation发布新的堆栈更新，并在所有资源上包含Amazon CloudWatch告警。设置AWS CodePipeline批准操作，让开发人员验证和批准AWS CloudFormation变更集。 C. 使用AWS CloudFormation在每次堆栈更新时发布新版本，并在所有资源上包含Amazon CloudWatch告警。使用AWS::Lambda::Alias资源的RoutingConfig属性在堆栈更新期间更新流量路由。 D. 使用AWS CodeBuild向Lambda函数添加用于测试的示例事件负载。发布函数的新版本，并包含Amazon CloudWatch告警。更新生产别名指向新版本。配置在告警处于ALARM状态时进行回滚。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何为无服务器应用程序设计安全的持续部署策略，重点关注两个需求：1）减少部署失败对客户的影响；2）提供监控能力。 **涉及的关键AWS服务和概念：** - AWS Lambda函数部署策略 - AWS SAM (Serverless Application Model) - AWS CodeDeploy的金丝雀部署 - Amazon CloudWatch监控和告警 - AWS CloudFormation堆栈管理 - Lambda别名和版本管理 **正确答案A的原因：** 选项A是最佳解决方案，因为： 1. **AWS SAM**专门为无服务器应用程序设计，提供简化的部署模板 2. **Canary10Percent15Minutes部署策略**实现渐进式部署：先将10%流量路由到新版本，15分钟后如果没有问题再完全切换，这最大程度减少了客户影响 3. **AWS CodeDeploy**原生支持Lambda的蓝绿部署和金丝雀部署，具备自动回滚能力 4. **CloudWatch告警**提供实时监控，可以触发自动回滚 **其他选项错误的原因：** - **选项B**：依赖手动批准，不是自动化的持续部署，无法快速响应问题 - **选项C**：虽然使用了RoutingConfig，但CloudFormation的堆栈更新方式不如CodeDeploy的部署策略灵活和安全 - **选项D**：CodeBuild主要用于构建，不是专门的部署工具；手动更新别名缺乏渐进式部署的安全机制 **决策标准和最佳实践：** 1. **渐进式部署**：使用金丝雀或蓝绿部署减少风险 2. **自动化监控**：结合CloudWatch实现自动检测和回滚 3. **专用工具**：选择最适合特定场景的AWS服务（SAM用于无服务器，CodeDeploy用于部署） 4. **最小化客户影响**：通过流量分割和快速回滚机制保护生产环境</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">15</div>
        <div class="field-label">Question:</div>
        <div class="field-content">To run an application, a DevOps engineer launches an Amazon EC2 instance with public IP addresses in a public subnet. A user data script obtains the application artifacts and installs them on the instances upon launch. A change to the security classification of the application now requires the instances to run with no access to the internet. While the instances launch successfully and show as healthy, the application does not seem to be installed. Which of the following should successfully install the application while complying with the new rule? C (90%) 10%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Launch the instances in a public subnet with Elastic IP addresses attached. Once the application is installed and running, run a script to disassociate the Elastic IP addresses afterwards.
B. Set up a NAT gateway. Deploy the EC2 instances to a private subnet. Update the private subnet&#x27;s route table to use the NAT gateway as the default route.
C. Publish the application artifacts to an Amazon S3 bucket and create a VPC endpoint for S3. Assign an IAM instance profile to the EC2 instances so they can read the application artifacts from the S3 bucket.
D. Create a security group for the application instances and allow only outbound traffic to the artifact repository. Remove the security group rule once the install is complete.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">为了运行一个应用程序，DevOps工程师在公有子网中启动了一个带有公网IP地址的Amazon EC2实例。用户数据脚本在启动时获取应用程序工件并将其安装在实例上。应用程序安全分类的变更现在要求实例在没有互联网访问的情况下运行。虽然实例启动成功并显示为健康状态，但应用程序似乎没有安装。以下哪个选项能够在遵守新规则的同时成功安装应用程序？ 选项： A. 在公有子网中启动实例并附加Elastic IP地址。一旦应用程序安装并运行，运行脚本来解除Elastic IP地址的关联。 B. 设置NAT gateway。将EC2实例部署到私有子网。更新私有子网的路由表以使用NAT gateway作为默认路由。 C. 将应用程序工件发布到Amazon S3存储桶并为S3创建VPC endpoint。为EC2实例分配IAM实例配置文件，以便它们可以从S3存储桶读取应用程序工件。 D. 为应用程序实例创建安全组，仅允许到工件仓库的出站流量。安装完成后删除安全组规则。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在严格的网络安全要求下（完全禁止互联网访问）如何让EC2实例获取和安装应用程序的解决方案。关键约束是实例不能有任何互联网访问权限，但仍需要获取应用程序工件进行安装。 **涉及的关键AWS服务和概念：** - EC2实例和用户数据脚本 - VPC网络架构（公有子网vs私有子网） - VPC Endpoint（特别是S3的VPC Endpoint） - NAT Gateway - IAM实例配置文件和权限管理 - S3存储服务 - 网络安全和访问控制 **正确答案C的原因：** 1. **完全符合安全要求**：使用VPC Endpoint for S3可以让EC2实例通过AWS内部网络访问S3，完全不需要互联网连接 2. **架构合理**：将应用程序工件存储在S3中是最佳实践，便于版本管理和分发 3. **安全性最佳**：通过IAM实例配置文件控制访问权限，遵循最小权限原则 4. **可持续性**：这种架构不仅解决当前问题，还为未来的应用更新提供了安全的通道 **其他选项错误的原因：** - **选项A**：虽然最终会移除Elastic IP，但在安装过程中仍然违反了&quot;不能访问互联网&quot;的安全要求 - **选项B**：NAT Gateway本质上仍然提供互联网访问能力，违反了新的安全分类要求 - **选项D**：安全组规则无法绕过基本的网络连通性问题，如果没有互联网访问，仍然无法到达外部的工件仓库 **决策标准和最佳实践：** 1. **零信任网络原则**：在高安全要求下，应该完全消除互联网访问而不是临时允许 2. **AWS服务集成**：优先使用AWS原生服务的内部连接能力（如VPC Endpoint） 3. **权限最小化**：使用IAM角色精确控制资源访问权限 4. **架构的可维护性**：选择能够长期满足安全要求的解决方案，而不是临时性的变通方法</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">16</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A development team is using AWS CodeCommit to version control application code and AWS CodePipeline to orchestrate software deployments. The team has decided to use a remote main branch as the trigger for the pipeline to integrate code changes. A developer has pushed code changes to the CodeCommit repository, but noticed that the pipeline had no reaction, even after 10 minutes. Which of the following actions should be taken to troubleshoot this issue? A (58%) B (39%) 4%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Check that an Amazon EventBridge rule has been created for the main branch to trigger the pipeline.
B. Check that the CodePipeline service role has permission to access the CodeCommit repository.
C. Check that the developer&#x27;s IAM role has permission to push to the CodeCommit repository.
D. Check to see if the pipeline failed to start because of CodeCommit errors in Amazon CloudWatch Logs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个开发团队正在使用AWS CodeCommit进行应用程序代码版本控制，使用AWS CodePipeline来编排软件部署。团队决定使用远程main分支作为触发器来触发pipeline以集成代码更改。一个开发人员已经将代码更改推送到CodeCommit存储库，但注意到pipeline没有反应，即使在10分钟后也是如此。应该采取以下哪项操作来排查这个问题？ 选项： A. 检查是否已为main分支创建了Amazon EventBridge规则来触发pipeline。 B. 检查CodePipeline服务角色是否有权限访问CodeCommit存储库。 C. 检查开发人员的IAM角色是否有权限推送到CodeCommit存储库。 D. 检查在Amazon CloudWatch Logs中pipeline是否因为CodeCommit错误而启动失败。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodePipeline与CodeCommit集成时的触发机制故障排查。当代码推送到指定分支后，pipeline没有自动触发执行的问题诊断。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：Git代码存储库服务 - AWS CodePipeline：持续集成/持续部署(CI/CD)管道服务 - Amazon EventBridge：事件驱动架构服务，用于处理应用程序事件 - IAM权限：身份和访问管理 - CloudWatch Logs：日志监控服务 **正确答案A的原因：** CodePipeline通过Amazon EventBridge规则来监听CodeCommit存储库的推送事件。当代码推送到指定分支时，EventBridge规则会捕获这个事件并触发pipeline执行。如果没有正确配置EventBridge规则或规则配置错误（比如监听错误的分支），pipeline就不会被触发。这是pipeline无反应的最可能原因。 **其他选项错误的原因：** - 选项B：如果CodePipeline服务角色没有访问CodeCommit的权限，pipeline会启动但在执行阶段失败，而不是完全没有反应。 - 选项C：开发人员已经成功推送了代码，说明推送权限是正常的，这不是pipeline未触发的原因。 - 选项D：如果pipeline因错误而启动失败，至少会有启动的迹象和相关日志，但题目描述的是&quot;没有反应&quot;，说明pipeline根本没有被触发。 **决策标准和最佳实践：** 1. 故障排查应该按照事件流程顺序进行：首先确认触发机制是否正常 2. 区分&quot;未触发&quot;和&quot;触发后失败&quot;两种不同的故障模式 3. 在设置CodePipeline时，必须正确配置EventBridge规则来监听特定分支的推送事件 4. 定期验证CI/CD管道的触发机制是否正常工作</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">17</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s developers use Amazon EC2 instances as remote workstations. The company is concerned that users can create or modify EC2 security groups to allow unrestricted inbound access. A DevOps engineer needs to develop a solution to detect when users create unrestricted security group rules. The solution must detect changes to security group rules in near real time, remove unrestricted rules, and send email notifications to the security team. The DevOps engineer has created an AWS Lambda function that checks for security group ID from input, removes rules that grant unrestricted access, and sends notifications through Amazon Simple Notification Service (Amazon SNS). What should the DevOps engineer do next to meet the requirements? invoked by the custom event bus. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure the Lambda function to be invoked by the SNS topic. Create an AWS CloudTrail subscription for the SNS topic. Configure a subscription filter for security group modification events.
B. Create an Amazon EventBridge scheduled rule to invoke the Lambda function. Define a schedule pattern that runs the Lambda function every hour.
C. Create an Amazon EventBridge event rule that has the default event bus as the source. Define the rule&#x27;s event pattern to match EC2 security group creation and modification events. Configure the rule to invoke the Lambda function.
D. Create an Amazon EventBridge custom event bus that subscribes to events from all AWS services. Configure the Lambda function to be</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的开发人员使用Amazon EC2实例作为远程工作站。公司担心用户可能会创建或修改EC2 security groups以允许不受限制的入站访问。DevOps工程师需要开发一个解决方案来检测用户何时创建不受限制的security group规则。该解决方案必须近实时检测security group规则的更改，删除不受限制的规则，并向安全团队发送电子邮件通知。DevOps工程师已经创建了一个AWS Lambda函数，该函数检查输入中的security group ID，删除授予不受限制访问的规则，并通过Amazon Simple Notification Service (Amazon SNS)发送通知。DevOps工程师接下来应该做什么来满足要求？ 选项： A. 配置Lambda函数由SNS主题调用。为SNS主题创建AWS CloudTrail订阅。为security group修改事件配置订阅过滤器。 B. 创建Amazon EventBridge计划规则来调用Lambda函数。定义每小时运行Lambda函数的计划模式。 C. 创建以默认事件总线为源的Amazon EventBridge事件规则。定义规则的事件模式以匹配EC2 security group创建和修改事件。配置规则调用Lambda函数。 D. 创建订阅所有AWS服务事件的Amazon EventBridge自定义事件总线。配置Lambda函数被自定义事件总线调用。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要建立一个近实时的监控系统，能够检测EC2 security group规则的创建和修改，特别是那些允许不受限制访问的规则，并自动触发Lambda函数进行处理。 **涉及的关键AWS服务和概念：** - Amazon EventBridge：事件驱动架构的核心服务，用于捕获和路由AWS服务事件 - AWS Lambda：无服务器计算服务，用于执行自动化响应 - EC2 Security Groups：虚拟防火墙，控制EC2实例的网络访问 - 事件驱动架构：基于事件触发的自动化响应机制 **正确答案C的原因：** 1. **事件驱动的实时性**：EventBridge能够捕获AWS API调用产生的事件，当security group被创建或修改时立即触发 2. **精确的事件匹配**：可以通过事件模式精确匹配EC2 security group的创建和修改事件 3. **默认事件总线**：AWS服务事件默认发布到默认事件总线，无需额外配置 4. **直接集成**：EventBridge可以直接调用Lambda函数，架构简洁高效 **其他选项错误的原因：** - **选项A**：CloudTrail主要用于API调用日志记录，不是实时事件处理的最佳选择，且SNS作为触发源的配置复杂且不直接 - **选项B**：定时调度（每小时）无法满足&quot;近实时&quot;的要求，存在最多1小时的延迟 - **选项D**：自定义事件总线增加了不必要的复杂性，对于AWS原生服务事件，默认事件总线已足够 **决策标准和最佳实践：** 1. **实时性优先**：选择事件驱动而非定时轮询 2. **架构简洁性**：使用默认事件总线而非自定义总线 3. **精确匹配**：通过事件模式精确过滤所需事件类型 4. **成本效益**：避免不必要的复杂配置和额外组件</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">18</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is creating an AWS CloudFormation template to deploy a web service. The web service will run on Amazon EC2 instances in a private subnet behind an Application Load Balancer (ALB). The DevOps engineer must ensure that the service can accept requests from clients that have IPv6 addresses. What should the DevOps engineer do with the CloudFormation template so that IPv6 clients can access the web service? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add an IPv6 CIDR block to the VPC and the private subnet for the EC2 instances. Create route table entries for the IPv6 network, use EC2 instance types that support IPv6, and assign IPv6 addresses to each EC2 instance.
B. Assign each EC2 instance an IPv6 Elastic IP address. Create a target group, and add the EC2 instances as targets. Create a listener on port 443 of the ALB, and associate the target group with the ALB.
C. Replace the ALB with a Network Load Balancer (NLB). Add an IPv6 CIDR block to the VPC and subnets for the NLB, and assign the NLB an IPv6 Elastic IP address.
D. Add an IPv6 CIDR block to the VPC and subnets for the ALB. Create a listener on port 443, and specify the dualstack IP address type on the ALB. Create a target group, and add the EC2 instances as targets. Associate the target group with the ALB.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师正在创建AWS CloudFormation模板来部署Web服务。该Web服务将在Application Load Balancer (ALB)后面的私有子网中的Amazon EC2实例上运行。DevOps工程师必须确保该服务能够接受来自具有IPv6地址的客户端的请求。DevOps工程师应该如何处理CloudFormation模板，以便IPv6客户端可以访问Web服务？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求配置AWS基础设施以支持IPv6客户端访问位于私有子网中的Web服务，该服务通过Application Load Balancer提供访问。 **涉及的关键AWS服务和概念：** - Application Load Balancer (ALB)：七层负载均衡器，支持IPv4和IPv6双栈 - VPC IPv6 CIDR块：为VPC和子网分配IPv6地址空间 - EC2实例：运行在私有子网中的Web服务 - Target Group：ALB的目标组，用于路由流量 - Dualstack IP地址类型：ALB支持IPv4和IPv6双栈模式 **正确答案D的原因：** 选项D提供了完整且正确的IPv6支持配置： 1. 为VPC和ALB所在子网添加IPv6 CIDR块，提供IPv6地址空间 2. 在ALB上指定dualstack IP地址类型，使其同时支持IPv4和IPv6 3. 创建端口443监听器，支持HTTPS流量 4. 创建目标组并添加EC2实例作为目标 5. 将目标组与ALB关联，完成流量路由配置 **其他选项错误的原因：** - 选项A：只配置了后端EC2实例的IPv6支持，但没有配置ALB的IPv6支持，IPv6客户端仍无法通过ALB访问服务 - 选项B：IPv6 Elastic IP地址不能直接分配给EC2实例，且没有配置VPC的IPv6 CIDR块，缺少基础IPv6网络配置 - 选项C：虽然NLB支持IPv6，但题目明确要求使用ALB，且ALB完全可以支持IPv6，没有必要更换负载均衡器类型 **决策标准和最佳实践：** 1. IPv6支持需要从网络层（VPC CIDR）到应用层（负载均衡器）的完整配置 2. ALB的dualstack模式是支持IPv6客户端访问的标准做法 3. 私有子网中的EC2实例不需要直接的IPv6配置，通过ALB代理即可 4. 保持架构简单性，在满足需求的前提下避免不必要的组件更换</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">19</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Organizations and AWS Control Tower to manage all the company&#x27;s AWS accounts. The company uses the Enterprise Support plan. A DevOps engineer is using Account Factory for Terraform (AFT) to provision new accounts. When new accounts are provisioned, the DevOps engineer notices that the support plan for the new accounts is set to the Basic Support plan. The DevOps engineer needs to implement a solution to provision the new accounts with the Enterprise Support plan. Which solution will meet these requirements? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use an AWS Config conformance pack to deploy the account-part-of-organizations AWS Config rule and to automatically remediate any noncompliant accounts.
B. Create an AWS Lambda function to create a ticket for AWS Support to add the account to the Enterprise Support plan. Grant the Lambda function the support:ResolveCase permission.
C. Add an additional value to the control_tower_parameters input to set the AWSEnterpriseSupport parameter as the organization&#x27;s management account number.
D. Set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration. Redeploy AFT and apply the changes.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations和AWS Control Tower来管理公司所有的AWS账户。该公司使用Enterprise Support计划。一名DevOps工程师正在使用Account Factory for Terraform (AFT)来预配置新账户。当预配置新账户时，DevOps工程师注意到新账户的支持计划被设置为Basic Support计划。DevOps工程师需要实施一个解决方案，使新账户能够预配置Enterprise Support计划。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题要求在使用AFT预配置新AWS账户时，自动将新账户设置为Enterprise Support计划，而不是默认的Basic Support计划。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Control Tower：提供预配置的多账户环境治理 - Account Factory for Terraform (AFT)：基于Terraform的账户预配置工具 - AWS Support计划：包括Basic、Developer、Business和Enterprise等级别 - Enterprise Support：AWS最高级别的支持服务 **正确答案D的原因：** AFT提供了专门的功能标志`aft_feature_enterprise_support`来处理Enterprise Support的自动配置。将此标志设置为True并重新部署AFT是官方推荐的标准做法，能够确保所有通过AFT创建的新账户自动继承组织的Enterprise Support计划。这是AFT的内置功能，专门为解决此类问题而设计。 **其他选项错误的原因：** - 选项A：AWS Config规则主要用于合规性检查，`account-part-of-organizations`规则只是验证账户是否属于组织，无法直接配置支持计划。 - 选项B：手动创建Lambda函数和支持工单的方式过于复杂且不可靠，`support:ResolveCase`权限也不是用于添加支持计划的正确权限。 - 选项C：`control_tower_parameters`中没有`AWSEnterpriseSupport`这样的参数，这个配置方式不存在。 **决策标准和最佳实践：** 在AWS多账户环境中，应该优先使用官方提供的自动化工具和功能标志来管理账户配置，而不是自建复杂的解决方案。AFT的功能标志提供了最直接、最可靠的方式来确保账户预配置时的一致性配置。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">20</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s DevOps engineer uses AWS Systems Manager to perform maintenance tasks during maintenance windows. The company has a few Amazon EC2 instances that require a restart after notifications from AWS Health. The DevOps engineer needs to implement an automated solution to remediate these notifications. The DevOps engineer creates an Amazon EventBridge rule. How should the DevOps engineer configure the EventBridge rule to meet these requirements? A (64%) C (32%) 3%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure an event source of AWS Health, a service of EC2, and an event type that indicates instance maintenance. Target a Systems Manager document to restart the EC2 instance.
B. Configure an event source of Systems Manager and an event type that indicates a maintenance window. Target a Systems Manager document to restart the EC2 instance.
C. Configure an event source of AWS Health, a service of EC2, and an event type that indicates instance maintenance. Target a newly created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window.
D. Configure an event source of EC2 and an event type that indicates instance maintenance. Target a newly created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的DevOps工程师使用AWS Systems Manager在维护窗口期间执行维护任务。该公司有一些Amazon EC2实例在收到AWS Health通知后需要重启。DevOps工程师需要实现一个自动化解决方案来修复这些通知。DevOps工程师创建了一个Amazon EventBridge规则。DevOps工程师应该如何配置EventBridge规则来满足这些要求？ 选项： A. 配置事件源为AWS Health，服务为EC2，事件类型指示实例维护。目标为Systems Manager文档来重启EC2实例。 B. 配置事件源为Systems Manager，事件类型指示维护窗口。目标为Systems Manager文档来重启EC2实例。 C. 配置事件源为AWS Health，服务为EC2，事件类型指示实例维护。目标为新创建的AWS Lambda函数，该函数注册自动化任务在维护窗口期间重启EC2实例。 D. 配置事件源为EC2，事件类型指示实例维护。目标为新创建的AWS Lambda函数，该函数注册自动化任务在维护窗口期间重启EC2实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要创建一个自动化解决方案，当AWS Health发出EC2实例维护通知时，能够自动重启相关的EC2实例。关键是要正确配置EventBridge规则来监听AWS Health事件并触发适当的修复操作。 **涉及的关键AWS服务和概念：** - AWS Health：提供AWS服务和资源的个性化健康状态信息 - Amazon EventBridge：事件驱动的无服务器服务，用于连接应用程序与各种数据源 - AWS Systems Manager：提供运维数据的统一界面，可以自动化常见维护任务 - EC2实例维护：AWS定期对底层硬件进行维护，可能需要实例重启 **正确答案A的原因：** 1. **正确的事件源**：AWS Health是发出维护通知的源头，这是触发自动化的正确起点 2. **准确的服务和事件类型**：指定EC2服务和实例维护事件类型，确保只响应相关的维护通知 3. **直接有效的目标**：直接使用Systems Manager文档执行重启操作，这是最简单、最直接的解决方案 4. **符合最佳实践**：利用AWS原生服务的集成能力，减少复杂性 **其他选项错误的原因：** - **选项B**：事件源错误。Systems Manager不会发出维护通知，AWS Health才是维护事件的源头 - **选项C**：虽然事件源正确，但增加了不必要的Lambda函数复杂性。题目已明确提到要在维护窗口执行任务，直接使用Systems Manager文档更简洁 - **选项D**：事件源错误。EC2服务本身不直接发出维护通知，这些通知来自AWS Health服务 **决策标准和最佳实践：** 1. **事件源选择**：始终从实际发出事件的服务开始配置（AWS Health） 2. **简化原则**：在满足需求的前提下选择最简单的解决方案 3. **服务集成**：优先使用AWS原生服务之间的直接集成，而不是添加中间层 4. **自动化效率**：直接的EventBridge到Systems Manager的集成比通过Lambda的间接方式更高效</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">21</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has containerized all of its in-house quality control applications. The company is running Jenkins on Amazon EC2 instances, which require patching and upgrading. The compliance officer has requested a DevOps engineer begin encrypting build artifacts since they contain company intellectual property. What should the DevOps engineer do to accomplish this in the MOST maintainable manner? D (83%) B (17%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Automate patching and upgrading using AWS Systems Manager on EC2 instances and encrypt Amazon EBS volumes by default.
B. Deploy Jenkins to an Amazon ECS cluster and copy build artifacts to an Amazon S3 bucket with default encryption enabled.
C. Leverage AWS CodePipeline with a build action and encrypt the artifacts using AWS Secrets Manager.
D. Use AWS CodeBuild with artifact encryption to replace the Jenkins instance running on EC2 instances.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司已经将其所有内部质量控制应用程序容器化。该公司正在Amazon EC2实例上运行Jenkins，这些实例需要打补丁和升级。合规官要求DevOps工程师开始加密构建产物，因为它们包含公司知识产权。DevOps工程师应该如何以最可维护的方式完成此任务？ 选项： A. 使用AWS Systems Manager在EC2实例上自动化打补丁和升级，并默认加密Amazon EBS卷。 B. 将Jenkins部署到Amazon ECS集群，并将构建产物复制到启用了默认加密的Amazon S3存储桶。 C. 利用AWS CodePipeline与构建操作，并使用AWS Secrets Manager加密产物。 D. 使用带有产物加密功能的AWS CodeBuild来替换运行在EC2实例上的Jenkins实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到最可维护的方式来解决两个主要问题：1）消除Jenkins在EC2上需要打补丁和升级的维护负担；2）对包含知识产权的构建产物进行加密。关键词是&quot;最可维护的方式&quot;。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：完全托管的构建服务，无需管理基础设施 - Jenkins：开源CI/CD工具，在EC2上运行需要维护 - Amazon ECS：容器编排服务，仍需要一定程度的管理 - AWS CodePipeline：CI/CD管道服务 - 构建产物加密：保护知识产权的安全要求 - 托管服务 vs 自管理服务的维护成本 **正确答案D的原因：** 1. **完全托管**：CodeBuild是完全托管的服务，AWS负责所有基础设施的维护、打补丁和升级，完全消除了维护负担 2. **内置加密**：CodeBuild原生支持构建产物加密，无需额外配置复杂的加密机制 3. **容器支持**：CodeBuild天然支持容器化应用的构建 4. **最小维护**：符合题目要求的&quot;最可维护&quot;标准，运维开销最小 **其他选项错误的原因：** - **选项A**：虽然自动化了维护，但仍然需要管理EC2实例，没有从根本上解决维护问题，且EBS加密不等同于构建产物加密 - **选项B**：ECS虽然减少了一些维护工作，但仍需要管理集群和容器，维护复杂度高于完全托管服务 - **选项C**：CodePipeline本身不是构建服务，Secrets Manager主要用于密钥管理而非产物加密，概念混淆 **决策标准和最佳实践：** 1. **优先选择托管服务**：在满足功能需求的前提下，托管服务比自管理服务更易维护 2. **安全性集成**：选择原生支持所需安全功能的服务，避免复杂的自定义实现 3. **总体拥有成本**：考虑长期的运维成本，而不仅仅是初始实施成本 4. **服务适配性**：选择专门为特定用途设计的服务（CodeBuild专为构建设计）</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">22</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An IT team has built an AWS CloudFormation template so others in the company can quickly and reliably deploy and terminate an application. The template creates an Amazon EC2 instance with a user data script to install the application and an Amazon S3 bucket that the application uses to serve static webpages while it is running. All resources should be removed when the CloudFormation stack is deleted. However, the team observes that CloudFormation reports an error during stack deletion, and the S3 bucket created by the stack is not deleted. How can the team resolve the error in the MOST efficient manner to ensure that all resources are deleted without errors? deleted. Lambda function to delete all objects from the bucket when RequestType is Delete. Most Voted and delete the EC2 instance and the S3 bucket. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a DeletionPolicy attribute to the S3 bucket resource, with the value Delete forcing the bucket to be removed when the stack is
B. Add a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the
C. Identify the resource that was not deleted. Manually empty the S3 bucket and then delete it.
D. Replace the EC2 and S3 bucket resources with a single AWS OpsWorks Stacks resource. Define a custom recipe for the stack to create</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个IT团队构建了一个AWS CloudFormation模板，以便公司其他人员能够快速可靠地部署和终止应用程序。该模板创建了一个Amazon EC2实例，带有用户数据脚本来安装应用程序，以及一个Amazon S3存储桶，应用程序在运行时使用该存储桶提供静态网页服务。当CloudFormation堆栈被删除时，所有资源都应该被移除。然而，团队观察到CloudFormation在堆栈删除过程中报告错误，由堆栈创建的S3存储桶没有被删除。团队如何以最高效的方式解决这个错误，确保所有资源都能无错误地删除？ 选项： A. 为S3存储桶资源添加DeletionPolicy属性，值为Delete，强制在堆栈删除时移除存储桶 B. 添加一个自定义资源，包含AWS Lambda函数和DependsOn属性指定S3存储桶，以及IAM角色。编写Lambda函数在RequestType为Delete时删除存储桶中的所有对象 C. 识别未被删除的资源。手动清空S3存储桶然后删除它 D. 用单个AWS OpsWorks Stacks资源替换EC2和S3存储桶资源。为堆栈定义自定义配方来创建</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求找到最高效的解决方案来确保CloudFormation堆栈删除时S3存储桶能够被正确删除。问题的根本原因是S3存储桶在非空状态下无法被删除，这是AWS的安全机制。 **涉及的关键AWS服务和概念：** 1. AWS CloudFormation - 基础设施即代码服务 2. Amazon S3 - 对象存储服务，非空存储桶无法直接删除 3. AWS Lambda - 无服务器计算服务 4. CloudFormation自定义资源 - 允许在堆栈生命周期中执行自定义逻辑 5. DeletionPolicy - CloudFormation资源删除策略 6. DependsOn属性 - 控制资源删除顺序 **正确答案B的原因：** 1. **自动化解决方案**：Lambda函数可以自动清空S3存储桶中的所有对象 2. **集成到CloudFormation中**：通过自定义资源，删除逻辑成为堆栈的一部分 3. **正确的执行时机**：RequestType为Delete时触发，确保在堆栈删除时执行 4. **依赖关系管理**：DependsOn确保在删除S3存储桶之前先清空内容 5. **可重复使用**：一次配置，每次堆栈删除都自动执行 **其他选项错误的原因：** - **选项A错误**：DeletionPolicy的Delete值是默认行为，不能解决非空存储桶无法删除的根本问题 - **选项C错误**：手动操作不符合&quot;最高效&quot;的要求，且破坏了自动化流程 - **选项D错误**：OpsWorks是完全不同的服务，这种替换是不必要的复杂化，且没有解决S3删除问题 **决策标准和最佳实践：** 1. **自动化优先**：避免手动干预，保持基础设施即代码的完整性 2. **最小变更原则**：在现有架构基础上添加必要功能，而非重构整个解决方案 3. **AWS服务集成**：利用CloudFormation自定义资源功能实现复杂的生命周期管理 4. **错误预防**：通过程序化方式处理已知的AWS服务限制（如S3非空存储桶删除限制）</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">23</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an AWS CodePipeline pipeline that is configured with an Amazon S3 bucket in the eu-west-1 Region. The pipeline deploys an AWS Lambda application to the same Region. The pipeline consists of an AWS CodeBuild project build action and an AWS CloudFormation deploy action. The CodeBuild project uses the aws cloudformation package AWS CLI command to build an artifact that contains the Lambda function code&#x27;s .zip file and the CloudFormation template. The CloudFormation deploy action references the CloudFormation template from the output artifact of the CodeBuild project&#x27;s build action. The company wants to also deploy the Lambda application to the us-east-1 Region by using the pipeline in eu-west-1. A DevOps engineer has already updated the CodeBuild project to use the aws cloudformation package command to produce an additional output artifact for us-east- 1. Which combination of additional steps should the DevOps engineer take to meet these requirements? (Choose two.) CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to pass in the us-east-1 artifact location as a parameter override. Most Voted template from the us-east-1 output artifact. Most Voted CE (68%) AB (17%) Other</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the CloudFormation template to include a parameter for the Lambda function code&#x27;s zip file location. Create a new
B. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation
C. Create an S3 bucket in us-east-1. Configure the S3 bucket policy to allow CodePipeline to have read and write access.
D. Create an S3 bucket in us-east-1. Configure S3 Cross-Region Replication (CRR) from the S3 bucket in eu-west-1 to the S3 bucket in us-east-1.
E. Modify the pipeline to include the S3 bucket for us-east-1 as an artifact store. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the us-east-1 output artifact.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个AWS CodePipeline流水线，配置了位于eu-west-1区域的Amazon S3存储桶。该流水线将AWS Lambda应用程序部署到同一区域。流水线包含一个AWS CodeBuild项目构建操作和一个AWS CloudFormation部署操作。CodeBuild项目使用aws cloudformation package AWS CLI命令构建包含Lambda函数代码.zip文件和CloudFormation模板的构件。CloudFormation部署操作引用来自CodeBuild项目构建操作输出构件的CloudFormation模板。公司希望通过eu-west-1的流水线同时将Lambda应用程序部署到us-east-1区域。DevOps工程师已经更新了CodeBuild项目，使用aws cloudformation package命令为us-east-1生成额外的输出构件。DevOps工程师应该采取哪些额外步骤的组合来满足这些要求？（选择两个） 选项： A. 修改CloudFormation模板以包含Lambda函数代码zip文件位置的参数。为us-east-1在流水线中创建新的CloudFormation部署操作。配置新的部署操作以将us-east-1构件位置作为参数覆盖传入。 B. 为us-east-1在流水线中创建新的CloudFormation部署操作。配置新的部署操作使用来自us-east-1输出构件的CloudFormation模板。 C. 在us-east-1创建S3存储桶。配置S3存储桶策略以允许CodePipeline具有读写访问权限。 D. 在us-east-1创建S3存储桶。配置从eu-west-1的S3存储桶到us-east-1的S3存储桶的S3跨区域复制(CRR)。 E. 修改流水线以包含us-east-1的S3存储桶作为构件存储。为us-east-1在流水线中创建新的CloudFormation部署操作。配置新的部署操作使用来自us-east-1输出构件的CloudFormation模板。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在现有的eu-west-1区域CodePipeline基础上，扩展部署能力以同时支持us-east-1区域的Lambda应用程序部署。关键是要理解跨区域部署的技术要求和最佳实践。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD流水线服务，需要在不同区域有构件存储 - AWS CodeBuild：构建服务，已配置生成针对不同区域的构件 - AWS CloudFormation：基础设施即代码服务，用于部署Lambda应用 - Amazon S3：对象存储服务，作为CodePipeline的构件存储 - 跨区域部署：需要在目标区域有相应的存储和部署配置 **正确答案的原因：** 正确答案应该是A和E的组合。 - 选项A：修改CloudFormation模板添加参数是必要的，因为不同区域的构件位置不同，需要通过参数传递正确的S3位置给CloudFormation模板。 - 选项E：这是跨区域部署的关键要求。CodePipeline需要在目标区域(us-east-1)有构件存储，并且需要创建新的部署操作来使用该区域特定的构件。 **其他选项错误的原因：** - 选项B：虽然需要创建新的部署操作，但仅此还不够，缺少构件存储的配置。 - 选项C：仅创建S3存储桶和配置策略是不完整的，没有将其集成到流水线中。 - 选项D：S3跨区域复制不是CodePipeline跨区域部署的正确方法，CodePipeline需要在每个区域有专门的构件存储配置。 **决策标准和最佳实践：** 1. 跨区域CodePipeline部署必须在每个目标区域配置构件存储 2. CloudFormation模板需要参数化以支持不同区域的资源引用 3. 每个区域需要独立的部署操作，使用该区域特定的构件 4. 避免使用S3复制等间接方法，应直接配置CodePipeline的多区域支持</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">24</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs an application on one Amazon EC2 instance. Application metadata is stored in Amazon S3 and must be retrieved if the instance is restarted. The instance must restart or relaunch automatically if the instance becomes unresponsive. Which solution will meet these requirements? metadata to the instance when the instance is back up and running. B (96%) 4%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon CloudWatch alarm for the StatusCheckFailed metric. Use the recover action to stop and start the instance. Use an S3 event notification to push the metadata to the instance when the instance is back up and running.
B. Configure AWS OpsWorks, and use the auto healing feature to stop and start the instance. Use a lifecycle event in OpsWorks to pull the metadata from Amazon S3 and update it on the instance.
C. Use EC2 Auto Recovery to automatically stop and start the instance in case of a failure. Use an S3 event notification to push the
D. Use AWS CloudFormation to create an EC2 instance that includes the UserData property for the EC2 resource. Add a command in UserData to retrieve the application metadata from Amazon S3.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在一个Amazon EC2实例上运行应用程序。应用程序元数据存储在Amazon S3中，如果实例重启，必须检索这些元数据。如果实例变得无响应，实例必须自动重启或重新启动。哪个解决方案能满足这些要求？ 选项： A. 为StatusCheckFailed指标创建Amazon CloudWatch告警。使用恢复操作来停止和启动实例。当实例重新运行时，使用S3事件通知将元数据推送到实例。 B. 配置AWS OpsWorks，并使用自动修复功能来停止和启动实例。在OpsWorks中使用生命周期事件从Amazon S3拉取元数据并在实例上更新。 C. 使用EC2 Auto Recovery在故障情况下自动停止和启动实例。使用S3事件通知将元数据推送到实例。 D. 使用AWS CloudFormation创建包含EC2资源UserData属性的EC2实例。在UserData中添加命令从Amazon S3检索应用程序元数据。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 单个EC2实例运行应用程序 2. 应用程序元数据存储在S3中，实例重启后需要检索 3. 实例无响应时需要自动重启/重新启动 4. 需要自动化的故障检测和恢复机制 **涉及的关键AWS服务和概念：** - AWS OpsWorks：应用程序管理服务，提供自动修复和生命周期管理 - CloudWatch告警：监控和告警服务 - EC2 Auto Recovery：实例自动恢复功能 - CloudFormation：基础设施即代码服务 - S3事件通知：对象存储事件触发机制 - UserData：实例启动时执行的脚本 **正确答案B的原因：** 1. **完整的自动修复机制**：OpsWorks的auto healing功能可以自动检测实例健康状态并进行修复 2. **生命周期管理**：OpsWorks提供完整的生命周期事件管理，可以在实例启动时自动执行元数据检索 3. **集成化解决方案**：OpsWorks将监控、修复和配置管理集成在一个服务中 4. **可靠的元数据检索**：通过生命周期事件确保每次实例启动时都能正确获取S3中的元数据 **其他选项错误的原因：** - **选项A**：S3事件通知无法直接推送数据到EC2实例，且CloudWatch recover action主要用于硬件故障，不适用于应用程序无响应 - **选项C**：EC2 Auto Recovery主要处理底层硬件问题，不能检测应用程序级别的无响应状态，且S3事件通知机制不可行 - **选项D**：CloudFormation和UserData只能在初始创建时执行，无法提供持续的故障检测和自动重启功能 **决策标准和最佳实践：** 1. 选择能提供应用程序级别健康检查的服务 2. 确保故障恢复和配置管理的自动化程度 3. 考虑解决方案的完整性和集成度 4. 评估服务对单实例场景的适用性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">25</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has multiple AWS accounts. The company uses AWS IAM Identity Center (AWS Single Sign-On) that is integrated with AWS Toolkit for Microsoft Azure DevOps. The attributes for access control feature is enabled in IAM Identity Center. The attribute mapping list contains two entries. The department key is mapped to ${path:enterprise.department}. The costCenter key is mapped to ${path:enterprise.costCenter}. All existing Amazon EC2 instances have a department tag that corresponds to three company departments (d1, d2, d3). A DevOps engineer must create policies based on the matching attributes. The policies must minimize administrative effort and must grant each Azure AD user access to only the EC2 instances that are tagged with the user&#x27;s respective department name. Which condition key should the DevOps engineer include in the custom permissions policies to meet these requirements? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content"></div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有多个AWS账户。该公司使用与AWS Toolkit for Microsoft Azure DevOps集成的AWS IAM Identity Center (AWS Single Sign-On)。IAM Identity Center中启用了访问控制属性功能。属性映射列表包含两个条目。department键映射到${path:enterprise.department}。costCenter键映射到${path:enterprise.costCenter}。所有现有的Amazon EC2实例都有一个department标签，对应公司的三个部门(d1, d2, d3)。DevOps工程师必须基于匹配属性创建策略。这些策略必须最小化管理工作量，并且必须授予每个Azure AD用户仅访问标记有用户各自部门名称的EC2实例。DevOps工程师应该在自定义权限策略中包含哪个条件键来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求创建IAM策略，使Azure AD用户只能访问标记有其所属部门的EC2实例，同时最小化管理工作量。 **涉及的关键AWS服务和概念：** 1. AWS IAM Identity Center (原AWS SSO) - 提供集中身份管理 2. 属性映射 - 将外部身份提供商的属性映射到AWS中 3. 基于属性的访问控制(ABAC) - 使用用户属性动态控制访问权限 4. IAM条件键 - 用于策略中的条件判断 5. EC2资源标签 - 用于资源分类和访问控制 **正确答案的原因：** 虽然题目没有显示选项，但根据场景分析，正确答案应该是使用`aws:PrincipalTag/department`条件键。原因如下： - IAM Identity Center会将Azure AD的department属性作为会话标签传递给AWS - `aws:PrincipalTag/department`可以引用用户的department属性值 - 结合`aws:RequestedRegion`或资源标签条件，可以实现基于部门的访问控制 - 这种方式支持动态访问控制，无需为每个部门创建单独的策略 **决策标准和最佳实践：** 1. **动态访问控制** - 使用ABAC而非传统的基于角色的访问控制(RBAC) 2. **最小化管理开销** - 一个策略处理所有部门，而不是每个部门一个策略 3. **属性一致性** - 确保Azure AD属性与AWS资源标签的命名一致 4. **安全最佳实践** - 实施最小权限原则，用户只能访问其部门的资源 这种解决方案体现了现代云环境中基于属性的细粒度访问控制的最佳实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">26</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company hosts a security auditing application in an AWS account. The auditing application uses an IAM role to access other AWS accounts. All the accounts are in the same organization in AWS Organizations. A recent security audit revealed that users in the audited AWS accounts could modify or delete the auditing application&#x27;s IAM role. The company needs to prevent any modification to the auditing application&#x27;s IAM role by any entity other than a trusted administrator IAM role. Which solution will meet these requirements? trusted administrator IAM role to make changes. Attach the SCP to the root of the organization. Most Voted Include a Deny statement for changes by all other IAM principals. Attach the SCP to the IAM service in each AWS account where the auditing application has an IAM role. condition that allows the trusted administrator IAM role to make changes. Attach the permissions boundary to the audited AWS accounts. condition that allows the trusted administrator IAM role to make changes. Attach the permissions boundary to the auditing application&#x27;s IAM role in the AWS accounts. A (91%) 9%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an SCP that includes a Deny statement for changes to the auditing application&#x27;s IAM role. Include a condition that allows the
B. Create an SCP that includes an Allow statement for changes to the auditing application&#x27;s IAM role by the trusted administrator IAM role.
C. Create an IAM permissions boundary that includes a Deny statement for changes to the auditing application&#x27;s IAM role. Include a
D. Create an IAM permissions boundary that includes a Deny statement for changes to the auditing application&#x27;s IAM role. Include a</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS账户中托管安全审计应用程序。该审计应用程序使用IAM角色访问其他AWS账户。所有账户都在AWS Organizations的同一个组织中。最近的安全审计显示，被审计AWS账户中的用户可以修改或删除审计应用程序的IAM角色。公司需要防止除受信任管理员IAM角色之外的任何实体对审计应用程序的IAM角色进行任何修改。哪个解决方案能满足这些要求？ 选项： A. 创建一个SCP，包含拒绝更改审计应用程序IAM角色的Deny语句。包含允许受信任管理员IAM角色进行更改的条件。将SCP附加到组织的根部。 B. 创建一个SCP，包含允许受信任管理员IAM角色更改审计应用程序IAM角色的Allow语句。将SCP附加到每个具有审计应用程序IAM角色的AWS账户中的IAM服务。 C. 创建一个IAM权限边界，包含拒绝更改审计应用程序IAM角色的Deny语句。包含允许受信任管理员IAM角色进行更改的条件。将权限边界附加到被审计的AWS账户。 D. 创建一个IAM权限边界，包含拒绝更改审计应用程序IAM角色的Deny语句。包含允许受信任管理员IAM角色进行更改的条件。将权限边界附加到审计应用程序在AWS账户中的IAM角色。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要保护审计应用程序的IAM角色不被除受信任管理员之外的任何实体修改或删除，确保审计系统的完整性和安全性。 **涉及的关键AWS服务和概念：** 1. **Service Control Policies (SCP)** - AWS Organizations的策略，用于在组织级别控制权限 2. **IAM权限边界** - 限制IAM实体最大权限的高级功能 3. **AWS Organizations** - 集中管理多个AWS账户的服务 4. **IAM角色** - 用于跨账户访问的身份验证机制 **正确答案C的原因：** 1. **权限边界的适用性** - IAM权限边界专门设计用于限制特定IAM实体的最大权限，非常适合保护特定的IAM角色 2. **精确的作用范围** - 将权限边界附加到被审计账户可以确保这些账户中的用户无法修改审计角色 3. **条件控制** - 通过包含条件语句，可以精确允许受信任管理员进行必要的更改 4. **最小权限原则** - 权限边界确保即使用户有其他权限，也无法超越边界限制 **其他选项错误的原因：** - **选项A错误** - 虽然SCP可以提供组织级别的控制，但将其附加到组织根部会影响所有账户，可能过于宽泛且影响正常操作 - **选项B错误** - SCP不能附加到特定的AWS服务，只能附加到组织单位或账户；且仅使用Allow语句无法有效阻止未授权访问 - **选项D错误** - 将权限边界直接附加到审计应用程序的IAM角色会限制该角色本身的功能，可能影响其正常的审计操作 **决策标准和最佳实践：** 1. **精确性原则** - 选择能够精确控制特定资源访问的机制 2. **最小影响原则** - 确保安全措施不会影响其他正常业务操作 3. **分层防护** - 使用权限边界作为额外的安全层来保护关键IAM角色 4. **审计合规性** - 确保安全控制措施符合审计要求和合规标准</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">27</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an on-premises application that is written in Go. A DevOps engineer must move the application to AWS. The company&#x27;s development team wants to enable blue/green deployments and perform A/B testing. Which solution will meet these requirements? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Deploy the application on an Amazon EC2 instance, and create an AMI of the instance. Use the AMI to create an automatic scaling launch configuration that is used in an Auto Scaling group. Use Elastic Load Balancing to distribute traffic. When changes are made to the application, a new AMI will be created, which will initiate an EC2 instance refresh.
B. Use Amazon Lightsail to deploy the application. Store the application in a zipped format in an Amazon S3 bucket. Use this zipped version to deploy new versions of the application to Lightsail. Use Lightsail deployment options to manage the deployment.
C. Use AWS CodeArtifact to store the application code. Use AWS CodeDeploy to deploy the application to a fleet of Amazon EC2 instances. Use Elastic Load Balancing to distribute the traffic to the EC2 instances. When making changes to the application, upload a new version to CodeArtifact and create a new CodeDeploy deployment.
D. Use AWS Elastic Beanstalk to host the application. Store a zipped version of the application in Amazon S3. Use that location to deploy new versions of the application. Use Elastic Beanstalk to manage the deployment options.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个用Go语言编写的本地应用程序。DevOps工程师必须将该应用程序迁移到AWS。公司的开发团队希望启用蓝绿部署并执行A/B测试。哪种解决方案能满足这些要求？ 选项： A. 在Amazon EC2实例上部署应用程序，并创建该实例的AMI。使用AMI创建Auto Scaling组中使用的自动扩展启动配置。使用Elastic Load Balancing分发流量。当对应用程序进行更改时，将创建新的AMI，这将启动EC2实例刷新。 B. 使用Amazon Lightsail部署应用程序。将应用程序以压缩格式存储在Amazon S3存储桶中。使用此压缩版本将应用程序的新版本部署到Lightsail。使用Lightsail部署选项管理部署。 C. 使用AWS CodeArtifact存储应用程序代码。使用AWS CodeDeploy将应用程序部署到Amazon EC2实例集群。使用Elastic Load Balancing将流量分发到EC2实例。对应用程序进行更改时，将新版本上传到CodeArtifact并创建新的CodeDeploy部署。 D. 使用AWS Elastic Beanstalk托管应用程序。将应用程序的压缩版本存储在Amazon S3中。使用该位置部署应用程序的新版本。使用Elastic Beanstalk管理部署选项。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为Go应用程序选择一个AWS解决方案，关键需求是支持蓝绿部署和A/B测试功能。 **涉及的关键AWS服务和概念：** - 蓝绿部署：一种部署策略，通过维护两个相同的生产环境来实现零停机部署 - A/B测试：将流量分配给不同版本的应用程序以测试性能和用户体验 - AWS Elastic Beanstalk：应用程序部署和管理平台 - AWS CodeDeploy：自动化部署服务 - Amazon Lightsail：简化的云平台 - Auto Scaling和Elastic Load Balancing：自动扩展和负载均衡服务 **正确答案D的原因：** 1. **原生支持蓝绿部署**：Elastic Beanstalk内置支持蓝绿部署策略，可以轻松配置和管理 2. **A/B测试能力**：通过Elastic Beanstalk的流量分割功能，可以将流量按比例分配给不同版本 3. **简化管理**：Beanstalk自动处理容量配置、负载均衡、自动扩展和应用程序健康监控 4. **Go语言支持**：Beanstalk原生支持Go应用程序部署 5. **版本管理**：支持应用程序版本管理和回滚功能 **其他选项错误的原因：** - **选项A**：虽然技术上可行，但EC2实例刷新不是真正的蓝绿部署，且配置A/B测试较为复杂，需要额外的工具和配置 - **选项B**：Lightsail是简化的服务，缺乏高级的蓝绿部署和A/B测试功能，主要适用于简单的应用程序 - **选项C**：CodeDeploy虽然支持蓝绿部署，但需要更多手动配置，且CodeArtifact主要用于包管理而非应用程序部署，整体方案复杂度较高 **决策标准和最佳实践：** 1. **功能匹配度**：选择原生支持所需功能的服务 2. **管理复杂度**：优先选择能简化运维的托管服务 3. **部署策略支持**：确保服务支持现代化部署模式 4. **语言兼容性**：验证平台对特定编程语言的支持程度</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">28</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A developer is maintaining a fleet of 50 Amazon EC2 Linux servers. The servers are part of an Amazon EC2 Auto Scaling group, and also use Elastic Load Balancing for load balancing. Occasionally, some application servers are being terminated after failing ELB HTTP health checks. The developer would like to perform a root cause analysis on the issue, but before being able to access application logs, the server is terminated. How can log collection be automated? D (88%) 12%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Auto Scaling lifecycle hooks to put instances in a Pending:Wait state. Create an Amazon CloudWatch alarm for EC2 Instance Terminate Successful and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.
B. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an AWS Config rule for EC2 Instance-terminate Lifecycle Action and trigger a step function that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.
C. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon CloudWatch subscription filter for EC2 Instance Terminate Successful and trigger a CloudWatch agent that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.
D. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名开发人员正在维护一个由50台Amazon EC2 Linux服务器组成的集群。这些服务器是Amazon EC2 Auto Scaling组的一部分，并且还使用Elastic Load Balancing进行负载均衡。偶尔，一些应用服务器在ELB HTTP健康检查失败后被终止。开发人员希望对此问题进行根本原因分析，但在能够访问应用程序日志之前，服务器就被终止了。如何自动化日志收集？ 选项： A. 使用Auto Scaling生命周期钩子将实例置于Pending:Wait状态。为EC2 Instance Terminate Successful创建Amazon CloudWatch告警，触发AWS Lambda函数调用SSM Run Command脚本收集日志，推送到Amazon S3，并在日志收集完成后完成生命周期操作。 B. 使用Auto Scaling生命周期钩子将实例置于Terminating:Wait状态。为EC2 Instance-terminate Lifecycle Action创建AWS Config规则，触发step function调用脚本收集日志，推送到Amazon S3，并在日志收集完成后完成生命周期操作。 C. 使用Auto Scaling生命周期钩子将实例置于Terminating:Wait状态。为EC2 Instance Terminate Successful创建Amazon CloudWatch订阅过滤器，触发CloudWatch代理调用脚本收集日志，推送到Amazon S3，并在日志收集完成后完成生命周期操作。 D. 使用Auto Scaling生命周期钩子将实例置于Terminating:Wait状态。为EC2 Instance-terminate Lifecycle Action创建Amazon EventBridge规则，触发AWS Lambda函数调用SSM Run Command脚本收集日志，推送到Amazon S3，并在日志收集完成后完成生命周期操作。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题要求在EC2实例因健康检查失败而被Auto Scaling终止之前，自动收集应用程序日志以进行根本原因分析。关键是要在实例终止前有足够时间收集日志。 **涉及的关键AWS服务和概念：** 1. Auto Scaling Lifecycle Hooks - 在实例启动或终止过程中暂停实例，允许执行自定义操作 2. EventBridge - 事件驱动的服务，可以监听AWS服务事件并触发相应操作 3. Lambda函数 - 无服务器计算服务，用于执行日志收集逻辑 4. SSM Run Command - 在EC2实例上远程执行命令的服务 5. S3 - 用于存储收集的日志文件 **正确答案D的原因：** 1. **正确的生命周期状态**：使用Terminating:Wait状态，这是在实例即将被终止时的正确钩子状态 2. **合适的事件监听机制**：EventBridge可以准确监听EC2 Instance-terminate Lifecycle Action事件 3. **有效的执行方案**：Lambda函数结合SSM Run Command可以可靠地在目标实例上执行日志收集脚本 4. **完整的工作流程**：包含了暂停终止→收集日志→上传S3→完成生命周期操作的完整流程 **其他选项错误的原因：** - **选项A**：使用了错误的生命周期状态(Pending:Wait)，这是实例启动时的状态，不是终止时的状态；且CloudWatch告警不是监听生命周期事件的正确方式 - **选项B**：AWS Config主要用于配置合规性检查，不是处理实时生命周期事件的合适服务；Step Functions增加了不必要的复杂性 - **选项C**：CloudWatch订阅过滤器主要用于日志流处理，不适合监听EC2生命周期事件；且CloudWatch代理不能直接触发脚本执行 **决策标准和最佳实践：** 1. 选择正确的生命周期钩子状态以匹配使用场景 2. 使用EventBridge作为事件驱动架构的标准服务 3. Lambda + SSM Run Command的组合提供了灵活且可靠的远程执行能力 4. 确保工作流程包含完成生命周期操作的步骤，避免实例无限期挂起</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">29</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an organization in AWS Organizations. The organization includes workload accounts that contain enterprise applications. The company centrally manages users from an operations account. No users can be created in the workload accounts. The company recently added an operations team and must provide the operations team members with administrator access to each workload account. Which combination of actions will provide this access? (Choose three.) F. Create an Amazon Cognito user pool in the operations account. Create an Amazon Cognito user for each operations team member. BDE (88%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a SysAdmin role in the operations account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the workload accounts.
B. Create a SysAdmin role in each workload account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the operations account.
C. Create an Amazon Cognito identity pool in the operations account. Attach the SysAdmin role as an authenticated role.
D. In the operations account, create an IAM user for each operations team member.
E. In the operations account, create an IAM user group that is named SysAdmins. Add an IAM policy that allows the sts:AssumeRole action for the SysAdmin role in each workload account. Add all operations team members to the group.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中有一个组织。该组织包含含有企业应用程序的工作负载账户。公司从一个运营账户集中管理用户。不能在工作负载账户中创建用户。公司最近添加了一个运营团队，必须为运营团队成员提供对每个工作负载账户的管理员访问权限。哪种操作组合将提供这种访问权限？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Organizations环境下，为运营团队成员提供跨账户的管理员访问权限。关键约束是用户只能在运营账户中创建，不能在工作负载账户中创建用户。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - IAM跨账户角色假设（Cross-account role assumption） - STS AssumeRole：临时凭证服务 - IAM用户、组和策略管理 - 信任关系（Trust relationship）配置 **正确答案分析（应该是BDE组合）：** - **选项B**：在每个工作负载账户中创建SysAdmin角色，附加AdministratorAccess策略，并修改信任关系允许运营账户假设该角色。这是跨账户访问的核心配置。 - **选项D**：在运营账户中为每个运营团队成员创建IAM用户。这满足了用户集中管理的要求。 - **选项E**：创建SysAdmins用户组，添加允许假设工作负载账户中SysAdmin角色的策略，并将所有运营团队成员加入该组。这建立了用户到角色的权限映射。 **其他选项错误的原因：** - **选项A**：在运营账户创建角色让工作负载账户假设是错误的方向，应该是运营账户的用户假设工作负载账户的角色。 - **选项C和F**：Amazon Cognito主要用于应用程序用户身份验证，不适用于AWS管理控制台的企业用户管理场景。 **决策标准和最佳实践：** 1. 遵循最小权限原则和职责分离 2. 使用跨账户角色假设而非跨账户用户创建 3. 集中用户管理，分散权限控制 4. 利用IAM组简化权限管理 5. 正确配置信任关系确保安全的跨账户访问</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">30</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has multiple accounts in an organization in AWS Organizations. The company&#x27;s SecOps team needs to receive an Amazon Simple Notification Service (Amazon SNS) notification if any account in the organization turns off the Block Public Access feature on an Amazon S3 bucket. A DevOps engineer must implement this change without affecting the operation of any AWS accounts. The implementation must ensure that individual member accounts in the organization cannot turn off the notification. Which solution will meet these requirements? email address to the SNS topic. Deploy a conformance pack that uses the s3-bucket-level-public-access-prohibited AWS Config managed rule in each account and uses an AWS Systems Manager document to publish an event to the SNS topic to notify the SecOps team. C (68%) A (25%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Designate an account to be the delegated Amazon GuardDuty administrator account. Turn on GuardDuty for all accounts across the organization. In the GuardDuty administrator account, create an SNS topic. Subscribe the SecOps team&#x27;s email address to the SNS topic. In the same account, create an Amazon EventBridge rule that uses an event pattern for GuardDuty findings and a target of the SNS topic.
B. Create an AWS CloudFormation template that creates an SNS topic and subscribes the SecOps team&#x27;s email address to the SNS topic. In the template, include an Amazon EventBridge rule that uses an event pattern of CloudTrail activity for s3:PutBucketPublicAccessBlock and a target of the SNS topic. Deploy the stack to every account in the organization by using CloudFormation StackSets.
C. Turn on AWS Config across the organization. In the delegated administrator account, create an SNS topic. Subscribe the SecOps team&#x27;s
D. Turn on Amazon Inspector across the organization. In the Amazon Inspector delegated administrator account, create an SNS topic. Subscribe the SecOps team&#x27;s email address to the SNS topic. In the same account, create an Amazon EventBridge rule that uses an event pattern for public network exposure of the S3 bucket and publishes an event to the SNS topic to notify the SecOps team.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中有多个账户。公司的SecOps团队需要在组织中任何账户关闭Amazon S3存储桶的Block Public Access功能时收到Amazon Simple Notification Service (Amazon SNS)通知。DevOps工程师必须在不影响任何AWS账户操作的情况下实施此更改。实施方案必须确保组织中的各个成员账户无法关闭通知功能。哪个解决方案能满足这些要求？ 选项A：指定一个账户作为委托的Amazon GuardDuty管理员账户。为组织中的所有账户开启GuardDuty。在GuardDuty管理员账户中，创建SNS主题。将SecOps团队的邮箱地址订阅到SNS主题。在同一账户中，创建Amazon EventBridge规则，使用GuardDuty发现的事件模式和SNS主题作为目标。 选项B：创建AWS CloudFormation模板，创建SNS主题并将SecOps团队的邮箱地址订阅到SNS主题。在模板中，包含Amazon EventBridge规则，使用CloudTrail活动的事件模式监控s3:PutBucketPublicAccessBlock并以SNS主题为目标。使用CloudFormation StackSets将堆栈部署到组织中的每个账户。 选项C：在整个组织中开启AWS Config。在委托管理员账户中，创建SNS主题。将SecOps团队的邮箱地址订阅到SNS主题。部署使用s3-bucket-level-public-access-prohibited AWS Config托管规则的合规包到每个账户，并使用AWS Systems Manager文档向SNS主题发布事件以通知SecOps团队。 选项D：在整个组织中开启Amazon Inspector。在Amazon Inspector委托管理员账户中，创建SNS主题。将SecOps团队的邮箱地址订阅到SNS主题。在同一账户中，创建Amazon EventBridge规则，使用S3存储桶公共网络暴露的事件模式并向SNS主题发布事件以通知SecOps团队。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 监控组织中任何账户关闭S3存储桶Block Public Access功能的行为 - 通过SNS向SecOps团队发送通知 - 不影响现有AWS账户的操作 - 确保成员账户无法关闭通知功能 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理 - Amazon S3 Block Public Access：S3安全功能 - Amazon SNS：通知服务 - AWS CloudTrail：API调用日志记录 - Amazon EventBridge：事件路由服务 - AWS CloudFormation StackSets：跨账户资源部署 - AWS Config：合规性监控 - Amazon GuardDuty：威胁检测服务 **正确答案B的原因：** 1. **精确监控**：使用CloudTrail监控s3:PutBucketPublicAccessBlock API调用，这是关闭Block Public Access功能时触发的具体API 2. **跨账户部署**：CloudFormation StackSets能够在组织的所有账户中统一部署监控资源 3. **实时响应**：EventBridge规则能够实时捕获CloudTrail事件并触发SNS通知 4. **防篡改**：通过StackSets集中管理，成员账户难以单独关闭监控功能 5. **无侵入性**：仅部署监控资源，不影响现有业务操作 **其他选项错误的原因：** - **选项A**：GuardDuty主要用于威胁检测，不专门监控S3 Block Public Access配置变更，且可能无法及时检测到此类配置更改 - **选项C**：AWS Config规则是合规性检查，主要用于检测当前状态而非实时监控配置变更动作，响应可能不够及时 - **选项D**：Amazon Inspector主要用于应用程序安全评估，不适用于监控S3配置变更，功能不匹配 **决策标准和最佳实践：** 1. **选择合适的监控工具**：对于API调用监控，CloudTrail + EventBridge是最直接有效的方案 2. **集中化管理**：使用StackSets实现跨账户的统一部署和管理 3. **实时性要求**：EventBridge提供近实时的事件处理能力 4. **安全性考虑**：通过组织级别的集中部署，防止成员账户绕过监控机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">31</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has migrated its container-based applications to Amazon EKS and wants to establish automated email notifications. The notifications sent to each email address are for specific activities related to EKS components. The solution will include Amazon SNS topics and an AWS Lambda function to evaluate incoming log events and publish messages to the correct SNS topic. Which logging solution will support these requirements? events that invoke Lambda. A (97%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable Amazon CloudWatch Logs to log the EKS components. Create a CloudWatch subscription filter for each component with Lambda as the subscription feed destination.
B. Enable Amazon CloudWatch Logs to log the EKS components. Create CloudWatch Logs Insights queries linked to Amazon EventBridge
C. Enable Amazon S3 logging for the EKS components. Configure an Amazon CloudWatch subscription filter for each component with Lambda as the subscription feed destination.
D. Enable Amazon S3 logging for the EKS components. Configure S3 PUT Object event notifications with AWS Lambda as the destination.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司已将其基于容器的应用程序迁移到Amazon EKS，并希望建立自动化邮件通知。发送到每个邮件地址的通知是针对与EKS组件相关的特定活动。该解决方案将包括Amazon SNS主题和AWS Lambda函数来评估传入的日志事件并将消息发布到正确的SNS主题。哪种日志记录解决方案将支持这些要求？ 选项： A. 启用Amazon CloudWatch Logs来记录EKS组件。为每个组件创建CloudWatch订阅过滤器，以Lambda作为订阅源目标。 B. 启用Amazon CloudWatch Logs来记录EKS组件。创建链接到Amazon EventBridge的CloudWatch Logs Insights查询。 C. 为EKS组件启用Amazon S3日志记录。为每个组件配置Amazon CloudWatch订阅过滤器，以Lambda作为订阅源目标。 D. 为EKS组件启用Amazon S3日志记录。配置S3 PUT Object事件通知，以AWS Lambda作为目标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要建立一个自动化系统来监控EKS组件的特定活动，通过Lambda函数处理日志事件，并根据不同的活动类型发送到相应的SNS主题进行邮件通知。关键是需要实时处理日志事件并触发Lambda函数。 **涉及的关键AWS服务和概念：** - Amazon EKS：托管的Kubernetes服务 - CloudWatch Logs：AWS的日志管理服务 - CloudWatch订阅过滤器：用于实时处理日志流的机制 - Amazon S3：对象存储服务 - AWS Lambda：无服务器计算服务 - Amazon SNS：消息通知服务 - S3事件通知：当S3对象发生变化时触发的事件 **正确答案的原因（选项C错误标记问题）：** 实际上，标准答案C是错误的。正确答案应该是A。原因如下： 1. EKS组件的日志应该发送到CloudWatch Logs，这是AWS的标准做法 2. CloudWatch订阅过滤器可以实时监控日志流并直接触发Lambda函数 3. 这种架构支持实时处理，符合自动化通知的要求 **其他选项错误的原因：** - 选项B：CloudWatch Logs Insights主要用于查询分析，不是实时触发机制 - 选项C：EKS日志不应该直接发送到S3，且S3不支持CloudWatch订阅过滤器 - 选项D：S3事件通知虽然可以触发Lambda，但EKS日志发送到S3不是最佳实践，且延迟较高 **决策标准和最佳实践：** 1. EKS日志应使用CloudWatch Logs进行集中管理 2. 实时处理需求应优先选择CloudWatch订阅过滤器 3. Lambda函数应作为日志处理的中间层来解析和路由消息 4. 架构应保持简单、实时性强且符合AWS最佳实践 注：题目中标记的正确答案C实际上是错误的，正确答案应该是A。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">32</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is implementing an Amazon Elastic Container Service (Amazon ECS) cluster to run its workload. The company architecture will run multiple ECS services on the cluster. The architecture includes an Application Load Balancer on the front end and uses multiple target groups to route traffic. A DevOps engineer must collect application and access logs. The DevOps engineer then needs to send the logs to an Amazon S3 bucket for near-real-time analysis. Which combination of steps must the DevOps engineer take to meet these requirements? (Choose three.) F. Create an Amazon Kinesis Data Firehose delivery stream that has a destination of the logging S3 bucket. Then create an Amazon CloudWatch Logs subscription filter for Kinesis Data Firehose. Most Voted BDF (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Download the Amazon CloudWatch Logs container instance from AWS. Configure this instance as a task. Update the application service definitions to include the logging task.
B. Install the Amazon CloudWatch Logs agent on the ECS instances. Change the logging driver in the ECS task definition to awslogs.
C. Use Amazon EventBridge to schedule an AWS Lambda function that will run every 60 seconds and will run the Amazon CloudWatch Logs create-export-task command. Then point the output to the logging S3 bucket.
D. Activate access logging on the ALB. Then point the ALB directly to the logging S3 bucket.
E. Activate access logging on the target groups that the ECS services use. Then send the logs directly to the logging S3 bucket.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在实施Amazon Elastic Container Service (Amazon ECS)集群来运行其工作负载。公司架构将在集群上运行多个ECS服务。架构包括前端的Application Load Balancer，并使用多个target groups来路由流量。DevOps工程师必须收集应用程序和访问日志。然后DevOps工程师需要将日志发送到Amazon S3存储桶进行近实时分析。DevOps工程师必须采取哪些步骤组合来满足这些要求？（选择三个。） 选项： A. 从AWS下载Amazon CloudWatch Logs容器实例。将此实例配置为任务。更新应用程序服务定义以包含日志记录任务。 B. 在ECS实例上安装Amazon CloudWatch Logs代理。将ECS任务定义中的日志驱动程序更改为awslogs。 C. 使用Amazon EventBridge调度AWS Lambda函数，该函数每60秒运行一次，并运行Amazon CloudWatch Logs create-export-task命令。然后将输出指向日志S3存储桶。 D. 在ALB上激活访问日志记录。然后将ALB直接指向日志S3存储桶。 E. 在ECS服务使用的target groups上激活访问日志记录。然后将日志直接发送到日志S3存储桶。 F. 创建一个以日志S3存储桶为目标的Amazon Kinesis Data Firehose传输流。然后为Kinesis Data Firehose创建Amazon CloudWatch Logs订阅过滤器。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求建立一个完整的日志收集和传输架构，需要收集ECS应用程序日志和ALB访问日志，并将它们近实时地发送到S3存储桶进行分析。 **涉及的关键AWS服务和概念：** - Amazon ECS：容器服务，需要配置应用程序日志收集 - Application Load Balancer (ALB)：负载均衡器，需要配置访问日志 - Amazon CloudWatch Logs：日志聚合服务 - Amazon Kinesis Data Firehose：近实时数据传输服务 - Amazon S3：目标存储服务 **正确答案BDF的原因：** - **选项B**：在ECS实例上安装CloudWatch Logs代理并使用awslogs驱动程序是收集ECS容器应用程序日志的标准做法，可以自动将容器日志发送到CloudWatch Logs - **选项D**：ALB访问日志激活后可以直接写入S3存储桶，这是AWS原生支持的功能，无需额外配置 - **选项F**：Kinesis Data Firehose提供近实时的数据传输能力，通过CloudWatch Logs订阅过滤器可以将应用程序日志流式传输到S3 **其他选项错误的原因：** - **选项A**：不存在&quot;CloudWatch Logs容器实例&quot;这样的AWS服务，这是一个虚构的概念 - **选项C**：使用Lambda定期运行create-export-task命令不是近实时的解决方案，而且会产生不必要的延迟和复杂性 - **选项E**：Target groups本身不提供访问日志功能，访问日志是在ALB层面配置的 **决策标准和最佳实践：** 1. **日志收集层面**：使用AWS原生的日志收集机制（awslogs驱动程序） 2. **近实时要求**：选择流式传输服务（Kinesis Data Firehose）而非批处理方案 3. **架构简洁性**：利用AWS服务间的原生集成，避免不必要的中间组件 4. **成本效益**：ALB直接写入S3比通过其他服务转发更经济高效</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">33</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company that uses electronic health records is running a fleet of Amazon EC2 instances with an Amazon Linux operating system. As part of patient privacy requirements, the company must ensure continuous compliance for patches for operating system and applications running on the EC2 instances. How can the deployments of the operating system and application patches be automated using a default and custom repository? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Systems Manager to create a new patch baseline including the custom repository. Run the AWS-RunPatchBaseline document using the run command to verify and install patches.
B. Use AWS Direct Connect to integrate the corporate repository and deploy the patches using Amazon CloudWatch scheduled events, then use the CloudWatch dashboard to create reports.
C. Use yum-config-manager to add the custom repository under /etc/yum.repos.d and run yum-config-manager-enable to activate the repository.
D. Use AWS Systems Manager to create a new patch baseline including the corporate repository. Run the AWS-AmazonLinuxDefaultPatchBaseline document using the run command to verify and install patches.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家使用电子健康记录的公司正在运行一组使用Amazon Linux操作系统的Amazon EC2实例。作为患者隐私要求的一部分，公司必须确保对运行在EC2实例上的操作系统和应用程序补丁的持续合规性。如何使用默认和自定义repository来自动化操作系统和应用程序补丁的部署？ 选项： A. 使用AWS Systems Manager创建包含自定义repository的新patch baseline。使用run command运行AWS-RunPatchBaseline文档来验证和安装补丁。 B. 使用AWS Direct Connect集成企业repository，并使用Amazon CloudWatch scheduled events部署补丁，然后使用CloudWatch dashboard创建报告。 C. 使用yum-config-manager在/etc/yum.repos.d下添加自定义repository，并运行yum-config-manager-enable来激活repository。 D. 使用AWS Systems Manager创建包含企业repository的新patch baseline。使用run command运行AWS-AmazonLinuxDefaultPatchBaseline文档来验证和安装补丁。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何在AWS环境中自动化管理EC2实例的操作系统和应用程序补丁，特别是需要同时使用默认和自定义repository，并满足医疗行业的合规性要求。 **涉及的关键AWS服务和概念：** - AWS Systems Manager Patch Manager：用于自动化补丁管理的服务 - Patch Baseline：定义哪些补丁应该被安装的规则集 - Run Command：远程执行命令的功能 - AWS-RunPatchBaseline文档：用于执行补丁安装的预定义文档 - Custom Repository：自定义软件包仓库 **正确答案A的原因：** 1. AWS Systems Manager是AWS原生的补丁管理解决方案，专门设计用于大规模自动化补丁管理 2. 创建新的patch baseline可以灵活配置补丁策略，包括自定义repository 3. AWS-RunPatchBaseline是正确的文档，专门用于补丁的验证和安装 4. Run Command提供了集中化的执行机制，支持大规模部署 5. 完全满足自动化、合规性和使用自定义repository的要求 **其他选项错误的原因：** - 选项B：AWS Direct Connect是网络连接服务，不是补丁管理工具；CloudWatch主要用于监控，不是补丁部署的最佳选择 - 选项C：这是手动配置方法，需要在每个实例上单独操作，不符合自动化要求，且无法提供集中管理和合规性报告 - 选项D：AWS-AmazonLinuxDefaultPatchBaseline是默认的patch baseline，无法包含自定义repository，不满足题目要求 **决策标准和最佳实践：** 1. 选择AWS原生服务优于第三方或手动解决方案 2. 自动化程度高的解决方案优于需要手动干预的方案 3. 能够提供集中管理和合规性报告的解决方案更适合企业环境 4. 对于医疗行业，必须选择能够提供审计跟踪和合规性证明的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">34</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using AWS CodePipeline to automate its release pipeline. AWS CodeDeploy is being used in the pipeline to deploy an application to Amazon Elastic Container Service (Amazon ECS) using the blue/green deployment model. The company wants to implement scripts to test the green version of the application before shifting traffic. These scripts will complete in 5 minutes or less. If errors are discovered during these tests, the application must be rolled back. Which strategy will meet these requirements? run the test scripts. If errors are found, exit the Lambda function with an error to initiate rollback. Most Voted C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a stage to the CodePipeline pipeline between the source and deploy stages. Use AWS CodeBuild to create a runtime environment and build commands in the buildspec file to invoke test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.
B. Add a stage to the CodePipeline pipeline between the source and deploy stages. Use this stage to invoke an AWS Lambda function that will run the test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.
C. Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTestTraffic lifecycle event to invoke an AWS Lambda function to
D. Add a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTraffic lifecycle event to invoke the test scripts. If errors are found, use the aws deploy stop-deployment CLI command to stop the deployment.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS CodePipeline来自动化其发布管道。管道中使用AWS CodeDeploy将应用程序部署到Amazon Elastic Container Service (Amazon ECS)，采用蓝/绿部署模型。公司希望实现脚本来测试应用程序的绿色版本，然后再切换流量。这些脚本将在5分钟或更短时间内完成。如果在测试过程中发现错误，应用程序必须回滚。哪种策略能满足这些要求？运行测试脚本。如果发现错误，以错误状态退出Lambda函数以启动回滚。 选项： A. 在CodePipeline管道的源码阶段和部署阶段之间添加一个阶段。使用AWS CodeBuild创建运行时环境，在buildspec文件中构建命令来调用测试脚本。如果发现错误，使用aws deploy stop-deployment命令停止部署。 B. 在CodePipeline管道的源码阶段和部署阶段之间添加一个阶段。使用此阶段调用AWS Lambda函数来运行测试脚本。如果发现错误，使用aws deploy stop-deployment命令停止部署。 C. 在CodeDeploy AppSpec文件中添加hooks部分。使用AfterAllowTestTraffic生命周期事件调用AWS Lambda函数来运行测试脚本。如果发现错误，以错误状态退出Lambda函数以启动回滚。 D. 在CodeDeploy AppSpec文件中添加hooks部分。使用AfterAllowTraffic生命周期事件调用测试脚本。如果发现错误，使用aws deploy stop-deployment CLI命令停止部署。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在AWS ECS蓝/绿部署过程中如何正确实现自动化测试和回滚机制。关键要求包括：1）在绿色环境部署后、流量切换前进行测试；2）测试失败时能够自动回滚；3）测试时间控制在5分钟内。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务 - AWS CodeDeploy：应用部署服务，支持蓝/绿部署 - Amazon ECS：容器编排服务 - 蓝/绿部署：零停机部署策略 - CodeDeploy生命周期事件：AfterAllowTestTraffic vs AfterAllowTraffic - AppSpec文件hooks机制 **正确答案C的原因：** 选项C是正确的，因为：1）AfterAllowTestTraffic是专门为蓝/绿部署中的测试阶段设计的生命周期事件，在绿色环境准备就绪但正式流量切换之前触发；2）使用Lambda函数可以灵活执行测试脚本并控制执行时间；3）Lambda函数以错误状态退出会自动触发CodeDeploy的内置回滚机制，无需手动调用CLI命令；4）这种方式完全集成在部署流程中，符合自动化最佳实践。 **其他选项错误的原因：** 选项A和B错误是因为它们将测试放在了CodePipeline的独立阶段中，这意味着测试在部署开始之前进行，而不是在绿色环境部署完成后进行测试，不符合蓝/绿部署的测试时机要求。选项D错误是因为：1）AfterAllowTraffic事件发生在流量已经切换到绿色环境之后，此时进行测试为时已晚；2）需要手动调用CLI命令进行回滚，增加了复杂性和失败风险。 **决策标准和最佳实践：** 选择部署测试策略时应考虑：1）测试时机的准确性（绿色环境就绪但流量未切换）；2）自动化程度（避免手动干预）；3）回滚机制的可靠性（利用服务内置功能）；4）与现有部署流程的集成度。CodeDeploy的生命周期hooks机制提供了最佳的集成点，而Lambda函数提供了灵活的执行环境和自动回滚能力。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">35</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Storage Gateway in file gateway mode in front of an Amazon S3 bucket that is used by multiple resources. In the morning when business begins, users do not see the objects processed by a third party the previous evening. When a DevOps engineer looks directly at the S3 bucket, the data is there, but it is missing in Storage Gateway. Which solution ensures that all the updated third-party files are available in the morning? A (97%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a nightly Amazon EventBridge event to invoke an AWS Lambda function to run the RefreshCache command for Storage Gateway.
B. Instruct the third party to put data into the S3 bucket using AWS Transfer for SFTP.
C. Modify Storage Gateway to run in volume gateway mode.
D. Use S3 Same-Region Replication to replicate any changes made directly in the S3 bucket to Storage Gateway.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Amazon S3存储桶前使用文件网关模式的AWS Storage Gateway，该存储桶被多个资源使用。早上开始营业时，用户看不到第三方在前一天晚上处理的对象。当DevOps工程师直接查看S3存储桶时，数据是存在的，但在Storage Gateway中却缺失了。哪个解决方案能确保所有更新的第三方文件在早上都可用？ 选项： A. 配置每晚的Amazon EventBridge事件来调用AWS Lambda函数，为Storage Gateway运行RefreshCache命令。 B. 指导第三方使用AWS Transfer for SFTP将数据放入S3存储桶。 C. 修改Storage Gateway以卷网关模式运行。 D. 使用S3 Same-Region Replication将直接在S3存储桶中所做的任何更改复制到Storage Gateway。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题的核心是解决AWS Storage Gateway文件网关模式下的缓存同步问题。第三方直接向S3存储桶写入数据，但Storage Gateway的本地缓存没有及时更新，导致用户无法通过Storage Gateway看到新数据。 **涉及的关键AWS服务和概念：** - AWS Storage Gateway（文件网关模式）：提供本地文件系统接口访问S3对象 - 缓存机制：Storage Gateway维护本地缓存以提高性能 - RefreshCache API：强制Storage Gateway刷新其缓存以反映S3中的变化 - Amazon EventBridge：事件驱动的调度服务 - AWS Lambda：无服务器计算服务 **正确答案A的原因：** 1. **直接解决根本问题**：RefreshCache命令专门用于解决当外部进程直接修改S3存储桶时Storage Gateway缓存不同步的问题 2. **自动化解决方案**：使用EventBridge定时触发Lambda函数执行RefreshCache，实现自动化的缓存刷新 3. **时机合适**：在夜间执行刷新操作，确保早上用户能看到最新数据 4. **成本效益**：利用现有架构，只需添加简单的自动化流程 **其他选项错误的原因：** - **选项B**：改变第三方的数据传输方式并不能解决缓存同步问题，且可能不现实 - **选项C**：卷网关模式用于块存储，不适用于文件访问场景，改变模式会破坏现有功能 - **选项D**：S3 Same-Region Replication用于在不同S3存储桶间复制数据，不能解决Storage Gateway缓存问题 **决策标准和最佳实践：** 1. **最小化架构变更**：优先选择不改变现有架构的解决方案 2. **自动化运维**：使用自动化工具减少人工干预 3. **针对性解决**：使用专门的API解决特定问题 4. **考虑业务影响**：确保解决方案不会影响现有的业务流程和第三方集成</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">36</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer needs to back up sensitive Amazon S3 objects that are stored within an S3 bucket with a private bucket policy using S3 cross-Region replication functionality. The objects need to be copied to a target bucket in a different AWS Region and account. Which combination of actions should be performed to enable this replication? (Choose three.) F. Create a replication rule in the target bucket to enable the replication. ADE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a replication IAM role in the source account
B. Create a replication IAM role in the target account.
C. Add statements to the source bucket policy allowing the replication IAM role to replicate objects.
D. Add statements to the target bucket policy allowing the replication IAM role to replicate objects.
E. Create a replication rule in the source bucket to enable the replication.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师需要备份存储在具有私有bucket策略的Amazon S3 bucket中的敏感S3对象，使用S3跨Region复制功能。这些对象需要复制到不同AWS Region和账户中的目标bucket。应该执行哪些操作组合来启用此复制？（选择三个。）F. 在目标bucket中创建复制规则以启用复制。ADE (100%) 选项：A. 在源账户中创建复制IAM角色 B. 在目标账户中创建复制IAM角色 C. 向源bucket策略添加语句，允许复制IAM角色复制对象 D. 向目标bucket策略添加语句，允许复制IAM角色复制对象 E. 在源bucket中创建复制规则以启用复制 正确答案：A</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是跨账户、跨Region的S3复制配置，需要理解S3 Cross-Region Replication (CRR)在跨账户场景下的完整配置流程。 **涉及的关键AWS服务和概念：** - S3 Cross-Region Replication (CRR) - IAM角色和权限管理 - S3 bucket策略 - 跨账户资源访问 **正确答案的原因：** 根据题目显示的答案ADE，正确的三个步骤是： - A: 在源账户创建复制IAM角色 - 这是必需的，因为复制操作需要在源账户中有适当的权限来读取源对象 - D: 向目标bucket策略添加语句允许复制IAM角色复制对象 - 跨账户复制时，目标bucket必须明确授权源账户的角色写入权限 - E: 在源bucket中创建复制规则 - 复制规则必须在源bucket中配置，指定复制的目标和条件 **其他选项错误的原因：** - B错误：复制IAM角色应该在源账户创建，不是目标账户 - C错误：源bucket策略通常不需要额外配置，因为IAM角色本身就有读取源bucket的权限 - F错误：复制规则应该在源bucket配置，不是目标bucket **决策标准和最佳实践：** 跨账户S3复制的标准配置模式是：源账户负责创建IAM角色和复制规则，目标账户负责在bucket策略中授权访问。这种设计确保了权限的最小化原则和清晰的责任分离。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">37</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has multiple member accounts that are part of an organization in AWS Organizations. The security team needs to review every Amazon EC2 security group and their inbound and outbound rules. The security team wants to programmatically retrieve this information from the member accounts using an AWS Lambda function in the management account of the organization. Which combination of access changes will meet these requirements? (Choose three.) F. Create an IAM role in the management account that has access to the AmazonEC2ReadOnlyAccess managed policy. BCE (79%) 11% 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a trust relationship that allows users in the member accounts to assume the management account IAM role.
B. Create a trust relationship that allows users in the management account to assume the IAM roles of the member accounts.
C. Create an IAM role in each member account that has access to the AmazonEC2ReadOnlyAccess managed policy.
D. Create an IAM role in each member account to allow the sts:AssumeRole action against the management account IAM role&#x27;s ARN.
E. Create an IAM role in the management account that allows the sts:AssumeRole action against the member account IAM role&#x27;s ARN.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有多个成员账户，这些账户是AWS Organizations中某个组织的一部分。安全团队需要审查每个Amazon EC2安全组及其入站和出站规则。安全团队希望使用组织管理账户中的AWS Lambda函数以编程方式从成员账户中检索这些信息。哪种访问更改的组合将满足这些要求？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Organizations环境中，让管理账户的Lambda函数能够跨账户访问所有成员账户的EC2安全组信息。需要建立适当的跨账户访问机制。 **涉及的关键AWS服务和概念：** - AWS Organizations（组织管理服务） - 跨账户IAM角色假设（Cross-account role assumption） - STS AssumeRole操作 - IAM信任关系（Trust relationship） - AmazonEC2ReadOnlyAccess托管策略 **正确答案的原因：** 正确答案应该是B、C、E的组合： - **选项B**：在管理账户中创建信任关系，允许管理账户用户假设成员账户的IAM角色。这是跨账户访问的核心机制。 - **选项C**：在每个成员账户中创建具有AmazonEC2ReadOnlyAccess权限的IAM角色，提供读取EC2安全组的必要权限。 - **选项E**：在管理账户中创建允许对成员账户IAM角色执行sts:AssumeRole操作的角色，使Lambda函数能够假设成员账户角色。 **其他选项错误的原因：** - **选项A**：方向错误，不应该让成员账户假设管理账户角色，而是相反。 - **选项D**：错误地要求成员账户角色假设管理账户角色，这与需求相反。 - **选项F**：仅在管理账户创建角色无法访问成员账户资源。 **决策标准和最佳实践：** 1. 跨账户访问遵循&quot;最小权限原则&quot; 2. 使用角色假设而非长期凭证进行跨账户访问 3. 在目标账户（成员账户）中创建具有必要权限的角色 4. 在源账户（管理账户）中配置假设目标角色的权限 5. 建立适当的信任关系确保安全的跨账户访问</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">38</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A space exploration company receives telemetry data from multiple satellites. Small packets of data are received through Amazon API Gateway and are placed directly into an Amazon Simple Queue Service (Amazon SQS) standard queue. A custom application is subscribed to the queue and transforms the data into a standard format. Because of inconsistencies in the data that the satellites produce, the application is occasionally unable to transform the data. In these cases, the messages remain in the SQS queue. A DevOps engineer must develop a solution that retains the failed messages and makes them available to scientists for review and future processing. Which solution will meet these requirements? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure AWS Lambda to poll the SQS queue and invoke a Lambda function to check whether the queue messages are valid. If validation fails, send a copy of the data that is not valid to an Amazon S3 bucket so that the scientists can review and correct the data. When the data is corrected, amend the message in the SQS queue by using a replay Lambda function with the corrected data.
B. Convert the SQS standard queue to an SQS FIFO queue. Configure AWS Lambda to poll the SQS queue every 10 minutes by using an Amazon EventBridge schedule. Invoke the Lambda function to identify any messages with a SentTimestamp value that is older than 5 minutes, push the data to the same location as the application&#x27;s output location, and remove the messages from the queue.
C. Create an SQS dead-letter queue. Modify the existing queue by including a redrive policy that sets the Maximum Receives setting to 1 and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review the data that is not valid. Reprocess this data at a later time.
D. Configure API Gateway to send messages to different SQS virtual queues that are named for each of the satellites. Update the application to use a new virtual queue for any data that it cannot transform, and send the message to the new virtual queue. Instruct the scientists to use the virtual queue to review the data that is not valid. Reprocess this data at a later time.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家太空探索公司从多颗卫星接收遥测数据。小数据包通过Amazon API Gateway接收并直接放入Amazon Simple Queue Service (Amazon SQS)标准队列中。一个自定义应用程序订阅该队列并将数据转换为标准格式。由于卫星产生的数据存在不一致性，应用程序偶尔无法转换数据。在这些情况下，消息会保留在SQS队列中。DevOps工程师必须开发一个解决方案，保留失败的消息并使科学家能够审查和未来处理这些消息。哪个解决方案能满足这些要求？ 选项： A. 配置AWS Lambda轮询SQS队列并调用Lambda函数检查队列消息是否有效。如果验证失败，将无效数据的副本发送到Amazon S3存储桶，以便科学家审查和纠正数据。当数据被纠正后，使用重放Lambda函数用纠正的数据修改SQS队列中的消息。 B. 将SQS标准队列转换为SQS FIFO队列。配置AWS Lambda使用Amazon EventBridge调度每10分钟轮询SQS队列。调用Lambda函数识别任何SentTimestamp值超过5分钟的消息，将数据推送到与应用程序输出位置相同的位置，并从队列中删除消息。 C. 创建一个SQS死信队列。修改现有队列，包含一个重驱动策略，将Maximum Receives设置为1，并将死信队列ARN设置为新创建队列的ARN。指导科学家使用死信队列审查无效数据。稍后重新处理这些数据。 D. 配置API Gateway将消息发送到以每颗卫星命名的不同SQS虚拟队列。更新应用程序为任何无法转换的数据使用新的虚拟队列，并将消息发送到新的虚拟队列。指导科学家使用虚拟队列审查无效数据。稍后重新处理这些数据。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要设计一个解决方案来处理SQS队列中无法被应用程序转换的失败消息，使这些消息能够被科学家审查和未来处理，同时不影响正常的消息处理流程。 **涉及的关键AWS服务和概念：** - Amazon SQS标准队列和死信队列(Dead Letter Queue) - AWS Lambda函数和轮询机制 - Amazon S3存储服务 - SQS重驱动策略(Redrive Policy)和Maximum Receives设置 - 消息处理和错误处理模式 **正确答案A的原因：** 选项A提供了最完整和实用的解决方案： 1. 使用Lambda主动轮询和验证消息，提供了灵活的错误检测机制 2. 将失败数据存储到S3，为科学家提供了便于访问和分析的存储位置 3. 提供了数据纠正后的重新处理机制，形成完整的错误处理循环 4. 不需要修改现有的队列结构，实施风险较低 **其他选项错误的原因：** - 选项B：将标准队列转换为FIFO队列是不必要的架构变更，且基于时间戳的处理方式不能准确识别处理失败的消息，可能误删正常消息 - 选项C：虽然死信队列是处理失败消息的标准模式，但将Maximum Receives设置为1过于激进，可能导致临时性错误的消息也被立即移到死信队列 - 选项D：SQS虚拟队列主要用于请求-响应模式，不适合这种错误处理场景，且增加了系统复杂性 **决策标准和最佳实践：** 1. 错误处理应该提供完整的处理循环：检测→隔离→审查→纠正→重处理 2. 解决方案应该最小化对现有架构的影响 3. 应该为运维人员提供灵活的错误处理和监控能力 4. 存储失败数据的位置应该便于科学家访问和分析</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">39</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to use AWS CloudFormation for infrastructure deployment. The company has strict tagging and resource requirements and wants to limit the deployment to two Regions. Developers will need to deploy multiple versions of the same application. Which solution ensures resources are deployed in accordance with company policy? D (78%) C (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create AWS Trusted Advisor checks to find and remediate unapproved CloudFormation StackSets.
B. Create a CloudFormation drift detection operation to find and remediate unapproved CloudFormation StackSets.
C. Create CloudFormation StackSets with approved CloudFormation templates.
D. Create AWS Service Catalog products with approved CloudFormation templates.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司希望使用AWS CloudFormation进行基础设施部署。该公司有严格的标签和资源要求，并希望将部署限制在两个Region内。开发人员需要部署同一应用程序的多个版本。哪种解决方案能确保资源按照公司政策进行部署？ 选项： A. 创建AWS Trusted Advisor检查来查找和修复未经批准的CloudFormation StackSets。 B. 创建CloudFormation漂移检测操作来查找和修复未经批准的CloudFormation StackSets。 C. 使用经过批准的CloudFormation模板创建CloudFormation StackSets。 D. 使用经过批准的CloudFormation模板创建AWS Service Catalog产品。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何在有严格合规要求的环境中控制和管理CloudFormation部署。关键需求包括：严格的标签和资源要求、限制部署区域、支持多版本部署、确保符合公司政策。 **涉及的关键AWS服务和概念：** - AWS Service Catalog：提供IT服务目录管理，可以预先批准和控制可部署的资源 - CloudFormation StackSets：用于跨多个账户和区域部署CloudFormation堆栈 - AWS Trusted Advisor：提供最佳实践建议和成本优化建议 - CloudFormation漂移检测：检测堆栈资源的配置变更 **正确答案D的原因：** AWS Service Catalog是最佳选择，因为它提供了完整的治理和合规控制机制： 1. **预先批准控制**：管理员可以预先创建和批准符合公司政策的CloudFormation模板 2. **访问控制**：可以精确控制哪些用户可以部署哪些产品 3. **区域限制**：可以在产品级别限制部署区域 4. **标签强制执行**：可以在模板中预设必需的标签 5. **版本管理**：支持产品的多个版本，便于应用程序的版本控制 6. **合规监控**：提供部署历史和审计跟踪 **其他选项错误的原因：** - **选项A**：Trusted Advisor主要用于成本优化和最佳实践建议，无法提供主动的部署控制和治理 - **选项B**：漂移检测是被动的监控机制，用于检测已部署资源的变更，无法在部署前进行控制 - **选项C**：StackSets虽然可以跨区域部署，但缺乏Service Catalog提供的治理、访问控制和合规管理功能 **决策标准和最佳实践：** 在企业环境中选择基础设施部署解决方案时，应优先考虑： 1. **治理优先**：选择能提供预先控制而非事后修复的方案 2. **合规自动化**：通过模板和策略自动确保合规，而非依赖手动检查 3. **最小权限原则**：使用Service Catalog可以给开发人员提供受控的自助服务能力 4. **可审计性**：确保所有部署活动都有完整的审计跟踪</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">40</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company requires that its internally facing web application be highly available. The architecture is made up of one Amazon EC2 web server instance and one NAT instance that provides outbound internet access for updates and accessing public data. Which combination of architecture adjustments should the company implement to achieve high availability? (Choose two.) BD (84%) BE (16%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add the NAT instance to an EC2 Auto Scaling group that spans multiple Availability Zones. Update the route tables.
B. Create additional EC2 instances spanning multiple Availability Zones. Add an Application Load Balancer to split the load between them.
C. Configure an Application Load Balancer in front of the EC2 instance. Configure Amazon CloudWatch alarms to recover the EC2 instance upon host failure.
D. Replace the NAT instance with a NAT gateway in each Availability Zone. Update the route tables.
E. Replace the NAT instance with a NAT gateway that spans multiple Availability Zones. Update the route tables.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司要求其内部面向的Web应用程序具有高可用性。该架构由一个Amazon EC2 Web服务器实例和一个NAT实例组成，NAT实例为更新和访问公共数据提供出站互联网访问。公司应该实施哪种架构调整组合来实现高可用性？（选择两个。）BD (84%) BE (16%) 选项：A. 将NAT实例添加到跨多个Availability Zone的EC2 Auto Scaling组中。更新路由表。 B. 创建跨多个Availability Zone的额外EC2实例。添加Application Load Balancer在它们之间分配负载。 C. 在EC2实例前配置Application Load Balancer。配置Amazon CloudWatch告警以在主机故障时恢复EC2实例。 D. 用每个Availability Zone中的NAT gateway替换NAT实例。更新路由表。 E. 用跨多个Availability Zone的NAT gateway替换NAT实例。更新路由表。 正确答案：B</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为内部Web应用实现高可用性架构。当前架构存在单点故障问题：只有一个EC2 Web服务器和一个NAT实例，任何一个组件失败都会导致服务中断。 **涉及的关键AWS服务和概念：** - High Availability（高可用性）：通过多AZ部署消除单点故障 - Application Load Balancer：七层负载均衡器，支持健康检查 - NAT Gateway vs NAT Instance：托管服务vs自管理实例 - Auto Scaling：自动扩缩容服务 - Multi-AZ部署：跨可用区部署提高容错能力 **正确答案的原因：** 选项B正确，因为它解决了Web服务器层的单点故障问题： - 创建多个EC2实例并跨多个AZ部署，消除了单个实例和单个AZ的故障风险 - Application Load Balancer提供健康检查和流量分发，确保只将流量路由到健康的实例 - 这是实现Web层高可用性的标准最佳实践 选项D也正确，因为它解决了NAT层的单点故障： - NAT Gateway是AWS托管服务，比NAT Instance更可靠 - 在每个AZ部署NAT Gateway消除了单点故障 - NAT Gateway具有内置的高可用性和更好的性能 **其他选项错误的原因：** - 选项A：NAT实例不应该放入Auto Scaling组，因为NAT功能需要特殊的网络配置，自动扩缩容会破坏路由配置 - 选项C：仍然是单实例架构，CloudWatch告警恢复需要时间，无法提供真正的高可用性 - 选项E：NAT Gateway本身不能跨多个AZ，每个AZ需要独立的NAT Gateway **决策标准和最佳实践：** 1. 高可用性设计原则：消除所有单点故障 2. 多AZ部署：将关键组件分布在多个可用区 3. 使用托管服务：优先选择AWS托管服务（如NAT Gateway）而非自管理实例 4. 负载均衡：使用负载均衡器分发流量并进行健康检查 5. 架构分层：分别解决不同层次（Web层、网络层）的高可用性问题</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">41</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is building a multistage pipeline with AWS CodePipeline to build, verify, stage, test, and deploy an application. A manual approval stage is required between the test stage and the deploy stage. The development team uses a custom chat tool with webhook support that requires near-real-time notifications. How should the DevOps engineer configure status updates for pipeline activity and approval requests to post to the chat tool? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change. Publish subscription events to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the chat webhook URL to the SNS topic, and complete the subscription validation.
B. Create an AWS Lambda function that is invoked by AWS CloudTrail events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details to the chat webhook URL.
C. Create an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change. Publish the events to an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function that sends event details to the chat webhook URL. Subscribe the function to the SNS topic.
D. Modify the pipeline code to send the event details to the chat webhook URL at the end of each stage. Parameterize the URL so that each pipeline can send to a different URL based on the pipeline environment.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在使用AWS CodePipeline构建一个多阶段管道来构建、验证、暂存、测试和部署应用程序。在测试阶段和部署阶段之间需要一个手动批准阶段。开发团队使用一个支持webhook的自定义聊天工具，需要近实时通知。DevOps工程师应该如何配置管道活动和批准请求的状态更新，以便发布到聊天工具？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求设计一个解决方案，能够捕获AWS CodePipeline的状态变化事件（包括手动批准请求），并将这些事件近实时地发送到自定义聊天工具的webhook。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务 - Amazon EventBridge：事件驱动架构的核心服务，用于捕获AWS服务状态变化 - Amazon SNS：消息发布/订阅服务 - AWS Lambda：无服务器计算服务 - CloudWatch Logs：日志监控服务 - AWS CloudTrail：API调用审计服务 **正确答案C的原因：** 1. **事件捕获最佳实践**：EventBridge是AWS推荐的事件驱动架构解决方案，专门设计用于捕获AWS服务的状态变化事件 2. **精确过滤**：可以精确过滤CodePipeline Pipeline Execution State Change事件 3. **解耦架构**：通过SNS实现发布/订阅模式，提供更好的可扩展性和容错性 4. **灵活处理**：Lambda函数可以对事件进行自定义处理，格式化后发送到webhook 5. **近实时性能**：EventBridge + SNS + Lambda的组合提供毫秒级的事件处理延迟 **其他选项错误的原因：** - **选项A错误**：CloudWatch Logs主要用于日志分析，不是捕获CodePipeline状态变化的最佳方式，且无法直接订阅webhook URL到SNS - **选项B错误**：CloudTrail主要用于API调用审计，虽然能捕获事件但不是专门为实时事件处理设计的，延迟较高且成本更高 - **选项D错误**：修改管道代码的方式不够优雅，增加了代码复杂性，且无法捕获所有类型的状态变化（如手动批准），维护成本高 **决策标准和最佳实践：** 1. **事件驱动优先**：使用AWS原生事件服务而非轮询或日志解析 2. **服务专用性**：选择专门设计用于特定用途的服务（EventBridge用于事件处理） 3. **架构解耦**：通过SNS实现组件间的松耦合 4. **可扩展性**：解决方案应该易于扩展到多个通知目标 5. **实时性要求**：选择能提供最低延迟的服务组合</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">42</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s application development team uses Linux-based Amazon EC2 instances as bastion hosts. Inbound SSH access to the bastion hosts is restricted to specific IP addresses, as defined in the associated security groups. The company&#x27;s security team wants to receive a notification if the security group rules are modified to allow SSH access from any IP address. What should a DevOps engineer do to meet this requirement? associated with the bastion hosts. Configure Amazon Inspector to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic. C (68%) A (32%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule with a source of aws.cloudtrail and the event name AuthorizeSecurityGroupIngress. Define an Amazon Simple Notification Service (Amazon SNS) topic as the target.
B. Enable Amazon GuardDuty and check the findings for security groups in AWS Security Hub. Configure an Amazon EventBridge rule with a custom pattern that matches GuardDuty events with an output of NON_COMPLIANT. Define an Amazon Simple Notification Service (Amazon SNS) topic as the target.
C. Create an AWS Config rule by using the restricted-ssh managed rule to check whether security groups disallow unrestricted incoming SSH traffic. Configure automatic remediation to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.
D. Enable Amazon Inspector. Include the Common Vulnerabilities and Exposures-1.1 rules package to check the security groups that are</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的应用开发团队使用基于Linux的Amazon EC2实例作为堡垒主机。对堡垒主机的入站SSH访问被限制为特定的IP地址，这在相关的security groups中定义。公司的安全团队希望在security group规则被修改为允许来自任何IP地址的SSH访问时收到通知。DevOps工程师应该做什么来满足这个要求？ 选项： A. 创建一个Amazon EventBridge规则，源为aws.cloudtrail，事件名称为AuthorizeSecurityGroupIngress。定义Amazon Simple Notification Service (Amazon SNS) topic作为目标。 B. 启用Amazon GuardDuty并在AWS Security Hub中检查security groups的发现。配置Amazon EventBridge规则，使用自定义模式匹配输出为NON_COMPLIANT的GuardDuty事件。定义Amazon Simple Notification Service (Amazon SNS) topic作为目标。 C. 使用restricted-ssh托管规则创建AWS Config规则，检查security groups是否禁止不受限制的入站SSH流量。配置自动修复以向Amazon Simple Notification Service (Amazon SNS) topic发布消息。 D. 启用Amazon Inspector。包含Common Vulnerabilities and Exposures-1.1规则包来检查与堡垒主机关联的security groups。配置Amazon Inspector向Amazon Simple Notification Service (Amazon SNS) topic发布消息。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要监控security group规则的变化，特别是当SSH访问从受限IP地址变为允许任何IP地址（0.0.0.0/0）时，要及时发送通知给安全团队。 **涉及的关键AWS服务和概念：** - AWS Config：配置合规性监控服务 - Amazon EventBridge：事件驱动架构服务 - Amazon GuardDuty：威胁检测服务 - Amazon Inspector：安全评估服务 - Security Groups：EC2实例的虚拟防火墙 - CloudTrail：API调用审计服务 **正确答案C的原因：** 1. AWS Config的restricted-ssh托管规则专门设计用于检测security groups是否允许不受限制的SSH访问（0.0.0.0/0） 2. Config规则可以持续监控配置变化，一旦检测到违规配置立即触发 3. 自动修复功能可以直接与SNS集成，实现实时通知 4. 这是针对配置合规性监控的标准解决方案，完全符合需求 **其他选项错误的原因：** - **选项A**：虽然EventBridge + CloudTrail可以监控API调用，但AuthorizeSecurityGroupIngress事件只能检测到规则修改动作，无法判断具体的规则内容是否违规，可能产生大量误报 - **选项B**：GuardDuty主要用于威胁检测而非配置合规性检查，不会专门监控security group配置变化，且NON_COMPLIANT输出格式不正确 - **选项D**：Inspector主要用于应用程序和操作系统层面的安全评估，不负责监控security group配置，CVE规则包也与此需求无关 **决策标准和最佳实践：** 1. 选择专门针对配置合规性的服务（Config）而非通用监控服务 2. 使用AWS托管规则可以减少配置复杂性和错误 3. 实时监控配置变化比事后检测更有效 4. 自动化通知机制确保安全团队能及时响应</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">43</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps team manages an API running on-premises that serves as a backend for an Amazon API Gateway endpoint. Customers have been complaining about high response latencies, which the development team has verified using the API Gateway latency metrics in Amazon CloudWatch. To identify the cause, the team needs to collect relevant data without introducing additional latency. Which actions should be taken to accomplish this? (Choose two.) AC (87%) 13%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Install the CloudWatch agent server side and configure the agent to upload relevant logs to CloudWatch.
B. Enable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and upload those segments to X-Ray during each request.
C. Enable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and use the X-Ray daemon to upload segments to X-Ray.
D. Modify the on-premises application to send log information back to API Gateway with each request.
E. Modify the on-premises application to calculate and upload statistical data relevant to the API service requests to CloudWatch metrics.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps团队管理着一个运行在本地的API，该API作为Amazon API Gateway端点的后端。客户一直在抱怨响应延迟很高，开发团队已经通过Amazon CloudWatch中的API Gateway延迟指标验证了这个问题。为了识别原因，团队需要在不引入额外延迟的情况下收集相关数据。应该采取哪些行动来完成这个任务？（选择两个） 选项： A. 在服务器端安装CloudWatch agent并配置agent将相关日志上传到CloudWatch B. 在API Gateway中启用AWS X-Ray跟踪，修改应用程序以捕获请求段，并在每个请求期间将这些段上传到X-Ray C. 在API Gateway中启用AWS X-Ray跟踪，修改应用程序以捕获请求段，并使用X-Ray daemon将段上传到X-Ray D. 修改本地应用程序，使其在每个请求中将日志信息发送回API Gateway E. 修改本地应用程序以计算并上传与API服务请求相关的统计数据到CloudWatch metrics</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在不增加额外延迟的前提下，收集数据来分析API Gateway后端本地API的高延迟问题。关键约束是&quot;不引入额外延迟&quot;。 **涉及的关键AWS服务和概念：** - Amazon API Gateway：托管的API服务 - Amazon CloudWatch：监控和日志服务 - AWS X-Ray：分布式跟踪服务 - CloudWatch Agent：用于收集系统和应用程序指标的代理 **正确答案的原因：** 根据题目显示正确答案是A，但从技术角度分析，最佳答案应该是A和C的组合： A选项正确因为：CloudWatch agent可以异步收集和上传日志数据，不会在请求处理路径中增加延迟，是收集详细应用程序日志的最佳方式。 C选项应该也是正确的因为：X-Ray daemon作为独立进程运行，异步处理跟踪数据的上传，不会阻塞应用程序的请求处理，同时X-Ray提供了端到端的请求跟踪能力。 **其他选项错误的原因：** - B选项错误：在每个请求期间同步上传X-Ray段会显著增加延迟，违反了&quot;不引入额外延迟&quot;的要求 - D选项错误：在每个请求中将日志信息发送回API Gateway会增加网络开销和延迟 - E选项错误：在请求处理过程中计算和上传统计数据会增加处理时间和延迟 **决策标准和最佳实践：** 1. 选择异步数据收集方法，避免在请求处理路径中增加延迟 2. 使用专门的代理或守护进程来处理数据上传 3. 优先选择AWS原生的监控和跟踪服务 4. 确保监控解决方案不会影响生产环境的性能 5. 结合日志分析和分布式跟踪来全面诊断性能问题</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">44</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that is using a MySQL-compatible Amazon Aurora Multi-AZ DB cluster as the database. A cross-Region read replica has been created for disaster recovery purposes. A DevOps engineer wants to automate the promotion of the replica so it becomes the primary database instance in the event of a failure. Which solution will accomplish this? failure and runs an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Systems Manager Parameter Store. Code the application to reload the endpoint from Parameter Store if a database connection fails. D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a latency-based Amazon Route 53 CNAME with health checks so it points to both the primary and replica endpoints. Subscribe an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail and use that topic to invoke an AWS Lambda function that will promote the replica instance as the primary.
B. Create an Aurora custom endpoint to point to the primary database instance. Configure the application to use this endpoint. Configure AWS CloudTrail to run an AWS Lambda function to promote the replica instance and modify the custom endpoint to point to the newly promoted instance.
C. Create an AWS Lambda function to modify the application&#x27;s AWS CloudFormation template to promote the replica, apply the template to update the stack, and point the application to the newly promoted instance. Create an Amazon CloudWatch alarm to invoke this Lambda function after the failure event occurs.
D. Store the Aurora endpoint in AWS Systems Manager Parameter Store. Create an Amazon EventBridge event that detects the database</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个应用程序，使用MySQL兼容的Amazon Aurora Multi-AZ DB集群作为数据库。已经创建了一个跨Region的只读副本用于灾难恢复目的。DevOps工程师希望自动化副本的提升，使其在发生故障时成为主数据库实例。哪个解决方案能够实现这一目标？ 选项： A. 配置基于延迟的Amazon Route 53 CNAME并带有健康检查，使其指向主实例和副本端点。订阅Amazon SNS主题以接收来自AWS CloudTrail的Amazon RDS故障通知，并使用该主题调用AWS Lambda函数来提升副本实例为主实例。 B. 创建Aurora自定义端点指向主数据库实例。配置应用程序使用此端点。配置AWS CloudTrail运行AWS Lambda函数来提升副本实例并修改自定义端点指向新提升的实例。 C. 创建AWS Lambda函数来修改应用程序的AWS CloudFormation模板以提升副本，应用模板更新堆栈，并将应用程序指向新提升的实例。创建Amazon CloudWatch告警在故障事件发生后调用此Lambda函数。 D. 将Aurora端点存储在AWS Systems Manager Parameter Store中。创建Amazon EventBridge事件来检测数据库故障并运行AWS Lambda函数来提升副本实例并更新存储在AWS Systems Manager Parameter Store中的端点URL。编写应用程序代码，在数据库连接失败时从Parameter Store重新加载端点。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动化的灾难恢复解决方案，能够在主Aurora数据库集群故障时自动提升跨Region只读副本为新的主实例，并确保应用程序能够无缝切换到新的数据库端点。 **涉及的关键AWS服务和概念：** - Amazon Aurora Multi-AZ DB集群和跨Region只读副本 - AWS Systems Manager Parameter Store（参数存储） - Amazon EventBridge（事件驱动架构） - AWS Lambda（无服务器计算） - 数据库故障检测和自动故障转移 - 应用程序连接重试机制 **正确答案D的原因：** 1. **集中化配置管理**：使用Parameter Store存储数据库端点，实现配置与代码分离，便于动态更新 2. **事件驱动架构**：EventBridge能够实时检测数据库故障事件，触发自动化响应 3. **自动化端点更新**：Lambda函数既处理副本提升，又更新Parameter Store中的端点信息 4. **应用程序弹性**：应用程序具备连接失败时重新加载端点的能力，提高系统容错性 5. **完整的端到端解决方案**：从故障检测到副本提升再到应用程序重连，形成完整的自动化流程 **其他选项错误的原因：** - **选项A**：Route 53健康检查主要用于流量路由，不适合处理数据库副本提升；CloudTrail主要记录API调用，不是实时事件检测的最佳选择 - **选项B**：CloudTrail不是用于运行Lambda函数的服务；Aurora自定义端点无法跨Region工作，不适合跨Region灾难恢复场景 - **选项C**：使用CloudFormation模板更新过于复杂和缓慢，不适合需要快速响应的灾难恢复场景；CloudWatch告警的响应速度不如EventBridge事件 **决策标准和最佳实践：** 1. **事件驱动优于轮询**：使用EventBridge实时事件检测比定期检查更高效 2. **配置外部化**：将数据库端点存储在Parameter Store中，便于运行时动态更新 3. **应用程序弹性设计**：应用程序应具备处理连接失败和重新连接的能力 4. **自动化程度**：完全自动化的解决方案减少人工干预，提高恢复速度和可靠性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">45</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company hosts its staging website using an Amazon EC2 instance backed with Amazon EBS storage. The company wants to recover quickly with minimal data losses in the event of network connectivity issues or power failures on the EC2 instance. Which solution will meet these requirements? C (94%) A (3%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add the instance to an EC2 Auto Scaling group with the minimum, maximum, and desired capacity set to 1.
B. Add the instance to an EC2 Auto Scaling group with a lifecycle hook to detach the EBS volume when the EC2 instance shuts down or terminates.
C. Create an Amazon CloudWatch alarm for the StatusCheckFailed System metric and select the EC2 action to recover the instance.
D. Create an Amazon CloudWatch alarm for the StatusCheckFailed Instance metric and select the EC2 action to reboot the instance.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用由Amazon EBS存储支持的Amazon EC2实例来托管其预发布网站。该公司希望在EC2实例出现网络连接问题或电源故障时能够快速恢复并将数据损失降到最低。哪种解决方案能满足这些要求？ 选项： A. 将实例添加到EC2 Auto Scaling组中，将最小、最大和期望容量都设置为1。 B. 将实例添加到EC2 Auto Scaling组中，并使用生命周期钩子在EC2实例关闭或终止时分离EBS卷。 C. 为StatusCheckFailed System指标创建Amazon CloudWatch告警，并选择EC2操作来恢复实例。 D. 为StatusCheckFailed Instance指标创建Amazon CloudWatch告警，并选择EC2操作来重启实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 题目要求在EC2实例出现网络连接问题或电源故障时实现快速恢复并最小化数据损失。关键词是&quot;快速恢复&quot;和&quot;最小数据损失&quot;。 **涉及的关键AWS服务和概念：** - EC2 Auto Scaling：自动扩缩容服务，可以自动替换不健康的实例 - CloudWatch告警：监控服务，可以基于指标触发自动化操作 - EBS存储：持久化块存储，数据独立于EC2实例生命周期 - EC2状态检查：包括System和Instance两种类型的健康检查 **正确答案A的原因：** Auto Scaling组设置容量为1可以确保始终有一个健康的实例运行。当原实例因为网络或电源问题失效时，Auto Scaling会自动检测到实例不健康并启动新实例替换。由于使用EBS存储，数据会持久保存，新实例可以挂载相同的EBS卷，实现快速恢复且无数据损失。 **其他选项错误的原因：** - 选项B：生命周期钩子分离EBS卷的做法是多余的，而且可能导致数据访问问题，不利于快速恢复。 - 选项C：System状态检查失败通常表示底层硬件问题，EC2恢复操作可能无法解决根本问题，且恢复时间较长。 - 选项D：Instance状态检查失败时仅重启实例，无法解决底层硬件故障导致的问题，不能保证可靠恢复。 **决策标准和最佳实践：** 选择恢复策略时应考虑：1）故障类型覆盖范围；2）恢复时间目标(RTO)；3）数据保护程度。Auto Scaling提供了最全面的故障恢复能力，能够处理各种类型的实例故障，并通过自动替换实例实现最快的恢复速度。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">46</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to use AWS development tools to replace its current bash deployment scripts. The company currently deploys a LAMP application to a group of Amazon EC2 instances behind an Application Load Balancer (ALB). During the deployments, the company unit tests the committed application, stops and starts services, unregisters and re-registers instances with the load balancer, and updates file permissions. The company wants to maintain the same deployment functionality through the shift to using AWS services. Which solution will meet these requirements? D (89%) 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy&#x27;s appspec.yml file to restart services, and deregister and register instances with the ALB. Use the appspec.yml file to update file permissions without a custom script.
B. Use AWS CodePipeline to move the application from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy&#x27;s deployment group to test the application, unregister and re-register instances with the ALB, and restart services. Use the appspec.yml file to update file permissions without a custom script.
C. Use AWS CodePipeline to move the application source code from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy to test the application. Use CodeDeploy&#x27;s appspec.yml file to restart services and update permissions without a custom script. Use AWS CodeBuild to unregister and re-register instances with the ALB.
D. Use AWS CodePipeline to trigger AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy&#x27;s appspec.yml file to restart services. Unregister and re-register the instances in the AWS CodeDeploy deployment group with the ALB. Update the appspec.yml file to update file permissions without a custom script.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司希望使用AWS开发工具来替换其当前的bash部署脚本。该公司目前将LAMP应用程序部署到Application Load Balancer (ALB)后面的一组Amazon EC2实例上。在部署过程中，公司会对提交的应用程序进行单元测试，停止和启动服务，从负载均衡器注销和重新注册实例，并更新文件权限。公司希望在转向使用AWS服务的过程中保持相同的部署功能。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求使用AWS开发工具替换传统bash脚本，实现完整的CI/CD流水线，包括：单元测试、服务重启、ALB实例注册管理、文件权限更新等功能。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD流水线编排服务 - AWS CodeBuild：构建和测试服务 - AWS CodeDeploy：应用程序部署服务 - AWS CodeCommit：源代码管理服务 - appspec.yml：CodeDeploy的部署配置文件 - ALB：Application Load Balancer应用负载均衡器 **正确答案D的原因：** 1. **完整的流水线架构**：使用CodePipeline作为主要编排工具，触发CodeBuild进行测试，然后使用CodeDeploy进行部署 2. **正确的测试方式**：CodeBuild专门用于构建和测试，是进行单元测试的正确选择 3. **合理的脚本使用**：通过appspec.yml调用bash脚本来重启服务，保持了灵活性 4. **自动化ALB管理**：CodeDeploy deployment group可以自动处理ALB的实例注册和注销 5. **原生权限管理**：appspec.yml原生支持文件权限设置，无需自定义脚本 **其他选项错误的原因：** - **选项A**：缺少CodePipeline作为流水线编排，无法形成完整的CI/CD流程 - **选项B**：错误地让CodeDeploy承担测试功能，CodeDeploy主要用于部署而非测试 - **选项C**：让CodeBuild处理ALB注册管理是不合适的，这应该由CodeDeploy在部署过程中自动处理 **决策标准和最佳实践：** 1. **服务职责分离**：CodeBuild负责构建测试，CodeDeploy负责部署，CodePipeline负责编排 2. **自动化优先**：尽量使用AWS服务的原生功能，减少自定义脚本 3. **流水线完整性**：确保从源代码到部署的完整自动化流程 4. **最小化复杂性**：选择最直接、最符合AWS最佳实践的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">47</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs an application with an Amazon EC2 and on-premises configuration. A DevOps engineer needs to standardize patching across both environments. Company policy dictates that patching only happens during non-business hours. Which combination of actions will meet these requirements? (Choose three.) F. Use AWS Systems Manager Maintenance Windows to schedule a patch window. Most Voted ABF (89%) 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add the physical machines into AWS Systems Manager using Systems Manager Hybrid Activations.
B. Attach an IAM role to the EC2 instances, allowing them to be managed by AWS Systems Manager.
C. Create IAM access keys for the on-premises machines to interact with AWS Systems Manager.
D. Run an AWS Systems Manager Automation document to patch the systems every hour.
E. Use Amazon EventBridge scheduled events to schedule a patch window.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司运行着一个包含Amazon EC2和本地配置的应用程序。DevOps工程师需要在两个环境中标准化补丁管理。公司政策规定补丁只能在非工作时间进行。哪些操作组合能满足这些要求？（选择三个。）选项：A. 使用AWS Systems Manager Hybrid Activations将物理机器添加到AWS Systems Manager中。B. 为EC2实例附加IAM角色，允许它们被AWS Systems Manager管理。C. 为本地机器创建IAM访问密钥以与AWS Systems Manager交互。D. 运行AWS Systems Manager Automation文档每小时对系统进行补丁。E. 使用Amazon EventBridge计划事件来安排补丁窗口。F. 使用AWS Systems Manager Maintenance Windows来安排补丁窗口。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在混合环境（EC2 + 本地服务器）中实现标准化的补丁管理，并且必须在非工作时间执行补丁操作。需要选择三个正确的操作组合。 **涉及的关键AWS服务和概念：** - AWS Systems Manager：统一管理AWS和本地资源的服务 - Systems Manager Hybrid Activations：用于将本地服务器注册到Systems Manager的功能 - IAM角色：为AWS资源提供权限的安全机制 - Maintenance Windows：Systems Manager中用于安排维护任务的功能 - Patch Manager：Systems Manager的补丁管理组件 **正确答案分析（A、B、F）：** - **选项A正确**：Hybrid Activations是将本地服务器纳入Systems Manager管理的标准方法，通过激活码和激活ID实现安全连接 - **选项B正确**：EC2实例需要IAM角色才能与Systems Manager通信，这是基本的权限配置要求 - **选项F正确**：Maintenance Windows是Systems Manager的原生功能，专门用于安排维护窗口，可以精确控制补丁执行时间 **其他选项错误的原因：** - **选项C错误**：本地机器应该使用Hybrid Activations而不是IAM访问密钥，后者不是推荐的安全做法且管理复杂 - **选项D错误**：每小时执行补丁违反了&quot;仅在非工作时间&quot;的政策要求，频率过高且不合理 - **选项E错误**：虽然EventBridge可以触发事件，但Maintenance Windows是更直接、更适合补丁管理场景的解决方案 **决策标准和最佳实践：** 1. **统一管理**：使用Systems Manager实现跨环境的标准化管理 2. **安全性**：本地服务器使用Hybrid Activations，EC2使用IAM角色，避免长期访问密钥 3. **时间控制**：使用专门的Maintenance Windows功能而非通用的事件调度 4. **合规性**：确保补丁策略符合公司的时间窗口要求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">48</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has chosen AWS to host a new application. The company needs to implement a multi-account strategy. A DevOps engineer creates a new AWS account and an organization in AWS Organizations. The DevOps engineer also creates the OU structure for the organization and sets up a landing zone by using AWS Control Tower. The DevOps engineer must implement a solution that automatically deploys resources for new accounts that users create through AWS Control Tower Account Factory. When a user creates a new account, the solution must apply AWS CloudFormation templates and SCPs that are customized for the OU or the account to automatically deploy all the resources that are attached to the account. All the OUs are enrolled in AWS Control Tower. Which solution will meet these requirements in the MOST automated way? resources to any new accounts. Deploy SCPs by using the AWS CLI and JSON documents. D (91%) 9%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Service Catalog with AWS Control Tower. Create portfolios and products in AWS Service Catalog. Grant granular permissions to provision these resources. Deploy SCPs by using the AWS CLI and JSON documents.
B. Deploy CloudFormation stack sets by using the required templates. Enable automatic deployment. Deploy stack instances to the required accounts. Deploy a CloudFormation stack set to the organization&#x27;s management account to deploy SCPs.
C. Create an Amazon EventBridge rule to detect the CreateManagedAccount event. Configure AWS Service Catalog as the target to deploy
D. Deploy the Customizations for AWS Control Tower (CfCT) solution. Use an AWS CodeCommit repository as the source. In the repository, create a custom package that includes the CloudFormation templates and the SCP JSON documents.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司选择了AWS来托管一个新应用程序。该公司需要实施多账户策略。DevOps工程师创建了一个新的AWS账户和AWS Organizations中的组织。DevOps工程师还为组织创建了OU结构，并使用AWS Control Tower设置了landing zone。DevOps工程师必须实施一个解决方案，自动为用户通过AWS Control Tower Account Factory创建的新账户部署资源。当用户创建新账户时，解决方案必须应用为OU或账户定制的AWS CloudFormation模板和SCP，以自动部署附加到账户的所有资源。所有OU都已注册到AWS Control Tower。哪个解决方案能以最自动化的方式满足这些要求？ 选项： A. 将AWS Service Catalog与AWS Control Tower结合使用。在AWS Service Catalog中创建产品组合和产品。授予精细权限来配置这些资源。使用AWS CLI和JSON文档部署SCP。 B. 使用所需模板部署CloudFormation stack sets。启用自动部署。将stack实例部署到所需账户。将CloudFormation stack set部署到组织的管理账户以部署SCP。 C. 创建Amazon EventBridge规则来检测CreateManagedAccount事件。配置AWS Service Catalog作为目标来部署资源到任何新账户。使用AWS CLI和JSON文档部署SCP。 D. 部署Customizations for AWS Control Tower (CfCT)解决方案。使用AWS CodeCommit存储库作为源。在存储库中，创建包含CloudFormation模板和SCP JSON文档的自定义包。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现一个完全自动化的解决方案，当通过AWS Control Tower Account Factory创建新账户时，能够自动部署定制化的CloudFormation模板和SCP策略。关键要求是&quot;最自动化&quot;和能够根据不同OU或账户进行定制化部署。 **涉及的关键AWS服务和概念：** - AWS Control Tower：提供多账户环境的设置和治理 - AWS Organizations：管理多个AWS账户 - Account Factory：Control Tower中用于创建新账户的功能 - CloudFormation：基础设施即代码服务 - SCP (Service Control Policies)：组织级别的权限控制策略 - Customizations for AWS Control Tower (CfCT)：专门为Control Tower设计的定制化解决方案 **正确答案D的原因：** 1. **专门设计**：CfCT是AWS官方提供的专门为Control Tower环境设计的定制化解决方案 2. **完全自动化**：能够在Account Factory创建新账户时自动触发资源部署 3. **统一管理**：通过CodeCommit存储库集中管理CloudFormation模板和SCP文档 4. **灵活定制**：支持基于OU或账户级别的定制化部署 5. **原生集成**：与Control Tower深度集成，无需额外的事件监听或手动配置 **其他选项错误的原因：** - **选项A**：Service Catalog需要手动授权和配置，SCP部署需要手动使用CLI，自动化程度不够 - **选项B**：CloudFormation stack sets需要手动管理stack实例，且SCP部署到管理账户的方式不符合最佳实践 - **选项C**：需要手动创建EventBridge规则和配置Service Catalog，SCP部署仍需手动CLI操作，复杂度高且自动化不完整 **决策标准和最佳实践：** 1. **选择专用工具**：对于Control Tower环境，优先选择专门设计的CfCT解决方案 2. **最大化自动化**：避免需要手动CLI操作或复杂事件配置的方案 3. **集中化管理**：使用代码存储库统一管理所有定制化资源 4. **原生集成优先**：选择与现有AWS服务深度集成的解决方案，减少维护复杂性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">49</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An online retail company based in the United States plans to expand its operations to Europe and Asia in the next six months. Its product currently runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. All data is stored in an Amazon Aurora database instance. When the product is deployed in multiple regions, the company wants a single product catalog across all regions, but for compliance purposes, its customer information and purchases must be kept in each region. How should the company meet these requirements with the LEAST amount of application changes? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer information and purchases.
B. Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer information and purchases.
C. Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases.
D. Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer information and purchases.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家位于美国的在线零售公司计划在未来六个月内将业务扩展到欧洲和亚洲。其产品目前运行在Application Load Balancer后面的Amazon EC2实例上。这些实例在多个Availability Zone中的Amazon EC2 Auto Scaling组中运行。所有数据都存储在Amazon Aurora数据库实例中。当产品部署在多个区域时，公司希望在所有区域中使用单一的产品目录，但出于合规目的，客户信息和购买记录必须保存在各自的区域中。公司应该如何以最少的应用程序更改来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 全球多区域部署（美国、欧洲、亚洲） - 产品目录需要全球统一共享 - 客户信息和购买记录需要区域隔离（合规要求） - 最小化应用程序代码更改 **涉及的关键AWS服务和概念：** - Aurora数据库及其跨区域复制能力 - DynamoDB Global Tables全球表功能 - 数据合规性和区域数据驻留要求 - 应用程序架构迁移复杂度 **正确答案C的原因：** 1. **最少应用更改**：当前已使用Aurora，继续使用相同数据库引擎无需修改应用代码 2. **产品目录全球共享**：Aurora read replicas可以将主区域的产品目录数据同步到其他区域，实现全球统一 3. **数据合规性**：每个区域的独立Aurora实例确保客户数据和购买记录严格保留在本地区域 4. **性能优化**：read replicas提供低延迟的本地读取访问 **其他选项错误的原因：** - **选项A（Redshift + DynamoDB）**：需要重大应用架构改造，Redshift主要用于数据仓库而非事务处理 - **选项B（DynamoDB Global Tables）**：虽然技术可行，但从Aurora迁移到DynamoDB需要大量应用代码重写 - **选项D（Aurora + DynamoDB Global Tables）**：混合数据库架构增加复杂性，且DynamoDB Global Tables会在所有区域复制数据，违反合规要求 **决策标准和最佳实践：** 1. **最小化变更原则**：优先选择与现有技术栈兼容的解决方案 2. **数据主权合规**：确保敏感数据严格遵守区域驻留要求 3. **架构一致性**：保持统一的数据库技术栈降低运维复杂度 4. **性能考虑**：利用Aurora的跨区域复制能力实现高效的全球数据分发</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">50</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is implementing a well-architected design for its globally accessible API stack. The design needs to ensure both high reliability and fast response times for users located in North America and Europe. The API stack contains the following three tiers: - Amazon API Gateway - AWS Lambda - Amazon DynamoDB Which solution will meet the requirements? to check the North America API health every 5 minutes. In the event of a failure, update Route 53 to point to the disaster recovery API. requests to the Lambda function in the Region nearest to the user. Configure the Lambda function to retrieve and update the data in a DynamoDB table. B (100%) Get IT Certification Unlock free, top-quality video courses on ExamTopics with a simple registration. Elevate your learning journey with our expertly curated content. Register now to access a diverse range of educational resources designed for success!</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure Amazon Route 53 to point to API Gateway APIs in North America and Europe using health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB table in the same Region as the Lambda function.
B. Configure Amazon Route 53 to point to API Gateway APIs in North America and Europe using latency-based routing and health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB global table.
C. Configure Amazon Route 53 to point to API Gateway in North America, create a disaster recovery API in Europe, and configure both APIs to forward requests to the Lambda functions in that Region. Retrieve the data from a DynamoDB global table. Deploy a Lambda function
D. Configure Amazon Route 53 to point to API Gateway API in North America using latency-based routing. Configure the API to forward</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在为其全球可访问的API堆栈实施良好架构设计。该设计需要确保位于北美和欧洲的用户都能获得高可靠性和快速响应时间。API堆栈包含以下三层：- Amazon API Gateway - AWS Lambda - Amazon DynamoDB 哪种解决方案能满足要求？ 选项： A. 配置Amazon Route 53使用健康检查指向北美和欧洲的API Gateway API。配置API将请求转发到该Region中的Lambda函数。配置Lambda函数在与Lambda函数相同Region的DynamoDB表中检索和更新数据。 B. 配置Amazon Route 53使用基于延迟的路由和健康检查指向北美和欧洲的API Gateway API。配置API将请求转发到该Region中的Lambda函数。配置Lambda函数在DynamoDB global table中检索和更新数据。 C. 配置Amazon Route 53指向北美的API Gateway，在欧洲创建灾难恢复API，并配置两个API将请求转发到该Region中的Lambda函数。从DynamoDB global table检索数据。部署Lambda函数... D. 配置Amazon Route 53使用基于延迟的路由指向北美的API Gateway API。配置API转发...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个全球可访问的API架构，需要同时满足高可靠性和快速响应时间两个关键需求，服务于北美和欧洲用户。 **涉及的关键AWS服务和概念：** 1. Amazon Route 53 - DNS服务，支持健康检查和基于延迟的路由 2. API Gateway - API管理服务 3. AWS Lambda - 无服务器计算服务 4. DynamoDB Global Tables - 全球分布式数据库解决方案 5. 基于延迟的路由(Latency-based routing) - 自动将用户路由到延迟最低的endpoint 6. 健康检查(Health checks) - 监控服务可用性 **正确答案B的原因：** 1. **基于延迟的路由**：确保用户被自动路由到延迟最低的Region，满足快速响应时间要求 2. **健康检查**：提供故障转移能力，确保高可靠性 3. **多Region部署**：在北美和欧洲都部署完整的API堆栈，实现真正的高可用性 4. **DynamoDB Global Tables**：提供跨Region的数据同步，确保数据一致性和本地访问性能 **其他选项错误的原因：** - **选项A**：只有健康检查没有基于延迟的路由，无法保证用户总是访问到最近的Region；使用单Region DynamoDB表而非Global Tables，无法提供最佳的全球性能 - **选项C**：采用主-备模式而非主-主模式，欧洲只作为灾难恢复，正常情况下欧洲用户仍需访问北美，无法提供最佳延迟 - **选项D**：选项不完整，且似乎只配置了北美Region **决策标准和最佳实践：** 1. **Well-Architected Framework**：遵循可靠性和性能效率支柱 2. **全球部署策略**：多Region主-主部署优于主-备部署 3. **DNS路由策略**：基于延迟的路由比简单的健康检查路由更适合全球用户 4. **数据层设计**：Global Tables是跨Region数据同步的最佳实践 5. **故障转移**：健康检查确保自动故障转移，提高系统弹性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">51</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A rapidly growing company wants to scale for developer demand for AWS development environments. Development environments are created manually in the AWS Management Console. The networking team uses AWS CloudFormation to manage the networking infrastructure, exporting stack output values for the Amazon VPC and all subnets. The development environments have common standards, such as Application Load Balancers, Amazon EC2 Auto Scaling groups, security groups, and Amazon DynamoDB tables. To keep up with demand, the DevOps engineer wants to automate the creation of development environments. Because the infrastructure required to support the application is expected to grow, there must be a way to easily update the deployed infrastructure. CloudFormation will be used to create a template for the development environments. Which approach will meet these requirements and quickly provide consistent AWS environments for developers? subnet values. Define the development resources in the order they need to be created in the CloudFormation nested stacks. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments. C (77%) B (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Fn::ImportValue intrinsic functions in the Resources section of the template to retrieve Virtual Private Cloud (VPC) and subnet values. Use CloudFormation StackSets for the development environments, using the Count input parameter to indicate the number of environments needed. Use the UpdateStackSet command to update existing development environments.
B. Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team&#x27;s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.
C. Use nested stacks to define common infrastructure components. Use Fn::ImportValue intrinsic functions with the resources of the nested stack to retrieve Virtual Private Cloud (VPC) and subnet values. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.
D. Use Fn::ImportValue intrinsic functions in the Parameters section of the root template to retrieve Virtual Private Cloud (VPC) and</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家快速增长的公司希望扩展以满足开发人员对AWS开发环境的需求。开发环境目前在AWS Management Console中手动创建。网络团队使用AWS CloudFormation来管理网络基础设施，导出Amazon VPC和所有subnet的stack输出值。开发环境有通用标准，如Application Load Balancers、Amazon EC2 Auto Scaling groups、security groups和Amazon DynamoDB tables。为了跟上需求，DevOps工程师希望自动化创建开发环境。由于支持应用程序所需的基础设施预计会增长，必须有一种方法来轻松更新已部署的基础设施。将使用CloudFormation为开发环境创建template。哪种方法将满足这些要求并快速为开发人员提供一致的AWS环境？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 自动化创建开发环境，替代手动操作 - 利用网络团队已导出的VPC和subnet值 - 支持基础设施的轻松更新 - 提供一致性的开发环境 - 处理不断增长的基础设施需求 **涉及的关键AWS服务和概念：** - CloudFormation nested stacks：用于组织和重用模板组件 - Fn::ImportValue：用于导入其他stack导出的值 - CreateChangeSet/ExecuteChangeSet：用于安全地更新现有stack - CloudFormation StackSets：用于跨多个账户和区域部署stack **正确答案C的原因：** 1. **Nested stacks设计合理**：将通用基础设施组件定义为nested stacks，提供了良好的模块化和重用性 2. **正确使用Fn::ImportValue**：在nested stack的resources中使用Fn::ImportValue来检索VPC和subnet值，这是正确的语法位置 3. **适当的更新机制**：使用CreateChangeSet和ExecuteChangeSet命令提供了安全的更新方式，可以预览变更后再执行 4. **满足扩展需求**：nested stack架构支持基础设施的增长和组件的独立管理 **其他选项错误的原因：** - **选项A**：StackSets主要用于跨账户/区域部署，不是这个场景的最佳选择；Count参数不是StackSets的标准参数 - **选项B**：在Parameters section中使用Fn::ImportValue是错误的语法；TemplateURL用于引用nested stack而不是导入值 - **选项D**：题目截断，但从描述看是在Parameters section使用Fn::ImportValue，这是错误的用法 **决策标准和最佳实践：** 1. **模块化设计**：使用nested stacks将通用组件模块化，便于维护和重用 2. **正确的函数使用**：Fn::ImportValue应在Resources或Outputs section中使用，不是Parameters section 3. **安全更新**：使用ChangeSet机制可以预览变更，降低更新风险 4. **架构可扩展性**：nested stack架构支持未来基础设施的增长和变更</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">52</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Organizations to manage multiple accounts. Information security policies require that all unencrypted Amazon EBS volumes be marked as non-compliant. A DevOps engineer needs to automatically deploy the solution and ensure that this compliance check is always present. Which solution will accomplish this? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CloudFormation template that defines an AWS Inspector rule to check whether EBS encryption is enabled. Save the template to an Amazon S3 bucket that has been shared with all accounts within the company. Update the account creation script pointing to the CloudFormation template in Amazon S3.
B. Create an AWS Config organizational rule to check whether EBS encryption is enabled and deploy the rule using the AWS CLI. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization.
C. Create an SCP in Organizations. Set the policy to prevent the launch of Amazon EC2 instances without encryption on the EBS volumes using a conditional expression. Apply the SCP to all AWS accounts. Use Amazon Athena to analyze the AWS CloudTrail output, looking for events that deny an ec2:RunInstances action.
D. Deploy an IAM role to all accounts from a single trusted account. Build a pipeline with AWS CodePipeline with a stage in AWS Lambda to assume the IAM role, and list all EBS volumes in the account. Publish a report to Amazon S3.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations来管理多个账户。信息安全策略要求所有未加密的Amazon EBS卷都被标记为不合规。DevOps工程师需要自动部署解决方案并确保此合规性检查始终存在。哪个解决方案能够实现这一目标？ 选项： A. 创建一个AWS CloudFormation模板，定义AWS Inspector规则来检查是否启用了EBS加密。将模板保存到与公司内所有账户共享的Amazon S3存储桶中。更新账户创建脚本，指向Amazon S3中的CloudFormation模板。 B. 创建AWS Config组织规则来检查是否启用了EBS加密，并使用AWS CLI部署该规则。创建并应用SCP来禁止在整个组织中停止和删除AWS Config。 C. 在Organizations中创建SCP。设置策略以防止在EBS卷未加密的情况下启动Amazon EC2实例，使用条件表达式。将SCP应用到所有AWS账户。使用Amazon Athena分析AWS CloudTrail输出，查找拒绝ec2:RunInstances操作的事件。 D. 从单个受信任账户向所有账户部署IAM角色。使用AWS CodePipeline构建管道，在AWS Lambda中设置阶段来承担IAM角色，并列出账户中的所有EBS卷。将报告发布到Amazon S3。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Organizations管理的多账户环境中，自动部署一个解决方案来检测和标记未加密的EBS卷为不合规状态，并确保这个合规性检查机制始终存在且不被删除。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Config：合规性监控和配置管理服务 - AWS Config组织规则：在组织层面统一部署的合规性规则 - SCP (Service Control Policy)：服务控制策略，用于限制账户权限 - EBS加密：存储卷加密功能 - AWS Inspector、CloudFormation、CodePipeline等其他相关服务 **正确答案B的原因：** 1. **AWS Config组织规则**：能够在组织层面统一部署合规性检查规则，自动检测所有账户中的EBS卷加密状态 2. **自动化部署**：通过AWS CLI可以实现自动化部署，满足DevOps需求 3. **持久性保护**：通过SCP禁止停止和删除AWS Config，确保合规性检查始终存在 4. **集中管理**：组织规则提供了集中管理和监控的能力 5. **实时监控**：Config规则可以实时监控资源状态变化并标记不合规资源 **其他选项错误的原因：** - **选项A**：AWS Inspector主要用于安全评估，不是专门的合规性监控工具；依赖手动脚本更新，自动化程度不够 - **选项C**：SCP只能预防性地阻止创建未加密卷，但无法检测和标记现有的未加密卷为不合规；这是预防而非检测方案 - **选项D**：这是一个自定义开发方案，复杂度高，维护成本大，且没有内置的合规性标记功能；缺乏持久性保护机制 **决策标准和最佳实践：** 1. **使用托管服务**：优先选择AWS原生的合规性服务而非自建方案 2. **组织级管理**：在多账户环境中使用组织级功能实现统一管理 3. **自动化优先**：选择能够自动化部署和管理的解决方案 4. **持久性保护**：通过SCP等机制确保合规性检查不被意外删除 5. **实时监控**：选择能够实时检测和响应配置变化的服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">53</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is performing vulnerability scanning for all Amazon EC2 instances across many accounts. The accounts are in an organization in AWS Organizations. Each account&#x27;s VPCs are attached to a shared transit gateway. The VPCs send traffic to the internet through a central egress VPC. The company has enabled Amazon Inspector in a delegated administrator account and has enabled scanning for all member accounts. A DevOps engineer discovers that some EC2 instances are listed in the &quot;not scanning&quot; tab in Amazon Inspector. Which combination of actions should the DevOps engineer take to resolve this issue? (Choose three.) F. Create a managed-instance activation. Use the Activation Code and the Activation ID to register the EC2 instances. ABE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Verify that AWS Systems Manager Agent is installed and is running on the EC2 instances that Amazon Inspector is not scanning.
B. Associate the target EC2 instances with security groups that allow outbound communication on port 443 to the AWS Systems Manager service endpoint.
C. Grant inspector:StartAssessmentRun permissions to the IAM role that the DevOps engineer is using.
D. Configure EC2 Instance Connect for the EC2 instances that Amazon Inspector is not scanning.
E. Associate the target EC2 instances with instance profiles that grant permissions to communicate with AWS Systems Manager.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在对AWS Organizations组织中多个账户的所有Amazon EC2实例执行漏洞扫描。这些账户都在一个组织中。每个账户的VPC都连接到共享的transit gateway。VPC通过中央出口VPC向互联网发送流量。该公司已在委托管理员账户中启用了Amazon Inspector，并为所有成员账户启用了扫描。DevOps工程师发现一些EC2实例在Amazon Inspector的&quot;not scanning&quot;选项卡中列出。DevOps工程师应该采取哪些操作组合来解决此问题？（选择三个。） 选项： A. 验证AWS Systems Manager Agent已安装并在Amazon Inspector未扫描的EC2实例上运行。 B. 将目标EC2实例与允许在端口443上与AWS Systems Manager服务端点进行出站通信的安全组关联。 C. 向DevOps工程师使用的IAM角色授予inspector:StartAssessmentRun权限。 D. 为Amazon Inspector未扫描的EC2实例配置EC2 Instance Connect。 E. 将目标EC2实例与授予与AWS Systems Manager通信权限的实例配置文件关联。 F. 创建托管实例激活。使用激活代码和激活ID注册EC2实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是Amazon Inspector漏洞扫描服务中EC2实例显示&quot;not scanning&quot;状态的故障排除。需要理解Inspector的工作原理以及它与AWS Systems Manager的依赖关系。 **涉及的关键AWS服务和概念：** 1. Amazon Inspector - AWS的漏洞评估服务 2. AWS Systems Manager (SSM) - 用于管理EC2实例的服务 3. SSM Agent - 在EC2实例上运行的代理程序 4. IAM角色和实例配置文件 - 用于权限管理 5. 安全组 - 控制网络访问的防火墙规则 **正确答案的原因：** 选择A、B、E是正确的，因为： A. **SSM Agent是必需的** - Amazon Inspector依赖AWS Systems Manager来与EC2实例通信并执行扫描。如果SSM Agent未安装或未运行，Inspector无法访问实例进行扫描。 B. **网络连接是必需的** - EC2实例必须能够通过HTTPS（端口443）与AWS Systems Manager服务端点通信。如果安全组阻止了这种出站连接，SSM Agent无法与服务通信。 E. **适当的IAM权限是必需的** - EC2实例需要具有允许与Systems Manager通信的IAM角色。没有正确的实例配置文件，实例无法向SSM服务进行身份验证。 **其他选项错误的原因：** C. **inspector:StartAssessmentRun权限不相关** - 这个权限用于启动评估运行，但不会解决实例不被扫描的问题。 D. **EC2 Instance Connect不相关** - 这是用于SSH连接的服务，与Inspector扫描无关。 F. **托管实例激活用于混合环境** - 这主要用于本地服务器或其他云中的实例，不适用于标准EC2实例。 **决策标准和最佳实践：** 1. 确保所有EC2实例都安装了最新版本的SSM Agent 2. 配置安全组允许必要的出站连接到AWS服务 3. 为EC2实例分配具有适当SSM权限的IAM角色 4. 定期检查Inspector控制台中的&quot;not scanning&quot;状态并及时解决 5. 在多账户环境中，确保委托管理员账户具有适当的跨账户权限</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">54</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A development team uses AWS CodeCommit for version control for applications. The development team uses AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy for CI/CD infrastructure. In CodeCommit, the development team recently merged pull requests that did not pass long-running tests in the code base. The development team needed to perform rollbacks to branches in the codebase, resulting in lost time and wasted effort. A DevOps engineer must automate testing of pull requests in CodeCommit to ensure that reviewers more easily see the results of automated tests as part of the pull request review. What should the DevOps engineer do to meet this requirement? CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete. C (65%) B (20%) D (16%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code review.
B. Create an Amazon EventBridge rule that reacts to the pullRequestCreated event. Create an AWS Lambda function that invokes a
C. Create an Amazon EventBridge rule that reacts to pullRequestCreated and pullRequestSourceBranchUpdated events. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code review.
D. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个开发团队使用AWS CodeCommit进行应用程序版本控制。开发团队使用AWS CodePipeline、AWS CodeBuild和AWS CodeDeploy构建CI/CD基础设施。在CodeCommit中，开发团队最近合并了一些没有通过代码库中长时间运行测试的pull request。开发团队需要对代码库中的分支执行回滚操作，导致时间损失和工作浪费。DevOps工程师必须自动化CodeCommit中pull request的测试，以确保审查者能够更容易地看到自动化测试结果作为pull request审查的一部分。DevOps工程师应该怎么做来满足这个要求？ 选项： A. 创建一个Amazon EventBridge规则来响应pullRequestStatusChanged事件。创建一个AWS Lambda函数来调用带有CodeBuild操作的CodePipeline pipeline来运行应用程序测试。编程Lambda函数将CodeBuild徽章作为评论发布到pull request上，以便开发者在代码审查中看到徽章。 B. 创建一个Amazon EventBridge规则来响应pullRequestCreated事件。创建一个AWS Lambda函数来调用带有CodeBuild操作的CodePipeline pipeline来运行应用程序测试。编程Lambda函数在测试结果完成时将CodeBuild测试结果作为评论发布到pull request上。 C. 创建一个Amazon EventBridge规则来响应pullRequestCreated和pullRequestSourceBranchUpdated事件。创建一个AWS Lambda函数来调用带有CodeBuild操作的CodePipeline pipeline来运行应用程序测试。编程Lambda函数将CodeBuild徽章作为评论发布到pull request上，以便开发者在代码审查中看到徽章。 D. 创建一个Amazon EventBridge规则来响应pullRequestStatusChanged事件。创建一个AWS Lambda函数来调用带有CodeBuild操作的CodePipeline pipeline来运行应用程序测试。编程Lambda函数在测试结果完成时将CodeBuild测试结果作为评论发布到pull request上。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现pull request的自动化测试，确保在代码合并前能够自动运行测试并将结果展示给审查者，避免合并未通过测试的代码导致的回滚问题。 **涉及的关键AWS服务和概念：** 1. AWS CodeCommit - Git版本控制服务，支持pull request功能 2. Amazon EventBridge - 事件驱动服务，可以监听CodeCommit事件 3. AWS Lambda - 无服务器计算服务，用于处理事件和编排流程 4. AWS CodePipeline - CI/CD管道服务 5. AWS CodeBuild - 构建和测试服务 6. CodeCommit事件类型：pullRequestCreated、pullRequestSourceBranchUpdated、pullRequestStatusChanged **正确答案D的原因：** 1. **事件选择合适**：pullRequestStatusChanged事件能够捕获pull request生命周期中的关键状态变化，包括创建、更新等多种情况 2. **完整的测试结果**：发布完整的测试结果而不仅仅是徽章，为审查者提供详细信息 3. **时机准确**：在测试结果完成时发布评论，确保信息的及时性和准确性 **其他选项错误的原因：** - **选项A**：虽然事件选择正确，但只发布CodeBuild徽章而不是详细的测试结果，信息不够完整 - **选项B**：只监听pullRequestCreated事件，无法处理pull request更新的情况，会遗漏后续的代码变更 - **选项C**：虽然监听了创建和更新事件，但同样只发布徽章而不是详细测试结果，且事件组合不如pullRequestStatusChanged全面 **决策标准和最佳实践：** 1. **全面的事件监听**：选择能够覆盖pull request完整生命周期的事件 2. **详细的反馈信息**：提供完整的测试结果而不仅仅是状态徽章 3. **自动化集成**：通过EventBridge + Lambda + CodePipeline/CodeBuild实现完全自动化的测试流程 4. **及时反馈**：确保测试结果能够及时反馈给开发团队，避免延误审查流程</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">55</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has deployed an application in a production VPC in a single AWS account. The application is popular and is experiencing heavy usage. The company&#x27;s security team wants to add additional security, such as AWS WAF, to the application deployment. However, the application&#x27;s product manager is concerned about cost and does not want to approve the change unless the security team can prove that additional security is necessary. The security team believes that some of the application&#x27;s demand might come from users that have IP addresses that are on a deny list. The security team provides the deny list to a DevOps engineer. If any of the IP addresses on the deny list access the application, the security team wants to receive automated notification in near real time so that the security team can document that the application needs additional security. The DevOps engineer creates a VPC flow log for the production VPC. Which set of additional steps should the DevOps engineer take to meet these requirements MOST cost-effectively? send alarm notices to the security team. Most Voted connector to the log group. Configure Athena to periodically query for all accepted traffic from the IP addresses on the deny list and to store the results in the S3 bucket. Configure an S3 event notification to automatically notify the security team through an Amazon Simple Notification Service (Amazon SNS) topic when new objects are added to the S3 bucket. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a log group in Amazon CloudWatch Logs. Configure the VPC flow log to capture accepted traffic and to send the data to the log group. Create an Amazon CloudWatch metric filter for IP addresses on the deny list. Create a CloudWatch alarm with the metric filter as input. Set the period to 5 minutes and the datapoints to alarm to 1. Use an Amazon Simple Notification Service (Amazon SNS) topic to
B. Create an Amazon S3 bucket for log files. Configure the VPC flow log to capture all traffic and to send the data to the S3 bucket. Configure Amazon Athena to return all log files in the S3 bucket for IP addresses on the deny list. Configure Amazon QuickSight to accept data from Athena and to publish the data as a dashboard that the security team can access. Create a threshold alert of 1 for successful access. Configure the alert to automatically notify the security team as frequently as possible when the alert threshold is met.
C. Create an Amazon S3 bucket for log files. Configure the VPC flow log to capture accepted traffic and to send the data to the S3 bucket. Configure an Amazon OpenSearch Service cluster and domain for the log files. Create an AWS Lambda function to retrieve the logs from the S3 bucket, format the logs, and load the logs into the OpenSearch Service cluster. Schedule the Lambda function to run every 5 minutes. Configure an alert and condition in OpenSearch Service to send alerts to the security team through an Amazon Simple Notification Service (Amazon SNS) topic when access from the IP addresses on the deny list is detected.
D. Create a log group in Amazon CloudWatch Logs. Create an Amazon S3 bucket to hold query results. Configure the VPC flow log to capture all traffic and to send the data to the log group. Deploy an Amazon Athena CloudWatch connector in AWS Lambda. Connect the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在单个AWS账户的生产VPC中部署了一个应用程序。该应用程序很受欢迎，使用量很大。公司的安全团队希望为应用程序部署添加额外的安全措施，如AWS WAF。但是，应用程序的产品经理担心成本，除非安全团队能够证明额外的安全措施是必要的，否则不想批准这个变更。安全团队认为应用程序的一些需求可能来自IP地址在拒绝列表上的用户。安全团队向DevOps工程师提供了拒绝列表。如果拒绝列表上的任何IP地址访问应用程序，安全团队希望近实时地收到自动通知，以便安全团队可以记录应用程序需要额外安全措施的证据。DevOps工程师为生产VPC创建了VPC flow log。DevOps工程师应该采取哪组额外步骤来最经济高效地满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 监控VPC中来自拒绝列表IP地址的访问 - 当检测到拒绝列表IP访问时，近实时通知安全团队 - 以最经济高效的方式实现 - 为安全团队提供证据证明需要额外安全措施 **涉及的关键AWS服务和概念：** - VPC Flow Logs：捕获VPC中网络流量信息 - CloudWatch Logs：日志存储和分析服务 - CloudWatch Metric Filter：从日志中提取特定模式的数据 - CloudWatch Alarm：基于指标触发告警 - Amazon SNS：消息通知服务 - Amazon S3：对象存储服务 - Amazon Athena：无服务器查询服务 - Amazon OpenSearch Service：搜索和分析服务 **正确答案A的原因：** 1. **实时性最佳**：CloudWatch Logs + Metric Filter + Alarm组合提供近实时监控（5分钟周期） 2. **成本最优**：只使用必要的AWS服务，避免了复杂的数据处理管道 3. **配置简单**：直接在CloudWatch中配置，无需额外的Lambda函数或复杂查询 4. **精准监控**：只捕获accepted traffic，减少不必要的数据处理 5. **自动化程度高**：一旦配置完成，完全自动化运行 **其他选项错误的原因：** - **选项B**：使用QuickSight dashboard需要人工查看，不是自动通知；Athena查询成本较高且不够实时 - **选项C**：OpenSearch Service集群成本高；Lambda函数每5分钟运行增加复杂性和成本；整体架构过于复杂 - **选项D**：描述不完整，且Athena connector方案比直接CloudWatch方案更复杂和昂贵 **决策标准和最佳实践：** 1. **成本效益**：选择最简单有效的解决方案，避免过度工程化 2. **实时性要求**：CloudWatch Logs的实时处理能力优于S3+批处理方案 3. **运维复杂度**：尽量减少需要管理的组件数量 4. **扩展性**：CloudWatch方案易于维护和扩展 5. **监控最佳实践**：使用AWS原生监控服务实现更好的集成和可靠性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">56</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer has automated a web service deployment by using AWS CodePipeline with the following steps: 1. An AWS CodeBuild project compiles the deployment artifact and runs unit tests. 2. An AWS CodeDeploy deployment group deploys the web service to Amazon EC2 instances in the staging environment. 3. A CodeDeploy deployment group deploys the web service to EC2 instances in the production environment. The quality assurance (QA) team requests permission to inspect the build artifact before the deployment to the production environment occurs. The QA team wants to run an internal penetration testing tool to conduct manual tests. The tool will be invoked by a REST API call. Which combination of actions should the DevOps engineer take to fulfill this request? (Choose two.) AE (77%) AD (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Insert a manual approval action between the test actions and deployment actions of the pipeline.
B. Modify the buildspec.yml file for the compilation stage to require manual approval before completion.
C. Update the CodeDeploy deployment groups so that they require manual approval to proceed.
D. Update the pipeline to directly call the REST API for the penetration testing tool.
E. Update the pipeline to invoke an AWS Lambda function that calls the REST API for the penetration testing tool.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师使用AWS CodePipeline自动化了Web服务部署，包含以下步骤：1. AWS CodeBuild项目编译部署构件并运行单元测试。2. AWS CodeDeploy部署组将Web服务部署到暂存环境的Amazon EC2实例。3. CodeDeploy部署组将Web服务部署到生产环境的EC2实例。质量保证(QA)团队请求在部署到生产环境之前检查构建构件的权限。QA团队希望运行内部渗透测试工具进行手动测试。该工具将通过REST API调用来触发。DevOps工程师应该采取哪些组合操作来满足这个请求？（选择两个）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** QA团队需要在生产环境部署前检查构建构件并运行渗透测试工具，这需要在pipeline中增加人工干预点和自动化调用外部测试工具的能力。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务，支持多阶段自动化部署 - Manual Approval Action：手动批准操作，允许人工干预pipeline流程 - AWS Lambda：无服务器计算服务，可以调用外部API - CodeBuild和CodeDeploy：构建和部署服务 - Pipeline Actions：管道中的各种操作类型 **正确答案的原因：** - 选项A：在测试和部署操作之间插入手动批准操作是标准做法，允许QA团队在生产部署前进行人工检查和批准 - 选项E：使用Lambda函数调用REST API是AWS推荐的集成外部服务的方式，可以自动化触发渗透测试工具 **其他选项错误的原因：** - 选项B：在buildspec.yml中要求手动批准不是正确的实现方式，buildspec主要用于定义构建步骤 - 选项C：CodeDeploy部署组本身不支持手动批准功能，这应该在pipeline级别实现 - 选项D：Pipeline不能直接调用REST API，需要通过Lambda等服务来实现 **决策标准和最佳实践：** 1. 使用CodePipeline的Manual Approval Action实现人工检查点 2. 通过Lambda函数集成外部API调用，保持架构的松耦合 3. 在适当的pipeline阶段插入批准步骤，确保生产环境的安全性 4. 遵循AWS服务的标准集成模式，避免不支持的配置方式</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">57</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is hosting a web application in an AWS Region. For disaster recovery purposes, a second region is being used as a standby. Disaster recovery requirements state that session data must be replicated between regions in near-real time and 1% of requests should route to the secondary region to continuously verify system functionality. Additionally, if there is a disruption in service in the main region, traffic should be automatically routed to the secondary region, and the secondary region must be able to scale up to handle all traffic. How should a DevOps engineer meet these requirements? Amazon Route 53 weighted routing policy with health checks to distribute the traffic across the regions. Most Voted policy with health checks to distribute the traffic across the regions. A (81%) D (19%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In both regions, deploy the application on AWS Elastic Beanstalk and use Amazon DynamoDB global tables for session data. Use an
B. In both regions, launch the application in Auto Scaling groups and use DynamoDB for session data. Use a Route 53 failover routing
C. In both regions, deploy the application in AWS Lambda, exposed by Amazon API Gateway, and use Amazon RDS for PostgreSQL with cross-region replication for session data. Deploy the web application with client-side logic to call the API Gateway directly.
D. In both regions, launch the application in Auto Scaling groups and use DynamoDB global tables for session data. Enable an Amazon CloudFront weighted distribution across regions. Point the Amazon Route 53 DNS record at the CloudFront distribution.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS区域中托管Web应用程序。出于灾难恢复目的，第二个区域被用作备用区域。灾难恢复要求规定会话数据必须在区域之间近实时复制，1%的请求应路由到辅助区域以持续验证系统功能。此外，如果主区域的服务出现中断，流量应自动路由到辅助区域，辅助区域必须能够扩展以处理所有流量。DevOps工程师应该如何满足这些要求？ 选项： A. 在两个区域中，在AWS Elastic Beanstalk上部署应用程序，并使用Amazon DynamoDB global tables处理会话数据。使用Amazon Route 53加权路由策略和健康检查在区域间分配流量。 B. 在两个区域中，在Auto Scaling groups中启动应用程序，并使用DynamoDB处理会话数据。使用Route 53故障转移路由策略和健康检查在区域间分配流量。 C. 在两个区域中，在AWS Lambda中部署应用程序，通过Amazon API Gateway暴露，并使用Amazon RDS for PostgreSQL跨区域复制处理会话数据。使用客户端逻辑直接调用API Gateway部署Web应用程序。 D. 在两个区域中，在Auto Scaling groups中启动应用程序，并使用DynamoDB global tables处理会话数据。启用Amazon CloudFront跨区域加权分配。将Amazon Route 53 DNS记录指向CloudFront分配。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是多区域灾难恢复架构设计，需要满足四个关键要求：1）会话数据近实时跨区域复制；2）1%流量路由到备用区域进行持续验证；3）主区域故障时自动切换；4）备用区域能够自动扩展处理全部流量。 **涉及的关键AWS服务和概念：** - DynamoDB Global Tables：提供跨区域的近实时数据复制 - Elastic Beanstalk：应用程序部署和管理平台，内置Auto Scaling能力 - Route 53加权路由：可以按百分比分配流量并支持健康检查 - Route 53故障转移路由：主要用于主备切换场景 - CloudFront：全球内容分发网络 - Lambda + API Gateway：无服务器架构 **正确答案A的原因：** 选项A完美满足所有要求：DynamoDB Global Tables提供近实时的跨区域会话数据复制；Elastic Beanstalk提供应用程序托管和自动扩展能力；Route 53加权路由策略可以精确控制1%流量到备用区域，同时通过健康检查实现故障时的自动切换。这是一个经典的主-备灾难恢复架构。 **其他选项错误的原因：** - 选项B：使用故障转移路由策略无法实现1%流量持续路由到备用区域的要求，故障转移路由是纯粹的主备模式 - 选项C：RDS跨区域复制不如DynamoDB Global Tables的近实时性能好，且Lambda可能面临冷启动问题，不适合需要持续1%流量验证的场景 - 选项D：CloudFront主要是CDN服务，虽然可以配置多源，但不是为灾难恢复场景设计的，且增加了不必要的复杂性 **决策标准和最佳实践：** 灾难恢复架构设计应优先考虑：数据复制的实时性和一致性、流量分配的精确控制能力、故障检测和自动切换机制、以及成本效益。DynamoDB Global Tables + Elastic Beanstalk + Route 53加权路由的组合是AWS推荐的标准灾难恢复解决方案，既满足技术要求又具有良好的成本控制。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">58</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs an application on Amazon EC2 instances. The company uses a series of AWS CloudFormation stacks to define the application resources. A developer performs updates by building and testing the application on a laptop and then uploading the build output and CloudFormation stack templates to Amazon S3. The developer&#x27;s peers review the changes before the developer performs the CloudFormation stack update and installs a new version of the application onto the EC2 instances. The deployment process is prone to errors and is time-consuming when the developer updates each EC2 instance with the new application. The company wants to automate as much of the application deployment process as possible while retaining a final manual approval step before the modification of the application or resources. The company already has moved the source code for the application and the CloudFormation templates to AWS CodeCommit. The company also has created an AWS CodeBuild project to build and test the application. Which combination of steps will meet the company&#x27;s requirements? (Choose two.) AD (69%) BD (31%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an application group and a deployment group in AWS CodeDeploy. Install the CodeDeploy agent on the EC2 instances.
B. Create an application revision and a deployment group in AWS CodeDeploy. Create an environment in CodeDeploy. Register the EC2 instances to the CodeDeploy environment.
C. Use AWS CodePipeline to invoke the CodeBuild job, run the CloudFormation update, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment.
D. Use AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, run the CloudFormation change sets and start the AWS CodeDeploy deployment.
E. Use AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Amazon EC2实例上运行应用程序。该公司使用一系列AWS CloudFormation堆栈来定义应用程序资源。开发人员通过在笔记本电脑上构建和测试应用程序，然后将构建输出和CloudFormation堆栈模板上传到Amazon S3来执行更新。开发人员的同事在开发人员执行CloudFormation堆栈更新并在EC2实例上安装新版本应用程序之前会审查这些更改。当开发人员需要更新每个EC2实例上的新应用程序时，部署过程容易出错且耗时。公司希望尽可能自动化应用程序部署过程，同时在修改应用程序或资源之前保留最终的手动批准步骤。公司已经将应用程序源代码和CloudFormation模板移动到AWS CodeCommit。公司还创建了AWS CodeBuild项目来构建和测试应用程序。哪种步骤组合将满足公司的要求？（选择两个）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动化的CI/CD流水线，需要满足以下关键需求： 1. 自动化应用程序部署过程 2. 保留手动批准步骤 3. 处理CloudFormation堆栈更新 4. 自动化EC2实例上的应用程序安装 5. 已有CodeCommit和CodeBuild基础设施 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：用于自动化应用程序部署到EC2实例 - AWS CodePipeline：编排整个CI/CD流水线 - AWS CloudFormation：基础设施即代码，管理AWS资源 - CloudFormation Change Sets：预览基础设施变更的机制 - 手动批准步骤：CodePipeline中的人工审核环节 **正确答案分析（选项A和D）：** 选项A正确的原因： - &quot;应用程序组&quot;和&quot;部署组&quot;是CodeDeploy的正确术语和概念 - 在EC2实例上安装CodeDeploy agent是必需的，这样CodeDeploy才能与实例通信并执行部署 - 这是设置CodeDeploy的标准和正确方式 选项D正确的原因： - 使用CodePipeline编排整个流程符合最佳实践 - CloudFormation change sets允许在实际执行前预览基础设施变更，满足审核需求 - 手动批准步骤满足公司要求 - 先执行CloudFormation change sets更新基础设施，再进行CodeDeploy部署应用程序，这个顺序是合理的 **其他选项错误的原因：** 选项B错误： - &quot;应用程序修订版本&quot;不是CodeDeploy中的标准术语 - &quot;环境&quot;和&quot;注册EC2实例到环境&quot;不是CodeDeploy的正确概念，这更像是Elastic Beanstalk的术语 - CodeDeploy使用的是应用程序组和部署组的概念 选项C错误： - 直接运行CloudFormation更新而不使用change sets缺乏预览机制 - 没有给审核者机会查看将要进行的基础设施变更 - 不符合最佳实践 选项E错误： - 创建了CloudFormation change sets但没有执行它们 - 这意味着基础设施变更不会被应用，可能导致应用程序部署失败 - 逻辑不完整 **决策标准和最佳实践：** 1. CodeDeploy的正确配置需要应用程序组、部署组和agent 2. 使用CloudFormation change sets进行基础设施变更的预览和审核 3. CodePipeline应该编排完整的流程：构建→基础设施变更→手动批准→应用程序部署 4. 基础设施更新应该在应用程序部署之前完成，确保环境准备就绪</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">59</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an EC2 Auto Scaling group across multiple Availability Zones. The engineer needs to implement a deployment strategy that: Launches a second fleet of instances with the same capacity as the original fleet. Maintains the original fleet unchanged while the second fleet is launched. Transitions traffic to the second fleet when the second fleet is fully deployed. Terminates the original fleet automatically 1 hour after transition. Which solution will satisfy these requirements? C (92%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use an AWS CloudFormation template with a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to reflect the new ALB.
B. Use two AWS Elastic Beanstalk environments to perform a blue/green deployment from the original environment to the new one. Create an application version lifecycle policy to terminate the original environment in 1 hour.
C. Use AWS CodeDeploy with a deployment group configured with a blue/green deployment configuration. Select the option Terminate the original instances in the deployment group with a waiting period of 1 hour.
D. Use AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using the Resources key that sets the deletion policy of the ALB to 1 hour, and deploy the application.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师管理着一个运行在Application Load Balancer (ALB)后面Amazon EC2实例上的Web应用程序。这些实例运行在跨多个Availability Zones的EC2 Auto Scaling组中。工程师需要实施一个部署策略，该策略需要：启动与原始集群相同容量的第二个实例集群。在启动第二个集群时保持原始集群不变。当第二个集群完全部署后将流量转移到第二个集群。在转移后1小时自动终止原始集群。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是蓝绿部署(Blue/Green Deployment)策略的实现。具体要求包括：1)创建相同容量的新实例集群；2)保持原集群运行状态；3)流量完全切换后再终止原集群；4)支持1小时延迟终止的自动化流程。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：专门的部署服务，支持多种部署策略 - 蓝绿部署：通过维护两套完全相同的生产环境来实现零停机部署 - Application Load Balancer：应用层负载均衡器，支持流量切换 - EC2 Auto Scaling：自动扩缩容服务 - AWS Elastic Beanstalk：应用程序部署和管理平台 **正确答案C的原因：** AWS CodeDeploy是专门设计用于处理各种部署策略的服务，完美支持蓝绿部署。它可以：1)自动创建与原始集群相同容量的新实例集群；2)在新集群部署期间保持原集群不变；3)通过ALB实现流量的平滑切换；4)提供精确的时间控制，支持1小时后自动终止原始实例的配置。CodeDeploy的蓝绿部署配置专门为这种场景设计，提供了完整的自动化流程。 **其他选项错误的原因：** 选项A错误：CloudFormation的retention policy主要用于资源删除保护，不是部署策略工具，且Route 53记录更新不能实现题目要求的蓝绿部署流程。选项B错误：虽然Elastic Beanstalk支持蓝绿部署，但application version lifecycle policy不是用于控制环境终止时间的正确机制，无法精确控制1小时后终止。选项D错误：Immutable部署策略是滚动更新的一种，不是蓝绿部署，且.ebextension中的deletion policy用于CloudFormation资源管理，不适用于此场景。 **决策标准和最佳实践：** 选择部署工具时应考虑：1)服务的专业性 - CodeDeploy专门用于部署管理；2)功能匹配度 - 是否原生支持所需的部署策略；3)自动化程度 - 能否提供完整的自动化流程；4)精确控制 - 是否支持精确的时间和流程控制。蓝绿部署的最佳实践是使用专门的部署工具，确保流程的可靠性和可控性。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">60</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A video-sharing company stores its videos in Amazon S3. The company has observed a sudden increase in video access requests, but the company does not know which videos are most popular. The company needs to identify the general access pattern for the video files. This pattern includes the number of users who access a certain file on a given day, as well as the number of pull requests for certain files. How can the company meet these requirements with the LEAST amount of effort? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Activate S3 server access logging. Import the access logs into an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns.
B. Activate S3 server access logging. Use Amazon Athena to create an external table with the log files. Use Athena to create a SQL query to analyze the access patterns.
C. Invoke an AWS Lambda function for every S3 object access event. Configure the Lambda function to write the file access information, such as user, S3 bucket, and file key, to an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns.
D. Record an Amazon CloudWatch Logs log message for every S3 object access event. Configure a CloudWatch Logs log stream to write the file access information, such as user, S3 bucket, and file key, to an Amazon Kinesis Data Analytics for SQL application. Perform a sliding window analysis.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家视频分享公司将其视频存储在Amazon S3中。该公司观察到视频访问请求突然增加，但公司不知道哪些视频最受欢迎。公司需要识别视频文件的一般访问模式。这种模式包括在给定日期访问某个文件的用户数量，以及某些文件的拉取请求数量。公司如何以最少的工作量满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案来分析S3中视频文件的访问模式，包括用户访问数量和文件请求次数，并且要求以最少的工作量实现。 **涉及的关键AWS服务和概念：** - S3 Server Access Logging：记录对S3存储桶的详细访问请求 - Amazon Athena：无服务器交互式查询服务，可直接查询S3中的数据 - Amazon Aurora：托管关系数据库服务 - AWS Lambda：无服务器计算服务 - Amazon CloudWatch Logs：日志监控服务 - Amazon Kinesis Data Analytics：实时数据分析服务 **正确答案B的原因：** 1. **最少工作量**：S3 Server Access Logging是原生功能，只需启用即可自动记录访问日志 2. **无服务器架构**：Athena是完全托管的服务，无需预置或管理基础设施 3. **直接查询**：Athena可以直接查询S3中的日志文件，无需数据导入过程 4. **成本效益**：按查询付费，没有持续运行的资源成本 5. **SQL支持**：提供标准SQL接口进行复杂的访问模式分析 **其他选项错误的原因：** - **选项A**：需要将日志导入Aurora数据库，增加了数据传输和存储成本，工作量更大 - **选项C**：需要为每个S3访问事件调用Lambda函数，这会产生大量的Lambda调用费用，并且需要配置事件触发器，工作量显著增加 - **选项D**：使用CloudWatch Logs和Kinesis Data Analytics过于复杂，适合实时分析而非历史访问模式分析，工作量大且成本高 **决策标准和最佳实践：** 1. **最小化运维开销**：选择托管服务而非自建解决方案 2. **成本优化**：按需付费模式优于持续运行的资源 3. **架构简洁性**：减少组件数量和数据传输步骤 4. **原生集成**：利用AWS服务间的原生集成能力 5. **可扩展性**：解决方案应能处理访问量的增长而无需额外配置</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">61</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A development team wants to use AWS CloudFormation stacks to deploy an application. However, the developer IAM role does not have the required permissions to provision the resources that are specified in the AWS CloudFormation template. A DevOps engineer needs to implement a solution that allows the developers to deploy the stacks. The solution must follow the principle of least privilege. Which solution will meet these requirements? D (82%) Other</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an IAM policy that allows the developers to provision the required resources. Attach the policy to the developer IAM role.
B. Create an IAM policy that allows full access to AWS CloudFormation. Attach the policy to the developer IAM role.
C. Create an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role a cloudformation:* action. Use the new service role during stack deployments.
D. Create an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role the iam:PassRole permission. Use the new service role during stack deployments.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个开发团队想要使用AWS CloudFormation堆栈来部署应用程序。但是，开发者IAM角色没有所需的权限来配置AWS CloudFormation模板中指定的资源。DevOps工程师需要实施一个解决方案，允许开发者部署堆栈。该解决方案必须遵循最小权限原则。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在遵循最小权限原则的前提下，如何让没有足够权限的开发者能够通过CloudFormation部署资源。关键是要在安全性和功能性之间找到平衡。 **涉及的关键AWS服务和概念：** - AWS CloudFormation：基础设施即代码服务 - IAM角色和权限管理 - CloudFormation服务角色（Service Role） - iam:PassRole权限 - 最小权限原则（Principle of Least Privilege） **正确答案分析（选项D）：** 选项D是最佳解决方案，因为： 1. 创建专门的CloudFormation服务角色，该角色拥有部署所需资源的权限 2. 只给开发者IAM角色授予iam:PassRole权限，允许其将服务角色传递给CloudFormation 3. 完美体现了最小权限原则：开发者本身不直接拥有创建资源的权限，只能通过指定的服务角色进行操作 4. 提供了权限隔离和审计追踪能力 **其他选项错误的原因：** - 选项A：直接给开发者授予资源配置权限，违反了最小权限原则，权限范围过大 - 选项B：授予CloudFormation完全访问权限是最糟糕的选择，严重违反安全最佳实践 - 选项C：cloudformation:*权限过于宽泛，同样违反了最小权限原则 **决策标准和最佳实践：** 1. 使用服务角色进行权限委托是AWS推荐的最佳实践 2. iam:PassRole是精确控制角色传递的关键权限 3. 通过服务角色可以实现权限的精细化管理和审计 4. 避免直接给用户过多权限，而是通过角色委托的方式实现功能需求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">62</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A production account has a requirement that any Amazon EC2 instance that has been logged in to manually must be terminated within 24 hours. All applications in the production account are using Auto Scaling groups with the Amazon CloudWatch Logs agent configured. How can this process be automated? function that terminates all instances with this tag. Most Voted D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CloudWatch Logs subscription to an AWS Step Functions application. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function once a day that will terminate all instances with this tag.
B. Create an Amazon CloudWatch alarm that will be invoked by the login event. Send the notification to an Amazon Simple Notification Service (Amazon SNS) topic that the operations team is subscribed to, and have them terminate the EC2 instance within 24 hours.
C. Create an Amazon CloudWatch alarm that will be invoked by the login event. Configure the alarm to send to an Amazon Simple Queue Service (Amazon SQS) queue. Use a group of worker instances to process messages from the queue, which then schedules an Amazon EventBridge rule to be invoked.
D. Create a CloudWatch Logs subscription to an AWS Lambda function. Configure the function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a daily Lambda</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个生产账户有一个要求，任何被手动登录过的Amazon EC2实例必须在24小时内终止。生产账户中的所有应用程序都使用配置了Amazon CloudWatch Logs代理的Auto Scaling组。如何自动化这个过程？ 选项： A. 创建一个CloudWatch Logs订阅到AWS Step Functions应用程序。配置AWS Lambda函数为产生登录事件的EC2实例添加标签并标记该实例待退役。创建Amazon EventBridge规则每天调用第二个Lambda函数来终止所有带有此标签的实例。 B. 创建一个由登录事件触发的Amazon CloudWatch告警。将通知发送到运维团队订阅的Amazon Simple Notification Service (Amazon SNS)主题，让他们在24小时内终止EC2实例。 C. 创建一个由登录事件触发的Amazon CloudWatch告警。配置告警发送到Amazon Simple Queue Service (Amazon SQS)队列。使用一组工作实例处理队列中的消息，然后调度Amazon EventBridge规则被调用。 D. 创建CloudWatch Logs订阅到AWS Lambda函数。配置函数为产生登录事件的EC2实例添加标签并标记该实例待退役。创建Amazon EventBridge规则每天调用Lambda函数终止所有带有此标签的实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动化解决方案，能够检测EC2实例的手动登录事件，并确保这些实例在24小时内被自动终止。关键要求包括：1）检测登录事件；2）标记被登录的实例；3）在24小时内自动终止这些实例；4）整个过程完全自动化。 **涉及的关键AWS服务和概念：** - CloudWatch Logs：收集和监控日志数据，包括登录事件 - Lambda：无服务器计算服务，用于处理事件和执行自动化任务 - EventBridge：事件路由服务，可以按计划触发Lambda函数 - EC2标签：用于标识和管理实例的元数据 - CloudWatch Logs订阅：实时处理日志流的机制 **正确答案D的原因：** 选项D提供了最直接和高效的解决方案：1）使用CloudWatch Logs订阅直接监听登录事件，实现实时检测；2）Lambda函数立即响应登录事件并给实例打标签；3）EventBridge规则每天定时触发另一个Lambda函数清理带标签的实例；4）整个流程完全自动化，无需人工干预；5）架构简洁，延迟最低。 **其他选项错误的原因：** 选项A错误：引入了不必要的Step Functions，增加了复杂性和成本，而且Step Functions不是处理日志事件的最佳选择。选项B错误：依赖人工操作，不符合&quot;自动化&quot;的要求，存在人为错误和延迟的风险。选项C错误：架构过于复杂，使用CloudWatch告警而非日志订阅来检测登录事件不够直接，而且需要额外的工作实例来处理SQS消息，增加了成本和复杂性。 **决策标准和最佳实践：** 选择自动化解决方案时应考虑：1）实时性 - CloudWatch Logs订阅比告警更适合实时事件处理；2）简洁性 - 避免不必要的中间服务；3）成本效益 - Lambda按需付费比持续运行的工作实例更经济；4）可靠性 - 减少组件数量降低故障点；5）完全自动化 - 避免任何人工干预环节。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">63</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has enabled all features for its organization in AWS Organizations. The organization contains 10 AWS accounts. The company has turned on AWS CloudTrail in all the accounts. The company expects the number of AWS accounts in the organization to increase to 500 during the next year. The company plans to use multiple OUs for these accounts. The company has enabled AWS Config in each existing AWS account in the organization. A DevOps engineer must implement a solution that enables AWS Config automatically for all future AWS accounts that are created in the organization. Which solution will meet this requirement? rule to invoke an AWS Lambda function that enables trusted access to AWS Config for the organization. deploy automatically when an account is created through Organizations. Most Voted the SCP to the root-level OU. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the organization&#x27;s management account, create an Amazon EventBridge rule that reacts to a CreateAccount API call. Configure the
B. In the organization&#x27;s management account, create an AWS CloudFormation stack set to enable AWS Config. Configure the stack set to
C. In the organization&#x27;s management account, create an SCP that allows the appropriate AWS Config API calls to enable AWS Config. Apply
D. In the organization&#x27;s management account, create an Amazon EventBridge rule that reacts to a CreateAccount API call. Configure the rule to invoke an AWS Systems Manager Automation runbook to enable AWS Config for the account.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司已为其在AWS Organizations中的组织启用了所有功能。该组织包含10个AWS账户。公司已在所有账户中开启了AWS CloudTrail。公司预计在明年组织中的AWS账户数量将增加到500个。公司计划为这些账户使用多个OU。公司已在组织中的每个现有AWS账户中启用了AWS Config。DevOps工程师必须实施一个解决方案，为组织中创建的所有未来AWS账户自动启用AWS Config。哪个解决方案能满足这个要求？ 选项： A. 在组织的管理账户中，创建一个Amazon EventBridge规则来响应CreateAccount API调用。配置规则调用AWS Lambda函数，为组织启用AWS Config的可信访问。 B. 在组织的管理账户中，创建AWS CloudFormation stack set来启用AWS Config。配置stack set在通过Organizations创建账户时自动部署。 C. 在组织的管理账户中，创建一个SCP，允许适当的AWS Config API调用来启用AWS Config。将SCP应用到根级OU。 D. 在组织的管理账户中，创建一个Amazon EventBridge规则来响应CreateAccount API调用。配置规则调用AWS Systems Manager Automation runbook来为账户启用AWS Config。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要实现一个自动化解决方案，为AWS Organizations中未来创建的所有新账户自动启用AWS Config服务。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Config：配置管理和合规性监控服务 - AWS CloudFormation StackSets：跨多个账户和区域部署资源的服务 - Amazon EventBridge：事件驱动架构服务 - Service Control Policies (SCP)：组织级别的权限控制策略 - AWS Lambda和Systems Manager：自动化执行服务 **正确答案B的原因：** CloudFormation StackSets是专门为跨多个AWS账户自动部署和管理资源而设计的服务。它具有以下优势： 1. 原生支持自动部署到新创建的组织账户 2. 可以配置为在新账户加入组织时自动执行部署 3. 提供集中管理和状态跟踪 4. 支持批量操作和回滚功能 5. 与AWS Organizations深度集成，是AWS推荐的最佳实践 **其他选项错误的原因：** - 选项A：Lambda函数方案需要自定义开发，复杂度高，且&quot;启用可信访问&quot;并不等同于在每个账户中启用AWS Config服务本身 - 选项C：SCP只是权限策略，它控制允许或拒绝的操作，但不能主动启用服务或部署资源 - 选项D：虽然技术上可行，但Systems Manager Automation runbook方案比StackSets更复杂，需要更多自定义配置和维护 **决策标准和最佳实践：** 1. 选择AWS原生服务和推荐的最佳实践解决方案 2. 优先考虑自动化程度高、维护成本低的方案 3. 选择与AWS Organizations深度集成的服务 4. 考虑解决方案的可扩展性（从10个账户扩展到500个账户） 5. 选择提供集中管理和监控能力的服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">64</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has many applications. Different teams in the company developed the applications by using multiple languages and frameworks. The applications run on premises and on different servers with different operating systems. Each team has its own release protocol and process. The company wants to reduce the complexity of the release and maintenance of these applications. The company is migrating its technology stacks, including these applications, to AWS. The company wants centralized control of source code, a consistent and automatic delivery pipeline, and as few maintenance tasks as possible on the underlying infrastructure. What should a DevOps engineer do to meet these requirements? D (93%) 7%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create one AWS CodeCommit repository for all applications. Put each application&#x27;s code in a different branch. Merge the branches, and use AWS CodeBuild to build the applications. Use AWS CodeDeploy to deploy the applications to one centralized application server.
B. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time. Use AWS CodeDeploy to deploy the applications to one centralized application server.
C. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time and to create one AMI for each server. Use AWS CloudFormation StackSets to automatically provision and decommission Amazon EC2 fleets by using these AMIs.
D. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build one Docker image for each application in Amazon Elastic Container Registry (Amazon ECR). Use AWS CodeDeploy to deploy the applications to Amazon Elastic Container Service (Amazon ECS) on infrastructure that AWS Fargate manages.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有很多应用程序。公司内不同的团队使用多种语言和框架开发了这些应用程序。这些应用程序运行在本地和不同操作系统的不同服务器上。每个团队都有自己的发布协议和流程。公司希望降低这些应用程序发布和维护的复杂性。公司正在将其技术栈（包括这些应用程序）迁移到AWS。公司希望实现源代码的集中控制、一致且自动化的交付管道，以及对底层基础设施尽可能少的维护任务。DevOps工程师应该怎么做来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何为多语言、多框架的应用程序设计一个统一的CI/CD解决方案，核心需求包括：1）源代码集中管理；2）一致且自动化的交付管道；3）最小化底层基础设施维护工作；4）支持多种技术栈。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：托管的Git代码仓库服务 - AWS CodeBuild：托管的构建服务 - AWS CodeDeploy：应用程序部署服务 - Amazon ECR：容器镜像仓库 - Amazon ECS：容器编排服务 - AWS Fargate：无服务器容器计算引擎 - AWS CloudFormation StackSets：跨账户和区域的基础设施管理 **正确答案D的原因：** 选项D完美满足所有要求：1）每个应用独立的CodeCommit仓库确保了代码隔离和团队自主性；2）使用Docker容器化解决了多语言多框架的兼容性问题；3）ECR提供统一的镜像管理；4）ECS on Fargate实现了完全托管的基础设施，无需维护服务器；5）CodeDeploy确保一致的部署流程。这种架构既保持了应用程序的独立性，又实现了标准化的交付管道。 **其他选项错误的原因：** 选项A将所有应用放在一个仓库的不同分支中，违背了应用独立性原则，且合并分支会造成部署复杂性。选项B使用单一应用服务器部署所有应用，存在单点故障风险且无法很好支持不同技术栈。选项C虽然使用了AMI但仍需要管理EC2实例，没有最小化基础设施维护工作。 **决策标准和最佳实践：** 在设计多应用CI/CD方案时，应遵循：1）应用程序独立性原则（独立仓库、独立部署）；2）容器化优先策略解决环境一致性问题；3）选择托管服务减少运维负担；4）标准化工具链确保一致的交付体验。Fargate作为无服务器容器平台，完美契合了&quot;最少维护任务&quot;的要求。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">65</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s application is currently deployed to a single AWS Region. Recently, the company opened a new office on a different continent. The users in the new office are experiencing high latency. The company&#x27;s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB) and uses Amazon DynamoDB as the database layer. The instances run in an EC2 Auto Scaling group across multiple Availability Zones. A DevOps engineer is tasked with minimizing application response times and improving availability for users in both Regions. Which combination of actions should be taken to address the latency issues? (Choose three.) F. Convert the DynamoDB table to a global table. Most Voted CDF (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new DynamoDB table in the new Region with cross-Region replication enabled.
B. Create new ALB and Auto Scaling group global resources and configure the new ALB to direct traffic to the new Auto Scaling group.
C. Create new ALB and Auto Scaling group resources in the new Region and configure the new ALB to direct traffic to the new Auto Scaling group.
D. Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB.
E. Create Amazon Route 53 aliases, health checks, and failover routing policies to route to the ALB.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的应用程序目前部署在单个AWS Region中。最近，该公司在不同大陆开设了新办公室。新办公室的用户遇到了高延迟问题。该公司的应用程序运行在Application Load Balancer (ALB)后面的Amazon EC2实例上，并使用Amazon DynamoDB作为数据库层。这些实例在跨多个Availability Zone的EC2 Auto Scaling组中运行。DevOps工程师的任务是最小化应用程序响应时间并提高两个Region用户的可用性。应该采取哪些操作组合来解决延迟问题？（选择三个。） 选项： A. 在新Region中创建新的DynamoDB表并启用跨Region复制。 B. 创建新的ALB和Auto Scaling组全局资源，并配置新的ALB将流量导向新的Auto Scaling组。 C. 在新Region中创建新的ALB和Auto Scaling组资源，并配置新的ALB将流量导向新的Auto Scaling组。 D. 创建Amazon Route 53记录、健康检查和基于延迟的路由策略来路由到ALB。 E. 创建Amazon Route 53别名、健康检查和故障转移路由策略来路由到ALB。 F. 将DynamoDB表转换为全局表。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这是一个典型的多Region部署架构问题，要求解决跨大陆用户的高延迟问题，同时提高可用性。需要选择三个正确的解决方案。 **涉及的关键AWS服务和概念：** - Multi-Region架构设计 - Application Load Balancer (ALB) - EC2 Auto Scaling组 - Amazon DynamoDB Global Tables - Amazon Route 53路由策略 - 延迟优化和高可用性设计 **正确答案分析（C、D、F）：** **选项C正确：** 在新Region中创建ALB和Auto Scaling组是解决延迟问题的核心。通过在用户附近的Region部署计算资源，可以显著减少网络延迟。这是标准的多Region部署模式。 **选项D正确：** Route 53的基于延迟的路由策略是关键组件，它能够自动将用户请求路由到延迟最低的Region，这正是解决跨大陆延迟问题的最佳实践。 **选项F正确：** DynamoDB Global Tables提供多Region数据库复制，确保数据在两个Region都可用，减少数据库访问延迟，同时提供高可用性。 **其他选项错误的原因：** **选项A错误：** DynamoDB没有&quot;跨Region复制&quot;这个具体功能。正确的做法是使用Global Tables（选项F），它提供了自动的多向复制。 **选项B错误：** 表述中&quot;全局资源&quot;概念不准确。ALB和Auto Scaling组都是Region级别的资源，不存在&quot;全局&quot;版本。必须在特定Region中创建这些资源。 **选项E错误：** 故障转移路由策略主要用于灾难恢复场景，不是解决延迟问题的最佳选择。对于性能优化，基于延迟的路由策略更合适。 **决策标准和最佳实践：** 1. **就近部署原则：** 在用户附近的Region部署应用组件 2. **智能路由：** 使用Route 53基于延迟的路由自动优化用户体验 3. **数据一致性：** 使用DynamoDB Global Tables确保数据在多Region间同步 4. **架构对称性：** 在新Region复制完整的应用架构栈 5. **自动化管理：** 利用AWS托管服务减少运维复杂性 这个解决方案实现了真正的多Region主-主架构，既解决了延迟问题又提高了整体可用性。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">66</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer needs to apply a core set of security controls to an existing set of AWS accounts. The accounts are in an organization in AWS Organizations. Individual teams will administer individual accounts by using the AdministratorAccess AWS managed policy. For all accounts, AWS CloudTrail and AWS Config must be turned on in all available AWS Regions. Individual account administrators must not be able to edit or delete any of the baseline resources. However, individual account administrators must be able to edit or delete their own CloudTrail trails and AWS Config rules. Which solution will meet these requirements in the MOST operationally efficient way? organization&#x27;s management account by using CloudFormation StackSets. Set the stack policy to deny Update:Delete actions. CloudTrail and AWS Config. Deploy AWS Config rules to the organization by using the AWS Config management account. Create a CloudTrail organization trail in the organization&#x27;s management account. Deny modification or deletion of the AWS Config recorders by using an SCP. Most Voted organization&#x27;s management account by using CloudFormation StackSets. Create an SCP that prevents updates or deletions to CloudTrail resources or AWS Config resources unless the principal is an administrator of the organization&#x27;s management account. Most Voted C (54%) D (38%) 9%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CloudFormation template that defines the standard account resources. Deploy the template to all accounts from the
B. Enable AWS Control Tower. Enroll the existing accounts in AWS Control Tower. Grant the individual account administrators access to
C. Designate an AWS Config management account. Create AWS Config recorders in all accounts by using AWS CloudFormation StackSets.
D. Create an AWS CloudFormation template that defines the standard account resources. Deploy the template to all accounts from the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师需要对AWS Organizations中现有的一组AWS账户应用核心安全控制。各个团队将使用AdministratorAccess AWS托管策略来管理各自的账户。对于所有账户，必须在所有可用的AWS区域中启用AWS CloudTrail和AWS Config。各个账户管理员不得编辑或删除任何基线资源。但是，各个账户管理员必须能够编辑或删除他们自己的CloudTrail跟踪和AWS Config规则。哪种解决方案能够以最具操作效率的方式满足这些要求？ 选项： A. 创建定义标准账户资源的AWS CloudFormation模板。使用CloudFormation StackSets从组织的管理账户将模板部署到所有账户。设置堆栈策略以拒绝Update:Delete操作。 B. 启用AWS Control Tower。将现有账户注册到AWS Control Tower中。授予各个账户管理员访问CloudTrail和AWS Config的权限。使用AWS Config管理账户将AWS Config规则部署到组织。在组织的管理账户中创建CloudTrail组织跟踪。使用SCP拒绝修改或删除AWS Config记录器。 C. 指定AWS Config管理账户。使用AWS CloudFormation StackSets在所有账户中创建AWS Config记录器。在组织的管理账户中创建CloudTrail组织跟踪。使用SCP拒绝修改或删除AWS Config记录器。 D. 创建定义标准账户资源的AWS CloudFormation模板。使用CloudFormation StackSets从组织的管理账户将模板部署到所有账户。创建SCP以防止更新或删除CloudTrail资源或AWS Config资源，除非主体是组织管理账户的管理员。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Organizations环境中实现以下目标：1）在所有账户的所有区域启用CloudTrail和Config；2）防止账户管理员删除基线安全资源；3）允许账户管理员管理自己创建的CloudTrail和Config资源；4）寻求最具操作效率的解决方案。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Config：配置合规性监控服务，支持管理账户模式 - AWS CloudTrail：API调用审计服务，支持组织级跟踪 - CloudFormation StackSets：跨账户资源部署工具 - SCP (Service Control Policy)：组织级权限控制策略 - AWS Control Tower：自动化治理服务 **正确答案C的原因：** 选项C采用了最优的架构设计：1）使用AWS Config管理账户功能，可以集中管理所有成员账户的Config记录器；2）使用CloudTrail组织跟踪，在管理账户中创建一个跟踪覆盖所有成员账户；3）通过SCP精确控制权限，只保护基线Config记录器不被删除，而不影响账户管理员创建自己的Config规则；4）这种方案操作效率最高，管理开销最小。 **其他选项错误的原因：** 选项A使用StackSets部署资源到各个账户，但CloudFormation堆栈策略无法区分基线资源和用户自定义资源，会阻止账户管理员创建自己的CloudTrail和Config资源。选项B的Control Tower虽然功能强大，但对于已有的组织来说实施复杂度高，不是最具操作效率的方案。选项D的SCP策略过于宽泛，会完全阻止账户管理员管理CloudTrail和Config资源，不满足允许管理自定义资源的要求。 **决策标准和最佳实践：** 在多账户环境中实施安全基线时，应优先考虑：1）使用AWS原生的集中管理功能（如Config管理账户、CloudTrail组织跟踪）；2）通过SCP实现精细化权限控制；3）平衡安全控制和操作灵活性；4）选择管理开销最小的方案。选项C完美体现了这些最佳实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">67</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has its AWS accounts in an organization in AWS Organizations. AWS Config is manually configured in each AWS account. The company needs to implement a solution to centrally configure AWS Config for all accounts in the organization. The solution also must record resource changes to a central account. Which combination of actions should a DevOps engineer perform to meet these requirements? (Choose two.) AE (84%) BD (16%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a delegated administrator account for AWS Config. Enable trusted access for AWS Config in the organization.
B. Configure a delegated administrator account for AWS Config. Create a service-linked role for AWS Config in the organization&#x27;s management account.
C. Create an AWS CloudFormation template to create an AWS Config aggregator. Configure a CloudFormation stack set to deploy the template to all accounts in the organization.
D. Create an AWS Config organization aggregator in the organization&#x27;s management account. Configure data collection from all AWS accounts in the organization and from all AWS Regions.
E. Create an AWS Config organization aggregator in the delegated administrator account. Configure data collection from all AWS accounts in the organization and from all AWS Regions.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中有多个AWS账户。AWS Config在每个AWS账户中都是手动配置的。该公司需要实施一个解决方案来为组织中的所有账户集中配置AWS Config。该解决方案还必须将资源变更记录到一个中央账户。DevOps工程师应该执行哪些操作组合来满足这些要求？（选择两个） 选项： A. 为AWS Config配置委托管理员账户。在组织中为AWS Config启用trusted access。 B. 为AWS Config配置委托管理员账户。在组织的管理账户中为AWS Config创建service-linked role。 C. 创建AWS CloudFormation模板来创建AWS Config aggregator。配置CloudFormation stack set将模板部署到组织中的所有账户。 D. 在组织的管理账户中创建AWS Config organization aggregator。配置从组织中所有AWS账户和所有AWS Regions收集数据。 E. 在委托管理员账户中创建AWS Config organization aggregator。配置从组织中所有AWS账户和所有AWS Regions收集数据。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 集中配置AWS Config到组织中的所有账户 2. 将资源变更记录到中央账户 3. 需要选择两个正确的操作组合 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Config：资源配置监控和合规性服务 - Delegated Administrator：委托管理员账户，用于集中管理特定服务 - Organization Aggregator：组织级别的配置聚合器 - Trusted Access：允许AWS服务访问组织信息的权限机制 - Service-linked Role：AWS服务自动创建和管理的IAM角色 **正确答案分析：** 根据题目显示正确答案是B，但从技术角度分析，最佳实践应该是A和E的组合： - **选项A正确**：配置委托管理员账户并启用trusted access是AWS Config组织级部署的标准第一步，这是AWS官方推荐的最佳实践 - **选项E正确**：在委托管理员账户中创建organization aggregator是正确的做法，因为委托管理员账户专门负责管理AWS Config服务 **其他选项分析：** - **选项B**：虽然配置委托管理员是正确的，但service-linked role通常由AWS服务自动创建，不需要手动创建 - **选项C**：使用CloudFormation stack set部署普通aggregator过于复杂，不如直接使用organization aggregator - **选项D**：在管理账户中创建aggregator不是最佳实践，应该使用委托管理员账户 **决策标准和最佳实践：** 1. 使用委托管理员账户而非管理账户来管理特定AWS服务 2. 启用trusted access是组织级服务集成的前提条件 3. Organization aggregator比普通aggregator更适合多账户环境 4. 遵循AWS Well-Architected Framework中的安全和运营卓越原则</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">68</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to migrate its content sharing web application hosted on Amazon EC2 to a serverless architecture. The company currently deploys changes to its application by creating a new Auto Scaling group of EC2 instances and a new Elastic Load Balancer, and then shifting the traffic away using an Amazon Route 53 weighted routing policy. For its new serverless application, the company is planning to use Amazon API Gateway and AWS Lambda. The company will need to update its deployment processes to work with the new application. It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base. Which deployment strategy will meet these requirements? API and Lambda functions. Shift traffic gradually using an Elastic Beanstalk blue/green deployment. changed, use OpsWorks to perform a blue/green deployment and shift traffic gradually. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CDK to deploy API Gateway and Lambda functions. When code needs to be changed, update the AWS CloudFormation stack and deploy the new version of the APIs and Lambda functions. Use a Route 53 failover routing policy for the canary release strategy.
B. Use AWS CloudFormation to deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete.
C. Use AWS Elastic Beanstalk to deploy API Gateway and Lambda functions. When code needs to be changed, deploy a new version of the
D. Use AWS OpsWorks to deploy API Gateway in the service layer and Lambda functions in a custom layer. When code needs to be</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司希望将其托管在Amazon EC2上的内容共享Web应用程序迁移到serverless架构。该公司目前通过创建新的Auto Scaling组EC2实例和新的Elastic Load Balancer来部署应用程序更改，然后使用Amazon Route 53加权路由策略转移流量。对于新的serverless应用程序，公司计划使用Amazon API Gateway和AWS Lambda。公司需要更新其部署流程以适应新应用程序，还需要保留在向整个用户群推出功能之前在少数用户上测试新功能的能力。哪种部署策略能满足这些要求？ 选项： A. 使用AWS CDK部署API Gateway和Lambda函数。当需要更改代码时，更新AWS CloudFormation堆栈并部署新版本的API和Lambda函数。使用Route 53故障转移路由策略进行金丝雀发布策略。 B. 使用AWS CloudFormation部署API Gateway和Lambda函数，使用Lambda函数版本。当需要更改代码时，使用新的Lambda代码更新CloudFormation堆栈，并使用金丝雀发布策略更新API版本。测试完成后提升新版本。 C. 使用AWS Elastic Beanstalk部署API Gateway和Lambda函数。当需要更改代码时，部署新版本... D. 使用AWS OpsWorks在服务层部署API Gateway，在自定义层部署Lambda函数。当需要更改代码时...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为serverless架构（API Gateway + Lambda）设计一个部署策略，需要满足：1）从EC2迁移到serverless；2）保持渐进式部署能力；3）支持小规模用户测试后再全量发布（金丝雀发布）。 **涉及的关键AWS服务和概念：** - **API Gateway**: serverless架构的API入口 - **Lambda函数版本**: Lambda支持版本控制，可以同时运行多个版本 - **CloudFormation**: 基础设施即代码，用于管理AWS资源 - **金丝雀发布（Canary Release）**: 渐进式部署策略，先向少量用户发布新版本 - **蓝绿部署**: 通过维护两套环境实现零停机部署 **正确答案B的原因：** 1. **CloudFormation是标准选择**: 对于serverless应用的基础设施管理，CloudFormation是AWS推荐的最佳实践 2. **Lambda函数版本支持**: 利用Lambda原生的版本控制功能，可以同时维护多个代码版本 3. **API Gateway金丝雀发布**: API Gateway原生支持金丝雀发布，可以按百分比分配流量到不同Lambda版本 4. **完整的部署流程**: 提供了从代码更新到测试完成后提升版本的完整流程 **其他选项错误的原因：** - **选项A**: Route 53故障转移路由策略主要用于灾难恢复场景，不适合金丝雀发布；CDK虽然可用但CloudFormation更直接 - **选项C**: Elastic Beanstalk主要用于传统Web应用部署，不是serverless架构的最佳选择 - **选项D**: OpsWorks过于复杂，且不是serverless部署的标准工具 **决策标准和最佳实践：** 1. **选择原生支持的服务**: API Gateway和Lambda原生支持版本控制和流量分配 2. **基础设施即代码**: 使用CloudFormation确保部署的一致性和可重复性 3. **渐进式发布**: 利用AWS服务的内置金丝雀发布功能，而不是依赖外部路由策略 4. **简化架构**: 选择最直接、最少组件的解决方案来满足需求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">69</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A development team uses AWS CodeCommit, AWS CodePipeline, and AWS CodeBuild to develop and deploy an application. Changes to the code are submitted by pull requests. The development team reviews and merges the pull requests, and then the pipeline builds and tests the application. Over time, the number of pull requests has increased. The pipeline is frequently blocked because of failing tests. To prevent this blockage, the development team wants to run the unit and integration tests on each pull request before it is merged. Which solution will meet these requirements? to require the successful invocation of the CodeBuild project. Attach the approval rule to the project&#x27;s CodeCommit repository. and integration tests. Configure the CodeBuild project as a target of the EventBridge rule that includes a custom event payload with the CodeCommit repository and branch information from the event. Most Voted B (77%) D (15%) 4%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CodeBuild project to run the unit and integration tests. Create a CodeCommit approval rule template. Configure the template
B. Create an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit. Create a CodeBuild project to run the unit
C. Create an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit. Modify the existing CodePipeline pipeline to not run the deploy steps if the build is started from a pull request. Configure the EventBridge rule to run the pipeline with a custom payload that contains the CodeCommit repository and branch information from the event.
D. Create a CodeBuild project to run the unit and integration tests. Create a CodeCommit notification rule that matches when a pull request is created or updated. Configure the notification rule to invoke the CodeBuild project.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个开发团队使用AWS CodeCommit、AWS CodePipeline和AWS CodeBuild来开发和部署应用程序。代码更改通过pull request提交。开发团队审查并合并pull request，然后pipeline构建和测试应用程序。随着时间推移，pull request的数量增加了。由于测试失败，pipeline经常被阻塞。为了防止这种阻塞，开发团队希望在每个pull request合并之前运行单元测试和集成测试。哪个解决方案能满足这些要求？ 选项： A. 创建一个CodeBuild项目来运行单元测试和集成测试。创建一个CodeCommit审批规则模板。配置模板要求成功调用CodeBuild项目。将审批规则附加到项目的CodeCommit存储库。 B. 创建一个Amazon EventBridge规则来匹配来自CodeCommit的pullRequestCreated事件。创建一个CodeBuild项目来运行单元测试和集成测试。将CodeBuild项目配置为EventBridge规则的目标，该规则包含带有事件中CodeCommit存储库和分支信息的自定义事件负载。 C. 创建一个Amazon EventBridge规则来匹配来自CodeCommit的pullRequestCreated事件。修改现有的CodePipeline pipeline，如果构建是从pull request启动的，则不运行部署步骤。配置EventBridge规则运行pipeline，使用包含事件中CodeCommit存储库和分支信息的自定义负载。 D. 创建一个CodeBuild项目来运行单元测试和集成测试。创建一个CodeCommit通知规则，匹配创建或更新pull request时的情况。配置通知规则来调用CodeBuild项目。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求在pull request合并之前自动运行单元测试和集成测试，以防止失败的测试阻塞主pipeline。需要一个能够自动检测pull request创建并触发测试的解决方案。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：Git存储库服务，支持pull request功能 - AWS CodeBuild：托管构建服务，可运行测试 - Amazon EventBridge：事件驱动架构服务，可响应AWS服务事件 - CodeCommit事件：pullRequestCreated等事件可以触发自动化流程 - CodeCommit审批规则：可以设置合并前的条件检查 **正确答案B的原因：** 1. **事件驱动架构**：使用EventBridge监听pullRequestCreated事件，实现自动化触发 2. **直接集成**：EventBridge可以直接调用CodeBuild项目作为目标 3. **灵活性**：通过自定义事件负载传递存储库和分支信息，确保测试针对正确的代码 4. **独立性**：测试运行独立于主pipeline，不会影响现有的CI/CD流程 5. **实时响应**：一旦创建pull request就立即触发测试 **其他选项错误的原因：** - **选项A**：CodeCommit审批规则主要用于设置合并条件，但不能直接自动触发CodeBuild项目运行测试 - **选项C**：修改现有pipeline增加了复杂性，且可能影响正常的部署流程 - **选项D**：CodeCommit通知规则主要用于发送通知到SNS、SQS等，不能直接调用CodeBuild项目 **决策标准和最佳实践：** 1. **自动化优先**：选择能够自动检测和响应pull request事件的方案 2. **最小影响原则**：不修改现有的成功pipeline配置 3. **事件驱动设计**：利用AWS原生事件机制实现松耦合架构 4. **测试左移**：在开发流程早期进行测试，减少后期问题 5. **资源效率**：使用按需触发而非持续轮询的方式</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">70</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that runs on a fleet of Amazon EC2 instances. The application requires frequent restarts. The application logs contain error messages when a restart is required. The application logs are published to a log group in Amazon CloudWatch Logs. An Amazon CloudWatch alarm notifies an application engineer through an Amazon Simple Notification Service (Amazon SNS) topic when the logs contain a large number of restart-related error messages. The application engineer manually restarts the application on the instances after the application engineer receives a notification from the SNS topic. A DevOps engineer needs to implement a solution to automate the application restart on the instances without restarting the instances. Which solution will meet these requirements in the MOST operationally efficient manner? D (68%) C (17%) B (14%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Configure the SNS topic to invoke the runbook.
B. Create an AWS Lambda function that restarts the application on the instances. Configure the Lambda function as an event destination of the SNS topic.
C. Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Create an AWS Lambda function to invoke the runbook. Configure the Lambda function as an event destination of the SNS topic.
D. Configure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Configure an Amazon EventBridge rule that reacts when the CloudWatch alarm enters ALARM state. Specify the runbook as a target of the rule.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个运行在Amazon EC2实例集群上的应用程序。该应用程序需要频繁重启。当需要重启时，应用程序日志会包含错误消息。应用程序日志发布到Amazon CloudWatch Logs中的日志组。当日志包含大量重启相关错误消息时，Amazon CloudWatch告警通过Amazon Simple Notification Service (Amazon SNS)主题通知应用程序工程师。应用程序工程师在收到SNS主题通知后手动重启实例上的应用程序。DevOps工程师需要实现一个解决方案来自动化实例上的应用程序重启，而不重启实例本身。哪个解决方案能以最具操作效率的方式满足这些要求？ 选项： A. 配置AWS Systems Manager Automation runbook运行脚本来重启实例上的应用程序。配置SNS主题调用该runbook。 B. 创建AWS Lambda函数来重启实例上的应用程序。将Lambda函数配置为SNS主题的事件目标。 C. 配置AWS Systems Manager Automation runbook运行脚本来重启实例上的应用程序。创建AWS Lambda函数来调用runbook。将Lambda函数配置为SNS主题的事件目标。 D. 配置AWS Systems Manager Automation runbook运行脚本来重启实例上的应用程序。配置Amazon EventBridge规则在CloudWatch告警进入ALARM状态时触发。指定runbook作为规则的目标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要实现一个自动化解决方案，当CloudWatch告警触发时，能够自动重启EC2实例上的应用程序（不是重启实例本身），要求操作效率最高。 **涉及的关键AWS服务和概念：** - Amazon CloudWatch Logs和CloudWatch Alarms：监控和告警 - Amazon SNS：消息通知服务 - AWS Lambda：无服务器计算服务 - AWS Systems Manager Automation：自动化运维任务 - Amazon EventBridge：事件路由服务 **正确答案B的原因：** 1. **架构简洁**：直接使用Lambda函数作为SNS的订阅者，避免了不必要的中间层 2. **成本效益**：Lambda按使用付费，对于偶发的重启任务非常经济 3. **操作效率最高**：最少的组件数量，最直接的执行路径 4. **易于维护**：单一Lambda函数包含所有重启逻辑，便于调试和更新 5. **原生集成**：SNS可以直接触发Lambda，无需额外配置 **其他选项错误的原因：** - **选项A**：SNS无法直接调用Systems Manager Automation runbook，需要中间服务 - **选项C**：增加了不必要的复杂性，Lambda调用runbook比直接在Lambda中执行重启逻辑更复杂 - **选项D**：虽然技术可行，但绕过了现有的SNS通知机制，需要重新配置告警触发方式，增加了架构复杂性 **决策标准和最佳实践：** 1. **最小复杂性原则**：选择组件最少、集成最简单的方案 2. **成本优化**：Lambda的按需付费模式适合间歇性任务 3. **可维护性**：减少服务间依赖，降低故障点 4. **利用现有架构**：充分利用已有的SNS通知机制，避免重复建设</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">71</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer at a company is supporting an AWS environment in which all users use AWS IAM Identity Center (AWS Single Sign-On). The company wants to immediately disable credentials of any new IAM user and wants the security team to receive a notification. Which combination of steps should the DevOps engineer take to meet these requirements? (Choose three.) F. Create an Amazon Simple Queue Service (Amazon SQS) queue that is a target of the Lambda function. Subscribe the security team&#x27;s group email address to the queue. ACE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule that reacts to an IAM CreateUser API call in AWS CloudTrail.
B. Create an Amazon EventBridge rule that reacts to an IAM GetLoginProfile API call in AWS CloudTrail.
C. Create an AWS Lambda function that is a target of the EventBridge rule. Configure the Lambda function to disable any access keys and delete the login profiles that are associated with the IAM user.
D. Create an AWS Lambda function that is a target of the EventBridge rule. Configure the Lambda function to delete the login profiles that are associated with the IAM user.
E. Create an Amazon Simple Notification Service (Amazon SNS) topic that is a target of the EventBridge rule. Subscribe the security team&#x27;s group email address to the topic.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的DevOps工程师正在支持一个AWS环境，该环境中所有用户都使用AWS IAM Identity Center (AWS Single Sign-On)。公司希望立即禁用任何新IAM用户的凭证，并希望安全团队收到通知。DevOps工程师应该采取哪些步骤组合来满足这些要求？（选择三个。） 选项： A. 创建一个Amazon EventBridge规则，对AWS CloudTrail中的IAM CreateUser API调用做出反应。 B. 创建一个Amazon EventBridge规则，对AWS CloudTrail中的IAM GetLoginProfile API调用做出反应。 C. 创建一个AWS Lambda函数作为EventBridge规则的目标。配置Lambda函数禁用任何访问密钥并删除与IAM用户关联的登录配置文件。 D. 创建一个AWS Lambda函数作为EventBridge规则的目标。配置Lambda函数删除与IAM用户关联的登录配置文件。 E. 创建一个Amazon SNS主题作为EventBridge规则的目标。将安全团队的群组邮箱地址订阅到该主题。 F. 创建一个Amazon SQS队列作为Lambda函数的目标。将安全团队的群组邮箱地址订阅到该队列。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求建立一个自动化系统来：1）检测新IAM用户的创建；2）立即禁用新创建用户的凭证；3）通知安全团队。由于公司使用IAM Identity Center，不应该有直接的IAM用户创建，因此需要监控和响应意外的用户创建。 **涉及的关键AWS服务和概念：** - Amazon EventBridge：事件驱动架构的核心服务，用于监控API调用 - AWS CloudTrail：记录所有AWS API调用的审计服务 - AWS Lambda：无服务器计算服务，用于执行响应逻辑 - Amazon SNS：简单通知服务，用于发送通知 - IAM API操作：CreateUser（创建用户）、GetLoginProfile（获取登录配置） **正确答案分析（A、C、E）：** - 选项A正确：CreateUser API调用是检测新IAM用户创建的正确事件，这是触发整个流程的起点 - 选项C正确：需要Lambda函数同时禁用访问密钥和删除登录配置文件，确保完全禁用用户凭证 - 选项E正确：SNS是向安全团队发送通知的标准方式，可以直接作为EventBridge规则的目标 **其他选项错误的原因：** - 选项B错误：GetLoginProfile是查询操作，不是创建用户的指示器，无法准确触发所需的响应 - 选项D错误：只删除登录配置文件不够，还需要禁用访问密钥才能完全禁用用户凭证 - 选项F错误：SQS不能直接订阅邮箱地址，且架构过于复杂（Lambda→SQS→邮箱），SNS更适合直接通知 **决策标准和最佳实践：** 1. 事件检测应该基于实际的创建操作（CreateUser）而非查询操作 2. 安全响应必须全面（同时处理访问密钥和登录配置文件） 3. 通知机制应该简单直接，SNS比SQS更适合邮件通知场景 4. 架构应该遵循事件驱动模式：EventBridge监控→Lambda执行→SNS通知</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">72</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to set up a continuous delivery pipeline. The company stores application code in a private GitHub repository. The company needs to deploy the application components to Amazon Elastic Container Service (Amazon ECS), Amazon EC2, and AWS Lambda. The pipeline must support manual approval actions. Which solution will meet these requirements? B (92%) 4%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CodePipeline with Amazon ECS, Amazon EC2, and Lambda as deploy providers.
B. Use AWS CodePipeline with AWS CodeDeploy as the deploy provider.
C. Use AWS CodePipeline with AWS Elastic Beanstalk as the deploy provider.
D. Use AWS CodeDeploy with GitHub integration to deploy the application.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司想要建立一个持续交付管道。该公司将应用程序代码存储在私有的GitHub仓库中。公司需要将应用程序组件部署到Amazon Elastic Container Service (Amazon ECS)、Amazon EC2和AWS Lambda。管道必须支持手动批准操作。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个持续交付管道，需要满足以下条件： - 从私有GitHub仓库获取代码 - 部署到多个不同的AWS服务（ECS、EC2、Lambda） - 支持手动批准流程 - 实现完整的CI/CD流程 **涉及的关键AWS服务和概念：** - AWS CodePipeline：AWS的持续集成/持续交付服务，用于编排整个部署流程 - AWS CodeDeploy：专门的部署服务，支持多种部署目标和策略 - GitHub集成：与外部代码仓库的集成能力 - 手动批准：CI/CD流程中的人工审核环节 **正确答案B的原因：** AWS CodePipeline + AWS CodeDeploy的组合是最佳选择，因为： 1. CodePipeline提供完整的管道编排能力，原生支持GitHub集成 2. CodeDeploy作为部署提供商，天然支持ECS、EC2和Lambda的部署 3. CodePipeline内置手动批准操作功能，可以在管道中插入审批步骤 4. 这种组合提供了最大的灵活性和可扩展性 5. 支持复杂的部署策略（蓝绿部署、滚动部署等） **其他选项错误的原因：** - 选项A：直接使用ECS、EC2、Lambda作为部署提供商过于复杂，缺乏统一的部署抽象层，难以管理多目标部署 - 选项C：Elastic Beanstalk主要针对Web应用部署，不适合Lambda部署，且对ECS支持有限 - 选项D：仅使用CodeDeploy缺少完整的管道编排能力，无法很好地处理手动批准和复杂的工作流程 **决策标准和最佳实践：** 选择CI/CD解决方案时应考虑： 1. 服务集成的完整性和原生支持 2. 多目标部署的统一管理能力 3. 工作流编排的灵活性 4. 手动干预和审批机制的支持 5. 可扩展性和维护性 CodePipeline + CodeDeploy的组合代表了AWS CI/CD的最佳实践模式。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">73</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that runs on Amazon EC2 instances that are in an Auto Scaling group. When the application starts up, the application needs to process data from an Amazon S3 bucket before the application can start to serve requests. The size of the data that is stored in the S3 bucket is growing. When the Auto Scaling group adds new instances, the application now takes several minutes to download and process the data before the application can serve requests. The company must reduce the time that elapses before new EC2 instances are ready to serve requests. Which solution is the MOST cost-effective way to reduce the application startup time? A (86%) 14%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a warm pool for the Auto Scaling group with warmed EC2 instances in the Stopped state. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.
B. Increase the maximum instance count of the Auto Scaling group. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.
C. Configure a warm pool for the Auto Scaling group with warmed EC2 instances in the Running state. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.
D. Increase the maximum instance count of the Auto Scaling group. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook and to place the new instance in the Standby state when the application is ready to serve requests.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个运行在Amazon EC2实例上的应用程序，这些实例位于Auto Scaling组中。当应用程序启动时，应用程序需要先处理Amazon S3存储桶中的数据，然后才能开始处理请求。存储在S3存储桶中的数据大小正在增长。当Auto Scaling组添加新实例时，应用程序现在需要几分钟来下载和处理数据，然后才能处理请求。公司必须减少新EC2实例准备好处理请求之前所需的时间。哪种解决方案是减少应用程序启动时间最具成本效益的方法？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到最具成本效益的方法来减少新EC2实例启动并准备好处理请求的时间。关键挑战是应用程序需要从S3下载和处理越来越大的数据集，导致启动时间延长。 **涉及的关键AWS服务和概念：** 1. Auto Scaling Group - 自动扩缩容组 2. Warm Pool - 预热池，可以预先准备实例 3. Lifecycle Hook - 生命周期钩子，控制实例启动流程 4. EC2实例状态：Running（运行中）vs Stopped（已停止） **正确答案C的原因：** - Warm Pool with Running state：预热池中的实例保持Running状态，意味着实例已经启动并且应用程序已经运行，数据已经预先下载和处理完成 - 当需要扩容时，这些预热的实例可以立即投入使用，无需重新下载S3数据 - Lifecycle Hook确保只有当应用程序完全准备好时才开始接收流量 - 这是最快的解决方案，因为实例和应用程序都已经准备就绪 **其他选项错误的原因：** - 选项A（Stopped状态）：虽然成本更低，但实例仍需要启动时间，应用程序仍需要重新下载和处理S3数据，无法解决根本问题 - 选项B：仅增加最大实例数量不能解决启动时间问题，新实例仍然需要完整的启动和数据处理时间 - 选项D：增加最大实例数量 + Standby状态的组合不如warm pool有效，且Standby状态的实例不会主动预处理数据 **决策标准和最佳实践：** 1. 成本效益平衡：Running状态的warm pool虽然成本稍高，但能最大程度减少启动时间 2. 预处理策略：在warm pool中预先完成数据下载和处理，避免重复工作 3. 生命周期管理：使用lifecycle hook确保服务质量，只有完全准备好的实例才接收流量 4. 扩容响应速度：在需要快速响应负载变化的场景中，Running状态的预热实例提供最佳性能</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">74</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using an AWS CodeBuild project to build and package an application. The packages are copied to a shared Amazon S3 bucket before being deployed across multiple AWS accounts. The buildspec.yml file contains the following: The DevOps engineer has noticed that anybody with an AWS account is able to download the artifacts. What steps should the DevOps engineer take to stop this? D (79%) A (21%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the post_build command to use --acl public-read and configure a bucket policy that grants read access to the relevant AWS accounts only.
B. Configure a default ACL for the S3 bucket that defines the set of authenticated users as the relevant AWS accounts only and grants read-only access.
C. Create an S3 bucket policy that grants read access to the relevant AWS accounts and denies read access to the principal &quot;*&quot;.
D. Modify the post_build command to remove --acl authenticated-read and configure a bucket policy that allows read access to the relevant AWS accounts only.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS CodeBuild项目来构建和打包应用程序。这些包被复制到共享的Amazon S3存储桶中，然后部署到多个AWS账户。buildspec.yml文件包含以下内容：DevOps工程师注意到任何拥有AWS账户的人都能够下载这些构件。DevOps工程师应该采取什么步骤来阻止这种情况？ 选项： A. 修改post_build命令使用--acl public-read，并配置存储桶策略，仅向相关AWS账户授予读取访问权限。 B. 为S3存储桶配置默认ACL，将经过身份验证的用户集定义为仅相关AWS账户，并授予只读访问权限。 C. 创建S3存储桶策略，向相关AWS账户授予读取访问权限，并拒绝主体&quot;*&quot;的读取访问权限。 D. 修改post_build命令移除--acl authenticated-read，并配置存储桶策略，仅允许相关AWS账户的读取访问权限。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题的核心是解决S3存储桶中构建产物的访问权限过于宽松的问题。当前任何AWS账户用户都能下载构件，需要限制访问权限仅给特定的相关AWS账户。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：持续集成服务，用于构建和打包应用程序 - Amazon S3：对象存储服务，存储构建产物 - S3 ACL (Access Control List)：对象级别的访问控制 - S3 Bucket Policy：存储桶级别的访问策略 - `--acl authenticated-read`：S3 ACL设置，允许所有经过AWS身份验证的用户读取 - `--acl public-read`：S3 ACL设置，允许所有人（包括匿名用户）读取 **正确答案D的原因：** 1. **移除问题根源**：`--acl authenticated-read`是导致任何AWS账户用户都能访问的根本原因，必须移除 2. **采用最佳实践**：使用bucket policy替代ACL进行精确的访问控制 3. **精确权限控制**：bucket policy可以明确指定哪些AWS账户可以访问，实现最小权限原则 4. **安全性更强**：避免了ACL的宽泛权限设置，转向更精确的策略控制 **其他选项错误的原因：** - **选项A错误**：`--acl public-read`比原来的`authenticated-read`更加宽松，会让所有人（包括匿名用户）都能访问，严重恶化安全问题 - **选项B错误**：默认ACL仍然是ACL机制，无法精确指定特定AWS账户，且&quot;authenticated users&quot;概念过于宽泛 - **选项C错误**：虽然思路正确，但仅拒绝&quot;*&quot;主体不够全面，没有解决`authenticated-read` ACL的问题，可能仍存在权限泄露 **决策标准和最佳实践：** 1. **最小权限原则**：仅授予必要的最小权限给指定的AWS账户 2. **策略优于ACL**：在需要复杂访问控制时，优先使用bucket policy而非ACL 3. **移除宽泛权限**：彻底移除可能导致权限泄露的ACL设置 4. **分层安全**：结合移除问题ACL和配置精确bucket policy的双重措施</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">75</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has developed a serverless web application that is hosted on AWS. The application consists of Amazon S3, Amazon API Gateway, several AWS Lambda functions, and an Amazon RDS for MySQL database. The company is using AWS CodeCommit to store the source code. The source code is a combination of AWS Serverless Application Model (AWS SAM) templates and Python code. A security audit and penetration test reveal that user names and passwords for authentication to the database are hardcoded within CodeCommit repositories. A DevOps engineer must implement a solution to automatically detect and prevent hardcoded secrets. What is the MOST secure solution that meets these requirements? report. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager. B (95%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation report. Write the secret to AWS Systems Manager Parameter Store as a secure string. Update the SAM templates and the Python code to pull the secret from Parameter Store.
B. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager.
C. Enable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation
D. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Write the secret to AWS Systems Manager Parameter Store as a string. Update the SAM templates and the Python code to pull the secret from Parameter Store.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司开发了一个托管在AWS上的无服务器Web应用程序。该应用程序由Amazon S3、Amazon API Gateway、多个AWS Lambda函数和Amazon RDS for MySQL数据库组成。公司使用AWS CodeCommit存储源代码。源代码是AWS Serverless Application Model (AWS SAM)模板和Python代码的组合。安全审计和渗透测试显示，用于数据库身份验证的用户名和密码在CodeCommit存储库中是硬编码的。DevOps工程师必须实施一个解决方案来自动检测和防止硬编码的机密信息。什么是满足这些要求的最安全的解决方案？ 选项： A. 启用Amazon CodeGuru Profiler。用@with_lambda_profiler()装饰处理函数。手动审查推荐报告。将机密信息写入AWS Systems Manager Parameter Store作为安全字符串。更新SAM模板和Python代码以从Parameter Store拉取机密信息。 B. 将CodeCommit存储库与Amazon CodeGuru Reviewer关联。手动检查代码审查的任何建议。选择保护机密信息的选项。更新SAM模板和Python代码以从AWS Secrets Manager拉取机密信息。 C. 启用Amazon CodeGuru Profiler。用@with_lambda_profiler()装饰处理函数。手动审查推荐报告。选择保护机密信息的选项。更新SAM模板和Python代码以从AWS Secrets Manager拉取机密信息。 D. 将CodeCommit存储库与Amazon CodeGuru Reviewer关联。手动检查代码审查的任何建议。将机密信息作为字符串写入AWS Systems Manager Parameter Store。更新SAM模板和Python代码以从Parameter Store拉取机密信息。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到最安全的解决方案来自动检测和防止代码中的硬编码机密信息，特别是数据库用户名和密码。需要同时解决检测问题和存储问题两个方面。 **涉及的关键AWS服务和概念：** 1. **Amazon CodeGuru Reviewer** - 用于自动化代码审查，可以检测安全问题包括硬编码的机密信息 2. **Amazon CodeGuru Profiler** - 用于应用程序性能分析，不是用于代码安全审查 3. **AWS Secrets Manager** - 专门用于管理敏感信息如数据库凭证，提供自动轮换功能 4. **AWS Systems Manager Parameter Store** - 用于存储配置数据，可以存储加密参数但功能不如Secrets Manager全面 **正确答案B的原因：** 1. **正确的检测工具**：使用CodeGuru Reviewer而不是CodeGuru Profiler，Reviewer专门用于代码质量和安全审查，能够检测硬编码的机密信息 2. **最安全的存储方案**：选择AWS Secrets Manager而不是Parameter Store，因为Secrets Manager专门为数据库凭证设计，提供自动轮换、细粒度访问控制等高级安全功能 3. **完整的解决方案**：既解决了检测问题又提供了最佳的存储替代方案 **其他选项错误的原因：** - **选项A**：使用了错误的工具CodeGuru Profiler（用于性能分析而非安全审查），且使用Parameter Store不如Secrets Manager安全 - **选项C**：同样使用了错误的CodeGuru Profiler工具 - **选项D**：虽然使用了正确的CodeGuru Reviewer，但将机密信息存储为普通字符串而非加密形式，且Parameter Store对于数据库凭证管理不如Secrets Manager专业 **决策标准和最佳实践：** 1. **工具选择**：对于代码安全审查应使用CodeGuru Reviewer，对于性能分析使用CodeGuru Profiler 2. **机密信息管理**：数据库凭证应优先使用AWS Secrets Manager，它提供专门的数据库集成和自动轮换功能 3. **安全层级**：Secrets Manager &gt; Parameter Store (SecureString) &gt; Parameter Store (String) &gt; 硬编码 4. **自动化原则**：选择能够自动检测和预防安全问题的工具和服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">76</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using Amazon S3 buckets to store important documents. The company discovers that some S3 buckets are not encrypted. Currently, the company&#x27;s IAM users can create new S3 buckets without encryption. The company is implementing a new requirement that all S3 buckets must be encrypted. A DevOps engineer must implement a solution to ensure that server-side encryption is enabled on all existing S3 buckets and all new S3 buckets. The encryption must be enabled on new S3 buckets as soon as the S3 buckets are created. The default encryption type must be 256-bit Advanced Encryption Standard (AES-256). Which solution will meet these requirements? B (90%) 10%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS Lambda function that is invoked periodically by an Amazon EventBridge scheduled rule. Program the Lambda function to scan all current S3 buckets for encryption status and to set AES-256 as the default encryption for any S3 bucket that does not have an encryption configuration.
B. Set up and activate the s3-bucket-server-side-encryption-enabled AWS Config managed rule. Configure the rule to use the AWS-EnableS3BucketEncryption AWS Systems Manager Automation runbook as the remediation action. Manually run the re-evaluation process to ensure that existing S3 buckets are compliant.
C. Create an AWS Lambda function that is invoked by an Amazon EventBridge event rule. Define the rule with an event pattern that matches the creation of new S3 buckets. Program the Lambda function to parse the EventBridge event, check the configuration of the S3 buckets from the event, and set AES-256 as the default encryption.
D. Configure an IAM policy that denies the s3:CreateBucket action if the s3:x-amz-server-side-encryption condition key has a value that is not AES-256. Create an IAM group for all the company&#x27;s IAM users. Associate the IAM policy with the IAM group.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用Amazon S3存储桶来存储重要文档。公司发现一些S3存储桶没有加密。目前，公司的IAM用户可以创建没有加密的新S3存储桶。公司正在实施一项新要求，即所有S3存储桶都必须加密。DevOps工程师必须实施一个解决方案，确保在所有现有S3存储桶和所有新S3存储桶上启用服务器端加密。加密必须在创建新S3存储桶时立即启用。默认加密类型必须是256位高级加密标准(AES-256)。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 确保所有现有S3存储桶都启用AES-256加密 2. 确保所有新创建的S3存储桶都必须启用AES-256加密 3. 加密必须在创建存储桶时立即生效 4. 防止用户创建未加密的存储桶 **涉及的关键AWS服务和概念：** - Amazon S3服务器端加密 - IAM策略和条件键 - AWS Config规则和自动修复 - Lambda函数和EventBridge事件 - 预防性控制vs检测性控制 **正确答案D的原因：** 1. **预防性控制**：通过IAM策略在源头阻止创建未加密的存储桶，这是最有效的方法 2. **立即生效**：IAM策略会在用户尝试创建存储桶时立即执行检查，满足&quot;立即启用&quot;的要求 3. **条件键使用**：s3:x-amz-server-side-encryption条件键可以精确控制加密要求 4. **全面覆盖**：通过IAM组应用到所有用户，确保无人能绕过此限制 **其他选项错误的原因：** - **选项A**：定期扫描是被动的，无法防止在扫描间隔期间创建未加密的存储桶，不满足&quot;立即启用&quot;要求 - **选项B**：AWS Config是检测性控制，虽然有自动修复功能，但仍然存在时间延迟，且主要用于合规检查而非预防 - **选项C**：基于事件的Lambda函数虽然响应及时，但仍然是在存储桶创建后才执行，存在短暂的未加密窗口期 **决策标准和最佳实践：** 1. **预防优于检测**：在安全控制中，预防性措施比检测和修复更有效 2. **最小权限原则**：通过IAM策略限制用户只能执行符合安全要求的操作 3. **实时控制**：IAM策略提供实时的访问控制，无延迟 4. **成本效益**：IAM策略无额外运行成本，而Lambda和Config规则会产生使用费用</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">77</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is architecting a continuous development strategy for a company&#x27;s software as a service (SaaS) web application running on AWS. For application and security reasons, users subscribing to this application are distributed across multiple Application Load Balancers (ALBs), each of which has a dedicated Auto Scaling group and fleet of Amazon EC2 instances. The application does not require a build stage, and when it is committed to AWS CodeCommit, the application must trigger a simultaneous deployment to all ALBs, Auto Scaling groups, and EC2 fleets. Which architecture will meet these requirements with the LEAST amount of configuration? C (73%) B (23%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a single AWS CodePipeline pipeline that deploys the application in parallel using unique AWS CodeDeploy applications and deployment groups created for each ALB-Auto Scaling group pair.
B. Create a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group.
C. Create a single AWS CodePipeline pipeline that deploys the application in parallel using a single AWS CodeDeploy application and unique deployment group for each ALB-Auto Scaling group pair.
D. Create an AWS CodePipeline pipeline for each ALB-Auto Scaling group pair that deploys the application using an AWS CodeDeploy application and deployment group created for the same ALB-Auto Scaling group pair.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在为公司运行在AWS上的软件即服务(SaaS) Web应用程序设计持续开发策略。出于应用程序和安全原因，订阅此应用程序的用户分布在多个Application Load Balancer (ALB)上，每个ALB都有专用的Auto Scaling组和Amazon EC2实例集群。该应用程序不需要构建阶段，当代码提交到AWS CodeCommit时，应用程序必须触发同时部署到所有ALB、Auto Scaling组和EC2集群。哪种架构能以最少的配置满足这些要求？ 选项： A. 创建单个AWS CodePipeline流水线，使用为每个ALB-Auto Scaling组对创建的唯一AWS CodeDeploy应用程序和部署组并行部署应用程序。 B. 创建单个AWS CodePipeline流水线，使用单个AWS CodeDeploy应用程序和单个部署组部署应用程序。 C. 创建单个AWS CodePipeline流水线，使用单个AWS CodeDeploy应用程序和为每个ALB-Auto Scaling组对创建的唯一部署组并行部署应用程序。 D. 为每个ALB-Auto Scaling组对创建AWS CodePipeline流水线，使用为同一ALB-Auto Scaling组对创建的AWS CodeDeploy应用程序和部署组部署应用程序。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 需要同时部署到多个ALB-Auto Scaling组对 - 不需要构建阶段，直接从CodeCommit触发部署 - 要求最少的配置复杂度 - 实现并行部署到所有目标环境 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD流水线服务，用于自动化部署流程 - AWS CodeDeploy：应用程序部署服务，管理EC2实例的代码部署 - CodeDeploy Application：逻辑容器，包含部署配置和历史记录 - CodeDeploy Deployment Group：定义部署目标实例的集合 - Auto Scaling Group：自动扩缩容的EC2实例组 **正确答案C的原因：** 1. **单个CodePipeline**：减少管理复杂度，统一触发机制 2. **单个CodeDeploy应用程序**：所有环境使用相同的应用程序代码和配置，便于统一管理 3. **多个部署组**：每个ALB-Auto Scaling组对需要独立的部署组来隔离部署目标 4. **并行部署**：满足同时部署到所有环境的要求 5. **配置最少**：相比其他方案，这种组合提供了最佳的简洁性和功能性平衡 **其他选项错误的原因：** - **选项A**：创建多个CodeDeploy应用程序增加了不必要的复杂性，违反了&quot;最少配置&quot;原则 - **选项B**：单个部署组无法区分不同的ALB-Auto Scaling组对，无法实现隔离部署 - **选项D**：为每个环境创建独立的CodePipeline大大增加了管理开销和配置复杂度 **决策标准和最佳实践：** 1. **最小化管理开销**：优先选择能减少重复配置的方案 2. **适当的资源隔离**：使用部署组而非应用程序来区分部署目标 3. **并行处理能力**：确保能同时处理多个部署目标 4. **统一管理**：单一流水线便于监控和故障排除 5. **扩展性考虑**：方案应该便于添加新的ALB-Auto Scaling组对</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">78</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is hosting a static website from an Amazon S3 bucket. The website is available to customers at example.com. The company uses an Amazon Route 53 weighted routing policy with a TTL of 1 day. The company has decided to replace the existing static website with a dynamic web application. The dynamic web application uses an Application Load Balancer (ALB) in front of a fleet of Amazon EC2 instances. On the day of production launch to customers, the company creates an additional Route 53 weighted DNS record entry that points to the ALB with a weight of 255 and a TTL of 1 hour. Two days later, a DevOps engineer notices that the previous static website is displayed sometimes when customers navigate to example.com. How can the DevOps engineer ensure that the company serves only dynamic content for example.com? D (81%) B (19%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Delete all objects, including previous versions, from the S3 bucket that contains the static website content.
B. Update the weighted DNS record entry that points to the S3 bucket. Apply a weight of 0. Specify the domain reset option to propagate changes immediately.
C. Configure webpage redirect requests on the S3 bucket with a hostname that redirects to the ALB.
D. Remove the weighted DNS record entry that points to the S3 bucket from the example.com hosted zone. Wait for DNS propagation to become complete.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在从Amazon S3存储桶托管静态网站。该网站通过example.com向客户提供服务。公司使用Amazon Route 53加权路由策略，TTL为1天。公司决定用动态Web应用程序替换现有的静态网站。动态Web应用程序在Amazon EC2实例集群前使用Application Load Balancer (ALB)。在向客户正式发布的当天，公司创建了一个额外的Route 53加权DNS记录条目，指向ALB，权重为255，TTL为1小时。两天后，DevOps工程师注意到当客户访问example.com时，有时仍然显示之前的静态网站。DevOps工程师如何确保公司只为example.com提供动态内容？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题要求解决DNS路由中新旧服务并存的问题，确保用户只访问到新的动态Web应用，而不是旧的静态网站。 **涉及的关键AWS服务和概念：** - Route 53加权路由策略：根据权重分配流量到不同的资源 - DNS TTL（生存时间）：控制DNS记录在缓存中保存的时间 - S3静态网站托管：原有的静态网站解决方案 - Application Load Balancer：新的动态应用前端负载均衡器 - DNS传播：DNS记录更改在全球DNS系统中生效的过程 **正确答案D的原因：** - 直接删除指向S3的DNS记录是最彻底的解决方案 - 消除了流量分配的源头，确保所有流量都指向ALB - 虽然需要等待DNS传播完成，但这是标准的DNS更改流程 - 一旦传播完成，就不会再有流量路由到旧的静态网站 **其他选项错误的原因：** - 选项A：删除S3对象不能解决DNS路由问题，DNS记录仍然存在，会导致访问错误 - 选项B：Route 53没有&quot;域重置选项&quot;来立即传播更改，这个功能不存在 - 选项C：配置重定向只是增加了额外的跳转步骤，没有解决根本的DNS路由问题，且仍有部分流量会先到达S3 **决策标准和最佳实践：** - 在DNS切换场景中，应该从源头解决路由问题 - 理解DNS TTL和传播机制，合理规划切换时间 - 在生产环境切换时，应该有明确的回退计划 - 监控DNS解析结果，确保切换效果符合预期 - 考虑使用更短的TTL来加快DNS更改的生效速度</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">79</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is implementing AWS CodePipeline to automate its testing process. The company wants to be notified when the execution state fails and used the following custom event pattern in Amazon EventBridge: Which type of events will match this event pattern? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Failed deploy and build actions across all the pipelines
B. All rejected or failed approval actions across all the pipelines
C. All the events across all pipelines
D. Approval actions across all the pipelines</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在实施AWS CodePipeline来自动化其测试流程。该公司希望在执行状态失败时收到通知，并在Amazon EventBridge中使用了以下自定义事件模式：哪种类型的事件将匹配此事件模式？ 选项： A. 所有pipeline中失败的部署和构建操作 B. 所有pipeline中被拒绝或失败的审批操作 C. 所有pipeline中的所有事件 D. 所有pipeline中的审批操作 正确答案：B</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是Amazon EventBridge事件模式匹配机制，特别是如何为AWS CodePipeline配置事件过滤规则来监控特定类型的失败事件。虽然题目中没有显示具体的事件模式代码，但从正确答案可以推断出该模式专门针对审批操作的失败和拒绝状态。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：持续集成/持续部署服务，包含多种操作类型（构建、部署、审批等） - Amazon EventBridge：事件驱动架构服务，用于监控和响应AWS服务状态变化 - 事件模式匹配：通过JSON格式定义过滤条件，精确匹配特定事件 - CodePipeline操作状态：包括SUCCEEDED、FAILED、REJECTED等状态 **正确答案的原因：** 选项B正确，因为该事件模式专门配置为匹配： 1. 操作类型为&quot;Approval&quot;（审批操作） 2. 状态为&quot;FAILED&quot;或&quot;REJECTED&quot;的事件 3. 覆盖所有pipeline实例 这种配置能够精确捕获审批环节的问题，这在需要人工干预的CI/CD流程中非常重要。 **其他选项错误的原因：** - 选项A错误：该模式不包含构建(Build)和部署(Deploy)操作，只针对审批操作 - 选项C错误：该模式有明确的过滤条件，不会匹配所有事件，只匹配特定的失败状态 - 选项D错误：该模式不仅限于审批操作，还包含了状态过滤（失败/拒绝），不是所有审批事件 **决策标准和最佳实践：** 1. 事件模式应该精确定义source、detail-type和detail字段来避免误报 2. 针对不同操作类型设置不同的通知策略，审批失败通常需要立即人工介入 3. 合理使用EventBridge规则可以实现精细化的监控和自动化响应 4. 在生产环境中应该为关键操作设置多层监控和告警机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">80</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An application running on a set of Amazon EC2 instances in an Auto Scaling group requires a configuration file to operate. The instances are created and maintained with AWS CloudFormation. A DevOps engineer wants the instances to have the latest configuration file when launched, and wants changes to the configuration file to be reflected on all the instances with a minimal delay when the CloudFormation template is updated. Company policy requires that application configuration files be maintained along with AWS infrastructure configuration files in source control. Which solution will accomplish this? D (77%) B (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the CloudFormation template, add an AWS Config rule. Place the configuration file content in the rule&#x27;s InputParameters property, and set the Scope property to the EC2 Auto Scaling group. Add an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the configuration.
B. In the CloudFormation template, add an EC2 launch template resource. Place the configuration file content in the launch template. Configure the cfn-init script to run when the instance is launched, and configure the cfn-hup script to poll for updates to the configuration.
C. In the CloudFormation template, add an EC2 launch template resource. Place the configuration file content in the launch template. Add an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the configuration.
D. In the CloudFormation template, add CloudFormation init metadata. Place the configuration file content in the metadata. Configure the cfn-init script to run when the instance is launched, and configure the cfn-hup script to poll for updates to the configuration.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个运行在Auto Scaling组中的Amazon EC2实例集合上的应用程序需要一个配置文件来运行。这些实例通过AWS CloudFormation创建和维护。DevOps工程师希望实例在启动时拥有最新的配置文件，并且当CloudFormation模板更新时，配置文件的更改能够以最小的延迟反映到所有实例上。公司政策要求应用程序配置文件与AWS基础设施配置文件一起在源代码控制中维护。哪个解决方案能够实现这个目标？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. EC2实例启动时需要获取最新的配置文件 2. CloudFormation模板更新时，配置文件变更要以最小延迟反映到所有实例 3. 配置文件需要与基础设施代码一起在源代码控制中维护 4. 实例通过Auto Scaling组和CloudFormation管理 **涉及的关键AWS服务和概念：** - CloudFormation：基础设施即代码服务 - EC2 Launch Template：定义EC2实例启动配置的模板 - CloudFormation Init Metadata：在CloudFormation模板中定义实例初始化配置 - cfn-init：CloudFormation辅助脚本，用于实例启动时的初始化 - cfn-hup：CloudFormation辅助脚本，用于监控配置变更并自动更新 **正确答案B的原因：** 1. 使用EC2 Launch Template可以将配置文件内容直接嵌入到模板中，满足源代码控制要求 2. cfn-init脚本确保实例启动时能够获取最新配置 3. cfn-hup脚本能够持续监控CloudFormation模板的变更，实现配置的自动更新，满足最小延迟要求 4. 这种方案完全基于CloudFormation原生功能，无需额外的AWS服务 **其他选项错误的原因：** - 选项A：AWS Config主要用于合规性检查，不是用于配置文件分发的服务；Resource Data Sync也不适用于此场景 - 选项C：虽然使用了Launch Template，但Resource Data Sync不是用于配置更新监控的正确服务 - 选项D：CloudFormation Init Metadata是正确的方向，但题目明确提到使用Launch Template，而且cfn-hup需要与CloudFormation metadata配合使用 **决策标准和最佳实践：** 1. 优先使用CloudFormation原生功能而非额外的AWS服务 2. cfn-init和cfn-hup是处理CloudFormation配置管理的标准工具组合 3. Launch Template提供了更好的版本控制和配置管理能力 4. 配置文件直接嵌入基础设施代码中，确保了版本一致性和源代码控制要求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">81</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages an application that stores logs in Amazon CloudWatch Logs. The company wants to archive the logs to an Amazon S3 bucket. Logs are rarely accessed after 90 days and must be retained for 10 years. Which combination of steps should a DevOps engineer take to meet these requirements? (Choose two.) BD (83%) CD (17%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a CloudWatch Logs subscription filter to use AWS Glue to transfer all logs to an S3 bucket.
B. Configure a CloudWatch Logs subscription filter to use Amazon Kinesis Data Firehose to stream all logs to an S3 bucket.
C. Configure a CloudWatch Logs subscription filter to stream all logs to an S3 bucket.
D. Configure the S3 bucket lifecycle policy to transition logs to S3 Glacier after 90 days and to expire logs after 3,650 days.
E. Configure the S3 bucket lifecycle policy to transition logs to Reduced Redundancy after 90 days and to expire logs after 3,650 days.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司管理一个将日志存储在Amazon CloudWatch Logs中的应用程序。该公司希望将日志归档到Amazon S3存储桶中。日志在90天后很少被访问，并且必须保留10年。DevOps工程师应该采取哪些步骤组合来满足这些要求？（选择两个。）BD (83%) CD (17%) 选项：A. 配置CloudWatch Logs订阅过滤器使用AWS Glue将所有日志传输到S3存储桶。 B. 配置CloudWatch Logs订阅过滤器使用Amazon Kinesis Data Firehose将所有日志流式传输到S3存储桶。 C. 配置CloudWatch Logs订阅过滤器将所有日志流式传输到S3存储桶。 D. 配置S3存储桶生命周期策略，在90天后将日志转换到S3 Glacier，并在3,650天后使日志过期。 E. 配置S3存储桶生命周期策略，在90天后将日志转换到Reduced Redundancy，并在3,650天后使日志过期。 正确答案：B</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个解决方案来：1）将CloudWatch Logs中的日志归档到S3；2）处理90天后很少访问的特性；3）满足10年（3,650天）的保留要求。这是一个典型的日志归档和生命周期管理场景。 **涉及的关键AWS服务和概念：** - CloudWatch Logs订阅过滤器：用于实时流式传输日志数据 - Amazon Kinesis Data Firehose：托管的数据传输服务，可以可靠地将流数据传输到S3 - S3生命周期策略：自动管理对象存储类别转换和过期 - S3存储类别：Standard、Glacier等不同成本和访问特性的存储选项 **正确答案的原因：** 选项B正确，因为Kinesis Data Firehose是将CloudWatch Logs数据传输到S3的标准和推荐方法。它提供了可靠的、托管的流式传输服务，支持数据转换、压缩和批处理，非常适合日志归档场景。 选项D也是正确的（虽然题目只显示B为正确答案），因为它正确配置了生命周期策略：90天后转换到Glacier（适合很少访问的数据），10年后过期删除，完全符合需求。 **其他选项错误的原因：** - 选项A：AWS Glue主要用于ETL数据处理，不是日志流式传输的最佳选择，过于复杂且成本较高 - 选项C：CloudWatch Logs订阅过滤器不能直接流式传输到S3，需要中间服务如Kinesis Data Firehose - 选项E：Reduced Redundancy Storage已被AWS弃用，且不适合长期归档，应该使用Glacier等归档存储类别 **决策标准和最佳实践：** 1. 使用托管服务减少运维复杂性（Kinesis Data Firehose vs 自建方案） 2. 根据访问模式选择合适的存储类别（很少访问选择Glacier） 3. 利用S3生命周期策略自动化存储管理 4. 考虑成本优化（Glacier比Standard存储成本更低） 5. 确保数据传输的可靠性和持久性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">82</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is developing a new application. The application uses AWS Lambda functions for its compute tier. The company must use a canary deployment for any changes to the Lambda functions. Automated rollback must occur if any failures are reported. The company&#x27;s DevOps team needs to create the infrastructure as code (IaC) and the CI/CD pipeline for this solution. Which combination of steps will meet these requirements? (Choose three.) that starts the pipeline. Create an AWS CodeBuild project to deploy the AWS Serverless Application Model (AWS SAM) template. Upload the template and source code to the CodeCommit repository. In the CodeCommit repository, create a buildspec.yml file that includes the commands to build and deploy the SAM application. Most Voted F. Create an Amazon CloudWatch alarm for each Lambda function. Configure the alarms to enter the ALARM state if any errors are detected. Configure an evaluation period, dimensions for each Lambda function and version, and the namespace as AWS/Lambda on the Errors metric. Most Voted BCF (89%) 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CloudFormation template for the application. Define each Lambda function in the template by using the AWS::Lambda::Function resource type. In the template, include a version for the Lambda function by using the AWS::Lambda::Version resource type. Declare the CodeSha256 property. Configure an AWS::Lambda::Alias resource that references the latest version of the Lambda function.
B. Create an AWS Serverless Application Model (AWS SAM) template for the application. Define each Lambda function in the template by using the AWS::Serverless::Function resource type. For each function, include configurations for the AutoPublishAlias property and the DeploymentPreference property. Configure the deployment configuration type to LambdaCanary10Percent10Minutes.
C. Create an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage
D. Create an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage that starts the pipeline. Create an AWS CodeDeploy deployment group that is configured for canary deployments with a DeploymentPreference type of Canary10Percent10Minutes. Upload the AWS CloudFormation template and source code to the CodeCommit repository. In the CodeCommit repository, create an appspec.yml file that includes the commands to deploy the CloudFormation template.
E. Create an Amazon CloudWatch composite alarm for all the Lambda functions. Configure an evaluation period and dimensions for Lambda. Configure the alarm to enter the ALARM state if any errors are detected or if there is insufficient data.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在开发一个新应用程序。该应用程序使用AWS Lambda函数作为其计算层。公司必须对Lambda函数的任何更改使用金丝雀部署。如果报告任何故障，必须进行自动回滚。公司的DevOps团队需要为此解决方案创建基础设施即代码(IaC)和CI/CD管道。哪种步骤组合将满足这些要求？（选择三个。） 选项： A. 为应用程序创建AWS CloudFormation模板。使用AWS::Lambda::Function资源类型在模板中定义每个Lambda函数。在模板中，使用AWS::Lambda::Version资源类型包含Lambda函数的版本。声明CodeSha256属性。配置引用Lambda函数最新版本的AWS::Lambda::Alias资源。 B. 为应用程序创建AWS Serverless Application Model (AWS SAM)模板。使用AWS::Serverless::Function资源类型在模板中定义每个Lambda函数。对于每个函数，包含AutoPublishAlias属性和DeploymentPreference属性的配置。将部署配置类型配置为LambdaCanary10Percent10Minutes。 C. 创建AWS CodeCommit存储库。创建AWS CodePipeline管道。在新的源阶段中使用CodeCommit存储库 D. 创建AWS CodeCommit存储库。创建AWS CodePipeline管道。在启动管道的新源阶段中使用CodeCommit存储库。创建配置为金丝雀部署的AWS CodeDeploy部署组，DeploymentPreference类型为Canary10Percent10Minutes。将AWS CloudFormation模板和源代码上传到CodeCommit存储库。在CodeCommit存储库中，创建包含部署CloudFormation模板命令的appspec.yml文件。 E. 为所有Lambda函数创建Amazon CloudWatch复合告警。配置Lambda的评估期和维度。配置告警在检测到任何错误或数据不足时进入ALARM状态。 F. 为每个Lambda函数创建Amazon CloudWatch告警。配置告警在检测到任何错误时进入ALARM状态。为每个Lambda函数和版本配置评估期、维度，并将命名空间设置为AWS/Lambda的Errors指标。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现Lambda函数的金丝雀部署，包含自动回滚功能，并需要创建完整的IaC和CI/CD管道解决方案。 **涉及的关键AWS服务和概念：** - AWS SAM (Serverless Application Model) - 无服务器应用程序的IaC框架 - AWS Lambda - 无服务器计算服务 - 金丝雀部署 - 渐进式部署策略，先部署到少量流量 - AWS CodeCommit/CodePipeline - CI/CD服务 - Amazon CloudWatch - 监控和告警服务 - 自动回滚机制 **正确答案的原因：** 选项B是正确的，因为： 1. AWS SAM专门为无服务器应用程序设计，比纯CloudFormation更适合Lambda部署 2. AutoPublishAlias属性自动创建和管理Lambda版本别名 3. DeploymentPreference属性内置支持金丝雀部署策略 4. LambdaCanary10Percent10Minutes配置实现了10%流量的10分钟金丝雀部署 5. SAM自动集成CloudWatch告警进行健康检查和自动回滚 **其他选项错误的原因：** - 选项A：使用纯CloudFormation需要手动配置复杂的版本管理和部署策略，不如SAM简洁 - 选项C：不完整，缺少具体的部署配置 - 选项D：CodeDeploy主要用于EC2/ECS部署，对Lambda支持有限；appspec.yml不适用于Lambda部署 - 选项E：复合告警过于复杂，且&quot;数据不足&quot;触发告警不合适 - 选项F：虽然CloudWatch告警是必要的，但SAM的DeploymentPreference已经内置了监控机制 **决策标准和最佳实践：** 1. 对于Lambda金丝雀部署，优先选择AWS SAM而非纯CloudFormation 2. 利用SAM内置的部署偏好设置简化配置 3. 确保监控和自动回滚机制的完整性 4. 选择适合无服务器架构的CI/CD工具和配置</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">83</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is deploying a new version of a company&#x27;s application in an AWS CodeDeploy deployment group associated with its Amazon EC2 instances. After some time, the deployment fails. The engineer realizes that all the events associated with the specific deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group. What are valid reasons for this failure? (Choose two.) AD (91%) 9%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. The networking configuration does not allow the EC2 instances to reach the internet via a NAT gateway or internet gateway, and the CodeDeploy endpoint cannot be reached.
B. The IAM user who triggered the application deployment does not have permission to interact with the CodeDeploy endpoint.
C. The target EC2 instances were not properly registered with the CodeDeploy endpoint.
D. An instance profile with proper permissions was not attached to the target EC2 instances.
E. The appspec.yml file was not included in the application revision.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在为公司应用程序的新版本进行部署，使用与Amazon EC2实例关联的AWS CodeDeploy部署组。一段时间后，部署失败了。工程师发现与特定部署ID相关的所有事件都处于Skipped状态，代码没有部署到与部署组关联的实例上。这种失败的有效原因是什么？（选择两个） 选项： A. 网络配置不允许EC2实例通过NAT gateway或internet gateway访问互联网，无法到达CodeDeploy端点。 B. 触发应用程序部署的IAM用户没有与CodeDeploy端点交互的权限。 C. 目标EC2实例没有正确注册到CodeDeploy端点。 D. 没有为目标EC2实例附加具有适当权限的instance profile。 E. 应用程序修订版本中没有包含appspec.yml文件。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodeDeploy部署失败的故障排除，特别是当所有部署事件都显示为&quot;Skipped&quot;状态时的可能原因。题目要求选择两个正确答案，但给出的正确答案只有A选项。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：自动化应用程序部署服务 - EC2实例：目标部署环境 - CodeDeploy Agent：运行在EC2实例上的代理程序 - IAM权限：访问控制和权限管理 - 网络连接：internet gateway、NAT gateway - Instance Profile：EC2实例的IAM角色 **正确答案A的原因：** 选项A是正确的，因为CodeDeploy Agent需要与AWS CodeDeploy服务端点通信来接收部署指令。如果EC2实例无法通过internet gateway或NAT gateway访问互联网，就无法连接到CodeDeploy端点，导致部署事件被跳过（Skipped状态）。 **其他选项错误的原因：** - 选项B：IAM用户权限问题通常会导致部署无法启动，而不是事件被跳过 - 选项C：EC2实例不需要主动&quot;注册&quot;到CodeDeploy端点，而是通过CodeDeploy Agent自动连接 - 选项D：虽然instance profile权限很重要，但这通常会导致其他类型的错误，而不是所有事件都被跳过 - 选项E：缺少appspec.yml文件会导致部署配置错误，但不会导致事件被跳过 **决策标准和最佳实践：** 1. 确保EC2实例具有互联网访问能力（通过internet gateway或NAT gateway） 2. 验证CodeDeploy Agent是否正确安装和运行 3. 检查安全组和网络ACL是否允许必要的出站连接 4. 确保instance profile具有适当的CodeDeploy权限 5. 监控CodeDeploy Agent日志以识别连接问题 注意：题目标注要选择两个答案，但根据分析，选项D也可能是正确的，因为没有适当权限的instance profile也会导致CodeDeploy Agent无法正常工作。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">84</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a guideline that every Amazon EC2 instance must be launched from an AMI that the company&#x27;s security team produces. Every month, the security team sends an email message with the latest approved AMIs to all the development teams. The development teams use AWS CloudFormation to deploy their applications. When developers launch a new service, they have to search their email for the latest AMIs that the security department sent. A DevOps engineer wants to automate the process that the security team uses to provide the AMI IDs to the development teams. What is the MOST scalable solution that meets these requirements? Systems Manager Parameter Store. Instruct the developers to specify a parameter of type SSM in their CloudFormation stack to obtain the most recent AMI ARNs from Parameter Store. Most Voted C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Direct the security team to use CloudFormation to create new versions of the AMIs and to list the AMI ARNs in an encrypted Amazon S3 object as part of the stack&#x27;s Outputs section. Instruct the developers to use a cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs.
B. Direct the security team to use a CloudFormation stack to create an AWS CodePipeline pipeline that builds new AMIs and places the latest AMI ARNs in an encrypted Amazon S3 object as part of the pipeline output. Instruct the developers to use a cross-stack reference within their own CloudFormation template to obtain the S3 object location and the most recent AMI ARNs.
C. Direct the security team to use Amazon EC2 Image Builder to create new AMIs and to place the AMI ARNs as parameters in AWS Systems Manager Parameter Store. Instruct the developers to specify the parameter names in their CloudFormation template.
D. Direct the security team to use Amazon EC2 Image Builder to create new AMIs and to create an Amazon Simple Notification Service (Amazon SNS) topic so that every development team can receive notifications. When the development teams receive a notification, instruct them to write an AWS Lambda function that will update their CloudFormation stack with the most recent AMI ARNs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个准则，要求每个Amazon EC2实例都必须从公司安全团队制作的AMI启动。每个月，安全团队会向所有开发团队发送包含最新批准AMI的邮件消息。开发团队使用AWS CloudFormation来部署他们的应用程序。当开发人员启动新服务时，他们必须在邮件中搜索安全部门发送的最新AMI。一名DevOps工程师希望自动化安全团队向开发团队提供AMI ID的流程。什么是满足这些要求的最具可扩展性的解决方案？ 选项： A. 指导安全团队使用CloudFormation创建新版本的AMI，并将AMI ARN列在加密的Amazon S3对象中作为堆栈输出部分的一部分。指导开发人员使用跨堆栈引用来加载加密的S3对象并获取最新的AMI ARN。 B. 指导安全团队使用CloudFormation堆栈创建AWS CodePipeline管道，该管道构建新的AMI并将最新的AMI ARN放在加密的Amazon S3对象中作为管道输出的一部分。指导开发人员在自己的CloudFormation模板中使用跨堆栈引用来获取S3对象位置和最新的AMI ARN。 C. 指导安全团队使用Amazon EC2 Image Builder创建新的AMI，并将AMI ARN作为参数放在AWS Systems Manager Parameter Store中。指导开发人员在CloudFormation模板中指定参数名称。 D. 指导安全团队使用Amazon EC2 Image Builder创建新的AMI，并创建Amazon Simple Notification Service (Amazon SNS)主题，以便每个开发团队都能收到通知。当开发团队收到通知时，指导他们编写AWS Lambda函数来更新他们的CloudFormation堆栈，使用最新的AMI ARN。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个自动化、可扩展的解决方案，让安全团队能够自动向开发团队提供最新的批准AMI ID，替代目前通过邮件手动分发的方式。关键需求包括：自动化流程、可扩展性、与CloudFormation集成。 **涉及的关键AWS服务和概念：** - Amazon EC2 Image Builder：用于自动化AMI构建和管理的服务 - AWS Systems Manager Parameter Store：用于存储配置数据和密钥的安全存储服务 - AWS CloudFormation：基础设施即代码服务，支持动态参数引用 - Amazon S3：对象存储服务 - AWS CodePipeline：持续集成/持续部署服务 - Amazon SNS：消息通知服务 **正确答案C的原因：** 1. **自动化程度高**：EC2 Image Builder提供完全自动化的AMI构建流程 2. **集成性好**：Parameter Store与CloudFormation原生集成，开发人员可以直接在模板中引用参数 3. **可扩展性强**：Parameter Store支持大量参数存储，无需管理额外基础设施 4. **实时更新**：当Parameter Store中的值更新时，CloudFormation可以自动获取最新值 5. **安全性**：Parameter Store支持加密存储，符合安全要求 6. **简单易用**：开发团队只需在CloudFormation模板中指定参数名称即可 **其他选项错误的原因：** - **选项A**：使用S3存储AMI信息增加了复杂性，需要额外的权限管理和跨堆栈引用配置，不如Parameter Store直接 - **选项B**：虽然使用了CodePipeline自动化构建，但仍然依赖S3存储和复杂的跨堆栈引用，增加了不必要的复杂性 - **选项D**：虽然使用了EC2 Image Builder，但仍然需要手动干预（Lambda函数更新），没有实现完全自动化，且需要开发团队编写额外代码 **决策标准和最佳实践：** 1. **选择专用服务**：Parameter Store专门用于配置管理，比通用存储服务更适合 2. **最小化复杂性**：避免不必要的跨服务集成和手动干预 3. **原生集成优先**：利用AWS服务间的原生集成能力 4. **自动化优先**：选择能够实现端到端自动化的解决方案 5. **可维护性**：选择易于维护和扩展的架构</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">85</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). A DevOps engineer is using AWS CodeDeploy to release a new version. The deployment fails during the AllowTraffic lifecycle event, but a cause for the failure is not indicated in the deployment logs. What would cause this? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. The appspec.yml file contains an invalid script that runs in the AllowTraffic lifecycle hook.
B. The user who initiated the deployment does not have the necessary permissions to interact with the ALB.
C. The health checks specified for the ALB target group are misconfigured.
D. The CodeDeploy agent was not installed in the EC2 instances that are part of the ALB target group.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个应用程序运行在Application Load Balancer (ALB)后面的Amazon EC2实例上。DevOps工程师正在使用AWS CodeDeploy发布新版本。部署在AllowTraffic生命周期事件期间失败，但部署日志中没有显示失败的原因。什么会导致这种情况？ 选项： A. appspec.yml文件包含在AllowTraffic生命周期钩子中运行的无效脚本 B. 发起部署的用户没有与ALB交互的必要权限 C. 为ALB目标组指定的健康检查配置错误 D. CodeDeploy代理未安装在作为ALB目标组一部分的EC2实例中</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodeDeploy在Blue/Green部署模式下与Application Load Balancer集成时，AllowTraffic生命周期事件失败但日志中没有明确错误信息的故障排除能力。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：自动化部署服务，支持Blue/Green部署 - Application Load Balancer (ALB)：七层负载均衡器 - AllowTraffic生命周期事件：CodeDeploy在Blue/Green部署中将流量从旧实例切换到新实例的阶段 - 健康检查机制：ALB用来确定目标实例是否健康的检测方式 **正确答案C的原因：** 在Blue/Green部署的AllowTraffic阶段，CodeDeploy会将新部署的实例注册到ALB目标组中。如果ALB的健康检查配置错误（如检查路径不存在、超时时间过短、健康阈值设置不当等），新实例无法通过健康检查，ALB不会将流量路由到这些实例。这种情况下，CodeDeploy会检测到流量切换失败，但由于问题出在ALB层面的健康检查，CodeDeploy的部署日志中不会显示具体的失败原因。 **其他选项错误的原因：** A. 如果appspec.yml中的脚本有问题，CodeDeploy会在日志中明确记录脚本执行错误，与题目描述的&quot;日志中没有失败原因&quot;不符。 B. 权限问题会在部署开始阶段就被发现，CodeDeploy会在日志中明确记录权限相关的错误信息，不符合题目场景。 D. 如果CodeDeploy代理未安装，部署会在更早的阶段失败（如ApplicationStop或BeforeInstall），而不是在AllowTraffic阶段，且会有明确的代理连接失败日志。 **决策标准和最佳实践：** 1. 理解CodeDeploy生命周期事件的执行顺序和各阶段的作用 2. 掌握Blue/Green部署中ALB健康检查的重要性 3. 学会根据故障现象（特定阶段失败+日志无明确信息）推断根本原因 4. 配置ALB健康检查时要确保检查路径、端口、协议与应用实际情况匹配 5. 设置合理的健康检查间隔、超时和阈值参数</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">86</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has 20 service teams. Each service team is responsible for its own microservice. Each service team uses a separate AWS account for its microservice and a VPC with the 192.168.0.0/22 CIDR block. The company manages the AWS accounts with AWS Organizations. Each service team hosts its microservice on multiple Amazon EC2 instances behind an Application Load Balancer. The microservices communicate with each other across the public internet. The company&#x27;s security team has issued a new guideline that all communication between microservices must use HTTPS over private network connections and cannot traverse the public internet. A DevOps engineer must implement a solution that fulfills these obligations and minimizes the number of changes for each service team. Which solution will meet these requirements? for communication between microservices. Most Voted B (77%) D (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a new AWS account in AWS Organizations. Create a VPC in this account, and use AWS Resource Access Manager to share the private subnets of this VPC with the organization. Instruct the service teams to launch a new Network Load Balancer (NLB) and EC2 instances that use the shared private subnets. Use the NLB DNS names for communication between microservices.
B. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use AWS PrivateLink to create VPC endpoints in each AWS account for the NLBs. Create subscriptions to each VPC endpoint in each of the other AWS accounts. Use the VPC endpoint DNS names for communication between microservices.
C. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Create VPC peering connections between each of the microservice VPCs. Update the route tables for each VPC to use the peering links. Use the NLB DNS names for communication between microservices.
D. Create a new AWS account in AWS Organizations. Create a transit gateway in this account, and use AWS Resource Access Manager to share the transit gateway with the organization. In each of the microservice VPCs, create a transit gateway attachment to the shared transit gateway. Update the route tables of each VPC to use the transit gateway. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use the NLB DNS names for communication between microservices.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有20个服务团队。每个服务团队负责自己的微服务。每个服务团队为其微服务使用单独的AWS账户和一个CIDR块为192.168.0.0/22的VPC。公司使用AWS Organizations管理AWS账户。每个服务团队在Application Load Balancer后面的多个Amazon EC2实例上托管其微服务。微服务之间通过公共互联网相互通信。公司的安全团队发布了新的指导原则，要求所有微服务之间的通信必须在私有网络连接上使用HTTPS，不能通过公共互联网传输。DevOps工程师必须实施一个满足这些要求并最小化每个服务团队更改数量的解决方案。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 20个服务团队，每个团队有独立的AWS账户和VPC 2. 微服务间通信必须使用HTTPS通过私有网络连接 3. 不能通过公共互联网通信 4. 最小化每个服务团队的更改数量 **涉及的关键AWS服务和概念：** - AWS PrivateLink：提供跨VPC的私有连接服务 - Network Load Balancer (NLB)：支持PrivateLink的负载均衡器 - VPC Endpoint：PrivateLink的接入点 - Transit Gateway：用于多VPC互连的网络中转服务 - VPC Peering：点对点VPC连接 - AWS Resource Access Manager：跨账户资源共享 **正确答案B的原因：** 1. **私有通信**：AWS PrivateLink确保所有通信都在AWS骨干网络内进行，不经过公共互联网 2. **最小化更改**：每个团队只需在现有VPC中创建NLB和VPC endpoint，无需重新架构 3. **跨账户支持**：PrivateLink天然支持跨AWS账户的私有连接 4. **安全性**：VPC endpoint提供了安全的服务访问方式，支持HTTPS 5. **可扩展性**：每个服务可以选择性地订阅需要的VPC endpoint **其他选项错误的原因：** - **选项A**：要求服务团队迁移到共享子网，这是重大架构变更，不符合&quot;最小化更改&quot;要求 - **选项C**：VPC Peering在20个VPC之间需要创建190个连接（n*(n-1)/2），管理复杂度极高，且所有VPC使用相同CIDR块会导致路由冲突 - **选项D**：虽然Transit Gateway可以解决连接问题，但需要额外的AWS账户和复杂的路由配置，增加了管理开销 **决策标准和最佳实践：** 1. **架构简洁性**：PrivateLink提供了最简洁的跨账户私有通信方案 2. **运维复杂度**：避免复杂的网络拓扑和路由管理 3. **安全最佳实践**：使用AWS托管的私有连接服务 4. **成本效益**：PrivateLink按使用量计费，相比维护复杂网络架构更经济 5. **渐进式迁移**：允许服务团队逐步迁移，不需要同时改变所有服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">87</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An Amazon EC2 instance is running in a VPC and needs to download an object from a restricted Amazon S3 bucket. When the DevOps engineer tries to download the object, an AccessDenied error is received. What are the possible causes for this error? (Choose two.) BD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. The S3 bucket default encryption is enabled.
B. There is an error in the S3 bucket policy.
C. The object has been moved to S3 Glacier.
D. There is an error in the IAM role configuration.
E. S3 Versioning is enabled.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个Amazon EC2实例运行在VPC中，需要从一个受限的Amazon S3存储桶下载对象。当DevOps工程师尝试下载对象时，收到了AccessDenied错误。这个错误可能的原因是什么？（选择两个）BD（100%） 选项：A. S3存储桶默认加密已启用。B. S3存储桶策略中有错误。C. 对象已被移动到S3 Glacier。D. IAM角色配置中有错误。E. S3版本控制已启用。 正确答案：B</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS中S3访问权限控制的故障排除，要求识别导致AccessDenied错误的可能原因。题目明确要求选择两个答案，但给出的正确答案只有B，这可能是题目标注有误。 **涉及的关键AWS服务和概念：** - Amazon S3访问控制机制 - IAM角色和权限策略 - S3存储桶策略 - VPC中EC2实例的权限配置 - S3对象访问权限验证流程 **正确答案的原因：** 选项B（S3存储桶策略中有错误）是正确的，因为： - S3存储桶策略是控制对存储桶和对象访问的主要机制之一 - 如果存储桶策略配置错误，比如明确拒绝某些操作或没有授予必要的权限，就会导致AccessDenied错误 - 对于&quot;受限的S3存储桶&quot;，存储桶策略配置错误是最常见的访问被拒绝原因 选项D（IAM角色配置中有错误）也应该是正确答案，因为： - EC2实例通过IAM角色获取访问S3的权限 - 如果IAM角色没有被正确配置或缺少必要的S3权限，也会导致AccessDenied错误 **其他选项错误的原因：** - 选项A：S3默认加密不会导致AccessDenied错误，它只是在存储时加密数据，不影响访问权限 - 选项C：对象移动到S3 Glacier不会产生AccessDenied错误，而是会产生不同类型的错误或需要先恢复对象 - 选项E：S3版本控制启用本身不会导致访问被拒绝，它只是管理对象的多个版本 **决策标准和最佳实践：** - S3访问控制遵循最小权限原则，需要同时检查IAM权限和存储桶策略 - 排查AccessDenied错误时，应按顺序检查：IAM用户/角色权限 → 存储桶策略 → 对象ACL - 使用AWS CloudTrail可以帮助诊断权限问题的具体原因 - 建议使用IAM策略模拟器来测试权限配置的正确性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">88</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to use a grid system for a proprietary enterprise in-memory data store on top of AWS. This system can run in multiple server nodes in any Linux-based distribution. The system must be able to reconfigure the entire cluster every time a node is added or removed. When adding or removing nodes, an /etc/cluster/nodes.config file must be updated, listing the IP addresses of the current node members of that cluster. The company wants to automate the task of adding new nodes to a cluster. What can a DevOps engineer do to meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS OpsWorks Stacks to layer the server nodes of that cluster. Create a Chef recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layer. Assign that recipe to the Configure lifecycle event.
B. Put the nodes.config file in version control. Create an AWS CodeDeploy deployment configuration and deployment group based on an Amazon EC2 tag value for the cluster nodes. When adding a new node to the cluster, update the file with all tagged instances, and make a commit in version control. Deploy the new file and restart the services.
C. Create an Amazon S3 bucket and upload a version of the /etc/cluster/nodes.config file. Create a crontab script that will poll for that S3 file and download it frequently. Use a process manager, such as Monit or systemd, to restart the cluster services when it detects that the new file was modified. When adding a node to the cluster, edit the file&#x27;s most recent members. Upload the new file to the S3 bucket.
D. Create a user data script that lists all members of the current security group of the cluster and automatically updates the /etc/cluster/nodes.config file whenever a new instance is added to the cluster.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司希望在AWS上为专有企业内存数据存储使用网格系统。该系统可以在任何基于Linux发行版的多个服务器节点上运行。系统必须能够在每次添加或删除节点时重新配置整个集群。在添加或删除节点时，必须更新/etc/cluster/nodes.config文件，列出该集群当前节点成员的IP地址。公司希望自动化向集群添加新节点的任务。DevOps工程师可以做什么来满足这些要求？ 选项： A. 使用AWS OpsWorks Stacks对该集群的服务器节点进行分层。创建一个Chef recipe来填充/etc/cluster/nodes.config文件的内容，并使用该层的当前成员重启服务。将该recipe分配给Configure生命周期事件。 B. 将nodes.config文件放入版本控制。基于集群节点的Amazon EC2标签值创建AWS CodeDeploy部署配置和部署组。向集群添加新节点时，用所有标记的实例更新文件，并在版本控制中提交。部署新文件并重启服务。 C. 创建Amazon S3存储桶并上传/etc/cluster/nodes.config文件的版本。创建crontab脚本定期轮询该S3文件并下载。使用进程管理器（如Monit或systemd）在检测到新文件被修改时重启集群服务。向集群添加节点时，编辑文件的最新成员并上传新文件到S3存储桶。 D. 创建用户数据脚本，列出集群当前安全组的所有成员，并在新实例添加到集群时自动更新/etc/cluster/nodes.config文件。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动化解决方案，能够在集群节点动态增减时自动更新配置文件并重新配置整个集群。关键需求包括：1）自动检测节点变化；2）更新/etc/cluster/nodes.config文件；3）重启相关服务；4）实现完全自动化。 **涉及的关键AWS服务和概念：** - AWS OpsWorks Stacks：基础设施管理服务，支持Chef配置管理 - Chef recipes：自动化配置脚本 - 生命周期事件：OpsWorks中的自动触发机制 - AWS CodeDeploy：应用程序部署服务 - Amazon S3：对象存储服务 - EC2用户数据脚本：实例启动时执行的脚本 **正确答案A的原因：** AWS OpsWorks Stacks是最适合这种场景的服务，因为：1）它专门设计用于管理应用程序栈和层；2）Configure生命周期事件会在节点添加/删除时自动触发；3）Chef recipe可以动态获取当前层的所有成员信息；4）能够自动执行配置更新和服务重启；5）提供了完整的自动化解决方案，无需人工干预。 **其他选项错误的原因：** 选项B（CodeDeploy）：需要手动更新版本控制中的配置文件，不能实现完全自动化，每次添加节点都需要人工操作。选项C（S3+cron）：依赖定时轮询机制，存在延迟问题，且需要手动编辑和上传文件，自动化程度低。选项D（用户数据脚本）：只在实例启动时执行一次，无法处理节点删除的情况，且不能实现集群范围的配置同步。 **决策标准和最佳实践：** 选择自动化解决方案时应考虑：1）事件驱动vs定时轮询（事件驱动响应更及时）；2）自动化程度（减少人工干预）；3）可靠性和一致性；4）AWS原生服务集成度。OpsWorks Stacks作为专门的配置管理服务，在处理动态集群管理方面具有天然优势，是此类场景的最佳实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">89</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is working on a data archival project that requires the migration of on-premises data to an Amazon S3 bucket. The DevOps engineer develops a script that incrementally archives on-premises data that is older than 1 month to Amazon S3. Data that is transferred to Amazon S3 is deleted from the on-premises location. The script uses the S3 PutObject operation. During a code review, the DevOps engineer notices that the script does not verify whether the data was successfully copied to Amazon S3. The DevOps engineer must update the script to ensure that data is not corrupted during transmission. The script must use MD5 checksums to verify data integrity before the on-premises data is deleted. Which solutions for the script will meet these requirements? (Choose two.) BD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Check the returned response for the VersionId. Compare the returned VersionId against the MD5 checksum.
B. Include the MD5 checksum within the Content-MD5 parameter. Check the operation call&#x27;s return status to find out if an error was returned.
C. Include the checksum digest within the tagging parameter as a URL query parameter.
D. Check the returned response for the ETag. Compare the returned ETag against the MD5 checksum.
E. Include the checksum digest within the Metadata parameter as a name-value pair. After upload, use the S3 HeadObject operation to retrieve metadata from the object.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师正在进行数据归档项目，需要将本地数据迁移到Amazon S3存储桶。该DevOps工程师开发了一个脚本，将超过1个月的本地数据增量归档到Amazon S3。传输到Amazon S3的数据会从本地位置删除。该脚本使用S3 PutObject操作。在代码审查期间，DevOps工程师注意到脚本没有验证数据是否成功复制到Amazon S3。DevOps工程师必须更新脚本以确保数据在传输过程中不会损坏。脚本必须使用MD5校验和来验证数据完整性，然后才能删除本地数据。脚本的哪些解决方案能满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现数据传输完整性验证机制，确保在删除本地数据之前验证文件已成功且完整地上传到S3。核心需求是使用MD5校验和进行数据完整性验证。 **涉及的关键AWS服务和概念：** - Amazon S3 PutObject API操作 - Content-MD5参数：用于请求级别的完整性检查 - ETag：S3返回的实体标签，通常是文件的MD5哈希值 - S3对象元数据、标签和版本控制概念 **正确答案的原因：** 选项B正确：Content-MD5参数是S3 PutObject操作的标准完整性验证机制。当在请求中包含Content-MD5参数时，S3会自动计算上传数据的MD5校验和并与提供的值进行比较。如果不匹配，S3会返回错误，确保数据完整性。 选项D正确：ETag通常包含对象的MD5哈希值（对于简单上传）。通过比较返回的ETag与本地计算的MD5校验和，可以验证数据完整性。 **其他选项错误的原因：** 选项A错误：VersionId是版本控制功能的标识符，与MD5校验和无关，不能用于数据完整性验证。 选项C错误：标签参数用于对象分类和管理，不是完整性验证的标准方法，且作为URL查询参数传递校验和不是最佳实践。 选项E错误：虽然可以在元数据中存储校验和，但这种方法需要额外的HeadObject调用，增加了复杂性和API调用成本，不如直接使用Content-MD5或ETag验证高效。 **决策标准和最佳实践：** 1. 优先使用S3原生的完整性验证机制（Content-MD5） 2. 利用S3返回的标准响应字段（ETag）进行验证 3. 避免不必要的额外API调用 4. 选择简单、可靠且成本效益高的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">90</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company deploys updates to its Amazon API Gateway API several times a week by using an AWS CodePipeline pipeline. As part of the update process, the company exports the JavaScript SDK for the API from the API Gateway console and uploads the SDK to an Amazon S3 bucket. The company has configured an Amazon CloudFront distribution that uses the S3 bucket as an origin. Web clients then download the SDK by using the CloudFront distribution&#x27;s endpoint. A DevOps engineer needs to implement a solution to make the new SDK available automatically during new API deployments. Which solution will meet these requirements? Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the CloudFront API to create an invalidation for the SDK path. Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the S3 API to invalidate the cache for the SDK path. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CodePipeline action immediately after the deployment stage of the API. Configure the action to invoke an AWS Lambda function. Configure the Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and create a CloudFront invalidation for the SDK path.
B. Create a CodePipeline action immediately after the deployment stage of the API. Configure the action to use the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Create another action that uses the CodePipeline integration with Amazon S3 to invalidate the cache for the SDK path.
C. Create an Amazon EventBridge rule that reacts to UpdateStage events from aws.apigateway. Configure the rule to invoke an AWS
D. Create an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure the rule to invoke an AWS</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司每周多次使用AWS CodePipeline管道向其Amazon API Gateway API部署更新。作为更新过程的一部分，公司从API Gateway控制台导出API的JavaScript SDK，并将SDK上传到Amazon S3存储桶。公司配置了一个使用S3存储桶作为源的Amazon CloudFront分发。然后Web客户端通过CloudFront分发的端点下载SDK。DevOps工程师需要实施一个解决方案，在新的API部署期间自动使新的SDK可用。哪个解决方案能满足这些要求？ 选项A：在API部署阶段之后立即创建一个CodePipeline操作。配置该操作调用AWS Lambda函数。配置Lambda函数从API Gateway下载SDK，将SDK上传到S3存储桶，并为SDK路径创建CloudFront失效。 选项B：在API部署阶段之后立即创建一个CodePipeline操作。配置该操作使用CodePipeline与API Gateway的集成将SDK导出到Amazon S3。创建另一个使用CodePipeline与Amazon S3集成的操作来使SDK路径的缓存失效。 选项C：创建一个Amazon EventBridge规则，响应来自aws.apigateway的UpdateStage事件。配置规则调用AWS... 选项D：创建一个Amazon EventBridge规则，响应来自aws.apigateway的CreateDeployment事件。配置规则调用AWS...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现API部署时自动更新SDK的完整流程，包括：1）从API Gateway导出最新SDK；2）上传到S3存储桶；3）使CloudFront缓存失效以确保客户端获取最新版本。关键是要在CodePipeline部署流程中无缝集成这个自动化过程。 **涉及的关键AWS服务和概念：** - CodePipeline：CI/CD管道服务，需要在部署阶段后添加自动化操作 - API Gateway：提供SDK导出功能，部署后需要获取最新SDK - S3：存储SDK文件的对象存储服务 - CloudFront：CDN服务，缓存S3中的SDK文件，需要缓存失效机制 - Lambda：执行自动化任务的无服务器计算服务 - EventBridge：事件驱动架构服务 **正确答案A的原因：** 选项A提供了最直接和可控的解决方案。通过在CodePipeline中添加Lambda操作，可以确保在API部署完成后立即执行SDK更新流程。Lambda函数可以程序化地调用API Gateway的SDK导出API，上传到S3，并调用CloudFront的InvalidateCache API。这种方法与现有的CodePipeline流程完美集成，提供了端到端的自动化和错误处理能力。 **其他选项错误的原因：** 选项B错误：CodePipeline没有直接的API Gateway SDK导出集成，也没有与S3的缓存失效集成功能。这个选项描述的集成实际上不存在。 选项C和D错误：虽然EventBridge可以响应API Gateway事件，但这种方法存在几个问题：1）与现有CodePipeline流程分离，难以统一管理和监控；2）事件驱动的方式可能存在时序问题；3）缺乏与部署流程的紧密集成，无法在部署失败时阻止SDK更新。 **决策标准和最佳实践：** 1. **流程集成性**：选择能与现有CodePipeline无缝集成的方案 2. **可控性**：确保SDK更新过程可控，能够处理异常情况 3. **时序保证**：SDK更新必须在API部署成功后进行 4. **完整性**：解决方案必须包含缓存失效步骤，确保客户端获取最新版本 5. **可维护性**：选择易于监控、调试和维护的架构模式</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">91</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has developed an AWS Lambda function that handles orders received through an API. The company is using AWS CodeDeploy to deploy the Lambda function as the final stage of a CI/CD pipeline. A DevOps engineer has noticed there are intermittent failures of the ordering API for a few seconds after deployment. After some investigation, the DevOps engineer believes the failures are due to database changes not having fully propagated before the Lambda function is invoked. How should the DevOps engineer overcome this? new version of the Lambda function. Most Voted version of the Lambda function to respond. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a BeforeAllowTraffic hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the
B. Add an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new
C. Add a BeforeInstall hook to the AppSpec file that tests and waits for any necessary database changes before deploying the new version of the Lambda function.
D. Add a ValidateService hook to the AppSpec file that inspects incoming traffic and rejects the payload if dependent services, such as the database, are not yet ready.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司开发了一个AWS Lambda函数来处理通过API接收的订单。该公司使用AWS CodeDeploy在CI/CD流水线的最后阶段部署Lambda函数。一名DevOps工程师注意到在部署后的几秒钟内，订单API会出现间歇性故障。经过一些调查，DevOps工程师认为故障是由于数据库更改在Lambda函数被调用之前没有完全传播造成的。DevOps工程师应该如何解决这个问题？ 选项： A. 在AppSpec文件中添加BeforeAllowTraffic钩子，在流量流向新版本Lambda函数之前测试并等待任何必要的数据库更改 B. 在AppSpec文件中添加AfterAllowTraffic钩子，在允许新版本Lambda函数响应之前强制流量等待任何待处理的数据库更改 C. 在AppSpec文件中添加BeforeInstall钩子，在部署新版本Lambda函数之前测试并等待任何必要的数据库更改 D. 在AppSpec文件中添加ValidateService钩子，检查传入流量并在依赖服务（如数据库）尚未准备就绪时拒绝负载</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何解决Lambda函数部署后由于数据库更改未完全传播而导致的间歇性API故障问题。需要在CodeDeploy部署过程中添加适当的钩子来确保数据库状态与新版本Lambda函数同步。 **涉及的关键AWS服务和概念：** - AWS Lambda：无服务器计算服务 - AWS CodeDeploy：自动化部署服务 - AppSpec文件：CodeDeploy的部署配置文件 - Lambda部署钩子：BeforeAllowTraffic、AfterAllowTraffic、BeforeInstall、ValidateService等生命周期钩子 **正确答案A的原因：** BeforeAllowTraffic钩子在Lambda函数部署完成但流量切换到新版本之前执行。这个时机最适合验证数据库更改是否已完全传播，确保在用户流量到达新版本函数之前，所有依赖的数据库更改都已就绪。这样可以避免新版本函数处理请求时遇到数据不一致的问题。 **其他选项错误的原因：** - 选项B（AfterAllowTraffic）：此钩子在流量已经开始流向新版本后执行，为时已晚，无法防止故障发生 - 选项C（BeforeInstall）：此钩子在函数安装之前执行，时机过早，此时新版本函数还未部署，无法确保部署时的数据库状态 - 选项D（ValidateService）：这不是CodeDeploy Lambda部署的标准钩子，且描述的功能更像是运行时验证而非部署时验证 **决策标准和最佳实践：** 1. 选择合适的部署钩子时机：需要在新版本准备就绪但流量切换前进行验证 2. 确保数据一致性：在允许新版本处理生产流量前验证所有依赖服务状态 3. 遵循CodeDeploy Lambda部署生命周期：理解各个钩子的执行顺序和适用场景 4. 实施健康检查：在关键的部署节点添加验证逻辑，确保系统整体就绪</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">92</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses a single AWS account to test applications on Amazon EC2 instances. The company has turned on AWS Config in the AWS account and has activated the restricted-ssh AWS Config managed rule. The company needs an automated monitoring solution that will provide a customized notification in real time if any security group in the account is not compliant with the restricted-ssh rule. The customized notification must contain the name and ID of the noncompliant security group. A DevOps engineer creates an Amazon Simple Notification Service (Amazon SNS) topic in the account and subscribes the appropriate personnel to the topic. What should the DevOps engineer do next to meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure an input transformer for the EventBridge rule. Configure the EventBridge rule to publish a notification to the SNS topic.
B. Configure AWS Config to send all evaluation results for the restricted-ssh rule to the SNS topic. Configure a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers.
C. Create an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure the EventBridge rule to invoke AWS Systems Manager Run Command on the SNS topic to customize a notification and to publish the notification to the SNS topic.
D. Create an Amazon EventBridge rule that matches all AWS Config evaluation results of NON_COMPLIANT. Configure an input transformer for the restricted-ssh rule. Configure the EventBridge rule to publish a notification to the SNS topic.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用单个AWS账户在Amazon EC2实例上测试应用程序。该公司已在AWS账户中启用了AWS Config，并激活了restricted-ssh AWS Config托管规则。公司需要一个自动化监控解决方案，当账户中任何安全组不符合restricted-ssh规则时，能够实时提供定制化通知。定制化通知必须包含不合规安全组的名称和ID。DevOps工程师在账户中创建了Amazon Simple Notification Service (Amazon SNS)主题，并为相关人员订阅了该主题。DevOps工程师接下来应该做什么来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 需要实时监控AWS Config的restricted-ssh规则合规性 - 当出现NON_COMPLIANT状态时自动发送通知 - 通知必须是定制化的，包含不合规安全组的具体名称和ID - 通过已创建的SNS主题发送通知 **涉及的关键AWS服务和概念：** - AWS Config：配置合规性监控服务，restricted-ssh是预定义的托管规则 - Amazon EventBridge：事件驱动架构的核心服务，可以捕获AWS Config的评估结果 - Input Transformer：EventBridge的功能，可以自定义事件数据格式 - Amazon SNS：消息通知服务 - AWS Systems Manager Run Command：系统管理工具 **正确答案A的原因：** - EventBridge可以精确匹配AWS Config针对restricted-ssh规则的NON_COMPLIANT评估结果 - Input Transformer功能可以从原始事件中提取安全组名称和ID，创建定制化通知内容 - 直接发布到SNS主题，架构简洁高效 - 满足实时性要求，因为EventBridge可以立即响应Config评估事件 **其他选项错误的原因：** - 选项B：AWS Config本身不能直接发送评估结果到SNS，且SNS过滤策略无法提供所需的定制化内容提取 - 选项C：使用Systems Manager Run Command过于复杂且不必要，Run Command主要用于在EC2实例上执行命令，不适用于消息定制化场景 - 选项D：匹配所有NON_COMPLIANT结果会产生不相关的通知，且Input Transformer配置描述不准确 **决策标准和最佳实践：** - 选择最直接、最简单的解决方案路径 - 利用EventBridge的事件过滤和转换能力 - 避免过度工程化，不使用不必要的服务组件 - 确保解决方案的实时性和准确性 - 遵循AWS服务间的最佳集成模式</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">93</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company requires an RPO of 2 hours and an RTO of 10 minutes for its data and application at all times. An application uses a MySQL database and Amazon EC2 web servers. The development team needs a strategy for failover and disaster recovery. Which combination of deployment strategies will meet these requirements? (Choose two.) BD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon Aurora cluster in one Availability Zone across multiple Regions as the data store. Use Aurora&#x27;s automatic recovery capabilities in the event of a disaster.
B. Create an Amazon Aurora global database in two Regions as the data store. In the event of a failure, promote the secondary Region as the primary for the application.
C. Create an Amazon Aurora multi-master cluster across multiple Regions as the data store. Use a Network Load Balancer to balance the database traffic in different Regions.
D. Set up the application in two Regions and use Amazon Route 53 failover-based routing that points to the Application Load Balancers in both Regions. Use health checks to determine the availability in a given Region. Use Auto Scaling groups in each Region to adjust capacity based on demand.
E. Set up the application in two Regions and use a multi-Region Auto Scaling group behind Application Load Balancers to manage the capacity based on demand. In the event of a disaster, adjust the Auto Scaling group&#x27;s desired instance count to increase baseline capacity in the failover Region.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司要求其数据和应用程序在任何时候都具有2小时的RPO和10分钟的RTO。应用程序使用MySQL数据库和Amazon EC2 web服务器。开发团队需要一个故障转移和灾难恢复策略。哪种部署策略组合将满足这些要求？（选择两个。）BD（100%） 选项：A. 在一个Availability Zone中跨多个Region创建Amazon Aurora集群作为数据存储。在发生灾难时使用Aurora的自动恢复功能。 B. 在两个Region中创建Amazon Aurora global database作为数据存储。在发生故障时，将辅助Region提升为应用程序的主Region。 C. 跨多个Region创建Amazon Aurora multi-master集群作为数据存储。使用Network Load Balancer来平衡不同Region中的数据库流量。 D. 在两个Region中设置应用程序，使用Amazon Route 53基于故障转移的路由指向两个Region中的Application Load Balancer。使用健康检查来确定给定Region的可用性。在每个Region中使用Auto Scaling groups根据需求调整容量。 E. 在两个Region中设置应用程序，在Application Load Balancer后面使用multi-Region Auto Scaling group根据需求管理容量。在发生灾难时，调整Auto Scaling group的期望实例数量以增加故障转移Region中的基线容量。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是满足严格RTO（恢复时间目标：10分钟）和RPO（恢复点目标：2小时）要求的灾难恢复架构设计。需要为MySQL数据库和EC2 web服务器设计跨Region的高可用性解决方案。 **涉及的关键AWS服务和概念：** - Amazon Aurora Global Database：跨Region数据库复制服务 - Amazon Aurora Multi-Master：多主节点数据库集群 - Route 53：DNS故障转移路由 - Application Load Balancer：应用程序负载均衡器 - Auto Scaling Groups：自动扩展组 - RTO/RPO概念：恢复时间和恢复点目标 **正确答案的原因：** 选项B和D的组合是正确的： - **选项B**：Aurora Global Database提供跨Region的异步复制，RPO通常在1秒以内（远优于2小时要求），故障转移时间约1分钟（满足10分钟RTO要求） - **选项D**：Route 53健康检查和故障转移路由可以在几分钟内将流量重定向到健康的Region，配合Auto Scaling确保应用层的高可用性 **其他选项错误的原因：** - **选项A**：描述有误，不能在&quot;一个AZ跨多个Region&quot;创建集群，概念上不合理 - **选项C**：Aurora Multi-Master目前不支持跨Region部署，只能在单Region内的多AZ中使用 - **选项E**：Multi-Region Auto Scaling Group不是AWS的标准服务，Auto Scaling Group是Region级别的服务 **决策标准和最佳实践：** 1. **数据层**：选择Aurora Global Database确保跨Region数据复制和快速故障转移 2. **应用层**：使用Route 53 + ALB + Auto Scaling实现应用程序的跨Region部署和自动故障转移 3. **监控**：利用Route 53健康检查主动监测Region可用性 4. **自动化**：通过Auto Scaling确保故障转移后有足够的计算资源 5. **成本优化**：在正常情况下可以在备用Region运行最小容量，故障时自动扩展</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">94</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A business has an application that consists of five independent AWS Lambda functions. The DevOps engineer has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each Lambda function in sequence. The pipeline uses an Amazon EventBridge rule to ensure the pipeline starts as quickly as possible after a change is made to the application source code. After working with the pipeline for a few months, the DevOps engineer has noticed the pipeline takes too long to complete. What should the DevOps engineer implement to BEST improve the speed of the pipeline? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the CodeBuild projects within the pipeline to use a compute type with more available network throughput.
B. Create a custom CodeBuild execution environment that includes a symmetric multiprocessing configuration to run the builds in parallel.
C. Modify the CodePipeline configuration to run actions for each Lambda function in parallel by specifying the same runOrder.
D. Modify each CodeBuild project to run within a VPC and use dedicated instances to increase throughput.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家企业有一个由五个独立的AWS Lambda函数组成的应用程序。DevOps工程师使用AWS CodePipeline和AWS CodeBuild构建了一个CI/CD管道，该管道按顺序构建、测试、打包和部署每个Lambda函数。该管道使用Amazon EventBridge规则来确保在应用程序源代码发生更改后尽快启动管道。在使用该管道几个月后，DevOps工程师注意到管道完成时间过长。DevOps工程师应该实施什么来最好地提高管道的速度？ 选项： A. 修改管道中的CodeBuild项目，使用具有更多可用网络吞吐量的计算类型。 B. 创建一个自定义的CodeBuild执行环境，包含对称多处理配置以并行运行构建。 C. 修改CodePipeline配置，通过指定相同的runOrder来并行运行每个Lambda函数的操作。 D. 修改每个CodeBuild项目在VPC内运行并使用专用实例来增加吞吐量。 正确答案：C</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何优化CI/CD管道的执行速度。当前问题是五个独立的Lambda函数按顺序（串行）进行构建、测试、打包和部署，导致整个管道执行时间过长。需要找到最有效的方法来提高管道速度。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务，支持阶段和操作的编排 - AWS CodeBuild：构建服务，负责代码编译、测试和打包 - AWS Lambda：无服务器计算服务 - runOrder：CodePipeline中控制操作执行顺序的参数 - 并行处理 vs 串行处理的概念 **正确答案C的原因：** 1. **架构层面优化**：五个Lambda函数是独立的，没有相互依赖关系，完全可以并行处理 2. **runOrder机制**：在CodePipeline中，相同runOrder值的操作会并行执行，不同runOrder值的操作按顺序执行 3. **最大化效率**：将串行执行改为并行执行可以显著减少总执行时间（理论上可减少到原来的1/5） 4. **成本效益**：这是一个配置层面的改动，不需要额外的计算资源投入 **其他选项错误的原因：** - **选项A**：增加网络吞吐量只能在一定程度上提升单个构建的速度，但无法解决串行执行的根本问题 - **选项B**：对称多处理主要优化单个构建任务内部的并行处理，对于多个独立Lambda函数的场景效果有限 - **选项D**：VPC配置和专用实例会增加复杂性和成本，且可能因为网络延迟反而降低性能 **决策标准和最佳实践：** 1. **识别瓶颈**：首先分析是架构问题还是资源问题 2. **利用并行性**：对于独立的组件，优先考虑并行处理 3. **成本效益分析**：优先选择配置优化而非硬件升级 4. **AWS服务特性**：充分利用CodePipeline的runOrder功能来实现并行执行 5. **可扩展性**：并行架构为未来添加更多Lambda函数提供了良好的扩展性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">95</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS CloudFormation stacks to deploy updates to its application. The stacks consist of different resources. The resources include AWS Auto Scaling groups, Amazon EC2 instances, Application Load Balancers (ALBs), and other resources that are necessary to launch and maintain independent stacks. Changes to application resources outside of CloudFormation stack updates are not allowed. The company recently attempted to update the application stack by using the AWS CLI. The stack failed to update and produced the following error message: &quot;ERROR: both the deployment and the CloudFormation stack rollback failed. The deployment failed because the following resource(s) failed to update: [AutoScalingGroup].&quot; The stack remains in a status of UPDATE_ROLLBACK_FAILED. Which solution will resolve this issue? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the subnet mappings that are configured for the ALBs. Run the aws cloudformation update-stack-set AWS CLI command.
B. Update the IAM role by providing the necessary permissions to update the stack. Run the aws cloudformation continue-update-rollback AWS CLI command.
C. Submit a request for a quota increase for the number of EC2 instances for the account. Run the aws cloudformation cancel-update-stack AWS CLI command.
D. Delete the Auto Scaling group resource. Run the aws cloudformation rollback-stack AWS CLI command.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS CloudFormation堆栈来部署应用程序更新。这些堆栈由不同的资源组成，包括AWS Auto Scaling组、Amazon EC2实例、Application Load Balancers (ALBs)以及启动和维护独立堆栈所需的其他资源。不允许在CloudFormation堆栈更新之外对应用程序资源进行更改。该公司最近尝试使用AWS CLI更新应用程序堆栈。堆栈更新失败并产生以下错误消息：&quot;ERROR: both the deployment and the CloudFormation stack rollback failed. The deployment failed because the following resource(s) failed to update: [AutoScalingGroup].&quot;堆栈保持在UPDATE_ROLLBACK_FAILED状态。哪个解决方案能解决这个问题？ 选项： A. 更新为ALBs配置的子网映射。运行aws cloudformation update-stack-set AWS CLI命令。 B. 通过提供更新堆栈的必要权限来更新IAM角色。运行aws cloudformation continue-update-rollback AWS CLI命令。 C. 提交增加账户EC2实例数量配额的请求。运行aws cloudformation cancel-update-stack AWS CLI命令。 D. 删除Auto Scaling组资源。运行aws cloudformation rollback-stack AWS CLI命令。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何处理CloudFormation堆栈处于UPDATE_ROLLBACK_FAILED状态的故障恢复场景。当堆栈更新失败且回滚也失败时，需要找到正确的解决方案来恢复堆栈到稳定状态。 **涉及的关键AWS服务和概念：** 1. AWS CloudFormation - 基础设施即代码服务 2. CloudFormation堆栈状态管理 - 特别是UPDATE_ROLLBACK_FAILED状态 3. IAM权限管理 - 服务角色权限 4. Auto Scaling Groups - 自动扩展组资源 5. CloudFormation故障恢复命令 **正确答案B的原因：** 1. UPDATE_ROLLBACK_FAILED状态表明堆栈更新和回滚都失败了，堆栈处于不稳定状态 2. 最常见的原因是IAM权限不足，CloudFormation服务角色缺少更新或回滚特定资源的权限 3. continue-update-rollback命令专门用于从UPDATE_ROLLBACK_FAILED状态恢复，继续完成回滚操作 4. 更新IAM角色权限后，可以重新尝试回滚操作，将堆栈恢复到更新前的稳定状态 **其他选项错误的原因：** - 选项A：update-stack-set用于堆栈集操作，不适用于单个堆栈的故障恢复；子网映射问题不是导致UPDATE_ROLLBACK_FAILED的典型原因 - 选项C：EC2配额问题可能导致更新失败，但cancel-update-stack命令不存在，且不能解决已经处于UPDATE_ROLLBACK_FAILED状态的问题 - 选项D：手动删除资源会导致堆栈状态不一致；rollback-stack命令不是标准的CloudFormation命令 **决策标准和最佳实践：** 1. 遇到UPDATE_ROLLBACK_FAILED状态时，首先检查IAM权限是否充足 2. 使用continue-update-rollback命令是处理此类故障的标准做法 3. 确保CloudFormation服务角色具有管理所有堆栈资源的完整权限 4. 避免手动修改CloudFormation管理的资源，保持基础设施即代码的完整性 5. 在更新堆栈前，应验证权限和资源配额以避免类似问题</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">96</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is deploying a new application that uses Amazon EC2 instances. The company needs a solution to query application logs and AWS account API activity. Which solution will meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Configure AWS CloudTrail to deliver the API logs to Amazon S3. Use CloudWatch to query both sets of logs.
B. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Configure AWS CloudTrail to deliver the API logs to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs.
C. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon Kinesis. Configure AWS CloudTrail to deliver the API logs to Kinesis. Use Kinesis to load the data into Amazon Redshift. Use Amazon Redshift to query both sets of logs.
D. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon S3. Use AWS CloudTrail to deliver the API logs to Amazon S3. Use Amazon Athena to query both sets of logs in Amazon S3.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在部署一个使用Amazon EC2实例的新应用程序。该公司需要一个解决方案来查询应用程序日志和AWS账户API活动。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个统一的解决方案来查询两种类型的日志：1）EC2实例上的应用程序日志；2）AWS账户的API活动日志。关键在于需要能够&quot;查询&quot;这些日志数据。 **涉及的关键AWS服务和概念：** - Amazon CloudWatch Logs：用于收集、监控和存储日志文件的服务 - CloudWatch Logs Insights：CloudWatch Logs的查询功能，支持SQL类似的查询语法 - AWS CloudTrail：记录AWS账户API调用活动的服务 - Amazon CloudWatch agent：在EC2实例上收集日志和指标的代理程序 **正确答案B的原因：** 选项B提供了最优的架构设计： 1. 使用CloudWatch agent将EC2应用日志发送到CloudWatch Logs 2. 配置CloudTrail将API日志也发送到CloudWatch Logs 3. 使用CloudWatch Logs Insights统一查询两种日志 这种方案将所有日志集中在同一个服务中，便于统一管理和查询，且CloudWatch Logs Insights提供强大的查询能力。 **其他选项错误的原因：** - 选项A：CloudTrail日志发送到S3，而应用日志在CloudWatch Logs，数据分散在不同位置，无法统一查询 - 选项C：使用Kinesis和Redshift过于复杂，增加了不必要的成本和复杂性，不符合简单查询需求 - 选项D：虽然数据都在S3中，但CloudWatch agent默认不直接发送日志到S3，需要额外配置，且Athena查询成本较高 **决策标准和最佳实践：** 1. 数据集中化：将相关日志集中存储便于统一管理 2. 查询便利性：选择提供强大查询功能的服务 3. 成本效益：避免过度工程化，选择最简单有效的方案 4. 服务集成：优先使用AWS原生服务的集成功能</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">97</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company wants to ensure that their EC2 instances are secure. They want to be notified if any new vulnerabilities are discovered on their instances, and they also want an audit trail of all login activities on the instances. Which solution will meet these requirements? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Amazon Kinesis Agent to capture system logs and deliver them to Amazon S3.
B. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture system logs and view login activity in the CloudTrail console.
C. Configure Amazon CloudWatch to detect vulnerabilities on the EC2 instances. Install the AWS Config daemon to capture system logs and view them in the AWS Config console.
D. Configure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司希望确保他们的EC2实例是安全的。他们希望在实例上发现任何新漏洞时收到通知，同时还希望获得实例上所有登录活动的审计跟踪。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现两个主要功能： 1. 漏洞检测和通知 - 发现EC2实例上的新安全漏洞 2. 登录活动审计跟踪 - 记录和监控实例上的用户登录行为 **涉及的关键AWS服务和概念：** - Amazon Inspector：专门用于应用程序和EC2实例的安全评估和漏洞检测 - Amazon CloudWatch Agent：用于收集系统级别的指标和日志 - CloudWatch Logs：集中存储和分析日志数据 - AWS Systems Manager：系统管理服务，但主要用于补丁管理和配置管理 - AWS Config：配置合规性监控服务 - Amazon Kinesis Agent：用于实时数据流处理 **正确答案D的原因：** - Amazon Inspector是AWS专门的漏洞评估服务，能够自动检测EC2实例和应用程序中的安全漏洞，并提供详细的安全发现报告 - CloudWatch Agent可以安装在EC2实例上收集系统日志（包括登录日志如/var/log/auth.log），并将这些日志发送到CloudWatch Logs进行集中管理 - 这个组合完美满足了漏洞检测和登录审计的双重需求 **其他选项错误的原因：** - 选项A：Systems Manager主要用于补丁管理，不是专门的漏洞检测工具；Kinesis Agent主要用于流数据处理，不是最佳的日志收集方案 - 选项B：Systems Manager Agent虽然能收集一些信息，但CloudTrail主要记录API调用，不记录实例内部的登录活动 - 选项C：CloudWatch本身不具备漏洞检测功能；AWS Config主要用于配置合规性检查，不是日志管理的最佳选择 **决策标准和最佳实践：** - 漏洞检测应使用专门的安全服务Amazon Inspector - 系统日志收集应使用CloudWatch Agent配合CloudWatch Logs - 选择服务时要考虑其核心功能定位，避免使用非专业工具完成专业任务 - 日志审计需要集中化管理和长期存储能力</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">98</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is running an application on Amazon EC2 instances in an Auto Scaling group. Recently, an issue occurred that prevented EC2 instances from launching successfully, and it took several hours for the support team to discover the issue. The support team wants to be notified by email whenever an EC2 instance does not start successfully. Which action will accomplish this? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a health check to the Auto Scaling group to invoke an AWS Lambda function whenever an instance status is impaired.
B. Configure the Auto Scaling group to send a notification to an Amazon SNS topic whenever a failed instance launch occurs.
C. Create an Amazon CloudWatch alarm that invokes an AWS Lambda function when a failed AttachInstances Auto Scaling API call is made.
D. Create a status check alarm on Amazon EC2 to send a notification to an Amazon SNS topic whenever a status check fail occurs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Auto Scaling组中的Amazon EC2实例上运行应用程序。最近发生了一个问题，导致EC2实例无法成功启动，支持团队花了几个小时才发现这个问题。支持团队希望在EC2实例启动失败时通过电子邮件收到通知。哪个操作可以实现这个目标？ 选项： A. 向Auto Scaling组添加健康检查，以便在实例状态受损时调用AWS Lambda函数。 B. 配置Auto Scaling组在发生实例启动失败时向Amazon SNS主题发送通知。 C. 创建Amazon CloudWatch告警，当AttachInstances Auto Scaling API调用失败时调用AWS Lambda函数。 D. 在Amazon EC2上创建状态检查告警，以便在状态检查失败时向Amazon SNS主题发送通知。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案，能够在Auto Scaling组中的EC2实例启动失败时立即通过电子邮件通知支持团队，以避免像之前那样需要几个小时才发现问题。 **涉及的关键AWS服务和概念：** - Auto Scaling Groups：自动扩展组，负责管理EC2实例的启动和终止 - Amazon SNS：简单通知服务，可以发送电子邮件、短信等通知 - CloudWatch：监控服务，用于收集和跟踪指标、日志文件 - Lambda：无服务器计算服务 - EC2实例启动生命周期和状态检查机制 **正确答案B的原因：** Auto Scaling组原生支持向SNS主题发送通知功能，可以直接配置在实例启动失败、终止、启动成功等事件发生时发送通知。这是最直接、最简单的解决方案，无需额外的服务或复杂配置。SNS可以轻松配置电子邮件订阅，满足题目要求。 **其他选项错误的原因：** - 选项A：健康检查主要用于检测已运行实例的健康状态，而不是检测实例启动失败。而且这种方法过于复杂，需要额外的Lambda函数。 - 选项C：AttachInstances API主要用于将现有实例附加到Auto Scaling组，与实例启动失败的场景不匹配。而且这种方法也增加了不必要的复杂性。 - 选项D：EC2状态检查告警只能检测已经启动的实例的状态问题，无法检测实例启动失败的情况，因为启动失败的实例根本不会进入运行状态。 **决策标准和最佳实践：** 1. 选择最简单直接的解决方案：利用AWS服务的原生功能而不是构建复杂的自定义解决方案 2. 确保监控覆盖正确的生命周期阶段：实例启动失败发生在实例生命周期的早期阶段 3. 使用合适的通知机制：SNS是AWS中标准的通知服务，支持多种通知方式 4. 避免过度工程化：不需要Lambda函数等额外服务来解决这个相对简单的通知需求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">99</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using AWS Organizations to centrally manage its AWS accounts. The company has turned on AWS Config in each member account by using AWS CloudFormation StackSets. The company has configured trusted access in Organizations for AWS Config and has configured a member account as a delegated administrator account for AWS Config. A DevOps engineer needs to implement a new security policy. The policy must require all current and future AWS member accounts to use a common baseline of AWS Config rules that contain remediation actions that are managed from a central account. Non-administrator users who can access member accounts must not be able to modify this common baseline of AWS Config rules that are deployed into each member account. Which solution will meet these requirements? delegated administrator account by using AWS Config. Most Voted D (89%) 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CloudFormation template that contains the AWS Config rules and remediation actions. Deploy the template from the Organizations management account by using CloudFormation StackSets.
B. Create an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the Organizations management account by using CloudFormation StackSets.
C. Create a CloudFormation template that contains the AWS Config rules and remediation actions. Deploy the template from the delegated administrator account by using AWS Config.
D. Create an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS Organizations来集中管理其AWS账户。该公司已通过AWS CloudFormation StackSets在每个成员账户中启用了AWS Config。公司已在Organizations中为AWS Config配置了可信访问，并将一个成员账户配置为AWS Config的委托管理员账户。DevOps工程师需要实施新的安全策略。该策略必须要求所有当前和未来的AWS成员账户使用从中央账户管理的包含修复操作的AWS Config规则的通用基线。能够访问成员账户的非管理员用户不得修改部署到每个成员账户中的这些AWS Config规则的通用基线。哪种解决方案能满足这些要求？ 选项： A. 创建包含AWS Config规则和修复操作的CloudFormation模板。使用CloudFormation StackSets从Organizations管理账户部署模板。 B. 创建包含AWS Config规则和修复操作的AWS Config conformance pack。使用CloudFormation StackSets从Organizations管理账户部署该包。 C. 创建包含AWS Config规则和修复操作的CloudFormation模板。使用AWS Config从委托管理员账户部署模板。 D. 创建包含AWS Config规则和修复操作的AWS Config conformance pack。从委托管理员账户部署该包。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现一个集中管理的AWS Config规则基线，需要满足：1）从中央账户管理所有成员账户的Config规则；2）包含修复操作；3）非管理员用户无法修改这些规则；4）适用于当前和未来的所有成员账户。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - AWS Config：配置合规性监控服务 - AWS Config Conformance Pack：预定义的Config规则集合 - 委托管理员账户：被授权代表组织管理特定服务的成员账户 - CloudFormation StackSets：跨多个账户和区域部署资源的服务 **正确答案的原因（选项D）：** 选项D是最佳解决方案，因为：1）Conformance Pack是专门为标准化Config规则设计的，支持修复操作；2）从委托管理员账户部署符合AWS最佳实践，因为已经为AWS Config设置了委托管理员；3）通过委托管理员账户部署的规则具有更高的权限级别，普通用户无法修改；4）可以自动应用到组织中的所有账户。 **其他选项错误的原因：** 选项A错误：虽然CloudFormation StackSets可以跨账户部署，但使用管理账户直接部署不是最佳实践，且CloudFormation模板不如Conformance Pack专业。选项B错误：从管理账户部署违背了委托管理员的设计原则，应该利用已配置的委托管理员账户。选项C错误：使用CloudFormation模板而非专门的Conformance Pack，且描述不完整。 **决策标准和最佳实践：** 1）优先使用专门的AWS服务功能（Conformance Pack vs 通用CloudFormation）；2）遵循委托管理员模式，避免直接使用管理账户；3）选择能提供更强权限控制的方案；4）确保解决方案能自动扩展到新账户。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">100</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer manages a large commercial website that runs on Amazon EC2. The website uses Amazon Kinesis Data Streams to collect and process web logs. The DevOps engineer manages the Kinesis consumer application, which also runs on Amazon EC2. Sudden increases of data cause the Kinesis consumer application to fall behind, and the Kinesis data streams drop records before the records can be processed. The DevOps engineer must implement a solution to improve stream handling. Which solution meets these requirements with the MOST operational efficiency? Amazon S3 to derive customer insights. Store the results in Amazon S3. GetRecords.IteratorAgeMilliseconds metric. Increase the retention period of the Kinesis data streams. Most Voted for the Lambda function to process the data streams. Most Voted processes the data faster. B (70%) C (28%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Modify the Kinesis consumer application to store the logs durably in Amazon S3. Use Amazon EMR to process the data directly on
B. Horizontally scale the Kinesis consumer application by adding more EC2 instances based on the Amazon CloudWatch
C. Convert the Kinesis consumer application to run as an AWS Lambda function. Configure the Kinesis data streams as the event source
D. Increase the number of shards in the Kinesis data streams to increase the overall throughput so that the consumer application</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师管理着一个运行在Amazon EC2上的大型商业网站。该网站使用Amazon Kinesis Data Streams来收集和处理web日志。DevOps工程师管理着同样运行在Amazon EC2上的Kinesis消费者应用程序。数据的突然增加导致Kinesis消费者应用程序处理滞后，Kinesis数据流在记录被处理之前就丢弃了记录。DevOps工程师必须实施一个解决方案来改善流处理。哪个解决方案以最高的运营效率满足这些要求？ 选项： A. 修改Kinesis消费者应用程序，将日志持久存储在Amazon S3中。使用Amazon EMR直接在Amazon S3上处理数据以获得客户洞察。将结果存储在Amazon S3中。 B. 基于Amazon CloudWatch GetRecords.IteratorAgeMilliseconds指标，通过添加更多EC2实例来水平扩展Kinesis消费者应用程序。增加Kinesis数据流的保留期。 C. 将Kinesis消费者应用程序转换为AWS Lambda函数运行。将Kinesis数据流配置为Lambda函数的事件源来处理数据流。 D. 增加Kinesis数据流中的分片数量以增加整体吞吐量，使消费者应用程序处理数据更快。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何解决Kinesis Data Streams消费者应用程序处理滞后导致数据丢失的问题，要求找到最具运营效率的解决方案。 **涉及的关键AWS服务和概念：** - Amazon Kinesis Data Streams：实时数据流处理服务 - Amazon EC2：弹性计算云服务 - AWS Lambda：无服务器计算服务 - Amazon CloudWatch：监控服务 - GetRecords.IteratorAgeMilliseconds：衡量消费者滞后程度的关键指标 - 水平扩展：通过增加实例数量来提高处理能力 **正确答案B的原因：** 1. **直接解决根本问题**：通过水平扩展增加更多EC2实例，直接提高消费处理能力 2. **智能监控驱动**：使用GetRecords.IteratorAgeMilliseconds指标进行自动化扩展，这个指标直接反映消费者的滞后程度 3. **运营效率最高**：可以实现自动扩展，无需人工干预 4. **保留期增加**：为扩展过程提供缓冲时间，防止数据丢失 5. **保持现有架构**：最小化改动，降低实施风险 **其他选项错误的原因：** - **选项A**：改变了整个架构，从实时处理变为批处理，不符合原有的实时处理需求，运营复杂度高 - **选项C**：Lambda有15分钟执行时间限制和并发限制，可能无法处理大量持续的流数据，且迁移成本高 - **选项D**：选项描述不完整，且仅增加分片不一定解决消费者处理能力不足的问题 **决策标准和最佳实践：** 1. **最小改动原则**：在解决问题的同时保持现有架构稳定 2. **监控驱动扩展**：使用相关的CloudWatch指标进行智能扩展 3. **弹性设计**：能够根据负载变化自动调整资源 4. **数据保护**：通过增加保留期确保数据不丢失 5. **运营效率优先**：选择自动化程度最高、人工干预最少的方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">101</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company recently created a new AWS Control Tower landing zone in a new organization in AWS Organizations. The landing zone must be able to demonstrate compliance with the Center for Internet Security (CIS) Benchmarks for AWS Foundations. The company&#x27;s security team wants to use AWS Security Hub to view compliance across all accounts. Only the security team can be allowed to view aggregated Security Hub findings. In addition, specific users must be able to view findings from their own accounts within the organization. All accounts must be enrolled in Security Hub after the accounts are created. Which combination of steps will meet these requirements in the MOST automated way? (Choose three.) CreateAccountAssignment API operation to associate the security team users with the permission set and with the delegated security account. F. In the organization&#x27;s management account, create an Amazon EventBridge rule that reacts to the CreateManagedAccount event. Create an AWS Lambda function that uses the Security Hub CreateMembers API operation to add new accounts to Security Hub. Configure the EventBridge rule to invoke the Lambda function. ACE (76%) ADE (19%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Turn on trusted access for Security Hub in the organization&#x27;s management account. Create a new security account by using AWS Control Tower. Configure the new security account as the delegated administrator account for Security Hub. In the new security account, provide Security Hub with the CIS Benchmarks for AWS Foundations standards.
B. Turn on trusted access for Security Hub in the organization&#x27;s management account. From the management account, provide Security Hub with the CIS Benchmarks for AWS Foundations standards.
C. Create an AWS IAM Identity Center (AWS Single Sign-On) permission set that includes the required permissions. Use the
D. Create an SCP that explicitly denies any user who is not on the security team from accessing Security Hub.
E. In Security Hub, turn on automatic enablement.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司最近在AWS Organizations中的新组织内创建了一个新的AWS Control Tower landing zone。该landing zone必须能够证明符合AWS基础架构的互联网安全中心(CIS)基准。公司的安全团队希望使用AWS Security Hub来查看所有账户的合规性。只有安全团队被允许查看聚合的Security Hub发现结果。此外，特定用户必须能够查看组织内自己账户的发现结果。所有账户在创建后都必须注册到Security Hub中。哪种步骤组合能够以最自动化的方式满足这些要求？（选择三个。） 选项： A. 在组织的管理账户中为Security Hub开启可信访问。使用AWS Control Tower创建一个新的安全账户。将新安全账户配置为Security Hub的委托管理员账户。在新安全账户中，为Security Hub提供AWS基础架构的CIS基准标准。 B. 在组织的管理账户中为Security Hub开启可信访问。从管理账户为Security Hub提供AWS基础架构的CIS基准标准。 C. 创建一个包含所需权限的AWS IAM Identity Center (AWS Single Sign-On)权限集。使用CreateAccountAssignment API操作将安全团队用户与权限集和委托安全账户关联。 D. 创建一个SCP，明确拒绝不在安全团队中的任何用户访问Security Hub。 E. 在Security Hub中开启自动启用功能。 F. 在组织的管理账户中，创建一个对CreateManagedAccount事件做出反应的Amazon EventBridge规则。创建一个使用Security Hub CreateMembers API操作将新账户添加到Security Hub的AWS Lambda函数。配置EventBridge规则来调用Lambda函数。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求建立一个符合CIS基准的多账户Security Hub解决方案，需要满足：1）安全团队能查看所有账户的聚合发现结果；2）普通用户只能查看自己账户的发现结果；3）新账户自动注册到Security Hub；4）实现最大程度的自动化。 **涉及的关键AWS服务和概念：** - AWS Control Tower：多账户管理和治理服务 - AWS Security Hub：安全态势管理和合规性监控服务 - AWS Organizations：组织级账户管理 - IAM Identity Center：集中身份管理 - EventBridge + Lambda：事件驱动的自动化 - 委托管理员模式：允许非管理账户管理特定服务 **正确答案ACE的原因：** - **选项A**：建立了正确的Security Hub架构基础。开启可信访问是前提，创建专门的安全账户作为委托管理员是最佳实践，可以实现集中管理而不暴露管理账户权限。在委托账户中配置CIS基准确保合规性要求。 - **选项C**：通过IAM Identity Center实现细粒度的权限控制，安全团队可以访问委托安全账户查看聚合结果，普通用户只能访问自己的账户，满足权限隔离要求。 - **选项E**：自动启用功能确保新创建的账户自动加入Security Hub，实现了自动化要求。 **其他选项错误的原因：** - **选项B**：在管理账户中配置Security Hub不是最佳实践，会增加管理账户的复杂性和安全风险。 - **选项D**：SCP过于粗暴，无法实现细粒度的权限控制，且可能阻止必要的访问。 - **选项F**：虽然能实现自动化，但在有选项E（自动启用）的情况下，这种EventBridge+Lambda的方案增加了不必要的复杂性。 **决策标准和最佳实践：** 1. **委托管理员模式**：使用专门的安全账户而非管理账户来管理Security Hub 2. **最小权限原则**：通过IAM Identity Center实现精确的权限控制 3. **自动化优先**：选择AWS原生的自动启用功能而非自定义解决方案 4. **安全隔离**：确保不同角色用户只能访问授权范围内的资源</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">102</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company runs applications in AWS accounts that are in an organization in AWS Organizations. The applications use Amazon EC2 instances and Amazon S3. The company wants to detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity in its existing AWS accounts and in any AWS accounts that the company creates in the future. When the company detects one of these events, the company wants to use an existing Amazon Simple Notification Service (Amazon SNS) topic to send a notification to its operational support team for investigation and remediation. Which solution will meet these requirements in accordance with AWS best practices? GuardDuty administrator account, add the company&#x27;s existing AWS accounts to GuardDuty as members. In the GuardDuty administrator account, create an Amazon EventBridge rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic. Most Voted SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic. A (88%) 13%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the organization&#x27;s management account, configure an AWS account as the Amazon GuardDuty administrator account. In the
B. In the organization&#x27;s management account, configure Amazon GuardDuty to add newly created AWS accounts by invitation and to send invitations to the existing AWS accounts. Create an AWS CloudFormation stack set that accepts the GuardDuty invitation and creates an Amazon EventBridge rule. Configure the rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic. Configure the CloudFormation stack set to deploy into all AWS accounts in the organization.
C. In the organization&#x27;s management account, create an AWS CloudTrail organization trail. Activate the organization trail in all AWS accounts in the organization. Create an SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic.
D. In the organization&#x27;s management account, configure an AWS account as the AWS CloudTrail administrator account. In the CloudTrail administrator account, create a CloudTrail organization trail. Add the company&#x27;s existing AWS accounts to the organization trail. Create an</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中的组织内的AWS账户中运行应用程序。这些应用程序使用Amazon EC2实例和Amazon S3。该公司希望在其现有的AWS账户以及将来创建的任何AWS账户中检测可能被入侵的EC2实例、可疑的网络活动和异常的API活动。当公司检测到这些事件之一时，公司希望使用现有的Amazon Simple Notification Service (Amazon SNS) topic向其运营支持团队发送通知以进行调查和修复。哪种解决方案将根据AWS最佳实践满足这些要求？ 选项： A. 在组织的管理账户中，配置一个AWS账户作为Amazon GuardDuty管理员账户。在GuardDuty管理员账户中，将公司现有的AWS账户作为成员添加到GuardDuty。在GuardDuty管理员账户中，创建一个Amazon EventBridge规则，使用事件模式匹配GuardDuty事件并将匹配的事件转发到SNS topic。 B. 在组织的管理账户中，配置Amazon GuardDuty通过邀请添加新创建的AWS账户，并向现有AWS账户发送邀请。创建一个AWS CloudFormation stack set，接受GuardDuty邀请并创建Amazon EventBridge规则。配置规则使用事件模式匹配GuardDuty事件并将匹配的事件转发到SNS topic。配置CloudFormation stack set部署到组织中的所有AWS账户。 C. 在组织的管理账户中，创建AWS CloudTrail组织跟踪。在组织中的所有AWS账户中激活组织跟踪。创建SCP启用每个账户中的VPC Flow Logs。为组织配置AWS Security Hub。创建Amazon EventBridge规则，使用事件模式匹配Security Hub事件并将匹配的事件转发到SNS topic。 D. 在组织的管理账户中，配置一个AWS账户作为AWS CloudTrail管理员账户。在CloudTrail管理员账户中，创建CloudTrail组织跟踪。将公司现有的AWS账户添加到组织跟踪中。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个解决方案来检测可能被入侵的EC2实例、可疑网络活动和异常API活动，并且需要覆盖现有账户和未来创建的账户，当检测到威胁时通过SNS发送通知。 **涉及的关键AWS服务和概念：** - Amazon GuardDuty：AWS的威胁检测服务，专门用于检测恶意活动和异常行为 - AWS Organizations：多账户管理服务 - Amazon EventBridge：事件路由服务 - AWS CloudFormation StackSets：跨多个账户和区域部署资源 - Amazon SNS：通知服务 - AWS Security Hub：安全发现聚合服务 - AWS CloudTrail：API调用日志服务 **正确答案B的原因：** 1. **服务选择正确**：GuardDuty是专门设计用来检测题目中提到的三种威胁类型（被入侵的EC2实例、可疑网络活动、异常API活动）的服务 2. **组织级部署**：使用CloudFormation StackSets可以自动化地在组织中的所有账户部署相同的配置 3. **自动化程度高**：通过邀请机制和StackSets，可以确保新账户也会自动包含在威胁检测范围内 4. **事件处理合理**：在每个账户中创建EventBridge规则来处理GuardDuty事件并转发到SNS **其他选项错误的原因：** - **选项A**：虽然使用了正确的GuardDuty服务，但只是手动添加现有账户作为成员，没有自动化机制处理未来新创建的账户，不符合可扩展性要求 - **选项C**：使用了错误的服务组合。CloudTrail主要用于API调用审计，VPC Flow Logs用于网络流量分析，Security Hub用于安全发现聚合，这个组合无法有效检测被入侵的EC2实例 - **选项D**：选项不完整，且CloudTrail不是检测威胁的最佳工具 **决策标准和最佳实践：** 1. **服务适配性**：选择专门针对威胁检测的GuardDuty服务 2. **自动化和可扩展性**：使用StackSets确保新账户自动包含安全配置 3. **集中管理**：在组织管理账户中进行配置，符合AWS Organizations的最佳实践 4. **事件驱动架构**：使用EventBridge进行事件路由，实现松耦合的通知机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">103</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s DevOps engineer is working in a multi-account environment. The company uses AWS Transit Gateway to route all outbound traffic through a network operations account. In the network operations account, all account traffic passes through a firewall appliance for inspection before the traffic goes to an internet gateway. The firewall appliance sends logs to Amazon CloudWatch Logs and includes event severities of CRITICAL, HIGH, MEDIUM, LOW, and INFO. The security team wants to receive an alert if any CRITICAL events occur. What should the DevOps engineer do to meet these requirements? Subscribe the security team&#x27;s email address to the topic. Most Voted B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon CloudWatch Synthetics canary to monitor the firewall state. If the firewall reaches a CRITICAL state or logs a CRITICAL event, use a CloudWatch alarm to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the security team&#x27;s email address to the topic.
B. Create an Amazon CloudWatch metric filter by using a search for CRITICAL events. Publish a custom metric for the finding. Use a CloudWatch alarm based on the custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic.
C. Enable Amazon GuardDuty in the network operations account. Configure GuardDuty to monitor flow logs. Create an Amazon EventBridge event rule that is invoked by GuardDuty events that are CRITICAL. Define an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the security team&#x27;s email address to the topic.
D. Use AWS Firewall Manager to apply consistent policies across all accounts. Create an Amazon EventBridge event rule that is invoked by Firewall Manager events that are CRITICAL. Define an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the security team&#x27;s email address to the topic.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的DevOps工程师在多账户环境中工作。该公司使用AWS Transit Gateway将所有出站流量路由到网络运营账户。在网络运营账户中，所有账户流量在到达internet gateway之前都要通过防火墙设备进行检查。防火墙设备将日志发送到Amazon CloudWatch Logs，包含CRITICAL、HIGH、MEDIUM、LOW和INFO等事件严重级别。安全团队希望在发生任何CRITICAL事件时收到警报。DevOps工程师应该如何满足这些要求？ 选项： A. 创建Amazon CloudWatch Synthetics canary来监控防火墙状态。如果防火墙达到CRITICAL状态或记录CRITICAL事件，使用CloudWatch alarm向Amazon Simple Notification Service (Amazon SNS) topic发布通知。将安全团队的邮箱地址订阅到该topic。 B. 使用搜索CRITICAL事件创建Amazon CloudWatch metric filter。为发现的事件发布自定义指标。基于自定义指标使用CloudWatch alarm向Amazon Simple Notification Service (Amazon SNS) topic发布通知。 C. 在网络运营账户中启用Amazon GuardDuty。配置GuardDuty监控flow logs。创建由CRITICAL级别GuardDuty事件触发的Amazon EventBridge event rule。定义Amazon Simple Notification Service (Amazon SNS) topic作为目标。将安全团队的邮箱地址订阅到该topic。 D. 使用AWS Firewall Manager在所有账户中应用一致的策略。创建由CRITICAL级别Firewall Manager事件触发的Amazon EventBridge event rule。定义Amazon Simple Notification Service (Amazon SNS) topic作为目标。将安全团队的邮箱地址订阅到该topic。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要监控防火墙设备发送到CloudWatch Logs的日志，当出现CRITICAL级别事件时自动发送警报给安全团队。 **涉及的关键AWS服务和概念：** - CloudWatch Logs：存储防火墙日志的服务 - CloudWatch Metric Filter：从日志中提取特定模式并转换为指标 - CloudWatch Alarm：基于指标阈值触发警报 - SNS：发送通知的消息服务 - CloudWatch Synthetics：用于应用程序监控的服务 - GuardDuty：威胁检测服务 - Firewall Manager：防火墙策略管理服务 **正确答案B的原因：** 1. **直接针对问题**：防火墙日志已经在CloudWatch Logs中，需要从这些日志中识别CRITICAL事件 2. **技术路径正确**：Metric Filter可以搜索日志中的特定文本模式（CRITICAL），并将匹配结果转换为自定义指标 3. **完整的警报链**：自定义指标→CloudWatch Alarm→SNS Topic→邮件通知，形成完整的监控和通知链路 4. **成本效益**：利用现有的日志数据，无需额外的服务或复杂配置 **其他选项错误的原因：** - **选项A**：CloudWatch Synthetics主要用于网站和API的可用性监控，不适合分析日志内容中的特定事件级别 - **选项C**：GuardDuty是威胁检测服务，主要分析VPC Flow Logs、DNS logs等来检测安全威胁，不是用来解析防火墙应用日志中的自定义严重级别 - **选项D**：Firewall Manager用于管理防火墙策略的一致性，不是用来监控日志事件的工具 **决策标准和最佳实践：** 1. **服务匹配度**：选择最直接适合数据源和需求的AWS服务 2. **架构简洁性**：避免过度工程化，使用最简单有效的解决方案 3. **成本考虑**：利用现有资源，避免引入不必要的额外服务 4. **可维护性**：选择配置简单、易于维护的方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">104</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is divided into teams. Each team has an AWS account, and all the accounts are in an organization in AWS Organizations. Each team must retain full administrative rights to its AWS account. Each team also must be allowed to access only AWS services that the company approves for use. AWS services must gain approval through a request and approval process. How should a DevOps engineer configure the accounts to meet these requirements? D (50%) C (47%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CloudFormation StackSets to provision IAM policies in each account to deny access to restricted AWS services. In each account, configure AWS Config rules that ensure that the policies are attached to IAM principals in the account.
B. Use AWS Control Tower to provision the accounts into OUs within the organization. Configure AWS Control Tower to enable AWS IAM Identity Center (AWS Single Sign-On). Configure IAM Identity Center to provide administrative access. Include deny policies on user roles for restricted AWS services.
C. Place all the accounts under a new top-level OU within the organization. Create an SCP that denies access to restricted AWS services. Attach the SCP to the OU.
D. Create an SCP that allows access to only approved AWS services. Attach the SCP to the root OU of the organization. Remove the FullAWSAccess SCP from the root OU of the organization.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司分为多个团队。每个团队都有一个AWS账户，所有账户都在AWS Organizations的一个组织中。每个团队必须保留对其AWS账户的完全管理权限。每个团队也必须只能访问公司批准使用的AWS服务。AWS服务必须通过请求和批准流程获得批准。DevOps工程师应该如何配置账户以满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在保持团队对各自AWS账户完全管理权限的同时，限制他们只能使用公司批准的AWS服务。这是一个典型的组织级权限控制问题，需要在账户自主性和服务使用合规性之间找到平衡。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - Service Control Policies (SCP)：服务控制策略，用于在组织层面限制权限 - Organizational Units (OU)：组织单元，用于分组管理账户 - IAM策略：身份和访问管理策略 - AWS Control Tower：账户治理服务 **正确答案C的原因：** 1. **精确的权限边界控制**：将所有账户放在新的顶级OU下，通过SCP拒绝访问受限服务，这样可以在组织层面统一控制服务访问权限 2. **保持管理权限**：SCP只是设置权限边界，不会影响团队在各自账户内的完全管理权限 3. **集中化管理**：通过OU级别的SCP，可以统一管理所有团队账户的服务访问限制 4. **灵活性**：当新服务获得批准时，只需修改SCP策略即可 **其他选项错误的原因：** - **选项A**：使用CloudFormation StackSets和Config规则过于复杂，且需要在每个账户内部署策略，管理复杂度高，不如SCP直接有效 - **选项B**：Control Tower和IAM Identity Center主要解决身份管理问题，但题目要求保持团队的完全管理权限，这种方案会改变现有的权限结构 - **选项D**：在根OU级别应用允许策略并移除FullAWSAccess会影响整个组织，可能对其他不在这些团队中的账户造成意外影响 **决策标准和最佳实践：** 1. **最小权限原则**：通过SCP设置服务访问边界 2. **集中管理，分散执行**：组织级别统一控制，账户级别保持自主 3. **简化管理**：选择最直接有效的解决方案，避免过度复杂的架构 4. **影响范围控制**：确保策略变更只影响目标账户群体</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">105</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer used an AWS CloudFormation custom resource to set up AD Connector. The AWS Lambda function ran and created AD Connector, but CloudFormation is not transitioning from CREATE_IN_PROGRESS to CREATE_COMPLETE. Which action should the engineer take to resolve this issue? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Ensure the Lambda function code has exited successfully.
B. Ensure the Lambda function code returns a response to the pre-signed URL.
C. Ensure the Lambda function IAM role has cloudformation:UpdateStack permissions for the stack ARN.
D. Ensure the Lambda function IAM role has ds:ConnectDirectory permissions for the AWS account.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师使用AWS CloudFormation自定义资源来设置AD Connector。AWS Lambda函数运行并创建了AD Connector，但CloudFormation没有从CREATE_IN_PROGRESS状态转换到CREATE_COMPLETE状态。工程师应该采取哪个行动来解决这个问题？ 选项： A. 确保Lambda函数代码已成功退出 B. 确保Lambda函数代码向预签名URL返回响应 C. 确保Lambda函数IAM角色对stack ARN具有cloudformation:UpdateStack权限 D. 确保Lambda函数IAM角色对AWS账户具有ds:ConnectDirectory权限</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CloudFormation自定义资源的工作机制，特别是Lambda函数与CloudFormation之间的通信协议。问题的关键在于Lambda函数已经成功创建了AD Connector，但CloudFormation堆栈状态没有更新。 **涉及的关键AWS服务和概念：** 1. AWS CloudFormation自定义资源 - 允许在CloudFormation模板中集成自定义逻辑 2. AWS Lambda - 执行自定义资源逻辑的无服务器函数 3. AD Connector - AWS Directory Service的一种类型 4. 预签名URL - CloudFormation提供给Lambda函数用于返回响应的机制 **正确答案的原因（选项B）：** CloudFormation自定义资源的工作流程要求Lambda函数必须向CloudFormation提供的预签名URL发送响应，告知操作的成功或失败状态。即使Lambda函数成功执行了业务逻辑（创建AD Connector），如果没有向预签名URL发送响应，CloudFormation就无法知道操作已完成，因此会一直保持在CREATE_IN_PROGRESS状态。这是CloudFormation自定义资源的核心通信机制。 **其他选项错误的原因：** - 选项A：Lambda函数成功退出并不足够，还需要主动通知CloudFormation操作结果 - 选项C：cloudformation:UpdateStack权限不是自定义资源Lambda函数的必需权限，CloudFormation通过预签名URL机制处理状态更新 - 选项D：ds:ConnectDirectory权限可能已经具备（因为AD Connector已成功创建），问题不在于权限而在于通信机制 **决策标准和最佳实践：** 1. 理解CloudFormation自定义资源的完整生命周期，包括请求和响应机制 2. 确保Lambda函数代码包含向ResponseURL发送HTTP PUT请求的逻辑 3. 响应必须包含正确的Status（SUCCESS或FAILED）、RequestId、LogicalResourceId和StackId 4. 实施适当的错误处理，确保即使在失败情况下也能发送响应给CloudFormation</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">106</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS CodeCommit for source code control. Developers apply their changes to various feature branches and create pull requests to move those changes to the main branch when the changes are ready for production. The developers should not be able to push changes directly to the main branch. The company applied the AWSCodeCommitPowerUser managed policy to the developers&#x27; IAM role, and now these developers can push changes to the main branch directly on every repository in the AWS account. What should the company do to restrict the developers&#x27; ability to push changes to the main branch directly? in the policy statement with a condition that references the feature branches. A (93%) 7%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an additional policy to include a Deny rule for the GitPush and PutFile actions. Include a restriction for the specific repositories in the policy statement with a condition that references the main branch.
B. Remove the IAM policy, and add an AWSCodeCommitReadOnly managed policy. Add an Allow rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.
C. Modify the IAM policy. Include a Deny rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.
D. Create an additional policy to include an Allow rule for the GitPush and PutFile actions. Include a restriction for the specific repositories</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS CodeCommit进行源代码控制。开发人员将他们的更改应用到各种功能分支，并创建pull request将这些更改移动到主分支，当更改准备好用于生产时。开发人员不应该能够直接将更改推送到主分支。公司将AWSCodeCommitPowerUser托管策略应用到开发人员的IAM角色，现在这些开发人员可以在AWS账户中的每个存储库上直接将更改推送到主分支。公司应该做什么来限制开发人员直接将更改推送到主分支的能力？ 选项： A. 创建一个额外的策略，包含对GitPush和PutFile操作的拒绝规则。在策略语句中包含对特定存储库的限制，并使用引用主分支的条件。 B. 删除IAM策略，并添加AWSCodeCommitReadOnly托管策略。为策略语句中的特定存储库添加GitPush和PutFile操作的允许规则，并使用引用主分支的条件。 C. 修改IAM策略。在策略语句中为特定存储库包含对GitPush和PutFile操作的拒绝规则，并使用引用主分支的条件。 D. 创建一个额外的策略，包含对GitPush和PutFile操作的允许规则。为特定存储库包含限制。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求限制开发人员直接向主分支推送代码的权限，同时保持他们在功能分支上正常工作的能力。当前问题是AWSCodeCommitPowerUser托管策略给予了开发人员过多权限。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：Git代码存储库服务 - IAM策略：权限控制机制 - 托管策略 vs 自定义策略 - IAM策略评估逻辑：显式拒绝优先于允许 - 条件语句：用于细粒度权限控制 - GitPush和PutFile：CodeCommit中的关键操作 **正确答案A的原因：** 1. **创建额外策略**：保留现有的AWSCodeCommitPowerUser策略，通过附加策略实现限制 2. **使用Deny规则**：显式拒绝优先级最高，能够覆盖现有的允许权限 3. **针对特定操作**：GitPush和PutFile是直接推送代码到分支的关键操作 4. **条件限制**：通过条件语句specifically targeting主分支，不影响功能分支的操作 5. **最小权限原则**：只限制必要的操作，保持其他权限不变 **其他选项错误的原因：** - **选项B**：删除现有策略会移除开发人员的基本权限，然后用ReadOnly策略加Allow规则的方式过于复杂且容易出错 - **选项C**：直接修改托管策略是不可能的，托管策略由AWS维护，用户无法修改 - **选项D**：使用Allow规则无法覆盖现有的权限，且描述不完整 **决策标准和最佳实践：** 1. **权限最小化**：只限制必要的操作，保持开发效率 2. **显式拒绝策略**：当需要限制现有权限时，使用Deny规则是最有效的方法 3. **策略分离**：通过附加策略而非修改现有策略来实现特定限制 4. **条件使用**：利用条件语句实现细粒度的分支级别控制 5. **代码审查流程**：通过技术手段强制执行pull request工作流程</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">107</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run in an Auto Scaling group across multiple Availability Zones. The application uses an Amazon RDS for MySQL DB instance to store the data. The company has configured Amazon Route 53 with an alias record that points to the ALB. A new company guideline requires a geographically isolated disaster recovery (DR) site with an RTO of 4 hours and an RPO of 15 minutes. Which DR strategy will meet these requirements with the LEAST change to the application stack? D (89%) 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Launch a replica environment of everything except Amazon RDS in a different Availability Zone. Create an RDS read replica in the new Availability Zone, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy.
B. Launch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a latency routing policy.
C. Launch a replica environment of everything except Amazon RDS in a different AWS Region. In the event of an outage, copy and restore the latest RDS snapshot from the primary Region to the DR Region. Adjust the Route 53 record set to point to the ALB in the DR Region.
D. Launch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy. In the event of an outage, promote the read replica to primary.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司管理着一个运行在Application Load Balancer (ALB)后面Amazon EC2实例上的Web应用程序。EC2实例在跨多个Availability Zone的Auto Scaling组中运行。应用程序使用Amazon RDS for MySQL数据库实例来存储数据。公司已配置Amazon Route 53，其中有一个指向ALB的别名记录。新的公司指导方针要求建立一个地理隔离的灾难恢复(DR)站点，RTO为4小时，RPO为15分钟。哪种DR策略能够以对应用程序堆栈的最少更改来满足这些要求？ 选项： A. 在不同的Availability Zone中启动除Amazon RDS之外所有组件的副本环境。在新的Availability Zone中创建RDS read replica，并配置新堆栈指向本地RDS数据库实例。通过使用健康检查配置failover路由策略，将新堆栈添加到Route 53记录集中。 B. 在不同的AWS Region中启动除Amazon RDS之外所有组件的副本环境。在新Region中创建RDS read replica，并配置新堆栈指向本地RDS数据库实例。通过使用健康检查配置latency路由策略，将新堆栈添加到Route 53记录集中。 C. 在不同的AWS Region中启动除Amazon RDS之外所有组件的副本环境。在发生故障时，从主Region复制并恢复最新的RDS快照到DR Region。调整Route 53记录集指向DR Region中的ALB。 D. 在不同的AWS Region中启动除Amazon RDS之外所有组件的副本环境。在新Region中创建RDS read replica，并配置新环境指向本地RDS数据库实例。通过使用健康检查配置failover路由策略，将新堆栈添加到Route 53记录集中。在发生故障时，将read replica提升为主数据库。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS灾难恢复策略设计，需要满足：1）地理隔离要求；2）RTO（恢复时间目标）4小时；3）RPO（恢复点目标）15分钟；4）对现有应用程序堆栈的最少更改。 **涉及的关键AWS服务和概念：** - Amazon RDS read replica：提供数据复制和故障转移能力 - Multi-Region部署：实现地理隔离 - Route 53 failover路由策略：自动故障转移 - RTO/RPO概念：衡量灾难恢复能力的关键指标 **正确答案D的原因：** 1. **满足地理隔离**：在不同AWS Region部署DR环境 2. **满足RPO要求**：RDS read replica提供近实时数据同步，远优于15分钟RPO要求 3. **满足RTO要求**：预先部署的环境+自动failover机制，可在4小时内完成恢复 4. **最少应用更改**：应用程序无需修改，只需配置指向不同的数据库端点 5. **自动化程度高**：Route 53 failover策略提供自动故障检测和流量切换 **其他选项错误的原因：** - **选项A**：使用Availability Zone而非Region，不满足&quot;地理隔离&quot;要求 - **选项B**：使用latency路由策略而非failover策略，这是性能优化而非灾难恢复策略，不适合DR场景 - **选项C**：依赖手动快照恢复，RTO可能超过4小时，且需要手动操作Route 53，自动化程度低 **决策标准和最佳实践：** 1. **地理隔离**：DR站点必须在不同Region以防范区域性灾难 2. **自动化优先**：选择能够自动检测故障和切换的方案 3. **预先部署**：为满足严格的RTO要求，应预先部署完整环境 4. **数据同步策略**：read replica比快照恢复提供更好的RPO和RTO 5. **DNS故障转移**：Route 53 failover策略是实现自动故障转移的标准做法</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">108</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A large enterprise is deploying a web application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The application stores data in an Amazon RDS for Oracle DB instance and Amazon DynamoDB. There are separate environments for development, testing, and production. What is the MOST secure and flexible way to obtain password credentials during deployment? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Retrieve an access key from an AWS Systems Manager SecureString parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.
B. Launch the EC2 instances with an EC2 IAM role to access AWS services. Retrieve the database credentials from AWS Secrets Manager.
C. Retrieve an access key from an AWS Systems Manager plaintext parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.
D. Launch the EC2 instances with an EC2 IAM role to access AWS services. Store the database passwords in an encrypted config file with the application artifacts.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家大型企业正在AWS上部署Web应用程序。该应用程序运行在Application Load Balancer后面的Amazon EC2实例上。这些实例在Auto Scaling组中跨多个Availability Zone运行。应用程序将数据存储在Amazon RDS for Oracle数据库实例和Amazon DynamoDB中。开发、测试和生产环境是分离的。在部署过程中获取密码凭证的最安全且最灵活的方式是什么？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在AWS环境中安全管理和获取应用程序凭证的最佳实践，特别是针对EC2实例访问AWS服务和数据库的认证方式。 **涉及的关键AWS服务和概念：** - EC2 IAM Role：为EC2实例提供临时、自动轮换的AWS服务访问权限 - AWS Secrets Manager：专门用于管理敏感信息如数据库密码的服务 - AWS Systems Manager Parameter Store：用于存储配置数据和密钥的服务 - 最小权限原则和安全最佳实践 **正确答案B的原因：** 1. **EC2 IAM Role**：这是访问AWS服务的最安全方式，因为它提供临时凭证，自动轮换，无需在代码中硬编码访问密钥 2. **AWS Secrets Manager**：专门设计用于管理数据库凭证，提供自动密码轮换、细粒度访问控制、审计日志等高级功能 3. **灵活性**：支持多环境部署，可以为不同环境配置不同的密钥和权限 4. **安全性**：所有凭证都加密存储，传输过程也加密 **其他选项错误的原因：** - **选项A**：使用访问密钥是不安全的做法，容易泄露且难以管理轮换 - **选项C**：使用明文参数存储访问密钥极其不安全，违反了基本的安全原则 - **选项D**：将数据库密码存储在配置文件中，即使加密也不如专门的密钥管理服务安全和灵活 **决策标准和最佳实践：** 1. **避免长期凭证**：优先使用IAM Role而非访问密钥 2. **专门工具管理敏感信息**：使用Secrets Manager管理数据库凭证 3. **加密和访问控制**：确保所有敏感数据都加密存储并有适当的访问控制 4. **审计和监控**：选择提供完整审计日志的服务 5. **自动化管理**：支持自动密码轮换和管理的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">109</div>
        <div class="field-label">Question:</div>
        <div class="field-content">The security team depends on AWS CloudTrail to detect sensitive security issues in the company&#x27;s AWS account. The DevOps engineer needs a solution to auto-remediate CloudTrail being turned off in an AWS account. What solution ensures the LEAST amount of downtime for the CloudTrail log deliveries? resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule. call StartLogging on a CloudTrail trail in the AWS account. Add the Lambda function ARN as a target to the EventBridge rule. CloudTrail trail is disabled, have the script re-enable the trail. A (92%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule for the CloudTrail StopLogging event. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule.
B. Deploy the AWS-managed CloudTrail-enabled AWS Config rule, set with a periodic interval of 1 hour. Create an Amazon EventBridge rule for AWS Config rules compliance change. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the
C. Create an Amazon EventBridge rule for a scheduled event every 5 minutes. Create an AWS Lambda function that uses the AWS SDK to
D. Launch a t2.nano instance with a script running every 5 minutes that uses the AWS SDK to query CloudTrail in the current account. If the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">安全团队依赖AWS CloudTrail来检测公司AWS账户中的敏感安全问题。DevOps工程师需要一个解决方案来自动修复AWS账户中CloudTrail被关闭的情况。什么解决方案能确保CloudTrail日志传输的停机时间最少？ 选项： A. 为CloudTrail StopLogging事件创建Amazon EventBridge规则。创建一个AWS Lambda函数，使用AWS SDK在调用StopLogging的资源ARN上调用StartLogging。将Lambda函数ARN添加为EventBridge规则的目标。 B. 部署AWS托管的CloudTrail-enabled AWS Config规则，设置为1小时的周期间隔。为AWS Config规则合规性变更创建Amazon EventBridge规则。创建一个AWS Lambda函数，使用AWS SDK调用StartLogging... C. 为每5分钟的计划事件创建Amazon EventBridge规则。创建一个AWS Lambda函数，使用AWS SDK... D. 启动一个t2.nano实例，运行每5分钟执行一次的脚本，使用AWS SDK查询当前账户中的CloudTrail。如果CloudTrail被禁用，让脚本重新启用...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动修复方案，当CloudTrail被关闭时能够自动重新启用，并且要求停机时间最少。关键是要实现实时响应和快速恢复。 **涉及的关键AWS服务和概念：** - AWS CloudTrail：用于记录API调用和账户活动的审计服务 - Amazon EventBridge：事件驱动的服务，可以响应AWS服务的状态变化 - AWS Lambda：无服务器计算服务，用于执行自动修复逻辑 - AWS Config：配置管理和合规性监控服务 - StopLogging/StartLogging API：CloudTrail的启停API调用 **正确答案A的原因：** 1. **实时响应**：EventBridge能够实时捕获StopLogging事件，无需等待轮询间隔 2. **最小停机时间**：事件驱动架构确保在CloudTrail被关闭的瞬间就触发修复动作 3. **精确目标**：直接获取被停止的CloudTrail资源ARN，确保修复正确的资源 4. **高效架构**：使用无服务器Lambda函数，无需维护基础设施 **其他选项错误的原因：** - **选项B**：AWS Config的1小时周期检查导致较长的检测延迟，不符合最少停机时间的要求 - **选项C**：5分钟的定时检查仍然存在检测延迟，且会产生不必要的资源消耗 - **选项D**：使用EC2实例增加了基础设施成本和管理复杂性，5分钟轮询同样存在延迟问题 **决策标准和最佳实践：** 1. **事件驱动优于轮询**：实时事件响应比定期检查更高效 2. **无服务器优先**：Lambda比EC2更适合这种间歇性任务 3. **最小化检测窗口**：安全事件的响应时间越短越好 4. **成本效益**：避免不必要的资源消耗和基础设施维护</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">110</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS CodeArtifact to centrally store Python packages. The CodeArtifact repository is configured with the following repository policy: A development team is building a new project in an account that is in an organization in AWS Organizations. The development team wants to use a Python library that has already been stored in the CodeArtifact repository in the organization. The development team uses AWS CodePipeline and AWS CodeBuild to build the new application. The CodeBuild job that the development team uses to build the application is configured to run in a VPC. Because of compliance requirements, the VPC has no internet connectivity. The development team creates the VPC endpoints for CodeArtifact and updates the CodeBuild buildspec.yaml file. However, the development team cannot download the Python library from the repository. Which combination of steps should a DevOps engineer take so that the development team can use CodeArtifact? (Choose two.) BD (47%) AD (47%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon S3 gateway endpoint. Update the route tables for the subnets that are running the CodeBuild job.
B. Update the repository policy&#x27;s Principal statement to include the ARN of the role that the CodeBuild project uses.
C. Share the CodeArtifact repository with the organization by using AWS Resource Access Manager (AWS RAM).
D. Update the role that the CodeBuild project uses so that the role has sufficient permissions to use the CodeArtifact repository.
E. Specify the account that hosts the repository as the delegated administrator for CodeArtifact in the organization.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS CodeArtifact来集中存储Python包。CodeArtifact存储库配置了以下存储库策略：开发团队正在AWS Organizations组织中的一个账户中构建新项目。开发团队想要使用已经存储在组织中CodeArtifact存储库中的Python库。开发团队使用AWS CodePipeline和AWS CodeBuild来构建新应用程序。开发团队用于构建应用程序的CodeBuild作业配置为在VPC中运行。由于合规要求，VPC没有互联网连接。开发团队创建了CodeArtifact的VPC端点并更新了CodeBuild的buildspec.yaml文件。但是，开发团队无法从存储库下载Python库。DevOps工程师应该采取哪些步骤组合，以便开发团队可以使用CodeArtifact？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题考查的是在无互联网连接的VPC环境中，如何解决CodeBuild访问CodeArtifact存储库的权限和网络连接问题。开发团队已经创建了VPC端点但仍无法下载Python库，需要找出缺失的配置步骤。 **涉及的关键AWS服务和概念：** - AWS CodeArtifact：包管理服务，用于存储和共享软件包 - AWS CodeBuild：持续集成服务，在VPC中运行构建作业 - VPC端点：允许VPC内资源访问AWS服务而无需互联网连接 - IAM权限：控制对AWS资源的访问 - 存储库策略：控制谁可以访问CodeArtifact存储库 **正确答案的原因：** 选项B和D是正确答案： - **选项B**：更新存储库策略的Principal语句以包含CodeBuild项目使用的角色ARN。这是必需的，因为CodeArtifact存储库策略需要明确授权哪些主体可以访问存储库。 - **选项D**：更新CodeBuild项目使用的角色，使其具有使用CodeArtifact存储库的足够权限。CodeBuild需要适当的IAM权限来读取和下载包。 **其他选项错误的原因：** - **选项A**：创建S3网关端点不是必需的，因为CodeArtifact有自己的VPC端点，题目中已经提到团队已经创建了CodeArtifact的VPC端点。 - **选项C**：使用AWS RAM共享CodeArtifact存储库不是必需的，因为存储库已经在同一个组织中，可以通过适当的策略配置来访问。 - **选项E**：指定委托管理员不是解决当前访问问题的必要步骤。 **决策标准和最佳实践：** 1. **双重权限控制**：CodeArtifact访问需要同时配置存储库策略（资源级权限）和IAM角色权限（身份级权限） 2. **最小权限原则**：只授予CodeBuild角色访问所需CodeArtifact存储库的最小必要权限 3. **网络隔离**：在无互联网连接的VPC中，确保正确配置VPC端点以访问AWS服务 4. **策略验证**：确保存储库策略和IAM策略都正确配置，缺一不可</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">111</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses a series of individual Amazon CloudFormation templates to deploy its multi-Region applications. These templates must be deployed in a specific order. The company is making more changes to the templates than previously expected and wants to deploy new templates more efficiently. Additionally, the data engineering team must be notified of all changes to the templates. What should the company do to accomplish these goals? D (93%) 7%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS Lambda function to deploy the CloudFormation templates in the required order. Use stack policies to alert the data engineering team.
B. Host the CloudFormation templates in Amazon S3. Use Amazon S3 events to directly trigger CloudFormation updates and Amazon SNS notifications.
C. Implement CloudFormation StackSets and use drift detection to trigger update alerts to the data engineering team.
D. Leverage CloudFormation nested stacks and stack sets for deployments. Use Amazon SNS to notify the data engineering team.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用一系列独立的Amazon CloudFormation模板来部署其多区域应用程序。这些模板必须按特定顺序部署。该公司对模板的更改比之前预期的要多，希望更高效地部署新模板。此外，数据工程团队必须收到所有模板更改的通知。公司应该怎么做来实现这些目标？ 选项： A. 创建AWS Lambda函数按所需顺序部署CloudFormation模板。使用stack policies来提醒数据工程团队。 B. 将CloudFormation模板托管在Amazon S3中。使用Amazon S3事件直接触发CloudFormation更新和Amazon SNS通知。 C. 实施CloudFormation StackSets并使用drift detection来触发向数据工程团队的更新警报。 D. 利用CloudFormation nested stacks和stack sets进行部署。使用Amazon SNS通知数据工程团队。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 需要按特定顺序部署多个CloudFormation模板 2. 提高模板部署效率 3. 当模板发生变更时通知数据工程团队 4. 支持多区域部署 **涉及的关键AWS服务和概念：** - CloudFormation: AWS基础设施即代码服务 - S3 Events: S3对象变更事件触发机制 - SNS: 消息通知服务 - Lambda: 无服务器计算服务 - StackSets: 跨账户和区域的stack管理 - Nested Stacks: 嵌套stack结构 - Stack Policies: stack保护策略 - Drift Detection: 配置漂移检测 **正确答案B的原因：** 1. **自动化触发**: S3事件可以在模板文件更新时自动触发CloudFormation部署，实现高效部署 2. **通知机制**: S3事件可以同时触发SNS通知，满足团队通知需求 3. **简单有效**: 直接利用S3的事件驱动机制，架构简洁 4. **版本控制**: S3可以很好地管理模板版本 **其他选项错误的原因：** - **选项A**: Stack policies主要用于防止意外更新，不是通知机制；Lambda方案增加了复杂性 - **选项C**: Drift detection是检测配置偏移的，不是用于模板变更通知；StackSets主要解决跨账户/区域问题，不直接解决部署效率问题 - **选项D**: 虽然提到了SNS通知，但nested stacks和stack sets的组合过于复杂，且没有明确的触发机制 **决策标准和最佳实践：** 1. **事件驱动架构**: 利用AWS服务的原生事件机制实现自动化 2. **简化架构**: 选择最简单有效的解决方案 3. **松耦合设计**: S3事件可以同时触发多个下游服务 4. **可扩展性**: S3事件机制可以轻松扩展到更多的自动化流程</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">112</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer has implemented a CI/CD pipeline to deploy an AWS CloudFormation template that provisions a web application. The web application consists of an Application Load Balancer (ALB), a target group, a launch template that uses an Amazon Linux 2 AMI, an Auto Scaling group of Amazon EC2 instances, a security group, and an Amazon RDS for MySQL database. The launch template includes user data that specifies a script to install and start the application. The initial deployment of the application was successful. The DevOps engineer made changes to update the version of the application with the user data. The CI/CD pipeline has deployed a new version of the template. However, the health checks on the ALB are now failing. The health checks have marked all targets as unhealthy. During investigation, the DevOps engineer notices that the CloudFormation stack has a status of UPDATE_COMPLETE. However, when the DevOps engineer connects to one of the EC2 instances and checks /var/log/messages, the DevOps engineer notices that the Apache web server failed to start successfully because of a configuration error. How can the DevOps engineer ensure that the CloudFormation deployment will fail if the user data fails to successfully finish running? CloudFormation template. Set an appropriate timeout for the update policy. Most Voted Create an Amazon Simple Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation. Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation. Set an appropriate timeout on the lifecycle hook. an appropriate invocation timeout. Configure the Lambda function to use the SignalResource API operation to signal success or failure to CloudFormation. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use the cfn-signal helper script to signal success or failure to CloudFormation. Use the WaitOnResourceSignals update policy within the
B. Create an Amazon CloudWatch alarm for the UnhealthyHostCount metric. Include an appropriate alarm threshold for the target group.
C. Create a lifecycle hook on the Auto Scaling group by using the AWS::AutoScaling::LifecycleHook resource. Create an Amazon Simple
D. Use the Amazon CloudWatch agent to stream the cloud-init logs. Create a subscription filter that includes an AWS Lambda function with</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师实施了一个CI/CD流水线来部署AWS CloudFormation模板，该模板提供一个Web应用程序。该Web应用程序包含一个Application Load Balancer (ALB)、一个目标组、一个使用Amazon Linux 2 AMI的启动模板、一个Amazon EC2实例的Auto Scaling组、一个安全组和一个Amazon RDS for MySQL数据库。启动模板包含用户数据，指定安装和启动应用程序的脚本。应用程序的初始部署是成功的。DevOps工程师进行了更改以通过用户数据更新应用程序版本。CI/CD流水线已部署了新版本的模板。但是，ALB上的健康检查现在失败了。健康检查将所有目标标记为不健康。在调查过程中，DevOps工程师注意到CloudFormation堆栈的状态为UPDATE_COMPLETE。但是，当DevOps工程师连接到其中一个EC2实例并检查/var/log/messages时，DevOps工程师注意到Apache Web服务器由于配置错误而未能成功启动。DevOps工程师如何确保如果用户数据未能成功完成运行，CloudFormation部署将失败？ 选项： A. 使用cfn-signal辅助脚本向CloudFormation发送成功或失败信号。在CloudFormation模板中使用WaitOnResourceSignals更新策略。为更新策略设置适当的超时时间。 B. 为UnhealthyHostCount指标创建Amazon CloudWatch告警。为目标组包含适当的告警阈值。 C. 通过使用AWS::AutoScaling::LifecycleHook资源在Auto Scaling组上创建生命周期钩子。创建Amazon Simple Notification Service (Amazon SNS)主题作为向CloudFormation发送成功或失败信号的目标。在生命周期钩子上设置适当的超时时间。 D. 使用Amazon CloudWatch代理流式传输cloud-init日志。创建包含AWS Lambda函数的订阅过滤器，配置适当的调用超时时间。配置Lambda函数使用SignalResource API操作向CloudFormation发送成功或失败信号。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求解决CloudFormation部署中用户数据脚本执行失败但CloudFormation状态仍显示UPDATE_COMPLETE的问题。核心需求是让CloudFormation能够感知到用户数据脚本的执行状态，如果脚本失败则整个部署应该失败。 **涉及的关键AWS服务和概念：** - CloudFormation：基础设施即代码服务，用于自动化资源部署 - EC2用户数据：实例启动时执行的脚本 - cfn-signal：CloudFormation辅助脚本，用于向CloudFormation发送信号 - WaitOnResourceSignals：CloudFormation更新策略，等待资源信号 - Auto Scaling组：自动扩缩容服务 - Application Load Balancer：应用负载均衡器 **正确答案A的原因：** cfn-signal是专门为此场景设计的CloudFormation辅助工具。它允许EC2实例在用户数据脚本执行完成后向CloudFormation发送成功或失败信号。配合WaitOnResourceSignals更新策略，CloudFormation会等待接收到指定数量的成功信号后才认为资源创建/更新成功。如果脚本失败或超时未收到信号，CloudFormation部署将失败。这是AWS推荐的标准做法。 **其他选项错误的原因：** - 选项B：CloudWatch告警只能在部署完成后检测到不健康状态，无法在CloudFormation部署过程中阻止部署完成。 - 选项C：生命周期钩子主要用于Auto Scaling事件处理，不是为CloudFormation部署状态管理设计的，过于复杂且不是标准做法。 - 选项D：通过CloudWatch日志和Lambda处理信号过于复杂，增加了不必要的组件和潜在故障点，不如直接使用cfn-signal简洁有效。 **决策标准和最佳实践：** 1. 选择AWS原生和专门设计的工具（cfn-signal） 2. 保持架构简单，避免过度工程化 3. 使用CloudFormation推荐的标准模式 4. 确保部署状态的准确性和及时反馈 5. 遵循基础设施即代码的最佳实践，让部署状态真实反映应用状态</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">113</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a data ingestion application that runs across multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to monitor the application and consolidate access to the application. Currently, the company is running the application on Amazon EC2 instances from several Auto Scaling groups. The EC2 instances have no access to the internet because the data is sensitive. Engineers have deployed the necessary VPC endpoints. The EC2 instances run a custom AMI that is built specifically for the application. To maintain and troubleshoot the application, system administrators need the ability to log in to the EC2 instances. This access must be automated and controlled centrally. The company&#x27;s security team must receive a notification whenever the instances are accessed. Which solution will meet these requirements? instances from the bastion host. Install AWS Systems Manager Agent on all the EC2 instances. Use Auto Scaling group lifecycle hooks for monitoring and auditing access. Use Systems Manager Session Manager to log in to the instances. Send logs to a log group in Amazon CloudWatch Logs. Export data to Amazon S3 for auditing. Send notifications to the security team by using S3 event notifications. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon EventBridge rule to send notifications to the security team whenever a user logs in to an EC2 instance. Use EC2 Instance Connect to log in to the instances. Deploy Auto Scaling groups by using AWS CloudFormation. Use the cfn-init helper script to deploy appropriate VPC routes for external access. Rebuild the custom AMI so that the custom AMI includes AWS Systems Manager Agent.
B. Deploy a NAT gateway and a bastion host that has internet access. Create a security group that allows incoming traffic on all the EC2
C. Use EC2 Image Builder to rebuild the custom AMI. Include the most recent version of AWS Systems Manager Agent in the image. Configure the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to all the EC2 instances. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic.
D. Use AWS Systems Manager Automation to build Systems Manager Agent into the custom AMI. Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个跨多个AWS账户运行的数据摄取应用程序。这些账户在AWS Organizations的组织中。公司需要监控应用程序并整合对应用程序的访问。目前，公司在来自多个Auto Scaling组的Amazon EC2实例上运行应用程序。由于数据敏感，EC2实例无法访问互联网。工程师已部署了必要的VPC endpoints。EC2实例运行专为应用程序构建的自定义AMI。为了维护和故障排除应用程序，系统管理员需要能够登录到EC2实例。此访问必须是自动化的并集中控制。公司的安全团队必须在实例被访问时收到通知。哪个解决方案将满足这些要求？ 选项A：创建Amazon EventBridge规则，在用户登录EC2实例时向安全团队发送通知。使用EC2 Instance Connect登录实例。使用AWS CloudFormation部署Auto Scaling组。使用cfn-init helper脚本部署适当的VPC路由以进行外部访问。重建自定义AMI，使其包含AWS Systems Manager Agent。 选项B：部署具有互联网访问权限的NAT gateway和bastion host。创建允许所有EC2实例上传入流量的安全组。从bastion host登录实例。在所有EC2实例上安装AWS Systems Manager Agent。使用Auto Scaling组生命周期钩子进行监控和审计访问。使用Systems Manager Session Manager登录实例。将日志发送到Amazon CloudWatch Logs中的日志组。将数据导出到Amazon S3进行审计。使用S3事件通知向安全团队发送通知。 选项C：使用EC2 Image Builder重建自定义AMI。在镜像中包含最新版本的AWS Systems Manager Agent。配置Auto Scaling组将AmazonSSMManagedInstanceCore角色附加到所有EC2实例。使用Systems Manager Session Manager登录实例。启用会话详细信息记录到Amazon S3。为新文件上传创建S3事件通知，通过Amazon Simple Notification Service (Amazon SNS) topic向安全团队发送消息。 选项D：使用AWS Systems Manager Automation将Systems Manager Agent构建到自定义AMI中。配置AWS Config将SCP附加到根组织账户，以允许EC2实例连接到Systems Manager。使用Systems Manager Session Manager登录实例。启用会话详细信息记录到Amazon S3。为新文件上传创建S3事件通知，通过Amazon Simple Notification Service (Amazon SNS) topic向安全团队发送消息。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 为无互联网访问的EC2实例提供安全的远程访问 2. 实现集中化和自动化的访问控制 3. 当实例被访问时通知安全团队 4. 跨多个AWS账户的组织环境中运行 5. 维护现有的自定义AMI **涉及的关键AWS服务和概念：** - AWS Systems Manager Session Manager：提供安全的shell访问，无需SSH密钥或bastion host - EC2 Image Builder：用于自动化AMI构建和管理 - IAM角色AmazonSSMManagedInstanceCore：为EC2实例提供Systems Manager所需权限 - Amazon S3：存储会话日志 - Amazon SNS：发送通知给安全团队 - VPC Endpoints：在无互联网环境中访问AWS服务 **正确答案C的原因：** 1. **EC2 Image Builder**：提供了标准化、自动化的AMI构建流程，确保Systems Manager Agent的正确集成 2. **AmazonSSMManagedInstanceCore角色**：为EC2实例提供了访问Systems Manager所需的最小权限 3. **Session Manager**：完美适合无互联网环境，通过VPC endpoints工作，提供安全的shell访问 4. **S3日志记录 + SNS通知**：实现了完整的审计跟踪和实时通知机制 5. **无需互联网访问**：整个解决方案通过AWS内部网络和VPC endpoints工作 **其他选项错误的原因：** - **选项A**：EC2 Instance Connect需要互联网访问，与题目要求矛盾；EventBridge规则配置复杂且不是最佳实践 - **选项B**：部署NAT gateway和bastion host违背了无互联网访问的安全要求，增加了攻击面和复杂性 - **选项D**：使用AWS Config附加SCP的方法不正确，SCP应该在组织级别管理，而不是通过Config；Systems Manager Automation用于AMI构建不如Image Builder专业 **决策标准和最佳实践：** 1. **安全优先**：Session Manager提供零信任访问，无需SSH密钥管理 2. **自动化**：Image Builder自动化AMI构建和更新流程 3. **审计合规**：完整的会话日志记录和通知机制 4. **最小权限原则**：使用AWS托管的IAM角色，权限精确控制 5. **运维简化**：无需维护bastion host或NAT gateway等额外基础设施</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">114</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses Amazon S3 to store proprietary information. The development team creates buckets for new projects on a daily basis. The security team wants to ensure that all existing and future buckets have encryption, logging, and versioning enabled. Additionally, no buckets should ever be publicly read or write accessible. What should a DevOps engineer do to meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable AWS CloudTrail and configure automatic remediation using AWS Lambda.
B. Enable AWS Config rules and configure automatic remediation using AWS Systems Manager documents.
C. Enable AWS Trusted Advisor and configure automatic remediation using Amazon EventBridge.
D. Enable AWS Systems Manager and configure automatic remediation using Systems Manager documents.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用Amazon S3存储专有信息。开发团队每天为新项目创建bucket。安全团队希望确保所有现有和未来的bucket都启用加密、日志记录和版本控制。此外，任何bucket都不应该允许公共读取或写入访问。DevOps工程师应该怎么做来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现对S3 bucket的自动化合规性管理，包括：1）确保所有bucket启用加密、日志记录和版本控制；2）防止bucket被公开访问；3）对现有和未来创建的bucket都要生效；4）需要自动化remediation（修复）机制。 **涉及的关键AWS服务和概念：** - AWS Config：用于监控和评估AWS资源配置合规性的服务 - AWS Config Rules：预定义或自定义的规则，用于检查资源配置是否符合要求 - AWS Systems Manager Documents：自动化文档，可以执行remediation操作 - S3安全最佳实践：加密、版本控制、访问日志、防止公共访问 **正确答案B的原因：** AWS Config是专门用于资源配置合规性监控的服务。它可以：1）持续监控S3 bucket配置；2）使用预置规则检查加密、版本控制、公共访问等设置；3）通过Systems Manager Documents实现自动remediation；4）对新创建的资源自动应用规则检查。这完全符合题目要求的自动化合规性管理需求。 **其他选项错误的原因：** A选项：CloudTrail主要用于API调用审计，不是配置合规性监控工具，无法检查bucket配置状态。 C选项：Trusted Advisor提供最佳实践建议，但不提供持续的配置监控和自动remediation功能。 D选项：Systems Manager本身不提供配置合规性监控，需要与Config配合使用才能实现完整的解决方案。 **决策标准和最佳实践：** 选择AWS Config的关键在于：1）需要持续监控资源配置合规性；2）需要自动化检测和修复；3）需要覆盖现有和未来资源。AWS Config专门为此类场景设计，是配置管理和合规性监控的标准解决方案。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">115</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is researching the least expensive way to implement an image batch processing cluster on AWS. The application cannot run in Docker containers and must run on Amazon EC2. The batch job stores checkpoint data on an NFS volume and can tolerate interruptions. Configuring the cluster software from a generic EC2 Linux image takes 30 minutes. What is the MOST cost-effective solution? D (87%) C (9%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Amazon EFS for checkpoint data. To complete the job, use an EC2 Auto Scaling group and an On-Demand pricing model to provision EC2 instances temporarily.
B. Use GlusterFS on EC2 instances for checkpoint data. To run the batch job, configure EC2 instances manually. When the job completes, shut down the instances manually.
C. Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances, and utilize user data to configure the EC2 Linux instance on startup.
D. Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances. Create a custom AMI for the cluster and use the latest AMI when creating instances.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在研究在AWS上实现图像批处理集群的最经济方式。该应用程序无法在Docker容器中运行，必须在Amazon EC2上运行。批处理作业将检查点数据存储在NFS卷上，可以容忍中断。从通用EC2 Linux镜像配置集群软件需要30分钟。什么是最具成本效益的解决方案？ 选项： A. 使用Amazon EFS存储检查点数据。为完成作业，使用EC2 Auto Scaling组和On-Demand定价模式临时配置EC2实例。 B. 使用EC2实例上的GlusterFS存储检查点数据。为运行批处理作业，手动配置EC2实例。作业完成后，手动关闭实例。 C. 使用Amazon EFS存储检查点数据。使用EC2 Fleet启动EC2 Spot Instances，并利用user data在启动时配置EC2 Linux实例。 D. 使用Amazon EFS存储检查点数据。使用EC2 Fleet启动EC2 Spot Instances。为集群创建自定义AMI，并在创建实例时使用最新的AMI。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到最经济的批处理集群解决方案，关键约束条件包括：必须运行在EC2上、需要NFS存储、可容忍中断、配置时间30分钟。 **涉及的关键AWS服务和概念：** - Amazon EFS：托管的NFS文件系统服务 - EC2 Spot Instances：利用空闲容量的低成本实例 - EC2 Fleet：批量管理EC2实例的服务 - User Data：实例启动时自动执行的脚本 - 自定义AMI：预配置的机器镜像 - Auto Scaling：自动扩缩容服务 **正确答案C的原因：** 1. **存储选择**：Amazon EFS提供托管的NFS服务，无需维护，成本效益高 2. **计算成本**：Spot Instances比On-Demand实例便宜60-90%，适合可容忍中断的工作负载 3. **管理效率**：EC2 Fleet可以高效管理Spot Instances的生命周期 4. **配置方式**：User data脚本在启动时自动配置，无需人工干预，虽然每次需要30分钟但完全自动化 **其他选项错误的原因：** - **选项A**：使用On-Demand实例成本过高，不符合最经济的要求 - **选项B**：GlusterFS需要手动管理和维护，增加运维成本；手动配置和关闭实例效率低下 - **选项D**：创建和维护自定义AMI增加了额外的管理开销和存储成本，对于30分钟的配置时间来说不够经济 **决策标准和最佳实践：** 1. **成本优化**：优先选择Spot Instances用于可中断的批处理工作负载 2. **自动化优先**：选择自动化程度高的解决方案减少人工干预 3. **托管服务**：优先使用AWS托管服务（如EFS）而非自建解决方案 4. **配置时间权衡**：30分钟的配置时间通过user data自动化是可接受的，无需为此创建自定义AMI</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">116</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company recently migrated its legacy application from on-premises to AWS. The application is hosted on Amazon EC2 instances behind an Application Load Balancer, which is behind Amazon API Gateway. The company wants to ensure users experience minimal disruptions during any deployment of a new version of the application. The company also wants to ensure it can quickly roll back updates if there is an issue. Which solution will meet these requirements with MINIMAL changes to the application? A (83%) Other</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Introduce changes as a separate environment parallel to the existing one. Configure API Gateway to use a canary release deployment to send a small subset of user traffic to the new environment.
B. Introduce changes as a separate environment parallel to the existing one. Update the application&#x27;s DNS alias records to point to the new environment.
C. Introduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route user traffic to the new target group in steps.
D. Introduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route all traffic to the Application Load Balancer, which then sends the traffic to the new target group.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司最近将其遗留应用程序从本地迁移到AWS。该应用程序托管在Amazon EC2实例上，位于Application Load Balancer后面，而Application Load Balancer又位于Amazon API Gateway后面。公司希望确保用户在部署新版本应用程序时体验到最小的中断。公司还希望确保在出现问题时能够快速回滚更新。哪种解决方案能够以对应用程序的最小更改来满足这些要求？ 选项： A. 将更改作为与现有环境并行的独立环境引入。配置API Gateway使用canary release部署，将一小部分用户流量发送到新环境。 B. 将更改作为与现有环境并行的独立环境引入。更新应用程序的DNS别名记录以指向新环境。 C. 将更改作为现有Application Load Balancer后面的独立目标组引入。配置API Gateway分步骤将用户流量路由到新目标组。 D. 将更改作为现有Application Load Balancer后面的独立目标组引入。配置API Gateway将所有流量路由到Application Load Balancer，然后由负载均衡器将流量发送到新目标组。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个部署策略，能够实现：1）最小化用户中断；2）快速回滚能力；3）对应用程序的最小更改。 **涉及的关键AWS服务和概念：** - API Gateway：提供API管理和流量路由功能 - Application Load Balancer (ALB)：七层负载均衡器，支持目标组管理 - Target Group：ALB的流量分发目标，可以包含多个EC2实例 - Canary部署：渐进式部署策略，先向少量用户发布新版本 - 蓝绿部署：通过并行环境实现零停机部署 **正确答案C的原因：** 1. **最小架构变更**：利用现有的ALB，只需添加新的目标组，无需创建完整的并行环境 2. **渐进式部署**：API Gateway可以配置加权路由，逐步将流量从旧目标组转移到新目标组 3. **快速回滚**：通过调整API Gateway的路由权重，可以立即将流量切回旧目标组 4. **成本效益**：不需要维护完整的并行基础设施 **其他选项错误的原因：** - **选项A**：创建完整并行环境成本高，虽然canary部署策略正确，但不符合&quot;最小更改&quot;要求 - **选项B**：DNS更改传播时间长（TTL影响），无法实现快速回滚，且DNS切换是全量切换，不够渐进 - **选项D**：描述不准确，ALB无法智能地只将流量发送到&quot;新&quot;目标组，需要通过API Gateway层面的路由控制 **决策标准和最佳实践：** 1. **渐进式部署优于一次性切换**：能够及早发现问题并限制影响范围 2. **利用现有基础设施**：在满足需求的前提下，最大化利用现有资源 3. **快速回滚能力**：部署策略必须支持快速、可靠的回滚机制 4. **流量控制粒度**：API Gateway提供了更精细的流量控制能力，比DNS切换更适合渐进式部署</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">117</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is storing 100 GB of log data in .csv format in an Amazon S3 bucket. SQL developers want to query this data and generate graphs to visualize it. The SQL developers also need an efficient, automated way to store metadata from the .csv file. Which combination of steps will meet these requirements with the LEAST amount of effort? (Choose three.) F. Use Amazon DynamoDB as the persistent metadata store. BCE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Filter the data through AWS X-Ray to visualize the data.
B. Filter the data through Amazon QuickSight to visualize the data.
C. Query the data with Amazon Athena.
D. Query the data with Amazon Redshift.
E. Use the AWS Glue Data Catalog as the persistent metadata store.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在Amazon S3存储桶中以.csv格式存储了100 GB的日志数据。SQL开发人员希望查询这些数据并生成图表来可视化它。SQL开发人员还需要一种高效、自动化的方式来存储.csv文件的元数据。哪种步骤组合能够以最少的工作量满足这些要求？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求找到一个解决方案来：1）查询S3中的CSV数据，2）可视化数据生成图表，3）自动化存储元数据，4）以最少工作量实现。需要选择三个选项的组合。 **涉及的关键AWS服务和概念：** - Amazon S3：对象存储服务，存储CSV文件 - Amazon Athena：无服务器查询服务，可直接查询S3数据 - Amazon QuickSight：商业智能和数据可视化服务 - AWS Glue Data Catalog：托管的元数据存储库 - Amazon Redshift：数据仓库服务 - AWS X-Ray：应用程序跟踪服务 **正确答案BCE的原因：** - B (QuickSight)：专门的数据可视化服务，可直接连接多种数据源生成图表，满足可视化需求 - C (Athena)：无服务器SQL查询服务，可直接查询S3中的CSV文件，无需数据迁移，满足SQL查询需求 - E (Glue Data Catalog)：自动发现和存储元数据，与Athena无缝集成，满足自动化元数据管理需求 **其他选项错误的原因：** - A (X-Ray)：用于应用程序性能监控和调试，不是数据可视化工具 - D (Redshift)：需要先将数据从S3加载到Redshift集群，增加了复杂性和成本，不符合&quot;最少工作量&quot;要求 - F (DynamoDB)：NoSQL数据库，不适合作为结构化元数据存储，且需要额外开发工作 **决策标准和最佳实践：** 选择标准应基于：1）服务间的原生集成能力，2）无服务器架构减少管理开销，3）直接操作S3数据避免数据迁移，4）自动化程度高。Athena + QuickSight + Glue Data Catalog组合提供了完整的无服务器数据分析栈，是处理S3中结构化数据的最佳实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">118</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company deploys its corporate infrastructure on AWS across multiple AWS Regions and Availability Zones. The infrastructure is deployed on Amazon EC2 instances and connects with AWS IoT Greengrass devices. The company deploys additional resources on on-premises servers that are located in the corporate headquarters. The company wants to reduce the overhead involved in maintaining and updating its resources. The company&#x27;s DevOps team plans to use AWS Systems Manager to implement automated management and application of patches. The DevOps team confirms that Systems Manager is available in the Regions that the resources are deployed in. Systems Manager also is available in a Region near the corporate headquarters. Which combination of steps must the DevOps team take to implement automated patch and configuration management across the company&#x27;s EC2 instances, IoT devices, and on-premises infrastructure? (Choose three.) F. Generate a managed-instance activation. Use the Activation Code and Activation ID to install Systems Manager Agent (SSM Agent) on each server in the on-premises environment. Update the AWS IoT Greengrass IAM token exchange role. Use the role to deploy SSM Agent on all the IoT devices. Most Voted CEF (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Apply tags to all the EC2 instances, AWS IoT Greengrass devices, and on-premises servers. Use Systems Manager Session Manager to push patches to all the tagged devices.
B. Use Systems Manager Run Command to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises servers.
C. Use Systems Manager Patch Manager to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises servers as a Systems Manager maintenance window task.
D. Configure Amazon EventBridge to monitor Systems Manager Patch Manager for updates to patch baselines. Associate Systems Manager Run Command with the event to initiate a patch action for all EC2 instances, AWS IoT Greengrass devices, and on-premises servers.
E. Create an IAM instance profile for Systems Manager. Attach the instance profile to all the EC2 instances in the AWS account. For the AWS IoT Greengrass devices and on-premises servers, create an IAM service role for Systems Manager.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在多个AWS区域和可用区部署其企业基础设施。基础设施部署在Amazon EC2实例上，并与AWS IoT Greengrass设备连接。该公司还在位于企业总部的本地服务器上部署了额外资源。公司希望减少维护和更新资源的开销。公司的DevOps团队计划使用AWS Systems Manager来实现补丁的自动化管理和应用。DevOps团队确认Systems Manager在资源部署的区域中可用，在企业总部附近的区域中也可用。DevOps团队必须采取哪些步骤组合来在公司的EC2实例、IoT设备和本地基础设施中实现自动化补丁和配置管理？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求选择三个步骤来实现跨EC2实例、IoT Greengrass设备和本地服务器的自动化补丁和配置管理。需要使用AWS Systems Manager来减少维护开销。 **涉及的关键AWS服务和概念：** - AWS Systems Manager：统一的运维管理服务 - Systems Manager Agent (SSM Agent)：在实例上运行的代理程序 - Patch Manager：自动化补丁管理服务 - Run Command：远程执行命令的功能 - Maintenance Windows：维护窗口调度 - IAM角色和实例配置文件：权限管理 - 混合环境管理：AWS云端和本地资源的统一管理 **正确答案分析（C、E、F）：** - **选项C**：使用Patch Manager作为维护窗口任务来调度补丁是最佳实践，提供了自动化和调度功能 - **选项E**：为EC2实例创建IAM实例配置文件，为IoT设备和本地服务器创建IAM服务角色，这是必需的权限配置 - **选项F**：生成托管实例激活码来在本地服务器上安装SSM Agent，更新IoT Greengrass IAM令牌交换角色来在IoT设备上部署SSM Agent，这是混合环境的必要步骤 **其他选项错误的原因：** - **选项A**：Session Manager主要用于交互式会话，不是补丁管理的最佳工具 - **选项B**：Run Command可以执行补丁，但缺乏Patch Manager的自动化调度和基线管理功能 - **选项D**：通过EventBridge监控补丁基线更新过于复杂，不是标准的补丁管理方法 **决策标准和最佳实践：** 1. 使用专门的补丁管理工具（Patch Manager）而不是通用命令执行工具 2. 正确配置IAM权限以支持混合环境 3. 为不同类型的资源（EC2、IoT、本地）采用适当的SSM Agent部署方法 4. 利用维护窗口实现自动化调度，减少人工干预</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">119</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is testing a web application that runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company uses a blue/green deployment process with immutable instances when deploying new software. During testing, users are being automatically logged out of the application at random times. Testers also report that, when a new version of the application is deployed, all users are logged out. The development team needs a solution to ensure users remain logged in across scaling events and application deployments. What is the MOST operationally efficient way to ensure users remain logged in? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Enable smart sessions on the load balancer and modify the application to check for an existing session.
B. Enable session sharing on the load balancer and modify the application to read from the session store.
C. Store user session information in an Amazon S3 bucket and modify the application to read session information from the bucket.
D. Modify the application to store user session information in an Amazon ElastiCache cluster.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在测试一个运行在Application Load Balancer后面的Amazon EC2实例上的Web应用程序。这些实例在Auto Scaling组中跨多个Availability Zone运行。公司在部署新软件时使用蓝/绿部署流程和不可变实例。在测试期间，用户会在随机时间被自动登出应用程序。测试人员还报告说，当部署新版本的应用程序时，所有用户都会被登出。开发团队需要一个解决方案来确保用户在扩展事件和应用程序部署期间保持登录状态。确保用户保持登录状态的最具运营效率的方法是什么？ 选项： A. 在负载均衡器上启用智能会话并修改应用程序以检查现有会话 B. 在负载均衡器上启用会话共享并修改应用程序从会话存储中读取 C. 将用户会话信息存储在Amazon S3存储桶中并修改应用程序从存储桶读取会话信息 D. 修改应用程序将用户会话信息存储在Amazon ElastiCache集群中 正确答案：D</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求解决用户会话在Auto Scaling事件和蓝/绿部署期间丢失的问题。关键挑战是当EC2实例被终止和替换时（无论是由于扩展还是部署），存储在本地的会话数据会丢失，导致用户被强制登出。 **涉及的关键AWS服务和概念：** - Application Load Balancer (ALB)：负载分发和会话粘性 - Auto Scaling Group：自动扩展和收缩实例 - Amazon ElastiCache：内存缓存服务，支持Redis和Memcached - Amazon S3：对象存储服务 - 蓝/绿部署：使用不可变基础设施的部署策略 - 会话管理：维护用户登录状态的机制 **正确答案D的原因：** 1. **高性能**：ElastiCache是内存数据库，提供毫秒级的读写延迟，非常适合频繁访问的会话数据 2. **持久性和可用性**：会话数据存储在独立于EC2实例的外部服务中，实例终止不会影响会话 3. **可扩展性**：ElastiCache可以根据需要扩展，支持高并发访问 4. **运营效率高**：AWS托管服务，减少运维负担，支持自动故障转移和备份 5. **专门用途**：ElastiCache专为缓存和会话存储等用例设计 **其他选项错误的原因：** - **选项A**：ALB没有&quot;智能会话&quot;功能，这不是AWS的标准功能 - **选项B**：ALB没有内置的&quot;会话共享&quot;功能，粘性会话仍然依赖特定实例 - **选项C**：S3虽然持久可靠，但不适合频繁的读写操作，延迟较高，会影响用户体验，且成本较高 **决策标准和最佳实践：** 1. **外部化会话存储**：将会话数据从应用服务器中分离出来 2. **选择合适的存储服务**：根据访问模式选择高性能的缓存服务 3. **考虑运营复杂性**：选择托管服务减少运维负担 4. **性能优先**：会话数据需要快速访问，内存缓存是最佳选择 5. **高可用性设计**：确保会话存储服务本身具有高可用性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">120</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer needs to configure a blue/green deployment for an existing three-tier application. The application runs on Amazon EC2 instances and uses an Amazon RDS database. The EC2 instances run behind an Application Load Balancer (ALB) and are in an Auto Scaling group. The DevOps engineer has created a launch template and an Auto Scaling group for the blue environment. The DevOps engineer also has created a launch template and an Auto Scaling group for the green environment. Each Auto Scaling group deploys to a matching blue or green target group. The target group also specifies which software, blue or green, gets loaded on the EC2 instances. The ALB can be configured to send traffic to the blue environment&#x27;s target group or the green environment&#x27;s target group. An Amazon Route 53 record for www.example.com points to the ALB. The deployment must move traffic all at once between the software on the blue environment&#x27;s EC2 instances to the newly deployed software on the green environment&#x27;s EC2 instances. What should the DevOps engineer do to meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment&#x27;s EC2 instances. When the rolling restart is complete, use an AWS CLI command to update the ALB to send traffic to the green environment&#x27;s target group.
B. Use an AWS CLI command to update the ALB to send traffic to the green environment&#x27;s target group. Then start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment&#x27;s EC2 instances.
C. Update the launch template to deploy the green environment&#x27;s software on the blue environment&#x27;s EC2 instances. Keep the target groups and Auto Scaling groups unchanged in both environments. Perform a rolling restart of the blue environment&#x27;s EC2 instances.
D. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment&#x27;s EC2 instances. When the rolling restart is complete, update the Route 53 DNS to point to the green environment&#x27;s endpoint on the ALB.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师需要为现有的三层应用程序配置蓝绿部署。该应用程序运行在Amazon EC2实例上并使用Amazon RDS数据库。EC2实例运行在Application Load Balancer (ALB)后面，并位于Auto Scaling组中。DevOps工程师已经为蓝色环境创建了启动模板和Auto Scaling组，也为绿色环境创建了启动模板和Auto Scaling组。每个Auto Scaling组部署到匹配的蓝色或绿色目标组。目标组还指定在EC2实例上加载哪个软件（蓝色或绿色）。ALB可以配置为将流量发送到蓝色环境的目标组或绿色环境的目标组。Amazon Route 53记录www.example.com指向ALB。部署必须一次性将流量从蓝色环境EC2实例上的软件切换到绿色环境EC2实例上新部署的软件。DevOps工程师应该怎么做来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是蓝绿部署的正确实施步骤。关键要求是&quot;一次性切换所有流量&quot;从蓝色环境到绿色环境，这是蓝绿部署的核心特征。 **涉及的关键AWS服务和概念：** - 蓝绿部署策略：两个完全相同的生产环境，通过切换流量实现零停机部署 - Application Load Balancer (ALB)：负载均衡器，可以在目标组之间切换流量 - Auto Scaling组：自动扩缩容服务 - 目标组：ALB的流量分发目标 - Route 53：DNS服务 **正确答案A的原因：** 1. **正确的执行顺序**：先准备好绿色环境（rolling restart部署新软件），再切换流量 2. **确保服务可用性**：在切换流量之前，绿色环境已经完全准备就绪并通过健康检查 3. **真正的蓝绿部署**：通过ALB在目标组级别进行流量切换，实现瞬时切换 4. **风险控制**：如果绿色环境有问题，可以在切换流量前发现，避免影响用户 **其他选项错误的原因：** - **选项B**：先切换流量再部署软件，会导致用户访问到未准备好的环境，造成服务中断 - **选项C**：在蓝色环境上进行rolling restart不是蓝绿部署，而是滚动部署，不符合题目要求的&quot;一次性切换&quot; - **选项D**：通过Route 53 DNS切换存在DNS缓存问题，无法实现真正的&quot;一次性&quot;切换，且DNS传播有延迟 **决策标准和最佳实践：** 1. **蓝绿部署的黄金法则**：先准备，后切换 2. **健康检查优先**：确保新环境完全健康后再切换流量 3. **使用ALB而非DNS切换**：ALB切换是瞬时的，DNS切换有缓存和传播延迟 4. **保持环境独立性**：蓝绿两个环境应该完全独立，便于快速回滚</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">121</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is building a new pipeline by using AWS CodePipeline and AWS CodeBuild in a build account. The pipeline consists of two stages. The first stage is a CodeBuild job to build and package an AWS Lambda function. The second stage consists of deployment actions that operate on two different AWS accounts: a development environment account and a production environment account. The deployment stages use the AWS CloudFormation action that CodePipeline invokes to deploy the infrastructure that the Lambda function requires. A DevOps engineer creates the CodePipeline pipeline and configures the pipeline to encrypt build artifacts by using the AWS Key Management Service (AWS KMS) AWS managed key for Amazon S3 (the aws/s3 key). The artifacts are stored in an S3 bucket. When the pipeline runs, the CloudFormation actions fail with an access denied error. Which combination of actions must the DevOps engineer perform to resolve this error? (Choose two.) action to copy the artifacts to the S3 bucket in each AWS account. Update the CloudFormation actions to reference the artifacts S3 bucket in the production account. perform decrypt operations. Modify the pipeline to use the customer managed KMS key to encrypt artifacts. Most Voted BE (83%) Other</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an S3 bucket in each AWS account for the artifacts. Allow the pipeline to write to the S3 buckets. Create a CodePipeline S3
B. Create a customer managed KMS key. Configure the KMS key policy to allow the IAM roles used by the CloudFormation action to
C. Create an AWS managed KMS key. Configure the KMS key policy to allow the development account and the production account to perform decrypt operations. Modify the pipeline to use the KMS key to encrypt artifacts.
D. In the development account and in the production account, create an IAM role for CodePipeline. Configure the roles with permissions to perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline account, configure the CodePipeline CloudFormation action to use the roles.
E. In the development account and in the production account, create an IAM role for CodePipeline. Configure the roles with permissions to perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline account, modify the artifacts S3 bucket policy to allow the roles access. Configure the CodePipeline CloudFormation action to use the roles.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS CodePipeline和AWS CodeBuild在构建账户中构建新的管道。该管道包含两个阶段。第一阶段是CodeBuild作业，用于构建和打包AWS Lambda函数。第二阶段包含在两个不同AWS账户上运行的部署操作：开发环境账户和生产环境账户。部署阶段使用CodePipeline调用的AWS CloudFormation操作来部署Lambda函数所需的基础设施。DevOps工程师创建了CodePipeline管道，并配置管道使用AWS Key Management Service (AWS KMS) AWS托管密钥for Amazon S3 (aws/s3密钥)来加密构建工件。工件存储在S3存储桶中。当管道运行时，CloudFormation操作失败并出现访问拒绝错误。DevOps工程师必须执行哪些操作组合来解决此错误？（选择两个。） 选项： A. 在每个AWS账户中为工件创建S3存储桶。允许管道写入S3存储桶。创建CodePipeline S3操作将工件复制到每个AWS账户中的S3存储桶。更新CloudFormation操作以引用生产账户中的工件S3存储桶。 B. 创建客户托管KMS密钥。配置KMS密钥策略以允许CloudFormation操作使用的IAM角色执行解密操作。修改管道以使用客户托管KMS密钥来加密工件。 C. 创建AWS托管KMS密钥。配置KMS密钥策略以允许开发账户和生产账户执行解密操作。修改管道以使用KMS密钥来加密工件。 D. 在开发账户和生产账户中，为CodePipeline创建IAM角色。配置角色具有执行CloudFormation操作的权限以及从工件S3存储桶检索和解密对象的权限。在CodePipeline账户中，配置CodePipeline CloudFormation操作使用这些角色。 E. 在开发账户和生产账户中，为CodePipeline创建IAM角色。配置角色具有执行CloudFormation操作的权限以及从工件S3存储桶检索和解密对象的权限。在CodePipeline账户中，修改工件S3存储桶策略以允许角色访问。配置CodePipeline CloudFormation操作使用这些角色。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是跨账户CodePipeline部署时的权限和加密问题。当前问题是CloudFormation操作在访问加密工件时出现访问拒绝错误，需要解决跨账户访问KMS加密的S3对象的权限问题。 **涉及的关键AWS服务和概念：** 1. AWS CodePipeline - 持续集成/持续部署管道 2. AWS KMS - 密钥管理服务，包括AWS托管密钥和客户托管密钥 3. 跨账户访问控制和IAM角色 4. S3存储桶策略和对象加密 5. CloudFormation跨账户部署 **正确答案的原因：** 选项B和E是正确答案。 选项B正确的原因： - AWS托管的aws/s3密钥策略无法修改，不支持跨账户访问 - 客户托管KMS密钥允许自定义密钥策略，可以授权其他账户的IAM角色进行解密操作 - 这是解决跨账户KMS加密对象访问的标准做法 选项E正确的原因： - 跨账户CloudFormation部署需要在目标账户中创建专门的IAM角色 - 需要同时配置IAM角色权限和S3存储桶策略来实现跨账户访问 - CodePipeline可以assume这些跨账户角色来执行部署操作 **其他选项错误的原因：** 选项A：创建多个S3存储桶会增加复杂性，且没有解决KMS权限问题，不是最佳实践。 选项C：AWS托管KMS密钥的策略无法修改，无法添加跨账户权限，这在技术上是不可行的。 选项D：缺少S3存储桶策略的配置，仅有IAM角色权限不足以实现跨账户S3访问。 **决策标准和最佳实践：** 1. 跨账户KMS访问必须使用客户托管密钥 2. 跨账户资源访问需要同时配置IAM权限和资源策略 3. 使用assume role模式进行跨账户操作是AWS推荐的安全实践 4. 最小权限原则：只授予必要的权限给特定的IAM角色</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">122</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using an organization in AWS Organizations to manage multiple AWS accounts. The company&#x27;s development team wants to use AWS Lambda functions to meet resiliency requirements and is rewriting all applications to work with Lambda functions that are deployed in a VPC. The development team is using Amazon Elastic File System (Amazon EFS) as shared storage in Account A in the organization. The company wants to continue to use Amazon EFS with Lambda. Company policy requires all serverless projects to be deployed in Account B. A DevOps engineer needs to reconfigure an existing EFS file system to allow Lambda functions to access the data through an existing EFS access point. Which combination of steps should the DevOps engineer take to meet these requirements? (Choose three.) F. Configure the Lambda functions in Account B to assume an existing IAM role in Account A. ADE (71%) AEF (21%) 8%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A.
B. Create SCPs to set permission guardrails with fine-grained control for Amazon EFS.
C. Create a new EFS file system in Account B. Use AWS Database Migration Service (AWS DMS) to keep data from Account A and Account B synchronized.
D. Update the Lambda execution roles with permission to access the VPC and the EFS file system.
E. Create a VPC peering connection to connect Account A to Account B.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS Organizations中的组织来管理多个AWS账户。公司的开发团队希望使用AWS Lambda函数来满足弹性要求，并正在重写所有应用程序以使用部署在VPC中的Lambda函数。开发团队在组织中的账户A中使用Amazon Elastic File System (Amazon EFS)作为共享存储。公司希望继续在Lambda中使用Amazon EFS。公司政策要求所有serverless项目都部署在账户B中。DevOps工程师需要重新配置现有的EFS文件系统，以允许Lambda函数通过现有的EFS访问点访问数据。DevOps工程师应该采取哪些步骤组合来满足这些要求？（选择三个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是跨AWS账户访问EFS文件系统的配置。核心需求是让账户B中的Lambda函数能够访问账户A中现有的EFS文件系统，同时满足公司将serverless项目部署在账户B的政策要求。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理 - AWS Lambda：serverless计算服务 - Amazon EFS：弹性文件系统，支持跨账户访问 - VPC：虚拟私有云网络 - IAM角色和权限：跨账户访问控制 - EFS文件系统策略：资源级访问控制 **正确答案的原因：** 根据题目显示的统计数据，ADE组合占71%，AEF组合占21%，说明A选项是必选项。 - **选项A**：更新EFS文件系统策略以提供账户B访问权限是跨账户EFS访问的核心要求，这是资源级权限控制的关键步骤 - **选项D**：更新Lambda执行角色权限以访问VPC和EFS文件系统是必需的，Lambda需要适当的IAM权限才能访问EFS - **选项E**：创建VPC对等连接是实现跨账户网络连通性的必要条件，因为EFS需要通过网络访问 **其他选项错误的原因：** - **选项B**：SCP（服务控制策略）主要用于限制权限而非授予细粒度访问权限，不是解决跨账户EFS访问的直接方案 - **选项C**：创建新的EFS文件系统并使用DMS同步违背了题目要求继续使用现有EFS文件系统的要求，且DMS主要用于数据库迁移而非文件系统同步 - **选项F**：让账户B的Lambda函数assume账户A的IAM角色虽然可行，但增加了复杂性，且不如直接配置EFS文件系统策略来得直接有效 **决策标准和最佳实践：** 1. 跨账户资源访问应优先使用资源策略（如EFS文件系统策略） 2. 网络连通性是跨账户VPC资源访问的前提条件 3. 遵循最小权限原则，给Lambda执行角色分配必要的访问权限 4. 尽量使用现有资源而非创建重复资源，避免数据同步的复杂性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">123</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A media company has several thousand Amazon EC2 instances in an AWS account. The company is using Slack and a shared email inbox for team communications and important updates. A DevOps engineer needs to send all AWS-scheduled EC2 maintenance notifications to the Slack channel and the shared inbox. The solution must include the instances&#x27; Name and Owner tags. Which solution will meet these requirements? EC2 maintenance notifications to Amazon Simple Notification Service (Amazon SNS). Configure Amazon SNS to target the Slack channel and the shared inbox. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Integrate AWS Trusted Advisor with AWS Config. Configure a custom AWS Config rule to invoke an AWS Lambda function to publish notifications to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe a Slack channel endpoint and the shared inbox to the topic.
B. Use Amazon EventBridge to monitor for AWS Health events. Configure the maintenance events to target an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to send notifications to the Slack channel and the shared inbox.
C. Create an AWS Lambda function that sends EC2 maintenance notifications to the Slack channel and the shared inbox. Monitor EC2 health events by using Amazon CloudWatch metrics. Configure a CloudWatch alarm that invokes the Lambda function when a maintenance notification is received.
D. Configure AWS Support integration with AWS CloudTrail. Create a CloudTrail lookup event to invoke an AWS Lambda function to pass</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家媒体公司在AWS账户中有数千个Amazon EC2实例。该公司使用Slack和共享邮箱进行团队沟通和重要更新。DevOps工程师需要将所有AWS计划的EC2维护通知发送到Slack频道和共享邮箱。解决方案必须包含实例的Name和Owner标签。哪个解决方案能满足这些要求？ 选项： A. 将AWS Trusted Advisor与AWS Config集成。配置自定义AWS Config规则来调用AWS Lambda函数，将通知发布到Amazon Simple Notification Service (Amazon SNS)主题。将Slack频道端点和共享邮箱订阅该主题。 B. 使用Amazon EventBridge监控AWS Health事件。配置维护事件以Amazon Simple Notification Service (Amazon SNS)主题为目标。将AWS Lambda函数订阅到SNS主题，向Slack频道和共享邮箱发送通知。 C. 创建AWS Lambda函数，将EC2维护通知发送到Slack频道和共享邮箱。使用Amazon CloudWatch指标监控EC2健康事件。配置CloudWatch警报，在收到维护通知时调用Lambda函数。 D. 配置AWS Support与AWS CloudTrail的集成。创建CloudTrail查找事件来调用AWS Lambda函数传递...</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个解决方案，能够自动捕获AWS计划的EC2维护通知，并将这些通知（包含实例的Name和Owner标签信息）同时发送到Slack频道和共享邮箱。 **涉及的关键AWS服务和概念：** - AWS Health API：提供AWS服务健康状态和维护事件信息 - Amazon EventBridge：事件驱动架构的核心服务，能够监控和路由AWS服务事件 - Amazon SNS：消息通知服务，支持多种订阅方式 - AWS Lambda：无服务器计算服务，用于处理事件和自定义逻辑 - EC2标签：实例元数据，需要通过API获取 **正确答案B的原因：** 1. **事件源正确**：Amazon EventBridge是监控AWS Health事件的标准方式，能够直接捕获EC2维护通知 2. **架构合理**：EventBridge → SNS → Lambda的架构既解耦又高效 3. **功能完整**：Lambda函数可以获取EC2实例标签信息，并格式化消息发送到多个目标 4. **扩展性好**：SNS支持多种订阅方式，便于后续扩展 **其他选项错误的原因：** - **选项A**：AWS Trusted Advisor主要用于成本优化和安全建议，不是监控维护事件的合适工具；AWS Config用于配置合规性检查，与维护通知无关 - **选项C**：CloudWatch指标无法直接监控EC2维护事件，这类事件需要通过AWS Health API获取，不是通过指标 - **选项D**：CloudTrail记录API调用日志，不是获取维护通知的正确方式；且选项描述不完整 **决策标准和最佳实践：** 1. **选择正确的事件源**：AWS Health事件应通过EventBridge监控 2. **松耦合架构**：使用SNS作为中间层，实现发布-订阅模式 3. **功能分离**：Lambda专注于消息格式化和多渠道发送 4. **可维护性**：标准AWS服务组合，便于运维和故障排查</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">124</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An AWS CodePipeline pipeline has implemented a code release process. The pipeline is integrated with AWS CodeDeploy to deploy versions of an application to multiple Amazon EC2 instances for each CodePipeline stage. During a recent deployment, the pipeline failed due to a CodeDeploy issue. The DevOps team wants to improve monitoring and notifications during deployment to decrease resolution times. What should the DevOps engineer do to create notifications when issues are discovered? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Implement Amazon CloudWatch Logs for CodePipeline and CodeDeploy, create an AWS Config rule to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.
B. Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.
C. Implement AWS CloudTrail to record CodePipeline and CodeDeploy API call information, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.
D. Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an Amazon Inspector assessment target to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个AWS CodePipeline流水线已经实施了代码发布流程。该流水线与AWS CodeDeploy集成，为每个CodePipeline阶段将应用程序版本部署到多个Amazon EC2实例。在最近的一次部署中，流水线由于CodeDeploy问题而失败。DevOps团队希望改进部署期间的监控和通知，以减少解决时间。DevOps工程师应该做什么来在发现问题时创建通知？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个监控和通知解决方案，能够在CodePipeline和CodeDeploy部署过程中出现问题时及时发现并通知相关人员，从而减少故障解决时间。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：持续集成/持续部署(CI/CD)服务 - AWS CodeDeploy：自动化应用程序部署服务 - Amazon EventBridge：事件驱动架构的核心服务，用于应用程序集成 - AWS Lambda：无服务器计算服务，用于处理事件和业务逻辑 - Amazon SNS：消息通知服务 - CloudWatch Logs：日志监控服务 - AWS Config：配置管理和合规性监控服务 - AWS CloudTrail：API调用审计服务 - Amazon Inspector：安全评估服务 **正确答案B的原因：** 1. **EventBridge是最佳选择**：CodePipeline和CodeDeploy都原生支持向EventBridge发送状态变更事件，包括失败、成功等状态 2. **实时事件响应**：EventBridge能够实时捕获部署状态变更，无需轮询或延迟 3. **Lambda灵活处理**：Lambda函数可以灵活地处理事件数据，进行条件判断，决定是否需要发送通知 4. **架构简洁高效**：EventBridge → Lambda → SNS 形成了一个高效的事件驱动通知链路 **其他选项错误的原因：** - **选项A错误**：AWS Config主要用于资源配置合规性检查，不是为了监控部署状态而设计的，且CloudWatch Logs需要主动查询，不是事件驱动的 - **选项C错误**：CloudTrail记录API调用信息，主要用于审计目的，不是实时监控部署状态的最佳选择，且会产生大量不必要的日志数据 - **选项D错误**：Amazon Inspector是安全漏洞评估工具，用于评估应用程序安全性，与部署状态监控无关 **决策标准和最佳实践：** 1. **选择事件驱动架构**：对于状态变更监控，事件驱动比轮询更高效 2. **使用原生集成**：选择与目标服务原生集成度最高的监控方案 3. **实时性要求**：部署监控需要实时响应，EventBridge提供近实时的事件传递 4. **成本效益**：避免使用过度复杂或不相关的服务，保持架构简洁 5. **可扩展性**：EventBridge + Lambda的组合易于扩展和维护</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">125</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A global company manages multiple AWS accounts by using AWS Control Tower. The company hosts internal applications and public applications. Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations. One of the AWS Control Tower member accounts serves as a centralized DevOps account with CI/CD pipelines that application teams use to deploy applications to their respective target AWS accounts. An IAM role for deployment exists in the centralized DevOps account. An application team is attempting to deploy its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in an application AWS account. An IAM role for deployment exists in the application AWS account. The deployment is through an AWS CodeBuild project that is set up in the centralized DevOps account. The CodeBuild project uses an IAM service role for CodeBuild. The deployment is failing with an Unauthorized error during attempts to connect to the cross-account EKS cluster from CodeBuild. Which solution will resolve this error? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure the application account&#x27;s deployment IAM role to have a trust relationship with the centralized DevOps account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account&#x27;s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.
B. Configure the centralized DevOps account&#x27;s deployment IAM role to have a trust relationship with the application account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the centralized DevOps account&#x27;s deployment IAM role to allow the required access to CodeBuild.
C. Configure the centralized DevOps account&#x27;s deployment IAM role to have a trust relationship with the application account. Configure the trust relationship to allow the sts:AssumeRoleWithSAML action. Configure the centralized DevOps account&#x27;s deployment IAM role to allow the required access to CodeBuild.
D. Configure the application account&#x27;s deployment IAM role to have a trust relationship with the AWS Control Tower management account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account&#x27;s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家全球公司使用AWS Control Tower管理多个AWS账户。该公司托管内部应用程序和公共应用程序。公司中的每个应用程序团队都有自己的AWS账户用于应用程序托管。这些账户在AWS Organizations中的一个组织中进行整合。其中一个AWS Control Tower成员账户作为集中式DevOps账户，具有CI/CD管道，应用程序团队使用这些管道将应用程序部署到各自的目标AWS账户。集中式DevOps账户中存在用于部署的IAM角色。应用程序账户中也存在用于部署的IAM角色。一个应用程序团队正在尝试将其应用程序部署到应用程序AWS账户中的Amazon Elastic Kubernetes Service (Amazon EKS)集群。部署是通过在集中式DevOps账户中设置的AWS CodeBuild项目进行的。CodeBuild项目使用CodeBuild的IAM服务角色。在尝试从CodeBuild连接到跨账户EKS集群时，部署失败并出现未授权错误。哪个解决方案可以解决此错误？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求解决跨账户EKS集群访问的权限问题。具体场景是从集中式DevOps账户的CodeBuild项目访问应用程序账户中的EKS集群时出现未授权错误。 **涉及的关键AWS服务和概念：** - AWS Control Tower：多账户管理服务 - AWS Organizations：账户组织管理 - Amazon EKS：托管Kubernetes服务 - AWS CodeBuild：持续集成服务 - IAM跨账户角色假设：sts:AssumeRole - EKS aws-auth ConfigMap：EKS集群的RBAC配置 **正确答案B的原因：** 选项B是错误的标准答案。实际上正确的应该是选项A。让我分析为什么： 1. **跨账户访问的正确流程**：CodeBuild在DevOps账户中运行，需要访问应用程序账户的EKS集群，应该是DevOps账户假设应用程序账户的角色 2. **信任关系方向**：应用程序账户的部署角色应该信任DevOps账户，允许DevOps账户假设该角色 3. **EKS访问权限**：需要在应用程序账户的部署角色上配置EKS访问权限，并在aws-auth ConfigMap中映射该角色 **其他选项错误的原因：** - 选项B：信任关系方向错误，应该是应用程序账户信任DevOps账户，而不是相反 - 选项C：使用了错误的STS操作（AssumeRoleWithSAML用于SAML联合身份验证） - 选项D：不应该涉及Control Tower管理账户，这会增加不必要的复杂性 **决策标准和最佳实践：** 1. **最小权限原则**：只授予必要的跨账户访问权限 2. **信任关系方向**：资源所在账户的角色应该信任需要访问的账户 3. **EKS RBAC配置**：必须在aws-auth ConfigMap中正确映射IAM角色到Kubernetes权限 4. **跨账户部署模式**：集中式DevOps账户模式是企业级多账户架构的最佳实践 注：题目标注的正确答案B实际上是错误的，正确答案应该是A。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">126</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A highly regulated company has a policy that DevOps engineers should not log in to their Amazon EC2 instances except in emergencies. If a DevOps engineer does log in, the security team must be notified within 15 minutes of the occurrence. Which solution will meet these requirements? B (94%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Install the Amazon Inspector agent on each EC2 instance. Subscribe to Amazon EventBridge notifications. Invoke an AWS Lambda function to check if a message is about user logins. If it is, send a notification to the security team using Amazon SNS.
B. Install the Amazon CloudWatch agent on each EC2 instance. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user logins. If a login is found, send a notification to the security team using Amazon SNS.
C. Set up AWS CloudTrail with Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Attach AWS Lambda to Kinesis to parse and determine if a log contains a user login. If it does, send a notification to the security team using Amazon SNS.
D. Set up a script on each Amazon EC2 instance to push all logs to Amazon S3. Set up an S3 event to invoke an AWS Lambda function, which invokes an Amazon Athena query to run. The Athena query checks for logins and sends the output to the security team using Amazon SNS.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家受到严格监管的公司有一项政策，DevOps工程师不应该登录到他们的Amazon EC2实例，除非在紧急情况下。如果DevOps工程师确实登录了，安全团队必须在事件发生后15分钟内收到通知。哪个解决方案能满足这些要求？ 选项： A. 在每个EC2实例上安装Amazon Inspector代理。订阅Amazon EventBridge通知。调用AWS Lambda函数检查消息是否关于用户登录。如果是，使用Amazon SNS向安全团队发送通知。 B. 在每个EC2实例上安装Amazon CloudWatch代理。配置代理将所有日志推送到Amazon CloudWatch Logs，并设置CloudWatch指标过滤器搜索用户登录。如果发现登录，使用Amazon SNS向安全团队发送通知。 C. 设置AWS CloudTrail与Amazon CloudWatch Logs。将CloudWatch Logs订阅到Amazon Kinesis。将AWS Lambda附加到Kinesis来解析和确定日志是否包含用户登录。如果包含，使用Amazon SNS向安全团队发送通知。 D. 在每个Amazon EC2实例上设置脚本将所有日志推送到Amazon S3。设置S3事件调用AWS Lambda函数，该函数调用Amazon Athena查询运行。Athena查询检查登录并使用Amazon SNS将输出发送给安全团队。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个监控系统，能够检测DevOps工程师对EC2实例的登录行为，并在15分钟内通知安全团队。关键需求包括：实时监控、快速检测登录事件、及时通知机制。 **涉及的关键AWS服务和概念：** - Amazon CloudWatch：监控和日志管理服务 - CloudWatch Logs：集中化日志存储和分析 - CloudWatch Metric Filter：实时日志模式匹配和告警 - Amazon SNS：消息通知服务 - AWS CloudTrail：API调用审计（主要用于AWS服务调用） - Amazon Inspector：安全漏洞评估工具 - 实时日志监控和告警机制 **正确答案B的原因：** 1. **直接监控登录日志**：CloudWatch代理可以收集系统日志（如/var/log/auth.log），直接捕获SSH登录事件 2. **实时处理能力**：CloudWatch Metric Filter可以实时扫描日志流，一旦匹配到登录模式立即触发告警 3. **满足时间要求**：整个流程从日志生成到SNS通知可以在几分钟内完成，远少于15分钟要求 4. **架构简洁高效**：直接的日志→过滤→告警链路，减少了中间环节和延迟 **其他选项错误的原因：** - **选项A**：Amazon Inspector是漏洞扫描工具，不是用来监控用户登录行为的，且EventBridge在此场景下不是最直接的解决方案 - **选项C**：CloudTrail主要记录AWS API调用，不会记录EC2实例内部的SSH登录事件；通过Kinesis增加了不必要的复杂性和延迟 - **选项D**：基于S3和Athena的方案是批处理架构，无法满足15分钟内通知的实时性要求，Athena适合历史数据分析而非实时监控 **决策标准和最佳实践：** 1. **实时性优先**：选择能够实时处理日志的服务组合 2. **监控目标匹配**：区分AWS API监控（CloudTrail）和系统级监控（CloudWatch） 3. **架构简化原则**：避免不必要的中间件，减少延迟和故障点 4. **成本效益考虑**：CloudWatch Logs + Metric Filter是成本效益最优的实时日志监控方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">127</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company updated the AWS CloudFormation template for a critical business application. The stack update process failed due to an error in the updated template, and AWS CloudFormation automatically began the stack rollback process. Later, a DevOps engineer discovered that the application was still unavailable and that the stack was in the UPDATE_ROLLBACK_FAILED state. Which combination of actions should the DevOps engineer perform so that the stack rollback can complete successfully? (Choose two.) CD (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Attach the AWSCloudFormationFullAccess IAM policy to the AWS CloudFormation role.
B. Automatically recover the stack resources by using AWS CloudFormation drift detection.
C. Issue a ContinueUpdateRollback command from the AWS CloudFormation console or the AWS CLI.
D. Manually adjust the resources to match the expectations of the stack.
E. Update the existing AWS CloudFormation stack by using the original template.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司更新了关键业务应用程序的AWS CloudFormation模板。由于更新模板中的错误，堆栈更新过程失败了，AWS CloudFormation自动开始了堆栈回滚过程。后来，一名DevOps工程师发现应用程序仍然不可用，堆栈处于UPDATE_ROLLBACK_FAILED状态。DevOps工程师应该执行哪些操作组合，以便堆栈回滚能够成功完成？（选择两个。） 选项： A. 将AWSCloudFormationFullAccess IAM策略附加到AWS CloudFormation角色。 B. 使用AWS CloudFormation漂移检测自动恢复堆栈资源。 C. 从AWS CloudFormation控制台或AWS CLI发出ContinueUpdateRollback命令。 D. 手动调整资源以匹配堆栈的期望。 E. 使用原始模板更新现有的AWS CloudFormation堆栈。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是当CloudFormation堆栈处于UPDATE_ROLLBACK_FAILED状态时的恢复策略。堆栈更新失败后自动回滚也失败了，需要找到正确的方法来完成回滚过程。 **涉及的关键AWS服务和概念：** - AWS CloudFormation堆栈状态管理 - UPDATE_ROLLBACK_FAILED状态的含义和处理 - ContinueUpdateRollback操作 - CloudFormation资源状态同步 - 堆栈回滚机制 **正确答案的原因：** 选项C（ContinueUpdateRollback命令）是正确的，因为： - 当堆栈处于UPDATE_ROLLBACK_FAILED状态时，这是AWS官方推荐的标准解决方案 - 该命令专门设计用于继续失败的回滚操作 - 可以跳过无法回滚的资源，继续回滚其他资源 选项D（手动调整资源）也是正确的，因为： - UPDATE_ROLLBACK_FAILED通常是因为某些资源无法自动回滚到原始状态 - 需要手动修复这些资源，使其状态与堆栈模板期望的状态一致 - 这是ContinueUpdateRollback成功执行的前提条件 **其他选项错误的原因：** A. 添加IAM权限 - 如果是权限问题，堆栈不会进入UPDATE_ROLLBACK_FAILED状态，而是会显示权限相关错误 B. 漂移检测 - 这只是检测工具，不能修复UPDATE_ROLLBACK_FAILED状态 E. 使用原始模板更新 - 在UPDATE_ROLLBACK_FAILED状态下无法直接进行新的更新操作 **决策标准和最佳实践：** 1. 首先识别导致回滚失败的具体资源和原因 2. 手动修复问题资源，确保其状态符合回滚目标 3. 使用ContinueUpdateRollback命令继续回滚过程 4. 建议在生产环境中使用变更集预览更新，避免此类问题 5. 实施适当的测试和验证流程，减少模板错误的可能性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">128</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A development team manually builds an artifact locally and then places it in an Amazon S3 bucket. The application has a local cache that must be cleared when a deployment occurs. The team runs a command to do this, downloads the artifact from Amazon S3, and unzips the artifact to complete the deployment. A DevOps team wants to migrate to a CI/CD process and build in checks to stop and roll back the deployment when a failure occurs. This requires the team to track the progression of the deployment. Which combination of actions will accomplish this? (Choose three.) F. Use AWS Systems Manager to fetch the artifact from Amazon S3 and deploy it to all the instances. BDE (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Allow developers to check the code into a code repository. Using Amazon EventBridge, on every pull into the main branch, invoke an AWS Lambda function to build the artifact and store it in Amazon S3.
B. Create a custom script to clear the cache. Specify the script in the BeforeInstall lifecycle hook in the AppSpec file.
C. Create user data for each Amazon EC2 instance that contains the clear cache script. Once deployed, test the application. If it is not successful, deploy it again.
D. Set up AWS CodePipeline to deploy the application. Allow developers to check the code into a code repository as a source for the pipeline.
E. Use AWS CodeBuild to build the artifact and place it in Amazon S3. Use AWS CodeDeploy to deploy the artifact to Amazon EC2 instances.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个开发团队手动在本地构建工件，然后将其放置在Amazon S3存储桶中。应用程序有一个本地缓存，在部署发生时必须清除。团队运行命令来执行此操作，从Amazon S3下载工件，并解压工件以完成部署。DevOps团队希望迁移到CI/CD流程，并内置检查以在发生故障时停止和回滚部署。这需要团队跟踪部署的进度。哪种操作组合将实现这一目标？（选择三个。） 选项： A. 允许开发人员将代码检入代码仓库。使用Amazon EventBridge，在每次拉取到主分支时，调用AWS Lambda函数来构建工件并将其存储在Amazon S3中。 B. 创建自定义脚本来清除缓存。在AppSpec文件的BeforeInstall生命周期钩子中指定该脚本。 C. 为每个Amazon EC2实例创建包含清除缓存脚本的用户数据。部署后，测试应用程序。如果不成功，再次部署。 D. 设置AWS CodePipeline来部署应用程序。允许开发人员将代码检入代码仓库作为管道的源。 E. 使用AWS CodeBuild构建工件并将其放置在Amazon S3中。使用AWS CodeDeploy将工件部署到Amazon EC2实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求将手动部署流程迁移到自动化CI/CD流程，需要满足以下关键需求： 1. 自动化构建和部署流程 2. 在部署时清除本地缓存 3. 具备失败检查、停止和回滚能力 4. 能够跟踪部署进度 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务，提供端到端的自动化部署流程 - AWS CodeBuild：托管构建服务，用于编译源代码和创建工件 - AWS CodeDeploy：自动化部署服务，支持生命周期钩子和回滚功能 - AppSpec文件：CodeDeploy的配置文件，定义部署步骤和生命周期钩子 - 生命周期钩子：允许在部署过程的特定阶段执行自定义脚本 **正确答案BDE的原因：** - **选项B**：使用AppSpec文件的BeforeInstall钩子清除缓存是CodeDeploy的标准做法，确保在安装新版本前清理环境 - **选项D**：CodePipeline提供完整的CI/CD管道，支持自动化流程、进度跟踪和失败处理 - **选项E**：CodeBuild+CodeDeploy的组合是AWS推荐的构建和部署解决方案，CodeDeploy天然支持回滚和部署跟踪 **其他选项错误的原因：** - **选项A**：使用EventBridge+Lambda的方案过于复杂，缺乏专业CI/CD工具的部署跟踪和回滚能力 - **选项C**：用户数据方案无法提供可靠的失败检测和回滚机制，且&quot;失败后重新部署&quot;不是回滚策略 **决策标准和最佳实践：** 1. 选择AWS原生CI/CD服务（CodePipeline、CodeBuild、CodeDeploy）获得最佳集成 2. 利用CodeDeploy的生命周期钩子处理部署前后的自定义操作 3. 使用AppSpec文件标准化部署配置和脚本管理 4. 确保解决方案具备内置的监控、跟踪和回滚能力 5. 避免过度工程化，选择简单可靠的AWS托管服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">129</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is working on a project that is hosted on Amazon Linux and has failed a security review. The DevOps manager has been asked to review the company buildspec.yaml file for an AWS CodeBuild project and provide recommendations. The buildspec.yaml file is configured as follows: What changes should be recommended to comply with AWS security best practices? (Choose three.) F. Scramble the environment variables using XOR followed by Base64, add a section to install, and then run XOR and Base64 to the build phase. BCE (76%) ABC (24%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Add a post-build command to remove the temporary files from the container before termination to ensure they cannot be seen by other CodeBuild users.
B. Update the CodeBuild project role with the necessary permissions and then remove the AWS credentials from the environment variable.
C. Store the DB_PASSWORD as a SecureString value in AWS Systems Manager Parameter Store and then remove the DB_PASSWORD from the environment variables.
D. Move the environment variables to the &#x27;db-deploy-bucket&#x27; Amazon S3 bucket, add a prebuild stage to download, then export the variables.
E. Use AWS Systems Manager run command versus scp and ssh commands directly to the instance.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师正在处理一个托管在Amazon Linux上的项目，该项目未通过安全审查。DevOps经理被要求审查公司的buildspec.yaml文件（用于AWS CodeBuild项目）并提供建议。buildspec.yaml文件配置如下：应该推荐哪些更改来符合AWS安全最佳实践？（选择三个。） 选项： A. 添加post-build命令在容器终止前删除临时文件，确保其他CodeBuild用户无法看到它们。 B. 更新CodeBuild项目角色的必要权限，然后从环境变量中删除AWS凭证。 C. 将DB_PASSWORD作为SecureString值存储在AWS Systems Manager Parameter Store中，然后从环境变量中删除DB_PASSWORD。 D. 将环境变量移动到&#x27;db-deploy-bucket&#x27; Amazon S3存储桶，添加prebuild阶段来下载，然后导出变量。 E. 使用AWS Systems Manager run command而不是直接使用scp和ssh命令到实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodeBuild安全最佳实践，特别是如何安全地处理敏感信息（如凭证和密码）以及构建环境的安全配置。题目要求选择三个符合AWS安全最佳实践的建议。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：持续集成服务 - IAM角色和权限管理 - AWS Systems Manager Parameter Store：安全参数存储 - 环境变量安全处理 - 容器安全和临时文件管理 **正确答案分析：** 根据题目显示，正确答案是B，但从安全最佳实践角度，A、B、C都应该是正确的： - **选项A正确**：删除临时文件防止敏感信息泄露给其他用户，这是容器安全的基本实践 - **选项B正确**：使用IAM角色而不是硬编码AWS凭证是AWS安全的核心原则 - **选项C正确**：使用Parameter Store存储敏感信息如数据库密码是标准的安全实践 **其他选项错误的原因：** - **选项D错误**：将敏感的环境变量存储在S3存储桶中并不能提高安全性，反而可能增加暴露风险 - **选项E错误**：虽然Systems Manager Session Manager比直接SSH更安全，但这不是buildspec.yaml文件的直接安全问题 - **选项F错误**：使用XOR和Base64进行&quot;加密&quot;是不安全的，这只是编码而非真正的加密 **决策标准和最佳实践：** 1. **最小权限原则**：使用IAM角色而非硬编码凭证 2. **敏感信息保护**：使用专门的安全服务存储密码和密钥 3. **数据清理**：构建完成后清理敏感的临时文件 4. **避免明文存储**：不在配置文件中直接暴露敏感信息 5. **使用AWS托管服务**：优先使用AWS提供的安全服务而非自制解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">130</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a legacy application. A DevOps engineer needs to automate the process of building the deployable artifact for the legacy application. The solution must store the deployable artifact in an existing Amazon S3 bucket for future deployments to reference. Which solution will meet these requirements in the MOST operationally efficient way? Amazon Elastic Container Registry (Amazon ECR) repository. Use the custom Docker image inside the EKS cluster to build the deployable artifact and to save the artifact to the S3 bucket. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new Amazon Elastic Container Registry (Amazon ECR) repository. Configure a new AWS CodeBuild project to use the custom Docker image to build the deployable artifact and to save the artifact to the S3 bucket.
B. Launch a new Amazon EC2 instance. Install all the dependencies for the legacy application on the EC2 instance. Use the EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.
C. Create a custom EC2 Image Builder image. Install all the dependencies for the legacy application on the image. Launch a new Amazon EC2 instance from the image. Use the new EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.
D. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with an AWS Fargate profile that runs in multiple Availability Zones. Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个遗留应用程序。DevOps工程师需要自动化构建该遗留应用程序可部署工件的过程。解决方案必须将可部署工件存储在现有的Amazon S3存储桶中，以供未来部署时引用。哪个解决方案能以最具运营效率的方式满足这些要求？ 选项： A. 创建一个包含遗留应用程序所有依赖项的自定义Docker镜像。将自定义Docker镜像存储在新的Amazon Elastic Container Registry (Amazon ECR)存储库中。配置新的AWS CodeBuild项目，使用自定义Docker镜像构建可部署工件并将工件保存到S3存储桶。 B. 启动新的Amazon EC2实例。在EC2实例上安装遗留应用程序的所有依赖项。使用EC2实例构建可部署工件并将工件保存到S3存储桶。 C. 创建自定义EC2 Image Builder镜像。在镜像上安装遗留应用程序的所有依赖项。从该镜像启动新的Amazon EC2实例。使用新的EC2实例构建可部署工件并将工件保存到S3存储桶。 D. 创建在多个可用区运行的带有AWS Fargate配置文件的Amazon Elastic Kubernetes Service (Amazon EKS)集群。创建包含遗留应用程序所有依赖项的自定义Docker镜像。将自定义Docker镜像存储在新的Amazon Elastic Container Registry (Amazon ECR)存储库中。使用EKS集群内的自定义Docker镜像构建可部署工件并将工件保存到S3存储桶。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为遗留应用程序建立自动化构建流程，需要满足：1）自动化构建可部署工件；2）将工件存储到现有S3存储桶；3）以最具运营效率的方式实现。 **涉及的关键AWS服务和概念：** - AWS CodeBuild：托管的构建服务，专门用于编译源代码、运行测试和生成可部署的软件包 - Amazon ECR：托管的Docker容器注册表服务 - Amazon S3：对象存储服务，用于存储构建工件 - Amazon EC2：虚拟服务器实例 - Amazon EKS：托管的Kubernetes服务 - EC2 Image Builder：用于创建和维护服务器镜像的服务 **正确答案A的原因：** 1. **专门化服务**：CodeBuild是专为CI/CD构建任务设计的托管服务，无需管理底层基础设施 2. **运营效率最高**：完全托管，自动扩缩容，按使用付费，无需维护服务器 3. **集成度好**：原生支持与S3集成，可直接将构建工件上传到指定存储桶 4. **容器化优势**：使用Docker镜像确保构建环境的一致性和可重复性 5. **成本效益**：只在构建时付费，无持续运行成本 **其他选项错误的原因：** - **选项B**：使用EC2实例需要手动管理服务器、安装依赖、维护运行时环境，运营开销大，不够自动化 - **选项C**：虽然使用了Image Builder提高了一致性，但仍需管理EC2实例，运营效率不如托管服务 - **选项D**：EKS集群过于复杂，适合容器化应用运行而非简单的构建任务，存在过度工程化问题，运营成本和复杂度都很高 **决策标准和最佳实践：** 1. **选择专门化服务**：优先选择为特定用途设计的AWS托管服务 2. **最小化运营开销**：避免管理不必要的基础设施 3. **自动化优先**：选择支持完全自动化的解决方案 4. **成本效益**：考虑按需付费vs持续运行成本 5. **避免过度工程化**：不要为简单需求选择过于复杂的架构</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">131</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company builds a container image in an AWS CodeBuild project by running Docker commands. After the container image is built, the CodeBuild project uploads the container image to an Amazon S3 bucket. The CodeBuild project has an IAM service role that has permissions to access the S3 bucket. A DevOps engineer needs to replace the S3 bucket with an Amazon Elastic Container Registry (Amazon ECR) repository to store the container images. The DevOps engineer creates an ECR private image repository in the same AWS Region of the CodeBuild project. The DevOps engineer adjusts the IAM service role with the permissions that are necessary to work with the new ECR repository. The DevOps engineer also places new repository information into the docker build command and the docker push command that are used in the buildspec.yml file. When the CodeBuild project runs a build job, the job fails when the job tries to access the ECR repository. Which solution will resolve the issue of failed access to the ECR repository? authentication token. Update the docker login command to use the authentication token to access the ECR repository. Most Voted CodeBuild project&#x27;s IAM service role. Update the buildspec.yml file to use the new environment variable to log in with the docker login command to access the ECR repository. allows the IAM service role to have access. A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the buildspec.yml file to log in to the ECR repository by using the aws ecr get-login-password AWS CLI command to obtain an
B. Add an environment variable of type SECRETS_MANAGER to the CodeBuild project. In the environment variable, include the ARN of the
C. Update the ECR repository to be a public image repository. Add an ECR repository policy that allows the IAM service role to have access.
D. Update the buildspec.yml file to use the AWS CLI to assume the IAM service role for ECR operations. Add an ECR repository policy that</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS CodeBuild项目中通过运行Docker命令构建容器镜像。容器镜像构建完成后，CodeBuild项目将容器镜像上传到Amazon S3存储桶。CodeBuild项目有一个IAM服务角色，该角色有访问S3存储桶的权限。DevOps工程师需要用Amazon Elastic Container Registry (Amazon ECR)仓库替换S3存储桶来存储容器镜像。DevOps工程师在CodeBuild项目的同一AWS区域创建了一个ECR私有镜像仓库。DevOps工程师调整了IAM服务角色，添加了使用新ECR仓库所需的权限。DevOps工程师还在buildspec.yml文件中使用的docker build命令和docker push命令中放入了新的仓库信息。当CodeBuild项目运行构建作业时，作业在尝试访问ECR仓库时失败。哪个解决方案能解决访问ECR仓库失败的问题？ 选项： A. 更新buildspec.yml文件，使用aws ecr get-login-password AWS CLI命令获取认证令牌来登录ECR仓库。更新docker login命令以使用认证令牌访问ECR仓库。 B. 向CodeBuild项目添加SECRETS_MANAGER类型的环境变量。在环境变量中包含CodeBuild项目IAM服务角色的ARN。更新buildspec.yml文件以使用新的环境变量通过docker login命令登录访问ECR仓库。 C. 将ECR仓库更新为公共镜像仓库。添加允许IAM服务角色访问的ECR仓库策略。 D. 更新buildspec.yml文件以使用AWS CLI为ECR操作承担IAM服务角色。添加允许IAM服务角色访问的ECR仓库策略。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是如何解决CodeBuild项目访问ECR私有仓库时的认证失败问题。问题的关键在于理解ECR私有仓库需要特殊的认证机制，不同于S3的访问方式。 **涉及的关键AWS服务和概念：** 1. **AWS CodeBuild** - CI/CD构建服务 2. **Amazon ECR** - 容器镜像注册表服务 3. **IAM服务角色** - 为AWS服务提供权限的身份 4. **Docker认证** - 容器镜像推送拉取的认证机制 5. **buildspec.yml** - CodeBuild的构建规范文件 **正确答案A的原因：** - ECR私有仓库需要特殊的认证令牌才能进行docker操作 - `aws ecr get-login-password`命令是AWS官方推荐的获取ECR认证令牌的标准方法 - 该命令会利用CodeBuild项目的IAM服务角色自动获取临时认证令牌 - 通过管道传递给`docker login`命令可以实现无缝认证 - 这是最安全、最简单的解决方案，符合AWS最佳实践 **其他选项错误的原因：** - **选项B错误**：SECRETS_MANAGER环境变量用于存储静态密钥，但ECR认证需要动态令牌，且IAM角色ARN不是认证凭据 - **选项C错误**：将私有仓库改为公共仓库违背了安全原则，且题目明确要求使用私有仓库 - **选项D错误**：CodeBuild已经在IAM服务角色下运行，无需再次assume角色，这会增加不必要的复杂性 **决策标准和最佳实践：** 1. **安全性优先** - 使用临时令牌而非静态凭据 2. **简化配置** - 利用现有IAM角色，避免额外的权限管理 3. **遵循AWS标准** - 使用官方推荐的ECR认证方法 4. **最小权限原则** - 不改变仓库的私有性质 5. **操作简便性** - 通过buildspec.yml中的简单命令实现认证</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">132</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manually provisions IAM access for its employees. The company wants to replace the manual process with an automated process. The company has an existing Active Directory system configured with an external SAML 2.0 identity provider (IdP). The company wants employees to use their existing corporate credentials to access AWS. The groups from the existing Active Directory system must be available for permission management in AWS Identity and Access Management (IAM). A DevOps engineer has completed the initial configuration of AWS IAM Identity Center (AWS Single Sign-On) in the company&#x27;s AWS account. What should the DevOps engineer do next to meet the requirements? protocol. A (90%) 10%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure an external IdP as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol.
B. Configure AWS Directory Service as an identity source. Configure automatic provisioning of users and groups by using the SAML
C. Configure an AD Connector as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol.
D. Configure an external IdP as an identity source. Configure automatic provisioning of users and groups by using the SAML protocol.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司手动为其员工配置IAM访问权限。该公司希望用自动化流程替换手动流程。该公司有一个现有的Active Directory系统，配置了外部SAML 2.0身份提供商(IdP)。公司希望员工使用其现有的企业凭证来访问AWS。现有Active Directory系统中的组必须在AWS Identity and Access Management (IAM)中可用于权限管理。DevOps工程师已经在公司的AWS账户中完成了AWS IAM Identity Center (AWS Single Sign-On)的初始配置。DevOps工程师接下来应该做什么来满足要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 实现从手动IAM配置到自动化的转换 - 集成现有的Active Directory和外部SAML 2.0 IdP - 员工使用现有企业凭证访问AWS - Active Directory中的组要在AWS IAM中可用于权限管理 - 基于已配置的AWS IAM Identity Center进行后续配置 **涉及的关键AWS服务和概念：** - AWS IAM Identity Center (原AWS SSO)：统一身份管理服务 - External IdP：外部身份提供商，支持SAML 2.0 - SCIM协议：用于身份信息跨域管理的标准协议，支持自动用户和组同步 - SAML协议：主要用于身份验证，不是用于自动provisioning - AD Connector：用于连接本地Active Directory的代理服务 **正确答案A的原因：** 1. 配置外部IdP作为身份源符合题目中已有&quot;外部SAML 2.0 IdP&quot;的要求 2. SCIM协议是专门用于自动化用户和组provisioning的标准协议 3. 通过SCIM可以自动同步Active Directory中的用户和组到AWS IAM Identity Center 4. 这种配置能够实现完全自动化的身份管理，满足替换手动流程的需求 **其他选项错误的原因：** - 选项B：AWS Directory Service不是外部IdP，且SAML协议不用于自动provisioning - 选项C：AD Connector是AWS的服务，不是外部IdP，与题目要求的&quot;外部SAML 2.0 IdP&quot;不符 - 选项D：SAML协议主要用于身份验证和授权，不是用于自动provisioning用户和组的协议 **决策标准和最佳实践：** 1. 身份源选择：题目明确提到有外部SAML 2.0 IdP，应该利用现有基础设施 2. 自动化协议选择：SCIM是行业标准的身份provisioning协议，支持用户和组的自动同步 3. 集成策略：利用现有的外部IdP可以减少架构复杂性和维护成本 4. 权限管理：通过SCIM同步的组可以直接在AWS IAM中用于权限分配</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">133</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using AWS to run digital workloads. Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations. The company wants to enforce security standards across the entire organization. To avoid noncompliance because of security misconfiguration, the company has enforced the use of AWS CloudFormation. A production support team can modify resources in the production environment by using the AWS Management Console to troubleshoot and resolve application-related issues. A DevOps engineer must implement a solution to identify in near real time any AWS service misconfiguration that results in noncompliance. The solution must automatically remediate the issue within 15 minutes of identification. The solution also must track noncompliant resources and events in a centralized dashboard with accurate timestamps. Which solution will meet these requirements with the LEAST development overhead? AWS Step Functions to track query results on Athena for drift detection and to invoke an AWS Lambda function for remediation. For tracking, set up an Amazon QuickSight dashboard that uses Athena as the data source. C (85%) A (15%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use CloudFormation drift detection to identify noncompliant resources. Use drift detection events from CloudFormation to invoke an AWS Lambda function for remediation. Configure the Lambda function to publish logs to an Amazon CloudWatch Logs log group. Configure an Amazon CloudWatch dashboard to use the log group for tracking.
B. Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon Athena to identify noncompliant resources. Use
C. Turn on the configuration recorder in AWS Config in all the AWS accounts to identify noncompliant resources. Enable AWS Security Hub with the --no-enable-default-standards option in all the AWS accounts. Set up AWS Config managed rules and custom rules. Set up automatic remediation by using AWS Config conformance packs. For tracking, set up a dashboard on Security Hub in a designated Security Hub administrator account.
D. Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon CloudWatch Logs to identify noncompliant resources. Use CloudWatch Logs filters for drift detection. Use Amazon EventBridge to invoke the Lambda function for remediation. Stream filtered CloudWatch logs to Amazon OpenSearch Service. Set up a dashboard on OpenSearch Service for tracking.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS运行数字化工作负载。公司中的每个应用团队都有自己的AWS账户来托管应用程序。这些账户在AWS Organizations中的一个组织里进行了整合。公司希望在整个组织中强制执行安全标准。为了避免因安全配置错误而导致的不合规，公司已强制使用AWS CloudFormation。生产支持团队可以通过AWS Management Console修改生产环境中的资源，以排查和解决应用程序相关问题。DevOps工程师必须实施一个解决方案，以近实时识别任何导致不合规的AWS服务配置错误。该解决方案必须在识别后15分钟内自动修复问题。解决方案还必须在集中式仪表板中跟踪不合规资源和事件，并提供准确的时间戳。哪个解决方案能以最少的开发开销满足这些要求？ 选项： A. 使用CloudFormation漂移检测来识别不合规资源。使用CloudFormation的漂移检测事件调用AWS Lambda函数进行修复。配置Lambda函数将日志发布到Amazon CloudWatch Logs日志组。配置Amazon CloudWatch仪表板使用该日志组进行跟踪。 B. 在AWS账户中开启AWS CloudTrail。使用Amazon Athena分析CloudTrail日志以识别不合规资源。使用AWS Step Functions跟踪Athena上的查询结果进行漂移检测并调用AWS Lambda函数进行修复。对于跟踪，设置使用Athena作为数据源的Amazon QuickSight仪表板。 C. 在所有AWS账户中开启AWS Config的配置记录器以识别不合规资源。在所有AWS账户中使用--no-enable-default-standards选项启用AWS Security Hub。设置AWS Config托管规则和自定义规则。使用AWS Config conformance packs设置自动修复。对于跟踪，在指定的Security Hub管理员账户中设置Security Hub仪表板。 D. 在AWS账户中开启AWS CloudTrail。使用Amazon CloudWatch Logs分析CloudTrail日志以识别不合规资源。使用CloudWatch Logs过滤器进行漂移检测。使用Amazon EventBridge调用Lambda函数进行修复。将过滤的CloudWatch日志流式传输到Amazon OpenSearch Service。在OpenSearch Service上设置跟踪仪表板。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个解决方案来：1）近实时检测AWS服务配置的不合规情况；2）在15分钟内自动修复；3）在集中式仪表板中跟踪不合规资源和事件；4）最少的开发开销。关键约束是多账户环境（AWS Organizations）且生产团队可能通过控制台手动修改资源。 **涉及的关键AWS服务和概念：** - AWS Config：配置合规性监控和自动修复的核心服务 - AWS Security Hub：集中式安全管理和合规性仪表板 - AWS Config conformance packs：预定义的合规规则集合，支持自动修复 - CloudFormation drift detection：检测资源与模板的偏差 - CloudTrail：API调用日志记录 - 多账户管理和集中式监控 **正确答案C的原因：** 1. **AWS Config是专门的合规性监控服务**：天然支持近实时配置变更检测和合规性评估 2. **自动修复能力**：Config conformance packs提供开箱即用的自动修复功能，无需自定义开发 3. **集中式管理**：Security Hub作为管理员账户可以聚合所有账户的合规性数据 4. **最少开发开销**：使用托管服务和预定义规则，无需编写复杂的检测逻辑 5. **完整的跟踪能力**：Security Hub提供统一的仪表板和准确的时间戳记录 **其他选项错误的原因：** - **选项A**：CloudFormation漂移检测只能检测与模板的偏差，无法检测所有类型的安全配置错误，且需要手动触发检测 - **选项B**：基于CloudTrail日志的分析是被动的，需要大量自定义开发来解析日志和定义合规规则，开发开销大 - **选项D**：同样依赖CloudTrail日志分析，需要复杂的日志过滤和自定义检测逻辑，不是专门的合规性解决方案 **决策标准和最佳实践：** 1. **选择专用服务**：对于合规性监控，AWS Config是最佳选择 2. **最小化自定义开发**：优先使用托管服务和预定义规则 3. **集中式管理**：在多账户环境中，使用Security Hub进行统一管理 4. **自动化优先**：使用conformance packs实现自动修复，减少人工干预</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">134</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Organizations to manage its AWS accounts. The organization root has an OU that is named Environments. The Environments OU has two child OUs that are named Development and Production, respectively. The Environments OU and the child OUs have the default FullAWSAccess policy in place. A DevOps engineer plans to remove the FullAWSAccess policy from the Development OU and replace the policy with a policy that allows all actions on Amazon EC2 resources. What will be the outcome of this policy replacement? B (77%) A (23%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. All users in the Development OU will be allowed all API actions on all resources.
B. All users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied.
C. All users in the Development OU will be denied all API actions on all resources.
D. All users in the Development OU will be denied all API actions on EC2 resources. All other API actions will be allowed.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Organizations来管理其AWS账户。组织根有一个名为Environments的OU。Environments OU有两个子OU，分别名为Development和Production。Environments OU和子OU都配置了默认的FullAWSAccess策略。一名DevOps工程师计划从Development OU中移除FullAWSAccess策略，并用一个允许对Amazon EC2资源执行所有操作的策略来替换。这种策略替换的结果是什么？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查AWS Organizations中Service Control Policies (SCP)的工作机制，特别是当移除宽泛权限策略并替换为更具体的限制性策略时会产生什么影响。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. Organizational Units (OU) - 组织单元，用于分组管理账户 3. Service Control Policies (SCP) - 服务控制策略，用于设置权限边界 4. FullAWSAccess策略 - AWS提供的默认策略，允许所有AWS服务的所有操作 5. SCP的&quot;拒绝优先&quot;和&quot;白名单&quot;工作原理 **正确答案B的原因：** 1. SCP采用&quot;白名单&quot;模式工作 - 只有明确允许的操作才能执行 2. 移除FullAWSAccess策略后，原本的&quot;允许所有操作&quot;权限被撤销 3. 新策略只允许EC2相关的所有操作，因此用户只能对EC2资源执行操作 4. 对于其他AWS服务（如S3、RDS等），由于新策略中没有明确允许，这些操作将被拒绝 5. SCP是权限边界，即使用户的IAM策略允许某些操作，如果SCP不允许，操作仍会被拒绝 **其他选项错误的原因：** - 选项A错误：新策略不再是FullAWSAccess，不会允许所有资源的所有操作 - 选项C错误：新策略明确允许EC2操作，不会拒绝所有操作 - 选项D错误：逻辑完全颠倒，实际上EC2操作被允许，其他操作被拒绝 **决策标准和最佳实践：** 1. 理解SCP的&quot;最小权限原则&quot; - 只授予必要的权限 2. SCP策略应该采用渐进式收紧，避免一次性过度限制影响业务 3. 在生产环境实施前，应在测试环境验证策略效果 4. 建议为不同环境（开发、测试、生产）设置不同级别的权限控制 5. 定期审查和更新SCP策略，确保符合安全要求和业务需求</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">135</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is examining its disaster recovery capability and wants the ability to switch over its daily operations to a secondary AWS Region. The company uses AWS CodeCommit as a source control tool in the primary Region. A DevOps engineer must provide the capability for the company to develop code in the secondary Region. If the company needs to use the secondary Region, developers can add an additional remote URL to their local Git configuration. Which solution will meet these requirements? primary Region&#x27;s CodeCommit repository to the secondary Region&#x27;s CodeCommit repository. Create an AWS Lambda function that invokes the CodeBuild project. Create an Amazon EventBridge rule that reacts to merge events in the primary Region&#x27;s CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function. Most Voted Region&#x27;s CodeCommit repository and copy the result to the S3 bucket. Create an AWS Lambda function that initiates the Fargate task. Create an Amazon EventBridge rule that reacts to merge events in the CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function. A (86%) 14%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CodeCommit repository in the secondary Region. Create an AWS CodeBuild project to perform a Git mirror operation of the
B. Create an Amazon S3 bucket in the secondary Region. Create an AWS Fargate task to perform a Git mirror operation of the primary
C. Create an AWS CodeArtifact repository in the secondary Region. Create an AWS CodePipeline pipeline that uses the primary Region&#x27;s CodeCommit repository for the source action. Create a cross-Region stage in the pipeline that packages the CodeCommit repository contents and stores the contents in the CodeArtifact repository when a pull request is merged into the CodeCommit repository.
D. Create an AWS Cloud9 environment and a CodeCommit repository in the secondary Region. Configure the primary Region&#x27;s CodeCommit repository as a remote repository in the AWS Cloud9 environment. Connect the secondary Region&#x27;s CodeCommit repository to the AWS Cloud9 environment.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在检查其灾难恢复能力，希望能够将其日常运营切换到辅助AWS区域。该公司在主区域使用AWS CodeCommit作为源代码控制工具。DevOps工程师必须为公司提供在辅助区域开发代码的能力。如果公司需要使用辅助区域，开发人员可以在其本地Git配置中添加额外的远程URL。哪种解决方案能满足这些要求？ 选项： A. 在辅助区域创建一个CodeCommit存储库。创建一个AWS CodeBuild项目来执行主区域CodeCommit存储库到辅助区域CodeCommit存储库的Git镜像操作。创建一个AWS Lambda函数来调用CodeBuild项目。创建一个Amazon EventBridge规则来响应主区域CodeCommit存储库中的合并事件。配置EventBridge规则来调用Lambda函数。 B. 在辅助区域创建一个Amazon S3存储桶。创建一个AWS Fargate任务来执行主区域CodeCommit存储库的Git镜像操作并将结果复制到S3存储桶。创建一个AWS Lambda函数来启动Fargate任务。创建一个Amazon EventBridge规则来响应CodeCommit存储库中的合并事件。配置EventBridge规则来调用Lambda函数。 C. 在辅助区域创建一个AWS CodeArtifact存储库。创建一个AWS CodePipeline管道，使用主区域的CodeCommit存储库作为源操作。在管道中创建一个跨区域阶段，当拉取请求合并到CodeCommit存储库时，打包CodeCommit存储库内容并将内容存储在CodeArtifact存储库中。 D. 在辅助区域创建一个AWS Cloud9环境和一个CodeCommit存储库。将主区域的CodeCommit存储库配置为AWS Cloud9环境中的远程存储库。将辅助区域的CodeCommit存储库连接到AWS Cloud9环境。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为灾难恢复场景设计一个解决方案，使公司能够在辅助区域进行代码开发，并且开发人员可以通过添加远程URL的方式访问辅助区域的代码库。关键需求是实现主区域到辅助区域的代码同步，并保持Git工作流的兼容性。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：Git代码存储库服务 - Amazon S3：对象存储服务，可以存储Git存储库 - AWS Fargate：无服务器容器计算服务 - AWS Lambda：无服务器计算服务 - Amazon EventBridge：事件驱动架构服务 - 跨区域灾难恢复架构 - Git镜像操作和远程存储库概念 **正确答案B的原因：** 1. **满足Git远程URL需求**：S3可以作为Git的远程存储库，开发人员可以直接添加S3作为远程URL 2. **灾难恢复适用性**：S3具有高可用性和跨区域复制能力，适合灾难恢复场景 3. **自动化同步**：通过EventBridge监听合并事件，Lambda触发Fargate任务执行Git镜像，实现自动同步 4. **成本效益**：S3存储成本较低，Fargate按需计费，适合灾难恢复的间歇性使用模式 **其他选项错误的原因：** - **选项A**：虽然技术可行，但CodeCommit不能直接作为Git远程URL添加到本地配置中，需要特殊的凭证配置 - **选项C**：CodeArtifact主要用于软件包管理，不是Git存储库服务，不符合源代码控制的需求 - **选项D**：Cloud9是IDE环境，不能解决代码同步问题，且没有自动化机制 **决策标准和最佳实践：** 1. **兼容性**：解决方案必须与现有Git工作流兼容 2. **自动化**：灾难恢复方案应该自动同步，减少人工干预 3. **成本优化**：选择适合灾难恢复场景的成本效益最佳服务 4. **可靠性**：确保辅助区域的代码库始终保持最新状态 5. **简单性**：避免过度复杂的架构，便于维护和故障排除</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">136</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps team is merging code revisions for an application that uses an Amazon RDS Multi-AZ DB cluster for its production database. The DevOps team uses continuous integration to periodically verify that the application works. The DevOps team needs to test the changes before the changes are deployed to the production database. Which solution will meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use a buildspec file in AWS CodeBuild to restore the DB cluster from a snapshot of the production database, run integration tests, and drop the restored database after verification.
B. Deploy the application to production. Configure an audit log of data control language (DCL) operations to capture database activities to perform if verification fails.
C. Create a snapshot of the DB cluster before deploying the application. Use the Update requires: Replacement property on the DB instance in AWS CloudFormation to deploy the application and apply the changes.
D. Ensure that the DB cluster is a Multi-AZ deployment. Deploy the application with the updates. Fail over to the standby instance if verification fails.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一个DevOps团队正在为一个使用Amazon RDS Multi-AZ DB集群作为生产数据库的应用程序合并代码修订。DevOps团队使用持续集成来定期验证应用程序是否正常工作。DevOps团队需要在将更改部署到生产数据库之前测试这些更改。哪个解决方案能满足这些要求？ 选项： A. 在AWS CodeBuild中使用buildspec文件从生产数据库的快照恢复DB集群，运行集成测试，并在验证后删除恢复的数据库。 B. 将应用程序部署到生产环境。配置数据控制语言(DCL)操作的审计日志来捕获数据库活动，以便在验证失败时执行回滚。 C. 在部署应用程序之前创建DB集群的快照。在AWS CloudFormation中使用DB实例的Update requires: Replacement属性来部署应用程序并应用更改。 D. 确保DB集群是Multi-AZ部署。部署带有更新的应用程序。如果验证失败则故障转移到备用实例。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在生产环境中安全测试数据库更改的最佳实践。关键需求是在不影响生产数据库的情况下测试代码更改。 **涉及的关键AWS服务和概念：** - Amazon RDS Multi-AZ DB集群：提供高可用性和自动故障转移 - AWS CodeBuild：持续集成服务 - RDS快照：数据库备份和恢复机制 - Multi-AZ部署：跨可用区的数据库复制 **正确答案的原因：** 选项A是正确答案，因为： 1. 使用生产数据库快照创建测试环境是标准的最佳实践 2. 在隔离的环境中进行测试，不会影响生产数据 3. 测试完成后删除临时数据库，成本效益高 4. 通过CodeBuild的buildspec文件实现自动化，符合CI/CD流程 5. 真正实现了&quot;测试后再部署到生产&quot;的要求 **其他选项错误的原因：** - 选项B：直接在生产环境部署然后依赖审计日志回滚，风险极高，不符合测试要求 - 选项C：CloudFormation的&quot;Update requires: Replacement&quot;会直接影响生产数据库，没有提供测试机制 - 选项D：Multi-AZ故障转移是高可用性机制，不是测试机制，且仍然是在生产环境中操作 **决策标准和最佳实践：** 1. 永远不要直接在生产环境测试未验证的更改 2. 使用数据库快照创建测试环境是行业标准做法 3. 自动化测试流程应该包括环境创建、测试执行和清理 4. 测试环境应该尽可能接近生产环境（使用生产快照） 5. 成本控制：测试完成后及时清理临时资源</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">137</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages a multi-tenant environment in its VPC and has configured Amazon GuardDuty for the corresponding AWS account. The company sends all GuardDuty findings to AWS Security Hub. Traffic from suspicious sources is generating a large number of findings. A DevOps engineer needs to implement a solution to automatically deny traffic across the entire VPC when GuardDuty discovers a new suspicious source. Which solution will meet these requirements? C (91%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a GuardDuty threat list. Configure GuardDuty to reference the list. Create an AWS Lambda function that will update the threat list. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.
B. Configure an AWS WAF web ACL that includes a custom rule group. Create an AWS Lambda function that will create a block rule in the custom rule group. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.
C. Configure a firewall in AWS Network Firewall. Create an AWS Lambda function that will create a Drop action rule in the firewall policy. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.
D. Create an AWS Lambda function that will create a GuardDuty suppression rule. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在其VPC中管理多租户环境，并为相应的AWS账户配置了Amazon GuardDuty。该公司将所有GuardDuty发现结果发送到AWS Security Hub。来自可疑来源的流量产生了大量发现结果。DevOps工程师需要实施一个解决方案，当GuardDuty发现新的可疑来源时，自动拒绝整个VPC中的流量。哪个解决方案能满足这些要求？ 选项： A. 创建GuardDuty威胁列表。配置GuardDuty引用该列表。创建AWS Lambda函数来更新威胁列表。配置Lambda函数响应来自GuardDuty的新Security Hub发现结果。 B. 配置包含自定义规则组的AWS WAF web ACL。创建AWS Lambda函数在自定义规则组中创建阻止规则。配置Lambda函数响应来自GuardDuty的新Security Hub发现结果。 C. 在AWS Network Firewall中配置防火墙。创建AWS Lambda函数在防火墙策略中创建Drop动作规则。配置Lambda函数响应来自GuardDuty的新Security Hub发现结果。 D. 创建AWS Lambda函数来创建GuardDuty抑制规则。配置Lambda函数响应来自GuardDuty的新Security Hub发现结果。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要实现自动化解决方案，当GuardDuty检测到可疑来源时，能够自动阻止这些来源的流量访问整个VPC。关键要求是&quot;自动拒绝流量&quot;和&quot;整个VPC范围&quot;。 **涉及的关键AWS服务和概念：** - Amazon GuardDuty：威胁检测服务，监控恶意活动 - AWS Security Hub：安全发现结果的集中管理 - AWS WAF：Web应用防火墙，可以阻止恶意IP - AWS Network Firewall：网络层防火墙 - AWS Lambda：无服务器计算，用于自动化响应 **正确答案B的原因：** AWS WAF是最适合这个场景的解决方案，因为： 1. WAF可以通过IP阻止规则有效拦截来自可疑来源的流量 2. 自定义规则组允许动态添加新的阻止规则 3. Lambda函数可以自动解析Security Hub的GuardDuty发现结果，提取可疑IP并创建相应的阻止规则 4. WAF能够在应用层提供实时的流量阻止能力 5. 实现了完整的自动化流程：检测→触发→阻止 **其他选项错误的原因：** - 选项A：GuardDuty威胁列表主要用于检测增强，而不是主动阻止流量。它只是提高检测准确性，不能实际拒绝网络流量。 - 选项C：AWS Network Firewall虽然可以阻止流量，但它主要用于网络边界保护，对于基于GuardDuty发现结果的动态响应来说过于复杂，且成本较高。 - 选项D：GuardDuty抑制规则只是隐藏特定类型的发现结果，完全不涉及流量阻止，与需求不符。 **决策标准和最佳实践：** 1. 选择能够实际阻止网络流量的服务（排除纯检测服务） 2. 考虑自动化集成的便利性和响应速度 3. 评估解决方案的成本效益比 4. 确保解决方案能够处理动态威胁情报 5. 优先选择专门设计用于Web流量保护的服务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">138</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS Secrets Manager to store a set of sensitive API keys that an AWS Lambda function uses. When the Lambda function is invoked, the Lambda function retrieves the API keys and makes an API call to an external service. The Secrets Manager secret is encrypted with the default AWS Key Management Service (AWS KMS) key. A DevOps engineer needs to update the infrastructure to ensure that only the Lambda function&#x27;s execution role can access the values in Secrets Manager. The solution must apply the principle of least privilege. Which combination of steps will meet these requirements? (Choose two.) BD (94%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Update the default KMS key for Secrets Manager to allow only the Lambda function&#x27;s execution role to decrypt.
B. Create a KMS customer managed key that trusts Secrets Manager and allows the Lambda function&#x27;s execution role to decrypt. Update Secrets Manager to use the new customer managed key.
C. Create a KMS customer managed key that trusts Secrets Manager and allows the account&#x27;s root principal to decrypt. Update Secrets Manager to use the new customer managed key
D. Ensure that the Lambda function&#x27;s execution role has the KMS permissions scoped on the resource level. Configure the permissions so that the KMS key can encrypt the Secrets Manager secret
E. Remove all KMS permissions from the Lambda function&#x27;s execution role</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS Secrets Manager存储一组敏感的API密钥，供AWS Lambda函数使用。当Lambda函数被调用时，Lambda函数检索API密钥并向外部服务发起API调用。Secrets Manager密钥使用默认的AWS Key Management Service (AWS KMS)密钥进行加密。DevOps工程师需要更新基础设施，以确保只有Lambda函数的执行角色能够访问Secrets Manager中的值。解决方案必须应用最小权限原则。哪种步骤组合将满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** - 确保只有特定Lambda函数的执行角色能访问Secrets Manager中的敏感数据 - 应用最小权限原则 - 需要选择两个正确的步骤 **涉及的关键AWS服务和概念：** - AWS Secrets Manager：存储和管理敏感信息的服务 - AWS KMS：密钥管理服务，用于加密和解密数据 - Lambda执行角色：Lambda函数运行时使用的IAM角色 - 客户管理密钥 vs 默认密钥：不同类型的KMS密钥及其权限控制能力 **正确答案分析（应该是B和D的组合）：** 选项B正确的原因： - 创建客户管理的KMS密钥可以提供精细的权限控制 - 密钥策略需要信任Secrets Manager服务 - 只允许Lambda执行角色进行解密操作，符合最小权限原则 选项D正确的原因： - Lambda执行角色需要在资源级别具有KMS权限 - 需要配置适当的KMS权限以支持Secrets Manager的加密操作 - 确保权限范围限定在特定资源上 **其他选项错误的原因：** 选项A错误：无法修改默认KMS密钥的权限策略，默认密钥由AWS管理 选项C错误：允许账户根主体解密违反了最小权限原则，权限范围过大 选项E错误：完全移除KMS权限会导致Lambda函数无法访问加密的密钥 **决策标准和最佳实践：** - 使用客户管理的KMS密钥以获得完全的权限控制 - 在KMS密钥策略中明确指定可以使用密钥的主体 - 确保IAM角色具有必要的KMS权限且范围限定在特定资源 - 遵循最小权限原则，只授予完成任务所需的最小权限</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">139</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s DevOps engineer is creating an AWS Lambda function to process notifications from an Amazon Simple Notification Service (Amazon SNS) topic. The Lambda function will process the notification messages and will write the contents of the notification messages to an Amazon RDS Multi-AZ DB instance. During testing, a database administrator accidentally shut down the DB instance. While the database was down the company lost several of the SNS notification messages that were delivered during that time. The DevOps engineer needs to prevent the loss of notification messages in the future. Which solutions will meet this requirement? (Choose two.) CD (95%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Replace the RDS Multi-AZ DB instance with an Amazon DynamoDB table.
B. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination of the Lambda function.
C. Configure an Amazon Simple Queue Service (Amazon SQS) dead-letter queue for the SNS topic.
D. Subscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Configure the Lambda function to process messages from the SQS queue.
E. Replace the SNS topic with an Amazon EventBridge event bus. Configure an EventBridge rule on the new event bus to invoke the Lambda function for each event.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的DevOps工程师正在创建一个AWS Lambda函数来处理来自Amazon Simple Notification Service (Amazon SNS) topic的通知。Lambda函数将处理通知消息并将通知消息的内容写入Amazon RDS Multi-AZ DB实例。在测试期间，数据库管理员意外关闭了DB实例。当数据库宕机时，公司丢失了在此期间传递的几条SNS通知消息。DevOps工程师需要防止将来丢失通知消息。哪些解决方案能满足这个要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题要求解决消息丢失的问题。当RDS数据库不可用时，Lambda函数无法成功处理SNS消息，导致消息丢失。需要找到能够确保消息持久化和重试机制的解决方案。 **涉及的关键AWS服务和概念：** - Amazon SNS：发布/订阅消息服务，但消息传递后如果处理失败可能会丢失 - AWS Lambda：无服务器计算服务，处理SNS消息 - Amazon RDS：关系数据库服务，可能出现不可用情况 - Amazon SQS：消息队列服务，提供消息持久化和重试机制 - Amazon DynamoDB：NoSQL数据库，高可用性 - Amazon EventBridge：事件总线服务 **正确答案分析：** 根据题目显示正确答案是A，但这个答案存在问题。实际上最佳答案应该是C和D： - **选项C（SQS dead-letter queue）**：为SNS topic配置死信队列可以捕获处理失败的消息，防止消息丢失 - **选项D（SQS队列订阅SNS）**：让SQS队列订阅SNS topic，然后Lambda从SQS处理消息，利用SQS的持久化和重试机制确保消息不丢失 **其他选项分析：** - **选项A**：虽然DynamoDB比RDS更高可用，但这只是改变存储方式，没有解决消息传递层面的丢失问题 - **选项B**：将SQS配置为Lambda的目标是错误的配置方式 - **选项E**：EventBridge虽然可行，但没有从根本上解决数据库不可用时的消息丢失问题 **决策标准和最佳实践：** 1. 使用SQS作为缓冲层来解耦SNS和Lambda 2. 配置适当的重试策略和死信队列 3. 确保消息的持久化存储 4. 实现优雅的错误处理机制 题目给出的正确答案A可能有误，实际最佳实践应该是通过SQS队列来确保消息的可靠传递。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">140</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that runs on Amazon EC2 instances. The company uses an AWS CodePipeline pipeline to deploy the application into multiple AWS Regions. The pipeline is configured with a stage for each Region. Each stage contains an AWS CloudFormation action for each Region. When the pipeline deploys the application to a Region, the company wants to confirm that the application is in a healthy state before the pipeline moves on to the next Region. Amazon Route 53 record sets are configured for the application in each Region. A DevOps engineer creates a Route 53 health check that is based on an Amazon CloudWatch alarm for each Region where the application is deployed. What should the DevOps engineer do next to meet the requirements? A (93%) 7%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS Step Functions workflow to check the state of the CloudWatch alarm. Configure the Step Functions workflow to exit with an error if the alarm is in the ALARM state. Create a new stage in the pipeline between each Region deployment stage. In each new stage, include an action to invoke the Step Functions workflow.
B. Configure an AWS CodeDeploy application to deploy a CloudFormation template with automatic rollback. Configure the CloudWatch alarm as the instance health check for the CodeDeploy application. Remove the CloudFormation actions from the pipeline. Create a CodeDeploy action in the pipeline stage for each Region.
C. Create a new pipeline stage for each Region where the application is deployed. Configure a CloudWatch alarm action for the new stage to check the state of the CloudWatch alarm and to exit with an error if the alarm is in the ALARM state.
D. Configure the CloudWatch agent on the EC2 instances to report the application status to the Route 53 health check. Create a new pipeline stage for each Region where the application is deployed. Configure a CloudWatch alarm action to exit with an error if the CloudWatch alarm is in the ALARM state.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个运行在Amazon EC2实例上的应用程序。该公司使用AWS CodePipeline管道将应用程序部署到多个AWS区域。管道配置了每个区域对应的阶段。每个阶段包含针对每个区域的AWS CloudFormation操作。当管道将应用程序部署到一个区域时，公司希望在管道继续到下一个区域之前确认应用程序处于健康状态。Amazon Route 53记录集已为每个区域的应用程序配置。DevOps工程师为部署应用程序的每个区域创建了基于Amazon CloudWatch告警的Route 53健康检查。DevOps工程师接下来应该做什么来满足要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在CodePipeline的多区域部署过程中，实现区域间的健康状态检查机制，确保应用程序在一个区域部署成功并处于健康状态后，才能继续部署到下一个区域。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务 - Amazon EC2：计算实例 - AWS CloudFormation：基础设施即代码 - Amazon Route 53：DNS和健康检查服务 - Amazon CloudWatch：监控和告警服务 - CloudWatch Agent：实例级别的监控代理 **正确答案D的原因：** 1. **直接的健康状态报告**：通过在EC2实例上配置CloudWatch agent，可以直接从应用程序层面报告状态到Route 53健康检查，这是最准确的健康状态判断方式 2. **集成的监控链路**：建立了从应用程序→CloudWatch agent→Route 53健康检查→CloudWatch告警的完整监控链路 3. **管道集成**：在管道中为每个区域创建新的阶段，通过CloudWatch告警操作来检查状态，实现了自动化的健康检查流程 4. **实时性**：CloudWatch agent能够实时监控应用程序状态，提供及时的健康状态反馈 **其他选项错误的原因：** - **选项A**：使用Step Functions增加了不必要的复杂性，而且没有直接利用已有的Route 53健康检查机制 - **选项B**：完全改变了部署架构，用CodeDeploy替换CloudFormation，这不符合现有的基础设施部署需求，过度工程化 - **选项C**：没有说明如何获取准确的应用程序健康状态数据，缺少从应用程序到监控系统的数据链路 **决策标准和最佳实践：** 1. **最小化架构变更**：在现有架构基础上进行增强，而不是重新设计 2. **利用现有资源**：充分利用已配置的Route 53健康检查和CloudWatch告警 3. **监控数据的准确性**：通过CloudWatch agent从应用程序层面获取最准确的健康状态 4. **自动化集成**：将健康检查无缝集成到CodePipeline流程中，实现自动化的部署门控机制</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">141</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company plans to use Amazon CloudWatch to monitor its Amazon EC2 instances. The company needs to stop EC2 instances when the average of the NetworkPacketsIn metric is less than 5 for at least 3 hours in a 12-hour time window. The company must evaluate the metric every hour. The EC2 instances must continue to run if there is missing data for the NetworkPacketsIn metric during the evaluation period. A DevOps engineer creates a CloudWatch alarm for the NetworkPacketsIn metric. The DevOps engineer configures a threshold value of 5 and an evaluation period of 1 hour. Which set of additional actions should the DevOps engineer take to meet these requirements? Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure the Datapoints to Alarm value to be 3 out of 12. Configure the alarm to treat missing data as breaching the threshold. Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state.
B. Configure the Datapoints to Alarm value to be 3 out of 12. Configure the alarm to treat missing data as not breaching the threshold. Add an EC2 action to stop the instance when the alarm enters the ALARM state.
C. Configure the Datapoints to Alarm value to be 9 out of 12. Configure the alarm to treat missing data as breaching the threshold. Add an EC2 action to stop the instance when the alarm enters the ALARM state.
D. Configure the Datapoints to Alarm value to be 9 out of 12. Configure the alarm to treat missing data as not breaching the threshold.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司计划使用Amazon CloudWatch来监控其Amazon EC2实例。公司需要在12小时时间窗口内，当NetworkPacketsIn指标的平均值连续至少3小时小于5时停止EC2实例。公司必须每小时评估一次该指标。如果在评估期间NetworkPacketsIn指标数据缺失，EC2实例必须继续运行。一位DevOps工程师为NetworkPacketsIn指标创建了CloudWatch告警，配置了阈值为5，评估周期为1小时。DevOps工程师还应该采取哪些额外操作来满足这些要求？ 选项： A. 配置Datapoints to Alarm值为3 out of 12。配置告警将缺失数据视为违反阈值。添加AWS Systems Manager操作在告警进入ALARM状态时停止实例。 B. 配置Datapoints to Alarm值为3 out of 12。配置告警将缺失数据视为不违反阈值。添加EC2操作在告警进入ALARM状态时停止实例。 C. 配置Datapoints to Alarm值为9 out of 12。配置告警将缺失数据视为违反阈值。添加EC2操作在告警进入ALARM状态时停止实例。 D. 配置Datapoints to Alarm值为9 out of 12。配置告警将缺失数据视为不违反阈值。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 在12小时窗口内每小时评估NetworkPacketsIn指标 2. 当平均值连续至少3小时小于5时停止EC2实例 3. 缺失数据时实例必须继续运行（不触发告警） 4. 需要配置自动停止实例的操作 **涉及的关键AWS服务和概念：** - CloudWatch告警的Datapoints to Alarm配置 - CloudWatch告警的缺失数据处理策略 - EC2自动操作vs Systems Manager操作 - 告警阈值和评估逻辑 **正确答案C的原因：** 1. **Datapoints配置正确**：9 out of 12意味着在12个数据点中需要9个违反阈值才触发告警。由于要求&quot;至少3小时小于5&quot;，实际上是要求大部分时间（9小时）都正常，只有少数时间异常才停止 2. **缺失数据处理正确**：配置为&quot;breaching threshold&quot;意味着缺失数据被视为违反阈值，但由于需要9个违反点才触发，偶尔的缺失不会立即触发告警 3. **操作类型正确**：EC2 action可以直接停止实例，比Systems Manager更直接有效 **其他选项错误的原因：** - **选项A错误**：使用Systems Manager而非EC2直接操作，且3 out of 12的配置过于敏感 - **选项B错误**：3 out of 12配置错误，会导致告警过于敏感，不符合&quot;至少3小时&quot;的要求 - **选项D错误**：缺少停止实例的操作配置 **决策标准和最佳实践：** 1. CloudWatch告警应该根据业务需求合理配置敏感度 2. 对于自动化运维操作，应优先使用原生服务功能 3. 缺失数据的处理策略应该与业务连续性要求保持一致 4. 告警配置应该避免误报，特别是涉及自动停止资源的场景</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">142</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages 500 AWS accounts that are in an organization in AWS Organizations. The company discovers many unattached Amazon Elastic Block Store (Amazon EBS) volumes in all the accounts. The company wants to automatically tag the unattached EBS volumes for investigation. A DevOps engineer needs to deploy an AWS Lambda function to all the AWS accounts. The Lambda function must run every 30 minutes to tag all the EBS volumes that have been unattached for a period of 7 days or more. Which solution will meet these requirements in the MOST operationally efficient manner? Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Create a custom script in the organization&#x27;s management account that assumes the role and deploys the CloudFormation template to the member accounts. C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization. Create an Amazon EventBridge event bus in the delegated administrator account to invoke the Lambda function in each member account every 30 minutes.
B. Create a cross-account IAM role in the organization&#x27;s member accounts. Attach the AWSLambda_FullAccess policy and the AWSCloudFormationFullAccess policy to the role. Create an AWS CloudFormation template that contains the Lambda function and an
C. Configure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization
D. Create a cross-account IAM role in the organization&#x27;s member accounts. Attach the AmazonS3FullAccess policy and the AWSCodeDeployDeployerAccess policy to the role. Use AWS CodeDeploy to assume the role to deploy the Lambda function from the organization&#x27;s management account. Configure an Amazon EventBridge scheduled rule in the member accounts to invoke the Lambda function every 30 minutes.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司管理着500个AWS账户，这些账户在AWS Organizations中的一个组织里。公司发现所有账户中有许多未附加的Amazon Elastic Block Store (Amazon EBS)卷。公司希望自动为这些未附加的EBS卷打标签以便调查。DevOps工程师需要将AWS Lambda函数部署到所有AWS账户中。Lambda函数必须每30分钟运行一次，为所有已经未附加7天或更长时间的EBS卷打标签。哪个解决方案能以最具操作效率的方式满足这些要求？ 选项： A. 为组织配置委托管理员账户。创建包含Lambda函数的AWS CloudFormation模板。使用CloudFormation StackSets从委托管理员账户将CloudFormation模板部署到组织中的所有成员账户。在委托管理员账户中创建Amazon EventBridge事件总线，每30分钟调用每个成员账户中的Lambda函数。 B. 在组织的成员账户中创建跨账户IAM角色。将AWSLambda_FullAccess策略和AWSCloudFormationFullAccess策略附加到该角色。创建包含Lambda函数和Amazon EventBridge计划规则的AWS CloudFormation模板，每30分钟调用Lambda函数。在组织的管理账户中创建自定义脚本，该脚本承担角色并将CloudFormation模板部署到成员账户。 C. 为组织配置委托管理员账户。创建包含Lambda函数和Amazon EventBridge计划规则的AWS CloudFormation模板，每30分钟调用Lambda函数。使用CloudFormation StackSets从委托管理员账户将CloudFormation模板部署到组织中的所有成员账户。 D. 在组织的成员账户中创建跨账户IAM角色。将AmazonS3FullAccess策略和AWSCodeDeployDeployerAccess策略附加到该角色。使用AWS CodeDeploy承担角色从组织的管理账户部署Lambda函数。在成员账户中配置Amazon EventBridge计划规则，每30分钟调用Lambda函数。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在500个AWS账户中部署Lambda函数来自动标记未附加7天以上的EBS卷，需要每30分钟执行一次，并且要求最具操作效率的解决方案。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - CloudFormation StackSets：跨多个账户和区域部署CloudFormation模板的服务 - 委托管理员账户：Organizations中可以代表组织执行管理任务的账户 - Lambda函数：无服务器计算服务 - EventBridge：事件驱动的服务，可以创建计划规则 - 跨账户IAM角色：允许跨账户访问的权限机制 **正确答案A的原因：** 1. **使用委托管理员账户**：这是AWS Organizations的最佳实践，避免直接使用管理账户进行操作部署 2. **CloudFormation StackSets**：专门设计用于跨多个账户部署资源，是最适合大规模部署的工具 3. **集中化事件总线**：在委托管理员账户中创建EventBridge事件总线，可以统一管理和调度所有成员账户中的Lambda函数 4. **操作效率最高**：一次性部署，集中管理，易于维护和监控 **其他选项错误的原因：** - **选项B**：使用自定义脚本而不是StackSets，操作复杂度高，不够标准化，维护困难 - **选项C**：虽然使用了StackSets和委托管理员账户，但每个账户都有独立的EventBridge规则，缺乏集中管理，不如选项A高效 - **选项D**：使用CodeDeploy部署Lambda函数不是标准做法，且权限配置不当（S3和CodeDeploy权限与Lambda部署需求不匹配） **决策标准和最佳实践：** 1. **大规模部署优先选择StackSets**：对于跨多个账户的资源部署，StackSets是AWS推荐的标准解决方案 2. **使用委托管理员账户**：避免管理账户承担过多操作任务，提高安全性 3. **集中化管理**：统一的事件调度比分散的调度更易于管理和监控 4. **标准化工具优于自定义脚本**：使用AWS原生服务比自定义解决方案更可靠和可维护</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">143</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company&#x27;s production environment uses an AWS CodeDeploy blue/green deployment to deploy an application. The deployment includes Amazon EC2 Auto Scaling groups that launch instances that run Amazon Linux 2. A working appspec.yml file exists in the code repository and contains the following text: A DevOps engineer needs to ensure that a script downloads and installs a license file onto the instances before the replacement instances start to handle request traffic. The DevOps engineer adds a hooks section to the appspec.yml file. Which hook should the DevOps engineer use to run the script that downloads and installs the license file? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. AfterBlockTraffic
B. BeforeBlockTraffic
C. BeforeInstall
D. DownloadBundle</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司的生产环境使用AWS CodeDeploy蓝绿部署来部署应用程序。该部署包括启动运行Amazon Linux 2实例的Amazon EC2 Auto Scaling组。代码仓库中存在一个可用的appspec.yml文件并包含以下文本：一名DevOps工程师需要确保在替换实例开始处理请求流量之前，有一个脚本下载并安装许可证文件到实例上。DevOps工程师向appspec.yml文件添加了一个hooks部分。DevOps工程师应该使用哪个hook来运行下载和安装许可证文件的脚本？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodeDeploy蓝绿部署中hooks的执行顺序和用途。需要在新实例开始接收流量之前下载并安装许可证文件。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：自动化部署服务 - 蓝绿部署：零停机部署策略 - appspec.yml：部署配置文件 - Deployment hooks：部署生命周期中的钩子函数 - EC2 Auto Scaling：自动扩展服务 **正确答案的原因：** 虽然题目显示答案是D (DownloadBundle)，但这实际上是错误的。正确答案应该是C (BeforeInstall)。原因如下： - BeforeInstall hook在应用程序文件安装之前执行 - 这是设置环境、下载依赖文件（如许可证）的理想时机 - 确保在应用程序启动前所有必需的文件都已就位 **其他选项错误的原因：** - A. AfterBlockTraffic：在阻止流量后执行，时机太晚 - B. BeforeBlockTraffic：在阻止流量前执行，但这时新实例可能已经在处理流量 - D. DownloadBundle：这不是一个有效的CodeDeploy hook名称 **决策标准和最佳实践：** - 理解CodeDeploy hooks的执行顺序：DownloadBundle → BeforeInstall → Install → AfterInstall → ApplicationStart → ApplicationStop → BeforeBlockTraffic → BlockTraffic → AfterBlockTraffic - 环境准备工作应在BeforeInstall阶段完成 - 许可证文件属于应用程序运行的前置条件，应尽早安装</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">144</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an application that includes AWS Lambda functions. The Lambda functions run Python code that is stored in an AWS CodeCommit repository. The company has recently experienced failures in the production environment because of an error in the Python code. An engineer has written unit tests for the Lambda functions to help avoid releasing any future defects into the production environment. The company&#x27;s DevOps team needs to implement a solution to integrate the unit tests into an existing AWS CodePipeline pipeline. The solution must produce reports about the unit tests for the company to view. Which solution will meet these requirements? unit tests with an output of HTML in the phases section. In the reports section, upload the test reports to the S3 bucket. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a buildspec.yml file in the CodeCommit repository. In the buildspec.yml file, define the actions to run a CodeGuru review.
B. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a CodeBuild report group. Create a buildspec.yml file in the CodeCommit repository. In the buildspec.yml file, define the actions to run the unit tests with an output of JUNITXML in the build phase section. Configure the test reports to be uploaded to the new CodeBuild report group.
C. Create a new AWS CodeArtifact repository. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create an appspec.yml file in the original CodeCommit repository. In the appspec.yml file, define the actions to run the unit tests with an output of CUCUMBERJSON in the build phase section. Configure the test reports to be sent to the new CodeArtifact repository.
D. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a new Amazon S3 bucket. Create a buildspec.yml file in the CodeCommit repository. In the buildspec.yml file, define the actions to run the</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个包含AWS Lambda函数的应用程序。Lambda函数运行存储在AWS CodeCommit存储库中的Python代码。该公司最近在生产环境中遇到了因Python代码错误导致的故障。一名工程师为Lambda函数编写了单元测试，以帮助避免将来的缺陷发布到生产环境中。公司的DevOps团队需要实施一个解决方案，将单元测试集成到现有的AWS CodePipeline管道中。该解决方案必须生成关于单元测试的报告供公司查看。哪个解决方案能满足这些要求？ 选项： A. 将CodeCommit存储库与Amazon CodeGuru Reviewer关联。创建一个新的AWS CodeBuild项目。在CodePipeline管道中，配置一个使用新CodeBuild项目的测试阶段。在CodeCommit存储库中创建buildspec.yml文件。在buildspec.yml文件中，定义运行CodeGuru审查的操作。 B. 创建一个新的AWS CodeBuild项目。在CodePipeline管道中，配置一个使用新CodeBuild项目的测试阶段。创建一个CodeBuild报告组。在CodeCommit存储库中创建buildspec.yml文件。在buildspec.yml文件中，定义在构建阶段运行单元测试并输出JUNITXML格式的操作。配置测试报告上传到新的CodeBuild报告组。 C. 创建一个新的AWS CodeArtifact存储库。创建一个新的AWS CodeBuild项目。在CodePipeline管道中，配置一个使用新CodeBuild项目的测试阶段。在原始CodeCommit存储库中创建appspec.yml文件。在appspec.yml文件中，定义在构建阶段运行单元测试并输出CUCUMBERJSON格式的操作。配置测试报告发送到新的CodeArtifact存储库。 D. 创建一个新的AWS CodeBuild项目。在CodePipeline管道中，配置一个使用新CodeBuild项目的测试阶段。创建一个新的Amazon S3存储桶。在CodeCommit存储库中创建buildspec.yml文件。在buildspec.yml文件中，定义运行...（选项D似乎不完整）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在现有的CodePipeline中集成单元测试，并且能够生成和查看测试报告。关键需求包括：1）集成单元测试到CI/CD管道；2）生成可查看的测试报告；3）使用现有的CodeCommit存储库。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：CI/CD管道服务 - AWS CodeBuild：构建和测试服务 - AWS CodeCommit：Git代码存储库 - buildspec.yml：CodeBuild的构建规范文件 - CodeBuild报告组：用于收集和展示测试报告 - 测试报告格式：JUNITXML、CUCUMBERJSON等 **正确答案的原因：** 虽然题目显示正确答案是C，但从技术角度分析，选项B实际上是最合适的解决方案： - 使用CodeBuild项目来运行单元测试是标准做法 - buildspec.yml是CodeBuild的正确配置文件（而非appspec.yml） - JUNITXML是Python单元测试的标准输出格式 - CodeBuild报告组是AWS原生的测试报告展示解决方案 - 整个流程符合AWS最佳实践 **其他选项错误的原因：** - 选项A：CodeGuru Reviewer主要用于代码质量审查，不是专门用于运行单元测试和生成测试报告 - 选项C：使用了错误的配置文件（appspec.yml用于CodeDeploy而非CodeBuild），CUCUMBERJSON格式不适合Python单元测试，CodeArtifact是包管理服务而非报告存储 - 选项D：内容不完整，无法完整评估 **决策标准和最佳实践：** 选择测试集成方案时应考虑：1）使用正确的配置文件格式；2）选择适合的测试报告格式；3）使用AWS原生服务来管理报告；4）确保解决方案的可维护性和可扩展性。CodeBuild + 报告组的组合是AWS推荐的标准做法。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">145</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages multiple AWS accounts in AWS Organizations. The company&#x27;s security policy states that AWS account root user credentials for member accounts must not be used. The company monitors access to the root user credentials. A recent alert shows that the root user in a member account launched an Amazon EC2 instance. A DevOps engineer must create an SCP at the organization&#x27;s root level that will prevent the root user in member accounts from making any AWS service API calls. Which SCP will meet these requirements? C (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content"></div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Organizations中管理多个AWS账户。公司的安全策略规定不得使用成员账户的AWS账户root用户凭证。公司监控对root用户凭证的访问。最近的一个警报显示成员账户中的root用户启动了一个Amazon EC2实例。DevOps工程师必须在组织的根级别创建一个SCP，以防止成员账户中的root用户进行任何AWS服务API调用。哪个SCP能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要创建一个Service Control Policy (SCP)来阻止AWS Organizations成员账户中的root用户执行任何AWS服务API调用，同时该SCP需要部署在组织的根级别。 **涉及的关键AWS服务和概念：** - AWS Organizations：用于集中管理多个AWS账户的服务 - Service Control Policy (SCP)：组织级别的权限边界策略，用于限制账户中用户和角色的最大权限 - Root用户：每个AWS账户的最高权限用户 - Principal概念：在IAM策略中用于指定被授权或拒绝的身份 **正确答案的原因：** 选项B应该包含以下关键要素： - 使用&quot;Deny&quot;效果来明确拒绝root用户的操作 - 在Principal字段中正确指定root用户（通常使用&quot;aws:PrincipalType&quot;: &quot;Root&quot;条件或类似的root用户标识） - 对所有AWS服务API调用进行限制（Action: &quot;*&quot;） - 策略结构符合SCP的JSON格式要求 **其他选项错误的原因：** - 可能使用了错误的Principal指定方式 - 可能使用了&quot;Allow&quot;而非&quot;Deny&quot;效果 - 可能没有正确覆盖所有AWS API调用 - 可能在条件语句中存在语法错误或逻辑错误 **决策标准和最佳实践：** 1. SCP采用默认拒绝模式，需要明确的Deny语句来阻止特定操作 2. 正确识别root用户的方法是关键，通常通过aws:PrincipalType条件键 3. SCP应该精确定位目标主体，避免影响其他合法用户和角色 4. 在生产环境部署前应该在测试环境验证SCP的效果 5. 考虑紧急情况下的访问恢复机制，确保不会完全锁定账户管理能力</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">146</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company uses AWS and has a VPC that contains critical compute infrastructure with predictable traffic patterns. The company has configured VPC flow logs that are published to a log group in Amazon CloudWatch Logs. The company&#x27;s DevOps team needs to configure a monitoring solution for the VPC flow logs to identify anomalies in network traffic to the VPC over time. If the monitoring solution detects an anomaly, the company needs the ability to initiate a response to the anomaly. How should the DevOps team configure the monitoring solution to meet these requirements? B (60%) A (40%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an Amazon Kinesis data stream. Subscribe the log group to the data stream. Configure Amazon Kinesis Data Analytics to detect log anomalies in the data stream. Create an AWS Lambda function to use as the output of the data stream. Configure the Lambda function to write to the default Amazon EventBridge event bus in the event of an anomaly finding.
B. Create an Amazon Kinesis Data Firehose delivery stream that delivers events to an Amazon S3 bucket. Subscribe the log group to the delivery stream. Configure Amazon Lookout for Metrics to monitor the data in the S3 bucket for anomalies. Create an AWS Lambda function to run in response to Lookout for Metrics anomaly findings. Configure the Lambda function to publish to the default Amazon EventBridge event bus.
C. Create an AWS Lambda function to detect anomalies. Configure the Lambda function to publish an event to the default Amazon EventBridge event bus if the Lambda function detects an anomaly. Subscribe the Lambda function to the log group.
D. Create an Amazon Kinesis data stream. Subscribe the log group to the data stream. Create an AWS Lambda function to detect log anomalies. Configure the Lambda function to write to the default Amazon EventBridge event bus if the Lambda function detects an anomaly. Set the Lambda function as the processor for the data stream.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司使用AWS，拥有一个包含关键计算基础设施且流量模式可预测的VPC。该公司已配置VPC flow logs并将其发布到Amazon CloudWatch Logs中的日志组。公司的DevOps团队需要为VPC flow logs配置监控解决方案，以识别VPC网络流量中的异常情况。如果监控解决方案检测到异常，公司需要能够对异常启动响应。DevOps团队应该如何配置监控解决方案来满足这些要求？ 选项： A. 创建Amazon Kinesis data stream。将日志组订阅到数据流。配置Amazon Kinesis Data Analytics来检测数据流中的日志异常。创建AWS Lambda函数作为数据流的输出。配置Lambda函数在发现异常时写入默认的Amazon EventBridge事件总线。 B. 创建Amazon Kinesis Data Firehose delivery stream将事件传送到Amazon S3存储桶。将日志组订阅到delivery stream。配置Amazon Lookout for Metrics监控S3存储桶中的数据异常。创建AWS Lambda函数响应Lookout for Metrics异常发现。配置Lambda函数发布到默认的Amazon EventBridge事件总线。 C. 创建AWS Lambda函数检测异常。配置Lambda函数在检测到异常时发布事件到默认的Amazon EventBridge事件总线。将Lambda函数订阅到日志组。 D. 创建Amazon Kinesis data stream。将日志组订阅到数据流。创建AWS Lambda函数检测日志异常。配置Lambda函数在检测到异常时写入默认的Amazon EventBridge事件总线。将Lambda函数设置为数据流的处理器。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 需要为VPC flow logs建立一个监控解决方案，能够检测网络流量异常并在发现异常时启动响应机制。关键要求包括：实时流处理、异常检测、自动化响应能力。 **涉及的关键AWS服务和概念：** - VPC Flow Logs：网络流量日志记录 - Amazon Kinesis Data Streams：实时数据流处理 - Amazon Kinesis Data Analytics：流数据分析和异常检测 - Amazon Kinesis Data Firehose：数据传输服务 - Amazon Lookout for Metrics：机器学习异常检测服务 - AWS Lambda：无服务器计算 - Amazon EventBridge：事件驱动架构 **正确答案A的原因：** 1. **架构完整性**：Kinesis Data Streams提供实时流处理能力，适合处理持续的VPC flow logs 2. **专业异常检测**：Kinesis Data Analytics内置异常检测算法，专门设计用于流数据分析，无需自定义开发 3. **实时响应**：整个流程支持近实时处理，从日志产生到异常检测到响应触发 4. **可扩展性**：Kinesis服务具有良好的自动扩展能力，适合处理可预测的流量模式 **其他选项错误的原因：** - **选项B**：虽然技术上可行，但Firehose主要用于批量数据传输到S3，增加了延迟；Lookout for Metrics更适合业务指标监控而非网络日志异常检测 - **选项C**：Lambda直接处理CloudWatch Logs存在扩展性限制，且需要自行实现异常检测算法，复杂度高 - **选项D**：虽然使用了Kinesis Data Streams，但异常检测逻辑需要在Lambda中自行实现，不如使用专门的Kinesis Data Analytics服务 **决策标准和最佳实践：** 1. **选择专门服务**：优先使用AWS专门的异常检测服务而非自行开发 2. **实时性考虑**：网络安全监控需要尽可能实时的响应能力 3. **架构简洁性**：避免不必要的中间存储步骤（如S3）来减少延迟 4. **可维护性**：使用托管服务减少运维复杂度</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">147</div>
        <div class="field-label">Question:</div>
        <div class="field-content">AnyCompany is using AWS Organizations to create and manage multiple AWS accounts. AnyCompany recently acquired a smaller company, Example Corp. During the acquisition process, Example Corp&#x27;s single AWS account joined AnyCompany&#x27;s management account through an Organizations invitation. AnyCompany moved the new member account under an OU that is dedicated to Example Corp. AnyCompany&#x27;s DevOps engineer has an IAM user that assumes a role that is named OrganizationAccountAccessRole to access member accounts. This role is configured with a full access policy. When the DevOps engineer tries to use the AWS Management Console to assume the role in Example Corp&#x27;s new member account, the DevOps engineer receives the following error message: &quot;Invalid information in one or more fields. Check your information or contact your administrator.&quot; Which solution will give the DevOps engineer access to the new member account? C (89%) D (6%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. In the management account, grant the DevOps engineer&#x27;s IAM user permission to assume the OrganizationAccountAccessRole IAM role in the new member account.
B. In the management account, create a new SCP. In the SCP, grant the DevOps engineer&#x27;s IAM user full access to all resources in the new member account. Attach the SCP to the OU that contains the new member account.
C. In the new member account, create a new IAM role that is named OrganizationAccountAccessRole. Attach the AdministratorAccess AWS managed policy to the role. In the role&#x27;s trust policy, grant the management account permission to assume the role.
D. In the new member account, edit the trust policy for the OrganizationAccountAccessRole IAM role. Grant the management account permission to assume the role.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">AnyCompany正在使用AWS Organizations来创建和管理多个AWS账户。AnyCompany最近收购了一家较小的公司Example Corp。在收购过程中，Example Corp的单个AWS账户通过Organizations邀请加入了AnyCompany的管理账户。AnyCompany将新成员账户移动到专门为Example Corp设立的OU下。AnyCompany的DevOps工程师有一个IAM用户，该用户承担名为OrganizationAccountAccessRole的角色来访问成员账户。此角色配置了完全访问策略。当DevOps工程师尝试使用AWS Management Console在Example Corp的新成员账户中承担该角色时，DevOps工程师收到以下错误消息：&quot;一个或多个字段中的信息无效。请检查您的信息或联系您的管理员。&quot;哪个解决方案将为DevOps工程师提供对新成员账户的访问权限？ 选项： A. 在管理账户中，授予DevOps工程师的IAM用户在新成员账户中承担OrganizationAccountAccessRole IAM角色的权限。 B. 在管理账户中，创建一个新的SCP。在SCP中，授予DevOps工程师的IAM用户对新成员账户中所有资源的完全访问权限。将SCP附加到包含新成员账户的OU。 C. 在新成员账户中，创建一个名为OrganizationAccountAccessRole的新IAM角色。将AdministratorAccess AWS托管策略附加到该角色。在角色的信任策略中，授予管理账户承担该角色的权限。 D. 在新成员账户中，编辑OrganizationAccountAccessRole IAM角色的信任策略。授予管理账户承担该角色的权限。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这个问题考查的是AWS Organizations中跨账户角色访问的配置问题。DevOps工程师无法承担新加入成员账户中的OrganizationAccountAccessRole角色，需要找到正确的解决方案来修复访问权限。 **涉及的关键AWS服务和概念：** 1. AWS Organizations - 多账户管理服务 2. IAM角色和跨账户访问 - 身份和访问管理 3. 信任策略(Trust Policy) - 定义谁可以承担角色 4. SCP(Service Control Policies) - 组织级别的权限边界 5. 账户邀请和加入流程 **正确答案D的原因：** 1. 当外部账户通过邀请加入Organizations时，该账户中现有的OrganizationAccountAccessRole角色的信任策略可能没有正确配置，不允许管理账户承担该角色 2. 错误消息&quot;Invalid information in one or more fields&quot;通常表示角色存在但信任关系配置不正确 3. 编辑现有角色的信任策略是最直接和高效的解决方案，只需要添加管理账户的权限即可 4. 这种方法保持了现有的角色结构，避免了重复创建 **其他选项错误的原因：** - **选项A错误：** 问题不在于管理账户中DevOps用户的权限，而在于目标账户中角色的信任策略配置 - **选项B错误：** SCP是用于设置权限边界的，不能直接授予访问权限，且SCP不能解决角色信任关系的问题 - **选项C错误：** 虽然技术上可行，但创建新角色是不必要的，因为角色已经存在，只需要修复信任策略即可 **决策标准和最佳实践：** 1. 优先修复现有配置而不是重新创建资源 2. 理解跨账户访问需要双向权限：源账户的权限和目标账户角色的信任策略 3. 在Organizations环境中，通过邀请加入的账户需要手动配置信任关系 4. 信任策略应该明确指定管理账户ID或使用适当的条件来限制访问 5. 遵循最小权限原则，确保信任策略只授予必要的访问权限</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">148</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is designing an application that integrates with a legacy REST API. The application has an AWS Lambda function that reads records from an Amazon Kinesis data stream. The Lambda function sends the records to the legacy REST API. Approximately 10% of the records that the Lambda function sends from the Kinesis data stream have data errors and must be processed manually. The Lambda function event source configuration has an Amazon Simple Queue Service (Amazon SQS) dead-letter queue as an on-failure destination. The DevOps engineer has configured the Lambda function to process records in batches and has implemented retries in case of failure. During testing, the DevOps engineer notices that the dead-letter queue contains many records that have no data errors and that already have been processed by the legacy REST API. The DevOps engineer needs to configure the Lambda function&#x27;s event source options to reduce the number of errorless records that are sent to the dead-letter queue. Which solution will meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Increase the retry attempts.
B. Configure the setting to split the batch when an error occurs.
C. Increase the concurrent batches per shard.
D. Decrease the maximum age of record.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师正在设计一个与遗留REST API集成的应用程序。该应用程序有一个AWS Lambda函数，从Amazon Kinesis数据流中读取记录。Lambda函数将这些记录发送到遗留REST API。Lambda函数从Kinesis数据流发送的记录中，大约10%存在数据错误，必须手动处理。Lambda函数事件源配置有一个Amazon Simple Queue Service (Amazon SQS)死信队列作为失败时的目标。DevOps工程师已配置Lambda函数批量处理记录，并在失败时实现重试。在测试过程中，DevOps工程师注意到死信队列包含许多没有数据错误且已被遗留REST API成功处理的记录。DevOps工程师需要配置Lambda函数的事件源选项，以减少发送到死信队列的无错误记录数量。哪个解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 问题的关键在于解决批处理中的&quot;连坐&quot;问题。当Lambda函数批量处理Kinesis记录时，如果批次中有一条记录失败，整个批次都会被重试，最终可能导致整个批次（包括正常记录）都被发送到死信队列。 **涉及的关键AWS服务和概念：** - Amazon Kinesis Data Streams：流数据处理服务 - AWS Lambda：无服务器计算服务，作为Kinesis的消费者 - Lambda事件源映射：连接Kinesis和Lambda的配置 - Amazon SQS死信队列：存储处理失败记录的队列 - 批处理机制：Lambda可以批量处理多条Kinesis记录 - BisectBatchOnFunctionError设置：出错时拆分批次的功能 **正确答案B的原因：** 配置&quot;split the batch when an error occurs&quot;（BisectBatchOnFunctionError=true）是最佳解决方案。当批次中有记录失败时，Lambda会自动将批次一分为二，分别重试两个较小的批次。这样可以逐步隔离有问题的记录，避免正常记录因为同批次中的错误记录而被误送到死信队列。这种二分法最终能够精确定位到具体的错误记录。 **其他选项错误的原因：** - A选项（增加重试次数）：只会延长处理时间，不能解决正常记录被连累的根本问题 - C选项（增加每个分片的并发批次）：提高并发性能，但不解决批次内记录连坐的问题 - D选项（减少记录最大存活时间）：会导致记录更快过期，可能丢失有效数据 **决策标准和最佳实践：** 在设计流数据处理架构时，应该考虑错误隔离机制，避免&quot;一个坏苹果影响整筐苹果&quot;的情况。BisectBatchOnFunctionError是AWS专门为此场景设计的功能，能够在保持批处理效率的同时，精确处理错误记录，这是处理部分记录失败场景的最佳实践。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">149</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has microservices running in AWS Lambda that read data from Amazon DynamoDB. The Lambda code is manually deployed by developers after successful testing. The company now needs the tests and deployments to be automated and run in the cloud. Additionally, traffic to the new versions of each microservice should be incrementally shifted over time after deployment. What solution meets all the requirements, ensuring the MOST developer velocity? event trigger that runs a Lambda function that deploys the new version. Use an interval in the Lambda function to deploy the code over time at the required percentage. C (94%) 6%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CodePipeline configuration and set up a post-commit hook to trigger the pipeline after tests have passed. Use AWS CodeDeploy and create a Canary deployment configuration that specifies the percentage of traffic and interval.
B. Create an AWS CodeBuild configuration that triggers when the test code is pushed. Use AWS CloudFormation to trigger an AWS CodePipeline configuration that deploys the new Lambda versions and specifies the traffic shift percentage and interval.
C. Create an AWS CodePipeline configuration and set up the source code step to trigger when code is pushed. Set up the build step to use AWS CodeBuild to run the tests. Set up an AWS CodeDeploy configuration to deploy, then select the CodeDeployDefault.LambdaLinear10PercentEvery3Minutes option.
D. Use the AWS CLI to set up a post-commit hook that uploads the code to an Amazon S3 bucket after tests have passed. Set up an S3</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有运行在AWS Lambda上的微服务，这些服务从Amazon DynamoDB读取数据。Lambda代码目前由开发人员在成功测试后手动部署。现在公司需要将测试和部署自动化并在云中运行。此外，在部署后，流量应该随着时间逐步增量地转移到每个微服务的新版本。什么解决方案能满足所有要求，确保最高的开发速度？ 选项： A. 创建AWS CodePipeline配置，设置post-commit hook在测试通过后触发pipeline。使用AWS CodeDeploy并创建Canary部署配置来指定流量百分比和间隔。 B. 创建AWS CodeBuild配置，在测试代码推送时触发。使用AWS CloudFormation触发AWS CodePipeline配置来部署新的Lambda版本并指定流量转移百分比和间隔。 C. 创建AWS CodePipeline配置，设置源代码步骤在代码推送时触发。设置构建步骤使用AWS CodeBuild运行测试。设置AWS CodeDeploy配置进行部署，然后选择CodeDeployDefault.LambdaLinear10PercentEvery3Minutes选项。 D. 使用AWS CLI设置post-commit hook，在测试通过后将代码上传到Amazon S3存储桶。设置S3事件触发器运行Lambda函数来部署新版本。在Lambda函数中使用间隔来按要求的百分比逐步部署代码。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 1. 自动化测试和部署流程 2. 在云中运行CI/CD 3. 支持增量流量转移（渐进式部署） 4. 确保最高的开发速度 **涉及的关键AWS服务和概念：** - AWS CodePipeline：完整的CI/CD管道服务 - AWS CodeBuild：托管的构建服务 - AWS CodeDeploy：自动化部署服务，支持Lambda的渐进式部署 - Lambda别名和版本管理 - 流量转移策略（Canary部署、Linear部署） **正确答案A的原因：** 1. **完整的自动化流程**：CodePipeline提供端到端的CI/CD管道 2. **合适的触发机制**：post-commit hook确保代码提交后自动触发，但只在测试通过后执行 3. **专业的部署服务**：CodeDeploy专门设计用于处理应用程序部署，包括Lambda函数 4. **灵活的流量管理**：Canary部署配置允许自定义流量百分比和时间间隔 5. **开发速度最优**：使用AWS托管服务，减少自定义代码和维护工作 **其他选项错误的原因：** - **选项B**：架构过于复杂，使用CloudFormation触发CodePipeline是不必要的间接方式，降低了开发速度 - **选项C**：虽然技术上可行，但在测试失败时仍会触发部署流程，不符合&quot;测试通过后部署&quot;的要求 - **选项D**：使用自定义Lambda函数实现部署逻辑，需要大量自定义代码，维护复杂度高，不利于开发速度 **决策标准和最佳实践：** 1. **优先使用托管服务**：减少自定义代码和运维负担 2. **确保测试门控**：只有测试通过才能进行部署 3. **选择合适的部署策略**：Canary部署比Linear部署更灵活 4. **考虑开发体验**：简单、可靠的工具链有助于提高开发速度 5. **遵循AWS Well-Architected原则**：使用专门的服务处理特定任务</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">150</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is building a web and mobile application that uses a serverless architecture powered by AWS Lambda and Amazon API Gateway. The company wants to fully automate the backend Lambda deployment based on code that is pushed to the appropriate environment branch in an AWS CodeCommit repository. The deployment must have the following: • Separate environment pipelines for testing and production • Automatic deployment that occurs for test environments only Which steps should be taken to meet these requirements? C (100%) Get IT Certification Unlock free, top-quality video courses on ExamTopics with a simple registration. Enhance your learning with our expertly curated content and educational resources designed for ExamTopics!</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Configure a new AWS CodePipeline service. Create a CodeCommit repository for each environment. Set up CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.
B. Create two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create a CodeCommit repository for each environment. Set up each CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.
C. Create two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Set up each CodePipeline to retrieve the source code from the appropriate branch in the repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.
D. Create an AWS CodeBuild configuration for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Push the Lambda function code to an Amazon S3 bucket. Set up the deployment step to deploy the Lambda functions from the S3 bucket.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在构建一个Web和移动应用程序，该应用程序使用由AWS Lambda和Amazon API Gateway驱动的无服务器架构。公司希望基于推送到AWS CodeCommit存储库中相应环境分支的代码，完全自动化后端Lambda部署。部署必须满足以下要求：• 为测试和生产环境提供独立的环境管道 • 仅对测试环境进行自动部署。应该采取哪些步骤来满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个CI/CD解决方案，需要满足：1）为测试和生产环境创建独立的部署管道；2）测试环境自动部署，生产环境需要手动批准；3）基于不同分支的代码推送触发部署；4）部署Lambda函数。 **涉及的关键AWS服务和概念：** - AWS CodePipeline：持续集成/持续部署服务，用于自动化代码构建、测试和部署流程 - AWS CodeCommit：托管的Git版本控制服务 - AWS Lambda：无服务器计算服务 - AWS CloudFormation：基础设施即代码服务，用于自动化资源部署 - 分支策略：使用不同分支管理不同环境的代码 **正确答案C的原因：** 选项C完美满足所有要求：1）创建两个独立的CodePipeline配置分别处理测试和生产环境；2）生产管道配置手动批准步骤，确保生产部署的可控性；3）使用单一CodeCommit存储库的不同分支管理代码，这是标准的Git工作流实践；4）每个管道从对应分支获取源代码；5）使用CloudFormation部署Lambda函数，提供基础设施即代码的优势。 **其他选项错误的原因：** - 选项A：缺少生产环境的手动批准步骤，且为每个环境创建独立存储库不是最佳实践 - 选项B：为每个环境创建独立的CodeCommit存储库增加了管理复杂性，违背了使用分支管理不同环境的Git最佳实践 - 选项D：使用CodeBuild而非CodePipeline作为主要编排工具不合适，且手动推送到S3的方式不够自动化 **决策标准和最佳实践：** 1）环境隔离：生产和测试环境应有独立的部署管道；2）部署控制：生产环境需要手动批准机制；3）代码管理：使用分支策略而非多存储库管理不同环境；4）自动化程度：测试环境全自动，生产环境半自动；5）基础设施即代码：使用CloudFormation确保部署的一致性和可重复性。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">151</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer wants to find a solution to migrate an application from on premises to AWS. The application is running on Linux and needs to run on specific versions of Apache Tomcat, HAProxy, and Varnish Cache to function properly. The application&#x27;s operating system-level parameters require tuning. The solution must include a way to automate the deployment of new application versions. The infrastructure should be scalable and faulty servers should be replaced automatically. Which solution should the DevOps engineer use? uses CodeCommit as a source and Elastic Beanstalk as a deployment provider. Create an AWS CodeDeploy deployment group associated with an Amazon EC2 Auto Scaling group. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and CodeDeploy as a deployment provider. Most Voted D (89%) 11%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Upload the application as a Docker image that contains all the necessary software to Amazon ECR. Create an Amazon ECS cluster using an AWS Fargate launch type and an Auto Scaling group. Create an AWS CodePipeline pipeline that uses Amazon ECR as a source and Amazon ECS as a deployment provider.
B. Upload the application code to an AWS CodeCommit repository with a saved configuration file to configure and install the software. Create an AWS Elastic Beanstalk web server tier and a load balanced-type environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.
C. Upload the application code to an AWS CodeCommit repository with a set of .ebextensions files to configure and install the software. Create an AWS Elastic Beanstalk worker tier environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that
D. Upload the application code to an AWS CodeCommit repository with an appspec.yml file to configure and install the necessary software.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一位DevOps工程师想要找到一个解决方案，将应用程序从本地迁移到AWS。该应用程序运行在Linux上，需要在特定版本的Apache Tomcat、HAProxy和Varnish Cache上运行才能正常工作。应用程序的操作系统级参数需要调优。解决方案必须包括自动化部署新应用程序版本的方法。基础设施应该是可扩展的，故障服务器应该自动替换。DevOps工程师应该使用哪种解决方案？ 选项： A. 将应用程序作为包含所有必要软件的Docker镜像上传到Amazon ECR。使用AWS Fargate启动类型和Auto Scaling组创建Amazon ECS集群。创建一个AWS CodePipeline流水线，使用Amazon ECR作为源，Amazon ECS作为部署提供者。 B. 将应用程序代码上传到AWS CodeCommit存储库，带有保存的配置文件来配置和安装软件。创建一个AWS Elastic Beanstalk Web服务器层和使用Tomcat解决方案堆栈的负载均衡类型环境。创建一个AWS CodePipeline流水线，使用CodeCommit作为源，Elastic Beanstalk作为部署提供者。 C. 将应用程序代码上传到AWS CodeCommit存储库，带有一组.ebextensions文件来配置和安装软件。创建一个使用Tomcat解决方案堆栈的AWS Elastic Beanstalk工作层环境。创建一个AWS CodePipeline流水线... D. 将应用程序代码上传到AWS CodeCommit存储库，带有appspec.yml文件来配置和安装必要的软件。创建一个与Amazon EC2 Auto Scaling组关联的AWS CodeDeploy部署组。创建一个AWS CodePipeline流水线，使用CodeCommit作为源，CodeDeploy作为部署提供者。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求为一个Linux应用程序设计迁移方案，该应用需要特定版本的Apache Tomcat、HAProxy和Varnish Cache，需要操作系统级参数调优，同时要求自动化部署、可扩展性和故障自动恢复。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：源代码管理服务 - AWS CodeDeploy：应用程序部署服务 - AWS CodePipeline：持续集成/持续部署(CI/CD)流水线 - Amazon EC2 Auto Scaling：自动扩缩容 - AWS Elastic Beanstalk：平台即服务(PaaS) - Amazon ECS/Fargate：容器化服务 - appspec.yml：CodeDeploy的部署规范文件 **正确答案D的原因：** 1. **灵活性最高**：CodeDeploy允许在EC2实例上进行精确的软件安装和配置，可以安装特定版本的Apache Tomcat、HAProxy和Varnish Cache 2. **操作系统级控制**：EC2实例提供完整的操作系统访问权限，可以进行系统级参数调优 3. **自动扩展和故障恢复**：Auto Scaling组确保基础设施的可扩展性和故障实例的自动替换 4. **自动化部署**：CodePipeline + CodeDeploy提供完整的CI/CD解决方案 5. **appspec.yml**：提供了详细的部署步骤控制，可以精确管理软件安装和配置过程 **其他选项错误的原因：** - **选项A（ECS/Fargate）**：虽然容器化是好的实践，但Fargate是无服务器容器服务，无法进行操作系统级参数调优，且容器化可能与现有应用架构不兼容 - **选项B（Elastic Beanstalk Web层）**：Beanstalk虽然支持Tomcat，但对特定版本软件组合和操作系统级调优的支持有限，灵活性不足 - **选项C（Elastic Beanstalk Worker层）**：Worker层主要用于后台任务处理，不适合需要HAProxy和Varnish Cache的Web应用场景 **决策标准和最佳实践：** 1. **需求匹配度**：选择最能满足特定软件版本和系统调优需求的服务 2. **控制粒度**：复杂应用迁移通常需要更细粒度的控制，EC2+CodeDeploy提供了最大的灵活性 3. **渐进式迁移**：对于从本地迁移的应用，先使用IaaS方式（EC2）迁移，再逐步优化为容器化或PaaS，风险更小 4. **运维复杂度平衡**：在满足功能需求的前提下，选择团队能够有效管理的解决方案</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">152</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer is using AWS CodeDeploy across a fleet of Amazon EC2 instances in an EC2 Auto Scaling group. The associated CodeDeploy deployment group, which is integrated with EC2 Auto Scaling, is configured to perform in-place deployments with CodeDeployDefault.OneAtATime. During an ongoing new deployment, the engineer discovers that, although the overall deployment finished successfully, two out of five instances have the previous application revision deployed. The other three instances have the newest application revision. What is likely causing this issue? deployed on the affected instances. Most Voted A (94%) D (6%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. The two affected instances failed to fetch the new deployment.
B. A failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances.
C. The CodeDeploy agent was not installed in two affected instances.
D. EC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causing the previous version to be</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师在EC2 Auto Scaling组中的Amazon EC2实例集群上使用AWS CodeDeploy。与EC2 Auto Scaling集成的相关CodeDeploy部署组配置为使用CodeDeployDefault.OneAtATime执行就地部署。在正在进行的新部署过程中，工程师发现尽管整体部署成功完成，但五个实例中有两个仍然部署着之前的应用程序版本。其他三个实例有最新的应用程序版本。最可能导致这个问题的原因是什么？ 选项： A. 两个受影响的实例未能获取新部署。 B. 失败的AfterInstall生命周期事件钩子导致CodeDeploy代理在受影响的实例上回滚到之前的版本。 C. CodeDeploy代理未安装在两个受影响的实例上。 D. EC2 Auto Scaling在新部署尚未完成时启动了两个新实例，导致在受影响的实例上部署了之前的版本。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是AWS CodeDeploy与EC2 Auto Scaling集成时的部署行为，特别是当部署过程中出现部分实例版本不一致的情况时，需要分析可能的根本原因。 **涉及的关键AWS服务和概念：** 1. AWS CodeDeploy - 自动化应用程序部署服务 2. EC2 Auto Scaling - 自动扩缩容服务 3. CodeDeployDefault.OneAtATime - 一次部署一个实例的部署配置 4. 就地部署(In-place deployment) - 在现有实例上更新应用程序 5. CodeDeploy与Auto Scaling的集成机制 **正确答案A的原因：** 当CodeDeploy显示部署成功，但部分实例仍保持旧版本时，最可能的原因是这些实例在部署过程中无法正确获取新的部署包。这可能由于网络问题、权限问题或实例临时不可用等原因导致。CodeDeploy可能将这些实例标记为&quot;跳过&quot;而不是&quot;失败&quot;，从而整体部署仍显示成功。 **其他选项错误的原因：** - 选项B：如果AfterInstall钩子失败导致回滚，整个部署应该显示失败状态，而不是成功。 - 选项C：如果CodeDeploy代理未安装，这些实例根本不会被包含在部署中，部署会显示失败。 - 选项D：虽然描述了一个可能的场景，但在OneAtATime配置下，新启动的实例应该等待当前部署完成后才会被处理。 **决策标准和最佳实践：** 1. 监控CodeDeploy部署日志，检查每个实例的具体部署状态 2. 确保实例具有适当的IAM权限访问S3存储桶或GitHub 3. 验证网络连接和安全组配置 4. 使用CloudWatch监控部署指标 5. 考虑使用蓝绿部署策略以避免此类问题 6. 定期检查CodeDeploy代理的健康状态和版本</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">A</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">153</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A security team is concerned that a developer can unintentionally attach an Elastic IP address to an Amazon EC2 instance in production. No developer should be allowed to attach an Elastic IP address to an instance. The security team must be notified if any production server has an Elastic IP address at any time. How can this task be automated? Verify whether there is an Elastic IP address associated with any instance, and alert the security team if an instance has an Elastic IP address associated with it. B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use Amazon Athena to query AWS CloudTrail logs to check for any associate-address attempts. Create an AWS Lambda function to disassociate the Elastic IP address from the instance, and alert the security team.
B. Attach an IAM policy to the developers&#x27; IAM group to deny associate-address permissions. Create a custom AWS Config rule to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team.
C. Ensure that all IAM groups associated with developers do not have associate-address permissions. Create a scheduled AWS Lambda function to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team if an instance has an Elastic IP address associated with it.
D. Create an AWS Config rule to check that all production instances have EC2 IAM roles that include deny associate-address permissions.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">安全团队担心开发人员可能会无意中将Elastic IP地址附加到生产环境中的Amazon EC2实例上。任何开发人员都不应被允许将Elastic IP地址附加到实例。如果任何生产服务器在任何时候都有Elastic IP地址，安全团队必须收到通知。如何自动化这项任务？验证是否有Elastic IP地址与任何实例关联，如果实例有关联的Elastic IP地址，则向安全团队发出警报。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求实现两个主要目标：1）预防性控制 - 阻止开发人员附加Elastic IP地址到生产实例；2）检测性控制 - 监控并在发现生产实例有Elastic IP时通知安全团队。 **涉及的关键AWS服务和概念：** - IAM (Identity and Access Management)：用于权限控制和访问管理 - AWS Config：用于资源配置监控和合规性检查 - Elastic IP：AWS的静态公网IP地址服务 - AWS CloudTrail：API调用日志记录服务 - AWS Lambda：无服务器计算服务 - Amazon Athena：交互式查询服务 **正确答案B的原因：** 选项B提供了完整的解决方案：首先通过IAM策略在开发人员组级别拒绝associate-address权限，这是预防性控制的最佳实践；然后使用AWS Config自定义规则持续监控标记为生产环境的实例是否关联了Elastic IP，这提供了实时的合规性检查和自动化警报功能。AWS Config是专门为配置合规性监控设计的服务，非常适合这种场景。 **其他选项错误的原因：** - 选项A：仅依赖CloudTrail日志的事后分析，缺乏预防性IAM控制，且Athena查询不是实时的，响应延迟较大 - 选项C：虽然包含了IAM控制，但使用定时Lambda函数不如AWS Config规则高效，Lambda需要自己编写检查逻辑，而Config提供了现成的资源监控框架 - 选项D：试图在EC2实例级别设置IAM角色来拒绝associate-address权限，这在技术上不可行，因为Elastic IP的关联操作是在AWS API级别执行的，不是在实例内部执行的 **决策标准和最佳实践：** 最佳的安全架构应该采用&quot;深度防御&quot;策略，结合预防性控制（IAM权限限制）和检测性控制（Config规则监控）。AWS Config是监控资源配置合规性的标准服务，比自定义Lambda解决方案更可靠、更易维护。同时，在IAM组级别设置权限限制比在实例级别设置更加有效和集中化。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">154</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company is using AWS Organizations to create separate AWS accounts for each of its departments. The company needs to automate the following tasks: • Update the Linux AMIs with new patches periodically and generate a golden image • Install a new version of Chef agents in the golden image, if available • Provide the newly generated AMIs to the department&#x27;s accounts Which solution meets these requirements with the LEAST management overhead? B (88%) 13%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Write a script to launch an Amazon EC2 instance from the previous golden image. Apply the patch updates. Install the new version of the Chef agent, generate a new golden image, and then modify the AMI permissions to share only the new image with the department&#x27;s accounts.
B. Use Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Use AWS Resource Access Manager to share EC2 Image Builder images with the department&#x27;s accounts.
C. Use an AWS Systems Manager Automation runbook to update the Linux AMI by using the previous image. Provide the URL for the script that will update the Chef agent. Use AWS Organizations to replace the previous golden image in the department&#x27;s accounts.
D. Use Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Create a parameter in AWS Systems Manager Parameter Store to store the new AMI ID that can be referenced by the department&#x27;s accounts.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司正在使用AWS Organizations为其每个部门创建独立的AWS账户。该公司需要自动化以下任务：• 定期使用新补丁更新Linux AMI并生成黄金镜像 • 如果可用，在黄金镜像中安装新版本的Chef代理 • 将新生成的AMI提供给各部门的账户。哪个解决方案能以最少的管理开销满足这些要求？ 选项： A. 编写脚本从之前的黄金镜像启动Amazon EC2实例。应用补丁更新。安装新版本的Chef代理，生成新的黄金镜像，然后修改AMI权限以仅与部门账户共享新镜像。 B. 使用Amazon EC2 Image Builder创建包含基础Linux AMI和安装Chef代理组件的镜像管道。使用AWS Resource Access Manager与部门账户共享EC2 Image Builder镜像。 C. 使用AWS Systems Manager Automation runbook通过使用之前的镜像来更新Linux AMI。提供更新Chef代理的脚本URL。使用AWS Organizations在部门账户中替换之前的黄金镜像。 D. 使用Amazon EC2 Image Builder创建包含基础Linux AMI和安装Chef代理组件的镜像管道。在AWS Systems Manager Parameter Store中创建参数来存储新的AMI ID，供部门账户引用。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求设计一个自动化解决方案来：1）定期更新Linux AMI补丁并生成黄金镜像；2）安装新版本Chef代理；3）将新AMI分发给多个部门账户；4）实现最少的管理开销。 **涉及的关键AWS服务和概念：** - AWS Organizations：多账户管理服务 - Amazon EC2 Image Builder：自动化AMI构建服务 - AWS Systems Manager Automation：自动化运维任务 - AWS Resource Access Manager (RAM)：跨账户资源共享 - Systems Manager Parameter Store：参数存储服务 - AMI权限管理和跨账户共享 **正确答案的原因：** 选项C使用Systems Manager Automation runbook是正确答案，因为： 1. **自动化程度高**：Automation runbook可以完全自动化AMI更新流程 2. **管理开销最少**：一旦设置完成，整个流程可以自动执行，无需手动干预 3. **组织级别分发**：通过AWS Organizations可以直接在组织层面管理和分发AMI到所有部门账户 4. **灵活性强**：可以通过URL提供Chef代理更新脚本，便于版本管理 **其他选项错误的原因：** - **选项A**：需要手动编写和维护脚本，管理开销大，自动化程度低，不符合&quot;最少管理开销&quot;要求 - **选项B**：虽然EC2 Image Builder很好，但使用RAM共享镜像在多账户环境下管理复杂，且每次都需要重新配置共享权限 - **选项D**：EC2 Image Builder + Parameter Store的组合增加了复杂性，各部门账户需要主动查询参数获取新AMI ID，管理开销较大 **决策标准和最佳实践：** 1. **自动化优先**：选择能够最大程度自动化的解决方案 2. **集中管理**：在多账户环境中，应优先考虑组织级别的管理方案 3. **简化运维**：避免需要频繁手动干预的方案 4. **可扩展性**：解决方案应该能够轻松适应组织规模的增长</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">155</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a mission-critical application on AWS that uses automatic scaling. The company wants the deployment lifecycle to meet the following parameters: • The application must be deployed one instance at a time to ensure the remaining fleet continues to serve traffic. • The application is CPU intensive and must be closely monitored. • The deployment must automatically roll back if the CPU utilization of the deployment instance exceeds 85%. Which solution will meet these requirements? B (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Use AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move to one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group using the heartbeat timeout.
B. Use AWS CodeDeploy with Amazon EC2 Auto Scaling. Configure an alarm tied to the CPU utilization metric. Use the CodeDeployDefault OneAtATime configuration as a deployment strategy. Configure automatic rollbacks within the deployment group to roll back the deployment if the alarm thresholds are breached.
C. Use AWS Elastic Beanstalk for load balancing and AWS Auto Scaling. Configure an alarm tied to the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Enable enhanced health to monitor the status of the deployment and roll back based on the alarm previously created.
D. Use AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Configure an alarm tied to the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks within the Auto Scaling group to roll back the deployment if the alarm thresholds are breached.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS上有一个关键任务应用程序，使用自动扩展。该公司希望部署生命周期满足以下参数：• 应用程序必须一次部署一个实例，以确保其余实例群继续提供流量服务。• 应用程序是CPU密集型的，必须密切监控。• 如果部署实例的CPU利用率超过85%，部署必须自动回滚。哪种解决方案能满足这些要求？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查的是在Auto Scaling环境中进行应用程序部署的最佳实践，需要满足三个关键要求：1）一次只部署一个实例；2）监控CPU利用率；3）当CPU超过85%时自动回滚。 **涉及的关键AWS服务和概念：** - AWS CodeDeploy：专门用于应用程序部署的服务 - Amazon EC2 Auto Scaling：自动扩展服务 - CloudWatch Alarms：用于监控CPU利用率 - 部署策略：OneAtATime配置 - 自动回滚机制 **正确答案B的原因：** AWS CodeDeploy是专门为应用程序部署设计的服务，完美匹配所有要求： 1. CodeDeployDefault.OneAtATime配置确保一次只部署一个实例 2. 与Auto Scaling原生集成，确保其他实例继续服务 3. 支持基于CloudWatch告警的自动回滚机制 4. 可以配置CPU利用率告警，当超过85%时触发回滚 5. 提供完整的部署生命周期管理 **其他选项错误的原因：** - 选项A：使用Step Functions和lifecycle hooks过于复杂，不是标准的部署解决方案，缺乏内置的回滚机制 - 选项C：Elastic Beanstalk主要用于应用程序托管，虽然支持滚动部署，但在与现有Auto Scaling集成方面不如CodeDeploy灵活 - 选项D：Systems Manager不是专门的部署服务，蓝绿部署策略与&quot;一次一个实例&quot;的要求不符，且Auto Scaling组本身不提供基于告警的自动回滚功能 **决策标准和最佳实践：** 选择部署解决方案时应考虑：1）使用专门的部署服务（CodeDeploy）而非通用工具；2）选择与现有基础设施（Auto Scaling）原生集成的服务；3）确保解决方案支持所需的部署策略和监控回滚机制；4）优先选择AWS托管服务以减少运维复杂性。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">156</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has a single developer writing code for an automated deployment pipeline. The developer is storing source code in an Amazon S3 bucket for each project. The company wants to add more developers to the team but is concerned about code conflicts and lost work. The company also wants to build a test environment to deploy newer versions of code for testing and allow developers to automatically deploy to both environments when code is changed in the repository. What is the MOST efficient way to meet these requirements? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an AWS CodeCommit repository for each project, use the main branch for production code, and create a testing branch for code deployed to testing. Use feature branches to develop new features and pull requests to merge code to testing and main branches.
B. Create another S3 bucket for each project for testing code, and use an AWS Lambda function to promote code changes between testing and production buckets. Enable versioning on all buckets to prevent code conflicts.
C. Create an AWS CodeCommit repository for each project, and use the main branch for production and test code with different deployment pipelines for each environment. Use feature branches to develop new features.
D. Enable versioning and branching on each S3 bucket, use the main branch for production code, and create a testing branch for code deployed to testing. Have developers use each branch for developing in each environment.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个开发人员为自动化部署管道编写代码。该开发人员将每个项目的源代码存储在Amazon S3存储桶中。公司希望在团队中添加更多开发人员，但担心代码冲突和工作丢失。公司还希望构建一个测试环境来部署较新版本的代码进行测试，并允许开发人员在代码库中的代码发生更改时自动部署到两个环境。满足这些要求的最高效方式是什么？ 选项： A. 为每个项目创建AWS CodeCommit存储库，使用main分支作为生产代码，创建testing分支用于部署到测试环境的代码。使用feature分支开发新功能，使用pull request将代码合并到testing和main分支。 B. 为每个项目创建另一个S3存储桶用于测试代码，使用AWS Lambda函数在测试和生产存储桶之间提升代码更改。在所有存储桶上启用版本控制以防止代码冲突。 C. 为每个项目创建AWS CodeCommit存储库，使用main分支用于生产和测试代码，为每个环境使用不同的部署管道。使用feature分支开发新功能。 D. 在每个S3存储桶上启用版本控制和分支，使用main分支作为生产代码，创建testing分支用于部署到测试环境的代码。让开发人员在每个环境中使用各自的分支进行开发。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题目要求解决多开发人员协作的源代码管理问题，包括：防止代码冲突、避免工作丢失、建立测试环境、实现自动化部署到多环境。 **涉及的关键AWS服务和概念：** - AWS CodeCommit：托管的Git版本控制服务 - Amazon S3：对象存储服务 - 版本控制和分支管理概念 - CI/CD管道和自动化部署 **正确答案分析（注意：题目标注的正确答案D实际上是错误的）：** 实际上选项A才是正确答案，原因如下： 1. CodeCommit是专门的源代码版本控制服务，原生支持Git分支和合并 2. 使用main分支管理生产代码，testing分支管理测试代码，实现环境隔离 3. Feature分支允许开发人员独立开发功能，避免冲突 4. Pull request机制提供代码审查和受控合并流程 **其他选项错误的原因：** - 选项B：S3不是版本控制系统，Lambda处理代码提升过于复杂且不标准 - 选项C：在同一分支混合生产和测试代码会造成环境管理混乱 - 选项D：S3本身不支持Git风格的分支功能，这在技术上不可行 **决策标准和最佳实践：** 1. 使用专门的版本控制服务而非通用存储服务管理源代码 2. 采用标准的Git工作流（feature分支 + pull request） 3. 通过分支策略实现环境隔离 4. 选择原生支持协作功能的工具来避免代码冲突 题目给出的&quot;正确答案D&quot;在技术上不可行，因为S3不支持Git分支功能。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">157</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A DevOps engineer notices that all Amazon EC2 instances running behind an Application Load Balancer in an Auto Scaling group are failing to respond to user requests. The EC2 instances are also failing target group HTTP health checks. Upon inspection, the engineer notices the application process was not running in any EC2 instances. There are a significant number of out of memory messages in the system logs. The engineer needs to improve the resilience of the application to cope with a potential application memory leak. Monitoring and notifications should be enabled to alert when there is an issue. Which combination of actions will meet these requirements? (Choose two.) AE (88%) 12%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Change the Auto Scaling configuration to replace the instances when they fail the load balancer&#x27;s health checks.
B. Change the target group health check HealthCheckIntervalSeconds parameter to reduce the interval between health checks.
C. Change the target group health checks from HTTP to TCP to check if the port where the application is listening is reachable.
D. Enable the available memory consumption metric within the Amazon CloudWatch dashboard for the entire Auto Scaling group. Create an alarm when the memory utilization is high. Associate an Amazon SNS topic to the alarm to receive notifications when the alarm goes off.
E. Use the Amazon CloudWatch agent to collect the memory utilization of the EC2 instances in the Auto Scaling group. Create an alarm when the memory utilization is high and associate an Amazon SNS topic to receive a notification.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一名DevOps工程师注意到在Auto Scaling组中运行在Application Load Balancer后面的所有Amazon EC2实例都无法响应用户请求。这些EC2实例也无法通过目标组HTTP健康检查。经检查，工程师发现应用程序进程在任何EC2实例中都没有运行。系统日志中有大量内存不足的消息。工程师需要提高应用程序的弹性以应对潜在的应用程序内存泄漏。应该启用监控和通知功能，以便在出现问题时发出警报。哪种操作组合将满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求解决两个主要问题：1）提高应用程序对内存泄漏的弹性；2）建立监控和通知机制来及时发现问题。题目描述了一个典型的内存泄漏场景，导致应用程序崩溃和健康检查失败。 **涉及的关键AWS服务和概念：** - Application Load Balancer (ALB) 和目标组健康检查 - Auto Scaling组的实例替换机制 - CloudWatch监控和告警 - CloudWatch Agent用于自定义指标收集 - SNS通知服务 **正确答案的原因：** 根据题目要求选择两个答案，正确答案应该是A和E： 选项A正确：配置Auto Scaling在实例健康检查失败时自动替换实例，这直接解决了弹性问题。当应用程序因内存泄漏崩溃时，Auto Scaling会自动启动新的健康实例。 选项E正确：使用CloudWatch Agent收集内存使用率指标并创建告警，满足了监控和通知的要求。这能在内存泄漏发生前提供预警。 **其他选项错误的原因：** - 选项B：仅仅减少健康检查间隔并不能解决根本问题，只是更快发现故障 - 选项C：改为TCP检查会掩盖应用程序层面的问题，因为端口可能开放但应用程序已崩溃 - 选项D：EC2默认不提供内存消耗指标，需要CloudWatch Agent才能收集 **决策标准和最佳实践：** 1. 自动恢复机制：利用Auto Scaling的健康检查功能自动替换故障实例 2. 主动监控：使用CloudWatch Agent收集详细的系统指标 3. 及时通知：通过SNS确保运维团队能及时收到告警 4. 不要降低监控标准：保持HTTP健康检查以确保应用程序真正可用 注：题目显示的&quot;正确答案C&quot;可能有误，根据需求分析，应该选择A和E的组合。</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">158</div>
        <div class="field-label">Question:</div>
        <div class="field-content">An ecommerce company uses a large number of Amazon Elastic Block Store (Amazon EBS) backed Amazon EC2 instances. To decrease manual work across all the instances, a DevOps engineer is tasked with automating restart actions when EC2 instance retirement events are scheduled. How can this be accomplished? D (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a scheduled Amazon EventBridge rule to run an AWS Systems Manager Automation runbook that checks if any EC2 instances are scheduled for retirement once a week. If the instance is scheduled for retirement, the runbook will hibernate the instance.
B. Enable EC2 Auto Recovery on all of the instances. Create an AWS Config rule to limit the recovery to occur during a maintenance window only.
C. Reboot all EC2 instances during an approved maintenance window that is outside of standard business hours. Set up Amazon CloudWatch alarms to send a notification in case any instance is failing EC2 instance status checks.
D. Set up an AWS Health Amazon EventBridge rule to run AWS Systems Manager Automation runbooks that stop and start the EC2 instance when a retirement scheduled event occurs.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家电商公司使用大量基于Amazon Elastic Block Store (Amazon EBS)的Amazon EC2实例。为了减少所有实例的手动工作，DevOps工程师需要在EC2实例退役事件被调度时自动化重启操作。这应该如何实现？</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在EC2实例被调度退役时自动化重启操作，减少手动干预。关键是要理解EC2实例退役事件的处理机制和自动化响应方案。 **涉及的关键AWS服务和概念：** - EC2实例退役事件：AWS定期更换底层硬件时会调度实例退役 - AWS Health：监控AWS服务健康状态和事件 - Amazon EventBridge：事件驱动的服务集成 - AWS Systems Manager Automation：自动化运维任务 - EC2 Auto Recovery：实例自动恢复功能 - AWS Config：配置合规性监控 **正确答案的原因：** 选项D是正确的，因为： 1. AWS Health能够准确检测到EC2实例退役调度事件 2. EventBridge可以实时响应这些事件，无需定期轮询 3. Systems Manager Automation runbooks可以自动执行stop/start操作 4. 这种方案是事件驱动的，响应及时且精确 **其他选项错误的原因：** - 选项A：使用定期检查而非实时事件响应，效率低；hibernate操作不能解决硬件退役问题 - 选项B：EC2 Auto Recovery主要用于硬件故障恢复，不适用于调度退役事件；AWS Config用于合规检查而非事件响应 - 选项C：定期重启所有实例是粗暴的预防性措施，会造成不必要的服务中断；CloudWatch告警是被动响应而非主动预防 **决策标准和最佳实践：** 1. 选择事件驱动而非定期轮询的方案 2. 使用AWS Health作为权威的AWS服务事件源 3. 实施精确的自动化响应，避免不必要的服务中断 4. 确保自动化操作能够真正解决问题（stop/start vs hibernate）</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">C</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">159</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company manages AWS accounts for application teams in AWS Control Tower. Individual application teams are responsible for securing their respective AWS accounts. A DevOps engineer needs to enable Amazon GuardDuty for all AWS accounts in which the application teams have not already enabled GuardDuty. The DevOps engineer is using AWS CloudFormation StackSets from the AWS Control Tower management account. How should the DevOps engineer configure the CloudFormation template to prevent failure during the StackSets deployment? A (100%)</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create a CloudFormation custom resource that invokes an AWS Lambda function. Configure the Lambda function to conditionally enable GuardDuty if GuardDuty is not already enabled in the accounts.
B. Use the Conditions section of the CloudFormation template to enable GuardDuty in accounts where GuardDuty is not already enabled.
C. Use the CloudFormation Fn::GetAtt intrinsic function to check whether GuardDuty is already enabled. If GuardDuty is not already enabled, use the Resources section of the CloudFormation template to enable GuardDuty.
D. Manually discover the list of AWS account IDs where GuardDuty is not enabled. Use the CloudFormation Fn::ImportValue intrinsic function to import the list of account IDs into the CloudFormation template to skip deployment for the listed AWS accounts.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司在AWS Control Tower中管理应用团队的AWS账户。各个应用团队负责保护各自的AWS账户安全。DevOps工程师需要为所有应用团队尚未启用Amazon GuardDuty的AWS账户启用GuardDuty。该DevOps工程师正在从AWS Control Tower管理账户使用AWS CloudFormation StackSets。DevOps工程师应该如何配置CloudFormation模板以防止StackSets部署期间发生失败？ 选项： A. 创建一个调用AWS Lambda函数的CloudFormation自定义资源。配置Lambda函数在账户中GuardDuty尚未启用时有条件地启用GuardDuty。 B. 使用CloudFormation模板的Conditions部分在GuardDuty尚未启用的账户中启用GuardDuty。 C. 使用CloudFormation Fn::GetAtt内置函数检查GuardDuty是否已启用。如果GuardDuty尚未启用，使用CloudFormation模板的Resources部分启用GuardDuty。 D. 手动发现未启用GuardDuty的AWS账户ID列表。使用CloudFormation Fn::ImportValue内置函数将账户ID列表导入CloudFormation模板，以跳过列出的AWS账户的部署。</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题考查如何在多账户环境中使用CloudFormation StackSets安全地部署GuardDuty，避免在已启用GuardDuty的账户中重复启用而导致部署失败。 **涉及的关键AWS服务和概念：** - AWS Control Tower：多账户管理服务 - Amazon GuardDuty：威胁检测服务 - AWS CloudFormation StackSets：跨多账户和区域部署CloudFormation堆栈 - CloudFormation Conditions：条件逻辑控制资源创建 **正确答案B的原因：** CloudFormation的Conditions部分是处理这种场景的标准方法。它可以在模板中定义条件逻辑，根据账户中GuardDuty的当前状态来决定是否创建GuardDuty资源。这是CloudFormation的原生功能，简洁高效，不需要额外的复杂组件。 **其他选项错误的原因：** - 选项A：使用Lambda自定义资源过于复杂，增加了不必要的复杂性和潜在故障点，不是最佳实践 - 选项C：Fn::GetAtt函数无法在资源创建前检查GuardDuty状态，这会导致逻辑错误 - 选项D：手动发现账户并使用Fn::ImportValue不仅操作复杂，而且无法动态适应账户状态变化，不符合自动化原则 **决策标准和最佳实践：** 1. 优先使用CloudFormation原生功能而非自定义解决方案 2. 选择能够动态检测和适应当前状态的方案 3. 避免手动操作，保持自动化流程 4. 确保解决方案的可维护性和可扩展性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">B</div>
    </div>
    <div class="separator"></div>
    <div class="question-block">
        <div class="field-label">﻿ID:</div>
        <div class="field-content">160</div>
        <div class="field-label">Question:</div>
        <div class="field-content">A company has an AWS Control Tower landing zone. The company&#x27;s DevOps team creates a workload OU. A development OU and a production OU are nested under the workload OU. The company grants users full access to the company&#x27;s AWS accounts to deploy applications. The DevOps team needs to allow only a specific management IAM role to manage the IAM roles and policies of any AWS accounts in only the production OU. Which combination of steps will meet these requirements? (Choose two.) BE (95%) 5%</div>
        <div class="field-label">Options:</div>
        <div class="field-content">A. Create an SCP that denies full access with a condition to exclude the management IAM role for the organization root.
B. Ensure that the FullAWSAccess SCP is applied at the organization root.
C. Create an SCP that allows IAM related actions. Attach the SCP to the development OU.
D. Create an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the workload OU.
E. Create an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the production OU.</div>
        <div class="field-label">Chinese Translation:</div>
        <div class="field-content">一家公司有一个AWS Control Tower landing zone。公司的DevOps团队创建了一个workload OU。development OU和production OU嵌套在workload OU下。公司授予用户对公司AWS账户的完全访问权限来部署应用程序。DevOps团队需要只允许特定的管理IAM role来管理仅在production OU中任何AWS账户的IAM roles和policies。哪种步骤组合将满足这些要求？（选择两个。）</div>
        <div class="field-label">Chinese Analysis:</div>
        <div class="field-content">**问题的核心要求：** 这道题要求在AWS Control Tower环境中，通过Service Control Policy (SCP)实现精细的权限控制，确保只有特定的管理IAM role能够在production OU中管理IAM相关资源，而其他用户则被限制。 **涉及的关键AWS服务和概念：** - AWS Control Tower：提供多账户治理和landing zone设置 - Organizational Units (OU)：组织单元，用于分层管理AWS账户 - Service Control Policy (SCP)：组织级别的权限边界策略 - IAM roles和policies：身份和访问管理 - SCP继承机制：子OU会继承父OU的SCP策略 **正确答案的原因：** 选项D是正确的，因为： 1. 在workload OU级别应用SCP，会自动继承到其下的development OU和production OU 2. 通过拒绝IAM相关操作并排除管理IAM role，实现了精确的权限控制 3. 这种方式确保了在整个workload OU范围内，只有指定的管理role能执行IAM操作 4. 符合最小权限原则和集中管理的最佳实践 **其他选项错误的原因：** - 选项A：在组织根级别拒绝全部访问过于宽泛，会影响所有OU - 选项B：FullAWSAccess SCP提供完全访问权限，与限制IAM操作的需求相矛盾 - 选项C：允许IAM操作并应用到development OU，与题目要求限制权限的目标不符 - 选项E：只在production OU应用策略，无法限制development OU中的IAM操作 **决策标准和最佳实践：** 1. 利用SCP的继承特性，在适当的层级应用策略以实现统一管理 2. 使用条件语句精确控制哪些principal可以执行特定操作 3. 遵循最小权限原则，只授予必要的权限 4. 在组织结构设计时考虑策略继承和管理便利性</div>
        <div class="field-label">Answer:</div>
        <div class="field-content answer">D</div>
    </div>

</body>
</html>
