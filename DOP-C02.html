<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CSV Data: aws_exam_analysis</title>
    
        <style>
            table {
                border-collapse: collapse;
                width: 100%;
                margin: 20px 0;
                font-family: Arial, sans-serif;
            }
            
            th, td {
                border: 1px solid #ddd;
                padding: 8px 12px;
                text-align: left;
            }
            
            th {
                background-color: #f2f2f2;
                font-weight: bold;
            }
            
            tr:nth-child(even) {
                background-color: #f9f9f9;
            }
            
            tr:hover {
                background-color: #f5f5f5;
            }
        </style>
        
    
    <style>
        @media (max-width: 768px) {
            table { font-size: 12px; }
            th, td { padding: 4px 6px; }
        }
    </style>
    
</head>
<body>
    <div style="max-width: 100%; overflow-x: auto;">
        <h1>CSV Data: aws_exam_analysis</h1>
        <p><small>Generated on 2025-09-18 14:17:28</small></p>

        <table>
            <thead>
                <tr>
                    <th>﻿问题编号</th>
                    <th>问题本身（英文）</th>
                    <th>各选项（英文）</th>
                    <th>翻译问题本身</th>
                    <th>分析各选项（中文）</th>
                    <th>答案</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>1</td>
                    <td>A company has a mobile application that makes HTTP API calls to an Application Load Balancer (ALB). The ALB routes requests to an AWS<br>Lambda function. Many different versions of the application are in use at any given time, including versions that are in testing by a subset of<br>users. The version of the application is dened in the user-agent header that is sent with all requests to the API.<br>After a series of recent changes to the API, the company has observed issues with the application. The company needs to gather a metric for<br>each API operation by response code for each version of the application that is in use. A DevOps engineer has modied the Lambda function<br>to extract the API operation name, version information from the user-agent header and response code.</td>
                    <td>A. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon<br>CloudWatch Logs log group. Congure a CloudWatch Logs metric lter that increments a metric for each API operation name. Specify<br>response code and application version as dimensions for the metric.<br>B. Modify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon<br>CloudWatch Logs log group. Congure a CloudWatch Logs Insights query to populate CloudWatch metrics from the log lines. Specify<br>response code and application version as dimensions for the metric.<br>C. Congure the ALB access logs to write to an Amazon CloudWatch Logs log group. Modify the Lambda function to respond to the ALB<br>with the API operation name, response code, and version number as response metadata. Congure a CloudWatch Logs metric lter that<br>increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.<br>D. Congure AWS X-Ray integration on the Lambda function. Modify the Lambda function to create an X-Ray subsegment with the API<br>operation name, response code, and version number. Congure X-Ray insights to extract an aggregated metric for each API operation<br>name and to publish the metric to Amazon CloudWatch. Specify response code and application version as dimensions for the metric.</td>
                    <td>一家公司有一个移动应用程序，它向应用程序负载均衡器(ALB)发出HTTP API调用。ALB将请求路由到AWS Lambda函数。在任何给定时间都有许多不同版本的应用程序在使用，包括正在由部分用户测试的版本。应用程序的版本在用户代理标头中定义，该标头与所有API请求一起发送。在最近对API进行一系列更改后，公司观察到应用程序出现问题。公司需要为正在使用的每个应用程序版本收集按响应代码分类的每个API操作的指标。DevOps工程师已修改Lambda函数以提取API操作名称、用户代理标头中的版本信息和响应代码。</td>
                    <td>选项A：修改Lambda函数将API操作名称、响应代码和版本号作为日志行写入CloudWatch Logs日志组，配置CloudWatch Logs指标过滤器为每个API操作名称递增指标，并指定响应代码和应用程序版本作为指标维度。这种方法可行，但指标过滤器在处理复杂的多维度指标时可能不够灵活。<br><br>选项B：修改Lambda函数将API操作名称、响应代码和版本号作为日志行写入CloudWatch Logs日志组，配置CloudWatch Logs Insights查询从日志行填充CloudWatch指标，并指定响应代码和应用程序版本作为指标维度。CloudWatch Logs Insights提供了更强大的查询能力，可以更好地处理复杂的日志分析和指标生成。<br><br>选项C：配置ALB访问日志写入CloudWatch Logs日志组，修改Lambda函数以响应元数据的形式向ALB返回API操作名称、响应代码和版本号，然后配置指标过滤器。这种方法过于复杂，且ALB访问日志可能不包含Lambda函数内部的详细信息。<br><br>选项D：在Lambda函数上配置AWS X-Ray集成，修改Lambda函数创建包含API操作名称、响应代码和版本号的X-Ray子段，配置X-Ray insights提取聚合指标并发布到CloudWatch。虽然X-Ray很强大，但对于这个特定需求来说可能过于复杂。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>2</td>
                    <td>A company provides an application to customers. The application has an Amazon API Gateway REST API that invokes an AWS Lambda<br>function. On initialization, the Lambda function loads a large amount of data from an Amazon DynamoDB table. The data load process results<br>in long cold-start times of 8-10 seconds. The DynamoDB table has DynamoDB Accelerator (DAX) congured.<br>Customers report that the application intermittently takes a long time to respond to requests. The application receives thousands of requests<br>throughout the day. In the middle of the day, the application experiences 10 times more requests than at any other time of the day. Near the<br>end of the day, the application&#x27;s request volume decreases to 10% of its normal total.<br>A DevOps engineer needs to reduce the latency of the Lambda function at all times of the day.</td>
                    <td>A. Congure provisioned concurrency on the Lambda function with a concurrency value of 1. Delete the DAX cluster for the DynamoDB<br>table.<br>B. Congure reserved concurrency on the Lambda function with a concurrency value of 0.<br>C. Congure provisioned concurrency on the Lambda function. Congure AWS Application Auto Scaling on the Lambda function with<br>provisioned concurrency values set to a minimum of 1 and a maximum of 100.<br>D. Congure reserved concurrency on the Lambda function. Congure AWS Application Auto Scaling on the API Gateway API with a<br>reserved concurrency maximum value of 100.</td>
                    <td>一家公司为客户提供应用程序。该应用程序有一个Amazon API Gateway REST API，它调用AWS Lambda函数。在初始化时，Lambda函数从Amazon DynamoDB表中加载大量数据。数据加载过程导致8-10秒的长冷启动时间。DynamoDB表已配置了DynamoDB Accelerator (DAX)。<br>客户报告应用程序间歇性地需要很长时间才能响应请求。应用程序全天接收数千个请求。在一天中的中午时段，应用程序的请求量比其他任何时间都多10倍。在一天结束时，应用程序的请求量减少到正常总量的10%。<br>DevOps工程师需要在一天中的所有时间都减少Lambda函数的延迟。</td>
                    <td>A. 在Lambda函数上配置并发值为1的预置并发。删除DynamoDB表的DAX集群。<br>这个选项有问题，因为只设置1个预置并发无法处理高峰期10倍的请求量，会导致性能瓶颈。删除DAX集群会增加DynamoDB访问延迟，与减少延迟的目标相反。<br><br>B. 在Lambda函数上配置并发值为0的保留并发。<br>这个选项完全错误，将保留并发设置为0意味着Lambda函数无法执行任何请求，这会完全阻止应用程序运行。<br><br>C. 在Lambda函数上配置预置并发。在Lambda函数上配置AWS Application Auto Scaling，预置并发值设置为最小1，最大100。<br>这是最佳选项。预置并发可以消除冷启动问题，而Auto Scaling可以根据请求量自动调整并发数量，在高峰期扩展到更多并发，在低峰期缩减资源，既保证性能又控制成本。<br><br>D. 在Lambda函数上配置保留并发。在API Gateway API上配置AWS Application Auto Scaling，保留并发最大值为100。<br>这个选项有误，API Gateway本身不支持Application Auto Scaling来调整Lambda的并发设置，而且保留并发不能解决冷启动问题。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>3</td>
                    <td>A company is adopting AWS CodeDeploy to automate its application deployments for a Java-Apache Tomcat application with an Apache<br>Webserver. The development team started with a proof of concept, created a deployment group for a developer environment, and performed<br>functional tests within the application. After completion, the team will create additional deployment groups for staging and production.<br>The current log level is congured within the Apache settings, but the team wants to change this conguration dynamically when the<br>deployment occurs, so that they can set different log level congurations depending on the deployment group without having a different<br>application revision for each group.<br>How can these requirements be met with the LEAST management overhead and without requiring different script versions for each<br>deployment group?</td>
                    <td>A. Tag the Amazon EC2 instances depending on the deployment group. Then place a script into the application revision that calls the<br>metadata service and the EC2 API to identify which deployment group the instance is part of. Use this information to congure the log<br>level settings. Reference the script as part of the AfterInstall lifecycle hook in the appspec.yml le.<br>B. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ NAME to identify which deployment group the<br>instance is part of. Use this information to congure the log level settings. Reference this script as part of the BeforeInstall lifecycle hook<br>in the appspec.yml le.<br>C. Create a CodeDeploy custom environment variable for each environment. Then place a script into the application revision that checks<br>this environment variable to identify which deployment group the instance is part of. Use this information to congure the log level<br>settings. Reference this script as part of the ValidateService lifecycle hook in the appspec.yml le.<br>D. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ID to identify which deployment group the<br>instance is part of to congure the log level settings. Reference this script as part of the Install lifecycle hook in the appspec.yml le.</td>
                    <td>一家公司正在采用AWS CodeDeploy来自动化其Java-Apache Tomcat应用程序与Apache Web服务器的应用部署。开发团队从概念验证开始，为开发环境创建了一个部署组，并在应用程序内执行了功能测试。完成后，团队将为预发布和生产环境创建额外的部署组。当前的日志级别在Apache设置中配置，但团队希望在部署发生时动态更改此配置，以便他们可以根据部署组设置不同的日志级别配置，而无需为每个组使用不同的应用程序版本。如何以最少的管理开销满足这些要求，且不需要为每个部署组使用不同的脚本版本？</td>
                    <td>选项A：通过标记EC2实例来识别部署组，然后在应用程序版本中放置脚本调用元数据服务和EC2 API来识别实例属于哪个部署组。这种方法需要额外的EC2标记管理和API调用，增加了复杂性和管理开销，不符合&quot;最少管理开销&quot;的要求。<br><br>选项B：创建脚本使用CodeDeploy环境变量DEPLOYMENT_GROUP_NAME来识别实例属于哪个部署组，并在BeforeInstall生命周期钩子中引用此脚本。这种方法直接利用CodeDeploy内置的环境变量，无需额外配置，管理开销最小，且BeforeInstall阶段适合进行配置更改。<br><br>选项C：为每个环境创建CodeDeploy自定义环境变量，然后在应用程序版本中放置脚本检查环境变量。这需要为每个部署组创建和管理自定义环境变量，增加了管理复杂性，且ValidateService阶段主要用于验证服务是否正常运行，不适合配置更改。<br><br>选项D：使用DEPLOYMENT_GROUP_ID环境变量并在Install生命周期钩子中执行。虽然使用了内置环境变量，但DEPLOYMENT_GROUP_ID是数字标识符，不如DEPLOYMENT_GROUP_NAME直观易用，且Install阶段主要用于文件复制，不是配置更改的最佳时机。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>4</td>
                    <td>A company requires its developers to tag all Amazon Elastic Block Store (Amazon EBS) volumes in an account to indicate a desired backup<br>frequency. This requirement Includes EBS volumes that do not require backups. The company uses custom tags named Backup_Frequency<br>that have values of none, dally, or weekly that correspond to the desired backup frequency. An audit nds that developers are occasionally not<br>tagging the EBS volumes.<br>A DevOps engineer needs to ensure that all EBS volumes always have the Backup_Frequency tag so that the company can perform backups at<br>least weekly unless a different value is specied.</td>
                    <td>A. Set up AWS Cong in the account. Create a custom rule that returns a compliance failure for all Amazon EC2 resources that do not have<br>a Backup Frequency tag applied. Congure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply<br>the Backup_Frequency tag with a value of weekly.<br>B. Set up AWS Cong in the account. Use a managed rule that returns a compliance failure for EC2::Volume resources that do not have a<br>Backup Frequency tag applied. Congure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply<br>the Backup_Frequency tag with a value of weekly.<br>C. Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events. Congure a<br>custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the<br>target of the rule.<br>D. Turn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events or EBS<br>ModifyVolume events. Congure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of<br>weekly. Specify the runbook as the target of the rule.</td>
                    <td>一家公司要求其开发人员为账户中的所有Amazon弹性块存储(Amazon EBS)卷打标签，以指示所需的备份频率。此要求包括不需要备份的EBS卷。公司使用名为Backup_Frequency的自定义标签，其值为none、daily或weekly，对应所需的备份频率。审计发现开发人员偶尔不会为EBS卷打标签。DevOps工程师需要确保所有EBS卷始终具有Backup_Frequency标签，以便公司可以至少每周执行一次备份，除非指定了不同的值。</td>
                    <td>选项A：使用AWS Config创建自定义规则，针对所有没有Backup_Frequency标签的Amazon EC2资源返回合规性失败。问题在于这里针对的是&quot;所有EC2资源&quot;而不是专门的EBS卷，范围过于宽泛，可能会影响其他不需要此标签的EC2资源类型。<br><br>选项B：使用AWS Config设置托管规则，专门针对EC2::Volume资源(即EBS卷)检查是否缺少Backup_Frequency标签。这个选项精确地针对EBS卷资源，使用托管规则比自定义规则更可靠，并配置修复操作使用Systems Manager自动化运行手册应用默认的weekly标签值。<br><br>选项C：使用CloudTrail和EventBridge监听EBS CreateVolume事件。虽然可以在创建新卷时自动添加标签，但这种方法只能处理新创建的卷，无法解决现有未标记卷的问题，且不能检测标签被删除的情况。<br><br>选项D：类似选项C，但增加了ModifyVolume事件监听。虽然覆盖面更广，但仍然存在与选项C相同的根本问题：无法处理现有未标记的卷，且ModifyVolume事件不一定与标签操作相关。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>5</td>
                    <td>A company is using an Amazon Aurora cluster as the data store for its application. The Aurora cluster is congured with a single DB instance.<br>The application performs read and write operations on the database by using the cluster&#x27;s instance endpoint.<br>The company has scheduled an update to be applied to the cluster during an upcoming maintenance window. The cluster must remain<br>available with the least possible interruption during the maintenance window.</td>
                    <td>A. Add a reader instance to the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the<br>Aurora cluster&#x27;s reader endpoint for reads.<br>B. Add a reader instance to the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora<br>cluster&#x27;s custom ANY endpoint for read and write operations.<br>C. Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations.<br>Update the Aurora cluster’s reader endpoint for reads.<br>D. Turn on the Multi-AZ option on the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the<br>Aurora cluster&#x27;s custom ANY endpoint for read and write operations</td>
                    <td>一家公司正在使用Amazon Aurora集群作为其应用程序的数据存储。Aurora集群配置了单个数据库实例。应用程序通过使用集群的实例端点对数据库执行读写操作。公司已安排在即将到来的维护窗口期间对集群应用更新。集群必须在维护窗口期间保持可用，并且中断时间尽可能少。</td>
                    <td>选项A：添加一个读取器实例到Aurora集群，更新应用程序使用Aurora集群端点进行写操作，使用Aurora集群的读取器端点进行读操作。这种方案可以提供读写分离，但在维护期间主实例仍可能不可用，无法完全解决维护期间的可用性问题。<br><br>选项B：添加读取器实例并创建自定义ANY端点用于读写操作。ANY端点会将连接路由到任何可用实例，但这可能导致读写操作路由到只读实例，造成写操作失败。<br><br>选项C：在Aurora集群上启用Multi-AZ选项，使用集群端点进行写操作，使用读取器端点进行读操作。Multi-AZ提供自动故障转移功能，在维护期间可以自动切换到备用实例，确保最小中断时间。<br><br>选项D：启用Multi-AZ并使用自定义ANY端点。虽然Multi-AZ提供高可用性，但ANY端点可能将写操作错误路由到只读实例。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>6</td>
                    <td>A company must encrypt all AMIs that the company shares across accounts. A DevOps engineer has access to a source account where an<br>unencrypted custom AMI has been built. The DevOps engineer also has access to a target account where an Amazon EC2 Auto Scaling group<br>will launch EC2 instances from the AMI. The DevOps engineer must share the AMI with the target account.<br>The company has created an AWS Key Management Service (AWS KMS) key in the source account.</td>
                    <td>A. In the source account, copy the unencrypted AMI to an encrypted AMI. Specify the KMS key in the copy action.<br>B. In the source account, copy the unencrypted AMI to an encrypted AMI. Specify the default Amazon Elastic Block Store (Amazon EBS)<br>encryption key in the copy action.<br>C. In the source account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role in the target<br>account.<br>D. In the source account, modify the key policy to give the target account permissions to create a grant. In the target account, create a<br>KMS grant that delegates permissions to the Auto Scaling group service-linked role.<br>E. In the source account, share the unencrypted AMI with the target account.<br>F. In the source account, share the encrypted AMI with the target account.</td>
                    <td>一家公司必须加密所有跨账户共享的AMI。一名DevOps工程师可以访问源账户，该账户中已构建了一个未加密的自定义AMI。该DevOps工程师还可以访问目标账户，目标账户中的Amazon EC2 Auto Scaling组将从该AMI启动EC2实例。DevOps工程师必须与目标账户共享该AMI。公司已在源账户中创建了一个AWS密钥管理服务(AWS KMS)密钥。</td>
                    <td>A. 在源账户中，将未加密的AMI复制为加密的AMI。在复制操作中指定KMS密钥。<br>这是正确的。要跨账户共享加密的AMI，首先需要将未加密的AMI复制为加密版本，使用公司提供的KMS密钥进行加密。<br><br>B. 在源账户中，将未加密的AMI复制为加密的AMI。在复制操作中指定默认的Amazon弹性块存储(Amazon EBS)加密密钥。<br>这是不正确的。使用默认EBS加密密钥无法实现跨账户共享，因为默认密钥是账户特定的，目标账户无法访问。<br><br>C. 在源账户中，创建一个KMS授权，将权限委托给目标账户中的Auto Scaling组服务链接角色。<br>这是正确的。为了让目标账户的Auto Scaling组能够使用加密的AMI启动实例，需要创建KMS授权给服务链接角色相应的权限。<br><br>D. 在源账户中，修改密钥策略以给目标账户创建授权的权限。在目标账户中，创建一个KMS授权，将权限委托给Auto Scaling组服务链接角色。<br>这也是一种可行的方法，通过修改密钥策略允许目标账户创建授权，然后在目标账户中创建相应的授权。<br><br>E. 在源账户中，与目标账户共享未加密的AMI。<br>这是不正确的。题目明确要求必须加密所有跨账户共享的AMI，共享未加密AMI不符合要求。<br><br>F. 在源账户中，与目标账户共享加密的AMI。<br>这个步骤是必要的，但仅仅共享加密AMI还不够，还需要配置相应的KMS权限。</td>
                    <td>ACD</td>
                </tr>
                <tr>
                    <td>7</td>
                    <td>A company uses AWS CodePipeline pipelines to automate releases of its application A typical pipeline consists of three stages build, test, and<br>deployment. The company has been using a separate AWS CodeBuild project to run scripts for each stage. However, the company now wants<br>to use AWS CodeDeploy to handle the deployment stage of the pipelines.<br>The company has packaged the application as an RPM package and must deploy the application to a eet of Amazon EC2 instances. The EC2<br>instances are in an EC2 Auto Scaling group and are launched from a common AMI.</td>
                    <td>A. Create a new version of the common AMI with the CodeDeploy agent installed. Update the IAM role of the EC2 instances to allow<br>access to CodeDeploy.<br>B. Create a new version of the common AMI with the CodeDeploy agent installed. Create an AppSpec le that contains application<br>deployment scripts and grants access to CodeDeploy.<br>C. Create an application in CodeDeploy. Congure an in-place deployment type. Specify the Auto Scaling group as the deployment target.<br>Add a step to the CodePipeline pipeline to use EC2 Image Builder to create a new AMI. Congure CodeDeploy to deploy the newly created<br>AMI.<br>D. Create an application in CodeDeploy. Congure an in-place deployment type. Specify the Auto Scaling group as the deployment target.<br>Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.<br>E. Create an application in CodeDeploy. Congure an in-place deployment type. Specify the EC2 instances that are launched from the<br>common AMI as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.</td>
                    <td>一家公司使用AWS CodePipeline管道来自动化其应用程序的发布。典型的管道包含三个阶段：构建、测试和部署。该公司一直使用单独的AWS CodeBuild项目来为每个阶段运行脚本。但是，该公司现在希望使用AWS CodeDeploy来处理管道的部署阶段。<br>该公司已将应用程序打包为RPM包，并且必须将应用程序部署到一组Amazon EC2实例。这些EC2实例位于EC2 Auto Scaling组中，并从通用AMI启动。</td>
                    <td>A. 创建安装了CodeDeploy代理的通用AMI新版本。更新EC2实例的IAM角色以允许访问CodeDeploy。<br>这个选项是正确的，因为要使用CodeDeploy部署到EC2实例，必须在实例上安装CodeDeploy代理，并且实例需要适当的IAM权限来与CodeDeploy服务通信。<br><br>B. 创建安装了CodeDeploy代理的通用AMI新版本。创建包含应用程序部署脚本的AppSpec文件并授予对CodeDeploy的访问权限。<br>这个选项部分正确，但AppSpec文件不是用来&quot;授予访问权限&quot;的，而是用来定义部署步骤的。权限应该通过IAM角色配置。<br><br>C. 在CodeDeploy中创建应用程序。配置就地部署类型。指定Auto Scaling组作为部署目标。向CodePipeline管道添加步骤以使用EC2 Image Builder创建新AMI。配置CodeDeploy部署新创建的AMI。<br>这个选项混淆了概念。CodeDeploy不是用来部署AMI的，而是用来部署应用程序代码到现有实例的。<br><br>D. 在CodeDeploy中创建应用程序。配置就地部署类型。指定Auto Scaling组作为部署目标。更新CodePipeline管道以使用CodeDeploy操作来部署应用程序。<br>这个选项是正确的，描述了正确的CodeDeploy配置方式，包括创建应用程序、配置就地部署、指定Auto Scaling组作为目标，并在管道中集成CodeDeploy操作。<br><br>E. 在CodeDeploy中创建应用程序。配置就地部署类型。指定从通用AMI启动的EC2实例作为部署目标。更新CodePipeline管道以使用CodeDeploy操作来部署应用程序。<br>这个选项不够理想，因为直接指定EC2实例而不是Auto Scaling组会使管理更复杂，特别是当实例数量动态变化时。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>8</td>
                    <td>A company’s security team requires that all external Application Load Balancers (ALBs) and Amazon API Gateway APIs are associated with<br>AWS WAF web ACLs. The company has hundreds of AWS accounts, all of which are included in a single organization in AWS Organizations. The<br>company has congured AWS Cong for the organization. During an audit, the company nds some externally facing ALBs that are not<br>associated with AWS WAF web ACLs.</td>
                    <td>A. Delegate AWS Firewall Manager to a security account.<br>B. Delegate Amazon GuardDuty to a security account.<br>C. Create an AWS Firewall Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.<br>D. Create an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.<br>E. Congure an AWS Cong managed rule to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.</td>
                    <td>一家公司的安全团队要求所有外部应用负载均衡器（ALB）和Amazon API Gateway API都必须关联AWS WAF web ACL。该公司拥有数百个AWS账户，所有账户都包含在AWS Organizations的单一组织中。公司已为该组织配置了AWS Config。在一次审计中，公司发现一些面向外部的ALB没有关联AWS WAF web ACL。</td>
                    <td>A. 将AWS Firewall Manager委托给安全账户 - 这是正确的。AWS Firewall Manager是一个集中管理防火墙规则的服务，可以跨多个AWS账户和资源统一管理WAF规则。将其委托给专门的安全账户是最佳实践，能够实现集中化的安全管理。<br><br>B. 将Amazon GuardDuty委托给安全账户 - 这个选项不正确。GuardDuty是威胁检测服务，主要用于识别恶意活动和异常行为，不能用于管理WAF web ACL的关联。<br><br>C. 创建AWS Firewall Manager策略来为任何新创建的ALB和API Gateway API附加AWS WAF web ACL - 这是正确的。Firewall Manager策略可以自动确保新创建的资源符合安全要求，自动附加WAF规则，这正是解决问题所需要的功能。<br><br>D. 创建Amazon GuardDuty策略来为任何新创建的ALB和API Gateway API附加AWS WAF web ACL - 这个选项不正确。GuardDuty不具备管理WAF web ACL附加的功能，它只是一个威胁检测服务。<br><br>E. 配置AWS Config管理规则来为任何新创建的ALB和API Gateway API附加AWS WAF web ACL - 这个选项不正确。AWS Config主要用于配置合规性检查和监控，它可以检测资源是否符合规则，但不能自动修复或附加WAF规则。</td>
                    <td>AC</td>
                </tr>
                <tr>
                    <td>9</td>
                    <td>A company uses AWS Key Management Service (AWS KMS) keys and manual key rotation to meet regulatory compliance requirements. The<br>security team wants to be notied when any keys have not been rotated after 90 days.</td>
                    <td>A. Congure AWS KMS to publish to an Amazon Simple Notication Service (Amazon SNS) topic when keys are more than 90 days old.<br>B. Congure an Amazon EventBridge event to launch an AWS Lambda function to call the AWS Trusted Advisor API and publish to an<br>Amazon Simple Notication Service (Amazon SNS) topic.<br>C. Develop an AWS Cong custom rule that publishes to an Amazon Simple Notication Service (Amazon SNS) topic when keys are more<br>than 90 days old.<br>D. Congure AWS Security Hub to publish to an Amazon Simple Notication Service (Amazon SNS) topic when keys are more than 90 days<br>old.</td>
                    <td>一家公司使用AWS密钥管理服务(AWS KMS)密钥和手动密钥轮换来满足监管合规要求。安全团队希望在任何密钥超过90天未轮换时收到通知。</td>
                    <td>A. 配置AWS KMS发布到Amazon简单通知服务(Amazon SNS)主题，当密钥超过90天时。<br>这个选项不正确，因为AWS KMS本身没有内置功能来监控密钥的轮换时间并自动发送通知。KMS不会主动跟踪密钥的使用时间或轮换周期，也不能直接与SNS集成来发送基于时间的警报。<br><br>B. 配置Amazon EventBridge事件来启动AWS Lambda函数，调用AWS Trusted Advisor API并发布到Amazon简单通知服务(Amazon SNS)主题。<br>这个选项不是最佳解决方案。虽然技术上可行，但Trusted Advisor主要用于成本优化、性能、安全性和容错性的最佳实践建议，而不是专门用于监控KMS密钥轮换的合规性检查。<br><br>C. 开发一个AWS Config自定义规则，当密钥超过90天时发布到Amazon简单通知服务(Amazon SNS)主题。<br>这是正确的选项。AWS Config可以创建自定义规则来评估AWS资源的配置合规性。可以编写自定义规则来检查KMS密钥的创建时间或最后轮换时间，当发现密钥超过90天未轮换时，触发SNS通知。<br><br>D. 配置AWS Security Hub发布到Amazon简单通知服务(Amazon SNS)主题，当密钥超过90天时。<br>这个选项不正确。虽然Security Hub是一个安全管理服务，但它主要聚合来自其他安全服务的发现，而不是专门监控KMS密钥轮换周期的工具。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>10</td>
                    <td>A security review has identied that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using<br>an unauthenticated request. The security team does not allow unauthenticated requests to S3 buckets for this project.<br>How can this issue be corrected in the MOST secure manner?</td>
                    <td>A. Add the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to<br>download the database population script.<br>B. Modify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the<br>token and download the database population script.<br>C. Remove unauthenticated access from the S3 bucket with a bucket policy. Modify the service role for the CodeBuild project to include<br>Amazon S3 access. Use the AWS CLI to download the database population script.<br>D. Remove unauthenticated access from the S3 bucket with a bucket policy. Use the AWS CLI to download the database population script<br>using an IAM access key and a secret access key.</td>
                    <td>安全审查发现一个AWS CodeBuild项目正在使用未经身份验证的请求从Amazon S3存储桶下载数据库填充脚本。安全团队不允许此项目对S3存储桶进行未经身份验证的请求。<br>如何以最安全的方式纠正这个问题？</td>
                    <td>A. 将存储桶名称添加到CodeBuild项目设置的AllowedBuckets部分。更新构建规范以使用AWS CLI下载数据库填充脚本。<br>这个选项不正确，因为CodeBuild项目设置中没有&quot;AllowedBuckets&quot;这样的配置项。这是一个虚构的配置选项，实际上不存在于AWS CodeBuild中。<br><br>B. 修改S3存储桶设置以启用HTTPS基本身份验证并指定令牌。更新构建规范以使用cURL传递令牌并下载数据库填充脚本。<br>这个选项不正确，因为S3不支持HTTPS基本身份验证机制。S3使用AWS的签名验证机制，而不是传统的HTTP基本身份验证。这种方法不符合AWS的安全最佳实践。<br><br>C. 使用存储桶策略移除S3存储桶的未经身份验证访问。修改CodeBuild项目的服务角色以包含Amazon S3访问权限。使用AWS CLI下载数据库填充脚本。<br>这是正确的选项。它遵循了AWS安全最佳实践：移除公共访问，使用IAM角色进行身份验证，通过服务角色授予必要的权限。这种方法最安全且符合AWS的推荐做法。<br><br>D. 使用存储桶策略移除S3存储桶的未经身份验证访问。使用IAM访问密钥和秘密访问密钥通过AWS CLI下载数据库填充脚本。<br>这个选项虽然能解决问题，但不是最安全的方式。硬编码访问密钥和秘密密钥存在安全风险，容易泄露。使用IAM角色比使用静态凭证更安全。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>11</td>
                    <td>An ecommerce company has chosen AWS to host its new platform. The company&#x27;s DevOps team has started building an AWS Control Tower<br>landing zone. The DevOps team has set the identity store within AWS IAM Identity Center (AWS Single Sign-On) to external identity provider<br>(IdP) and has congured SAML 2.0.<br>The DevOps team wants a robust permission model that applies the principle of least privilege. The model must allow the team to build and<br>manage only the team&#x27;s own resources.</td>
                    <td>A. Create IAM policies that include the required permissions. Include the aws:PrincipalTag condition key.<br>B. Create permission sets. Attach an inline policy that includes the required permissions and uses the aws:PrincipalTag condition key to<br>scope the permissions.<br>C. Create a group in the IdP. Place users in the group. Assign the group to accounts and the permission sets in IAM Identity Center.<br>D. Create a group in the IdP. Place users in the group. Assign the group to OUs and IAM policies.<br>E. Enable attributes for access control in IAM Identity Center. Apply tags to users. Map the tags as key-value pairs.<br>F. Enable attributes for access control in IAM Identity Center. Map attributes from the IdP as key-value pairs.</td>
                    <td>一家电商公司选择了AWS来托管其新平台。该公司的DevOps团队已经开始构建AWS Control Tower着陆区。DevOps团队已将AWS IAM Identity Center（AWS Single Sign-On）中的身份存储设置为外部身份提供商（IdP），并配置了SAML 0。<br><br>DevOps团队希望建立一个强大的权限模型，应用最小权限原则。该模型必须允许团队仅构建和管理团队自己的资源。</td>
                    <td>A. 创建包含所需权限的IAM策略。包含aws:PrincipalTag条件键。<br>这个选项是正确的。aws:PrincipalTag条件键可以基于用户的标签来限制权限，实现细粒度的访问控制，确保用户只能访问与其标签匹配的资源，符合最小权限原则。<br><br>B. 创建权限集。附加包含所需权限的内联策略，并使用aws:PrincipalTag条件键来限定权限范围。<br>这个选项是正确的。在IAM Identity Center中，权限集是管理权限的标准方式。使用内联策略配合aws:PrincipalTag条件键可以实现基于用户属性的精确权限控制。<br><br>C. 在IdP中创建组。将用户放入组中。在IAM Identity Center中将组分配给账户和权限集。<br>这个选项是正确的。这是IAM Identity Center的标准用法，通过IdP中的组来管理用户，然后将组映射到AWS账户和权限集，实现统一的身份和访问管理。<br><br>D. 在IdP中创建组。将用户放入组中。将组分配给OU和IAM策略。<br>这个选项不完全正确。虽然创建组是好的做法，但直接将组分配给OU和IAM策略不是IAM Identity Center的标准做法，应该通过权限集来管理。<br><br>E. 在IAM Identity Center中启用属性访问控制。对用户应用标签。将标签映射为键值对。<br>这个选项部分正确，但不完整。仅启用属性访问控制和应用标签还不足以实现完整的权限模型。<br><br>F. 在IAM Identity Center中启用属性访问控制。将IdP中的属性映射为键值对。<br>这个选项部分正确，但同样不完整。属性映射是有用的，但需要与其他组件配合才能形成完整的权限模型。</td>
                    <td>ABC</td>
                </tr>
                <tr>
                    <td>12</td>
                    <td>An ecommerce company is receiving reports that its order history page is experiencing delays in reecting the processing status of orders.<br>The order processing system consists of an AWS Lambda function that uses reserved concurrency. The Lambda function processes order<br>messages from an Amazon Simple Queue Service (Amazon SQS) queue and inserts processed orders into an Amazon DynamoDB table. The<br>DynamoDB table has auto scaling enabled for read and write capacity.</td>
                    <td>A. Check the ApproximateAgeOfOldestMessage metric for the SQS queue. Increase the Lambda function concurrency limit.<br>B. Check the ApproximateAgeOfOldestMessage metnc for the SQS queue Congure a redrive policy on the SQS queue.<br>C. Check the NumberOfMessagesSent metric for the SQS queue. Increase the SQS queue visibility timeout.<br>D. Check the WriteThrottleEvents metric for the DynamoDB table. Increase the maximum write capacity units (WCUs) for the table&#x27;s scaling<br>policy.<br>E. Check the Throttles metric for the Lambda function. Increase the Lambda function timeout.</td>
                    <td>一家电商公司收到报告称其订单历史页面在反映订单处理状态时出现延迟。订单处理系统由一个使用预留并发的AWS Lambda函数组成。该Lambda函数处理来自Amazon Simple Queue Service (Amazon SQS)队列的订单消息，并将处理后的订单插入到Amazon DynamoDB表中。DynamoDB表已启用读写容量的自动扩展。</td>
                    <td>A. 检查SQS队列的ApproximateAgeOfOldestMessage指标。增加Lambda函数的并发限制。<br>这个选项是正确的。ApproximateAgeOfOldestMessage指标显示队列中最旧消息的年龄，如果这个值很高，说明消息在队列中等待时间过长，可能是因为Lambda函数的并发限制导致处理能力不足。增加Lambda函数的并发限制可以提高处理速度。<br><br>B. 检查SQS队列的ApproximateAgeOfOldestMessage指标。在SQS队列上配置重驱动策略。<br>检查ApproximateAgeOfOldestMessage是正确的，但配置重驱动策略主要用于处理失败的消息，而不是解决处理延迟问题。重驱动策略不会提高处理速度。<br><br>C. 检查SQS队列的NumberOfMessagesSent指标。增加SQS队列的可见性超时。<br>NumberOfMessagesSent指标显示发送到队列的消息数量，但不能直接反映处理延迟问题。增加可见性超时只是延长消息被其他消费者处理的时间，不会解决根本的处理能力问题。<br><br>D. 检查DynamoDB表的WriteThrottleEvents指标。增加表扩展策略的最大写容量单位(WCUs)。<br>这个选项是正确的。WriteThrottleEvents指标显示写入请求被限制的次数。如果DynamoDB表的写入容量不足，会导致Lambda函数写入数据时被限制，从而造成整体处理延迟。增加最大WCUs可以提高写入性能。<br><br>E. 检查Lambda函数的Throttles指标。增加Lambda函数超时时间。<br>虽然检查Throttles指标是有意义的，但增加超时时间不会解决限制问题。如果Lambda函数被限制，需要的是增加并发限制而不是超时时间。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>13</td>
                    <td>A company has a single AWS account that runs hundreds of Amazon EC2 instances in a single AWS Region. New EC2 instances are launched<br>and terminated each hour in the account. The account also includes existing EC2 instances that have been running for longer than a week.<br>The company&#x27;s security policy requires all running EC2 instances to use an EC2 instance prole. If an EC2 instance does not have an instance<br>prole attached, the EC2 instance must use a default instance prole that has no IAM permissions assigned.<br>A DevOps engineer reviews the account and discovers EC2 instances that are running without an instance prole. During the review, the<br>DevOps engineer also observes that new EC2 instances are being launched without an instance prole.</td>
                    <td>A. Congure an Amazon EventBridge rule that reacts to EC2 RunInstances API calls. Congure the rule to invoke an AWS Lambda function<br>to attach the default instance prole to the EC2 instances.<br>B. Congure the ec2-instance-prole-attached AWS Cong managed rule with a trigger type of conguration changes. Congure an<br>automatic remediation action that invokes an AWS Systems Manager Automation runbook to attach the default instance prole to the EC2<br>instances.<br>C. Congure an Amazon EventBridge rule that reacts to EC2 StartInstances API calls. Congure the rule to invoke an AWS Systems<br>Manager Automation runbook to attach the default instance prole to the EC2 instances<br>D. Congure the iam-role-managed-policy-check AWS Cong managed rule with a trigger type of conguration changes. Congure an<br>automatic remediation action that invokes an AWS Lambda function to attach the default instance prole to the EC2 instances.</td>
                    <td>一家公司在单个AWS区域的单个AWS账户中运行数百个Amazon EC2实例。该账户中每小时都会启动和终止新的EC2实例。该账户还包括已运行超过一周的现有EC2实例。<br>公司的安全策略要求所有正在运行的EC2实例都使用EC2实例配置文件。如果EC2实例没有附加实例配置文件，该EC2实例必须使用没有分配IAM权限的默认实例配置文件。<br>DevOps工程师审查该账户时发现有EC2实例在没有实例配置文件的情况下运行。在审查过程中，DevOps工程师还观察到新的EC2实例在启动时没有实例配置文件。</td>
                    <td>A. 配置Amazon EventBridge规则响应EC2 RunInstances API调用，配置规则调用AWS Lambda函数为EC2实例附加默认实例配置文件。这个选项有一定可行性，但EventBridge响应RunInstances API调用可能存在时机问题，且Lambda函数需要额外开发和维护。<br><br>B. 配置ec2-instance-profile-attached AWS Config托管规则，触发类型为配置更改。配置自动修复操作，调用AWS Systems Manager自动化运行手册为EC2实例附加默认实例配置文件。这个选项使用了专门针对EC2实例配置文件的Config规则，能够检测现有和新创建的实例是否缺少配置文件，并通过自动修复功能及时处理。<br><br>C. 配置Amazon EventBridge规则响应EC2 StartInstances API调用，配置规则调用AWS Systems Manager自动化运行手册为EC2实例附加默认实例配置文件。这个选项的问题是StartInstances API主要用于启动已停止的实例，而不是新创建的实例，无法覆盖所有场景。<br><br>D. 配置iam-role-managed-policy-check AWS Config托管规则，触发类型为配置更改。配置自动修复操作，调用AWS Lambda函数为EC2实例附加默认实例配置文件。这个Config规则主要检查IAM角色的托管策略，而不是专门检查EC2实例配置文件的附加情况，不够精准。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>14</td>
                    <td>A DevOps engineer is building a continuous deployment pipeline for a serverless application that uses AWS Lambda functions. The company<br>wants to reduce the customer impact of an unsuccessful deployment. The company also wants to monitor for issues.</td>
                    <td>A. Use an AWS Serverless Application Model (AWS SAM) template to dene the serverless application. Use AWS CodeDeploy to deploy the<br>Lambda functions with the Canary10Percent15Minutes Deployment Preference Type. Use Amazon CloudWatch alarms to monitor the<br>health of the functions.<br>B. Use AWS CloudFormation to publish a new stack update, and include Amazon CloudWatch alarms on all resources. Set up an AWS<br>CodePipeline approval action for a developer to verify and approve the AWS CloudFormation change set.<br>C. Use AWS CloudFormation to publish a new version on every stack update, and include Amazon CloudWatch alarms on all resources. Use<br>the RoutingCong property of the AWS::Lambda::Alias resource to update the trac routing during the stack update.<br>D. Use AWS CodeBuild to add sample event payloads for testing to the Lambda functions. Publish a new version of the functions, and<br>include Amazon CloudWatch alarms. Update the production alias to point to the new version. Congure rollbacks to occur when an alarm<br>is in the ALARM state.</td>
                    <td>一名DevOps工程师正在为使用AWS Lambda函数的无服务器应用程序构建持续部署管道。公司希望减少部署失败对客户的影响。公司还希望监控问题。</td>
                    <td>选项A：使用AWS无服务器应用程序模型(AWS SAM)模板来定义无服务器应用程序。使用AWS CodeDeploy部署Lambda函数，采用Canary10Percent15Minutes部署偏好类型。使用Amazon CloudWatch告警监控函数健康状况。这个选项提供了渐进式部署策略，通过金丝雀部署只将10%的流量路由到新版本，持续15分钟，可以有效减少部署失败的影响范围，同时CloudWatch告警提供了监控能力。<br><br>选项B：使用AWS CloudFormation发布新的堆栈更新，并在所有资源上包含Amazon CloudWatch告警。设置AWS CodePipeline批准操作，让开发人员验证和批准CloudFormation变更集。虽然提供了人工审批机制，但这种方式是一次性全量部署，无法减少部署失败对客户的影响，不符合渐进式部署的需求。<br><br>选项C：使用AWS CloudFormation在每次堆栈更新时发布新版本，并在所有资源上包含Amazon CloudWatch告警。使用AWS::Lambda::Alias资源的RoutingConfig属性在堆栈更新期间更新流量路由。虽然可以实现流量分配，但CloudFormation本身不提供自动化的渐进式部署和回滚机制，需要手动管理。<br><br>选项D：使用AWS CodeBuild为Lambda函数添加测试事件负载。发布函数的新版本，并包含Amazon CloudWatch告警。更新生产别名指向新版本。配置在告警处于ALARM状态时进行回滚。这种方式仍然是一次性切换到新版本，无法实现渐进式部署来减少客户影响。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>15</td>
                    <td>To run an application, a DevOps engineer launches an Amazon EC2 instance with public IP addresses in a public subnet. A user data script<br>obtains the application artifacts and installs them on the instances upon launch. A change to the security classication of the application now<br>requires the instances to run with no access to the internet. While the instances launch successfully and show as healthy, the application<br>does not seem to be installed.</td>
                    <td>A. Launch the instances in a public subnet with Elastic IP addresses attached. Once the application is installed and running, run a script to<br>disassociate the Elastic IP addresses afterwards.<br>B. Set up a NAT gateway. Deploy the EC2 instances to a private subnet. Update the private subnet&#x27;s route table to use the NAT gateway as<br>the default route.<br>C. Publish the application artifacts to an Amazon S3 bucket and create a VPC endpoint for S3. Assign an IAM instance prole to the EC2<br>instances so they can read the application artifacts from the S3 bucket.<br>D. Create a security group for the application instances and allow only outbound trac to the artifact repository. Remove the security<br>group rule once the install is complete.</td>
                    <td>为了运行一个应用程序，DevOps工程师在公有子网中启动了一个带有公网IP地址的Amazon EC2实例。用户数据脚本在启动时获取应用程序构件并将其安装在实例上。由于应用程序安全分类的变更，现在要求实例在没有互联网访问权限的情况下运行。虽然实例启动成功并显示为健康状态，但应用程序似乎没有被安装。</td>
                    <td>A. 在公有子网中启动实例并附加弹性IP地址，应用程序安装运行后，运行脚本解除弹性IP地址关联。这个方案虽然可以暂时解决安装问题，但存在安全风险，因为在安装过程中实例仍然可以访问互联网，不符合新的安全分类要求。而且这种方法增加了操作复杂性，需要额外的脚本来管理IP地址的关联和解除关联。<br><br>B. 设置NAT网关，将EC2实例部署到私有子网，更新私有子网的路由表使用NAT网关作为默认路由。这个方案可以让实例访问互联网下载构件，但仍然提供了互联网访问能力，不完全符合&quot;没有互联网访问权限&quot;的安全要求。NAT网关虽然提供了一定的安全性，但实例仍然可以主动访问互联网资源。<br><br>C. 将应用程序构件发布到Amazon S3存储桶并创建S3的VPC端点，为EC2实例分配IAM实例配置文件以便从S3存储桶读取应用程序构件。这个方案完全符合安全要求，实例不需要互联网访问就能获取构件，通过VPC端点实现私有连接，既安全又高效。<br><br>D. 为应用程序实例创建安全组，仅允许到构件仓库的出站流量，安装完成后删除安全组规则。这个方案仍然需要互联网连接来访问外部构件仓库，不符合完全隔离互联网的安全要求，而且管理复杂度较高。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>16</td>
                    <td>A development team is using AWS CodeCommit to version control application code and AWS CodePipeline to orchestrate software<br>deployments. The team has decided to use a remote main branch as the trigger for the pipeline to integrate code changes. A developer has<br>pushed code changes to the CodeCommit repository, but noticed that the pipeline had no reaction, even after 10 minutes.</td>
                    <td>A. Check that an Amazon EventBridge rule has been created for the main branch to trigger the pipeline.<br>B. Check that the CodePipeline service role has permission to access the CodeCommit repository.<br>C. Check that the developer’s IAM role has permission to push to the CodeCommit repository.<br>D. Check to see if the pipeline failed to start because of CodeCommit errors in Amazon CloudWatch Logs.</td>
                    <td>一个开发团队正在使用AWS CodeCommit进行应用程序代码版本控制，使用AWS CodePipeline来编排软件部署。团队决定使用远程主分支作为触发器来启动管道以集成代码更改。一名开发人员已经将代码更改推送到CodeCommit存储库，但注意到管道没有任何反应，即使在10分钟后也是如此。</td>
                    <td>A. 检查是否为主分支创建了Amazon EventBridge规则来触发管道。<br>这个选项是正确的。当CodePipeline需要监听CodeCommit存储库的特定分支变化时，需要通过Amazon EventBridge（以前称为CloudWatch Events）来设置事件规则。如果没有正确配置EventBridge规则来监听主分支的推送事件，管道就不会被触发。这是管道无反应的最可能原因。<br><br>B. 检查CodePipeline服务角色是否有权限访问CodeCommit存储库。<br>虽然这个权限很重要，但如果权限不足，通常会在管道尝试执行时产生错误信息，而不是完全没有反应。权限问题通常会导致管道启动但随后失败，而不是根本不启动。<br><br>C. 检查开发人员的IAM角色是否有权限推送到CodeCommit存储库。<br>这个选项不正确，因为题目已经明确说明开发人员已经成功将代码更改推送到了CodeCommit存储库，这说明开发人员具有推送权限。<br><br>D. 检查管道是否因为CodeCommit错误而启动失败，查看Amazon CloudWatch Logs。<br>这个选项也不太可能，因为如果管道尝试启动但失败了，通常会有相应的状态显示或错误信息。题目描述的是管道完全没有反应，说明管道根本没有被触发启动。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>17</td>
                    <td>A company&#x27;s developers use Amazon EC2 instances as remote workstations. The company is concerned that users can create or modify EC2<br>security groups to allow unrestricted inbound access.<br>A DevOps engineer needs to develop a solution to detect when users create unrestricted security group rules. The solution must detect<br>changes to security group rules in near real time, remove unrestricted rules, and send email notications to the security team. The DevOps<br>engineer has created an AWS Lambda function that checks for security group ID from input, removes rules that grant unrestricted access, and<br>sends notications through Amazon Simple Notication Service (Amazon SNS).</td>
                    <td>A. Congure the Lambda function to be invoked by the SNS topic. Create an AWS CloudTrail subscription for the SNS topic. Congure a<br>subscription lter for security group modication events.<br>B. Create an Amazon EventBridge scheduled rule to invoke the Lambda function. Dene a schedule pattern that runs the Lambda function<br>every hour.<br>C. Create an Amazon EventBridge event rule that has the default event bus as the source. Dene the rule’s event pattern to match EC2<br>security group creation and modication events. Congure the rule to invoke the Lambda function.<br>D. Create an Amazon EventBridge custom event bus that subscribes to events from all AWS services. Congure the Lambda function to be<br>invoked by the custom event bus.</td>
                    <td>一家公司的开发人员使用Amazon EC2实例作为远程工作站。公司担心用户可能创建或修改EC2安全组以允许不受限制的入站访问。<br>DevOps工程师需要开发一个解决方案来检测用户何时创建不受限制的安全组规则。该解决方案必须近实时检测安全组规则的更改，删除不受限制的规则，并向安全团队发送电子邮件通知。DevOps工程师已经创建了一个AWS Lambda函数，该函数检查输入中的安全组ID，删除授予不受限制访问的规则，并通过Amazon Simple Notification Service (Amazon SNS)发送通知。</td>
                    <td>A. 配置Lambda函数由SNS主题调用，为SNS主题创建AWS CloudTrail订阅，为安全组修改事件配置订阅过滤器。这个方案存在架构问题，CloudTrail不能直接订阅SNS主题，而是应该将日志发送到CloudWatch Logs或S3，然后通过其他方式触发事件。此外，这种方式无法实现近实时检测。<br><br>B. 创建Amazon EventBridge计划规则来调用Lambda函数，定义每小时运行Lambda函数的计划模式。这是一个定时执行的方案，无法满足&quot;近实时检测&quot;的要求，因为安全组的修改可能在任何时间发生，每小时检查一次会有很大的延迟。<br><br>C. 创建以默认事件总线为源的Amazon EventBridge事件规则，定义规则的事件模式以匹配EC2安全组创建和修改事件，配置规则调用Lambda函数。这个方案能够实时捕获EC2安全组的变更事件，当安全组被创建或修改时立即触发Lambda函数执行，完全满足近实时检测的要求。<br><br>D. 创建订阅所有AWS服务事件的Amazon EventBridge自定义事件总线，配置Lambda函数由自定义事件总线调用。虽然技术上可行，但创建自定义事件总线来处理所有AWS服务的事件是过度设计，会产生不必要的复杂性和成本，而且默认事件总线已经能够满足需求。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>18</td>
                    <td>A DevOps engineer is creating an AWS CloudFormation template to deploy a web service. The web service will run on Amazon EC2 instances<br>in a private subnet behind an Application Load Balancer (ALB). The DevOps engineer must ensure that the service can accept requests from<br>clients that have IPv6 addresses.</td>
                    <td>A. Add an IPv6 CIDR block to the VPC and the private subnet for the EC2 instances. Create route table entries for the IPv6 network, use<br>EC2 instance types that support IPv6, and assign IPv6 addresses to each EC2 instance.<br>B. <br>C. Replace the ALB with a Network Load Balancer (NLB). Add an IPv6 CIDR block to the VPC and subnets for the NLB, and assign the NLB<br>an IPv6 Elastic IP address.<br>D. Add an IPv6 CIDR block to the VPC and subnets for the AL</td>
                    <td>一名DevOps工程师正在创建AWS CloudFormation模板来部署Web服务。该Web服务将在位于Application Load Balancer (ALB)后面私有子网中的Amazon EC2实例上运行。DevOps工程师必须确保该服务能够接受来自具有IPv6地址的客户端的请求。</td>
                    <td>选项A：为VPC和EC2实例的私有子网添加IPv6 CIDR块，为IPv6网络创建路由表条目，使用支持IPv6的EC2实例类型，并为每个EC2实例分配IPv6地址。这个方案过于复杂，实际上不需要为私有子网中的EC2实例配置IPv6，因为ALB可以处理IPv6到IPv4的转换。<br><br>选项B：选项内容缺失，无法进行分析。但根据参考答案显示B是正确答案，推测应该是为VPC和ALB所在的公有子网添加IPv6 CIDR块，并为ALB启用IPv6支持的方案。<br><br>选项C：将ALB替换为Network Load Balancer (NLB)，为VPC和NLB的子网添加IPv6 CIDR块，并为NLB分配IPv6弹性IP地址。虽然技术上可行，但没有必要替换ALB，因为ALB本身就支持IPv6。<br><br>选项D：选项内容不完整，无法进行完整分析。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>19</td>
                    <td>A company uses AWS Organizations and AWS Control Tower to manage all the company&#x27;s AWS accounts. The company uses the Enterprise<br>Support plan.<br>A DevOps engineer is using Account Factory for Terraform (AFT) to provision new accounts. When new accounts are provisioned, the DevOps<br>engineer notices that the support plan for the new accounts is set to the Basic Support plan. The DevOps engineer needs to implement a<br>solution to provision the new accounts with the Enterprise Support plan.</td>
                    <td>A. Use an AWS Cong conformance pack to deploy the account-part-of-organizations AWS Cong rule and to automatically remediate any<br>noncompliant accounts.<br>B. Create an AWS Lambda function to create a ticket for AWS Support to add the account to the Enterprise Support plan. Grant the Lambda<br>function the support:ResolveCase permission.<br>C. Add an additional value to the control_tower_parameters input to set the AWSEnterpriseSupport parameter as the organization&#x27;s<br>management account number.<br>D. Set the aft_feature_enterprise_support feature ag to True in the AFT deployment input conguration. Redeploy AFT and apply the<br>changes.</td>
                    <td>一家公司使用AWS Organizations和AWS Control Tower来管理公司所有的AWS账户。该公司使用企业支持计划。<br>一名DevOps工程师正在使用Account Factory for Terraform (AFT)来配置新账户。当配置新账户时，DevOps工程师注意到新账户的支持计划被设置为基础支持计划。DevOps工程师需要实施一个解决方案，以便为新账户配置企业支持计划。</td>
                    <td>A. 使用AWS Config一致性包来部署account-part-of-organizations AWS Config规则并自动修复任何不合规的账户。这个选项不正确，因为AWS Config规则主要用于监控和合规性检查，而不是用于设置支持计划。account-part-of-organizations规则只是检查账户是否属于组织，与支持计划设置无关。<br><br>B. 创建AWS Lambda函数为AWS Support创建工单，将账户添加到企业支持计划中，并授予Lambda函数support:ResolveCase权限。这个选项不正确，因为support:ResolveCase权限是用于解决现有案例的，而不是用于更改支持计划。此外，这种方法过于复杂且不是AFT的标准做法。<br><br>C. 在control_tower_parameters输入中添加额外的值，将AWSEnterpriseSupport参数设置为组织管理账户号码。这个选项不正确，因为control_tower_parameters主要用于Control Tower的配置，而不是AFT特定的企业支持设置。<br><br>D. 在AFT部署输入配置中将aft_feature_enterprise_support功能标志设置为True，重新部署AFT并应用更改。这个选项正确，因为AFT提供了专门的功能标志来启用企业支持计划，这是AFT框架中处理此需求的标准方法。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>20</td>
                    <td>A company&#x27;s DevOps engineer uses AWS Systems Manager to perform maintenance tasks during maintenance windows. The company has a<br>few Amazon EC2 instances that require a restart after notications from AWS Health. The DevOps engineer needs to implement an automated<br>solution to remediate these notications. The DevOps engineer creates an Amazon EventBridge rule.<br>How should the DevOps engineer congure the EventBridge rule to meet these requirements?</td>
                    <td>A. Congure an event source of AWS Health, a service of EC2. and an event type that indicates instance maintenance. Target a Systems<br>Manager document to restart the EC2 instance.<br>B. Congure an event source of Systems Manager and an event type that indicates a maintenance window. Target a Systems Manager<br>document to restart the EC2 instance.<br>C. Congure an event source of AWS Health, a service of EC2, and an event type that indicates instance maintenance. Target a newly<br>created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window.<br>D. Congure an event source of EC2 and an event type that indicates instance maintenance. Target a newly created AWS Lambda function<br>that registers an automation task to restart the EC2 instance during a maintenance window.</td>
                    <td>一家公司的DevOps工程师使用AWS Systems Manager在维护窗口期间执行维护任务。该公司有一些Amazon EC2实例在收到AWS Health通知后需要重启。DevOps工程师需要实现一个自动化解决方案来修复这些通知。DevOps工程师创建了一个Amazon EventBridge规则。<br>DevOps工程师应该如何配置EventBridge规则来满足这些要求？</td>
                    <td>选项A：配置AWS Health作为事件源，EC2作为服务，以及表示实例维护的事件类型。目标为Systems Manager文档来重启EC2实例。这个选项正确地识别了事件源（AWS Health），并且直接使用Systems Manager文档来执行重启操作，符合题目要求的自动化修复AWS Health通知的需求。<br><br>选项B：配置Systems Manager作为事件源，以及表示维护窗口的事件类型。目标为Systems Manager文档来重启EC2实例。这个选项错误地将Systems Manager作为事件源，但题目明确提到需要响应AWS Health的通知，而不是Systems Manager的维护窗口事件。<br><br>选项C：配置AWS Health作为事件源，EC2作为服务，以及表示实例维护的事件类型。目标为新创建的AWS Lambda函数，该函数注册自动化任务在维护窗口期间重启EC2实例。虽然事件源配置正确，但增加了不必要的复杂性，通过Lambda函数来间接执行重启操作，而直接使用Systems Manager文档更简单有效。<br><br>选项D：配置EC2作为事件源，以及表示实例维护的事件类型。目标为新创建的AWS Lambda函数，该函数注册自动化任务在维护窗口期间重启EC2实例。这个选项错误地将EC2作为事件源，但题目明确说明需要响应AWS Health的通知，不是EC2本身的事件。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>21</td>
                    <td>A company has containerized all of its in-house quality control applications. The company is running Jenkins on Amazon EC2 instances, which<br>require patching and upgrading. The compliance ocer has requested a DevOps engineer begin encrypting build artifacts since they contain<br>company intellectual property.</td>
                    <td>A. Automate patching and upgrading using AWS Systems Manager on EC2 instances and encrypt Amazon EBS volumes by default.<br>B. Deploy Jenkins to an Amazon ECS cluster and copy build artifacts to an Amazon S3 bucket with default encryption enabled.<br>C. Leverage AWS CodePipeline with a build action and encrypt the artifacts using AWS Secrets Manager.<br>D. Use AWS CodeBuild with artifact encryption to replace the Jenkins instance running on EC2 instances.</td>
                    <td>一家公司已经将其所有内部质量控制应用程序容器化。该公司在Amazon EC2实例上运行Jenkins，这些实例需要打补丁和升级。合规官要求DevOps工程师开始加密构建工件，因为它们包含公司的知识产权。</td>
                    <td>A. 使用AWS Systems Manager在EC2实例上自动化打补丁和升级，并默认加密Amazon EBS卷。这个选项虽然解决了自动化补丁管理的问题，但仍然需要维护EC2实例，没有从根本上解决基础设施管理的复杂性。EBS卷加密只是加密存储，不是专门针对构建工件的加密。<br><br>B. 将Jenkins部署到Amazon ECS集群，并将构建工件复制到启用默认加密的Amazon S3存储桶。这个方案减少了一些基础设施管理负担，通过容器化Jenkins来简化部署，S3的默认加密可以保护构建工件。但仍需要管理ECS集群和Jenkins配置。<br><br>C. 利用AWS CodePipeline与构建操作，并使用AWS Secrets Manager加密工件。这个选项存在概念错误，AWS Secrets Manager主要用于管理敏感信息如密码、API密钥等，而不是用来加密构建工件的工具。<br><br>D. 使用带有工件加密功能的AWS CodeBuild来替换运行在EC2实例上的Jenkins。这是完全托管的解决方案，消除了基础设施管理需求，CodeBuild原生支持构建工件加密，完美满足合规要求，且与AWS生态系统深度集成。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>22</td>
                    <td>An IT team has built an AWS CloudFormation template so others in the company can quickly and reliably deploy and terminate an application.<br>The template creates an Amazon EC2 instance with a user data script to install the application and an Amazon S3 bucket that the application<br>uses to serve static webpages while it is running.<br>All resources should be removed when the CloudFormation stack is deleted. However, the team observes that CloudFormation reports an error<br>during stack deletion, and the S3 bucket created by the stack is not deleted.<br>How can the team resolve the error in the MOST ecient manner to ensure that all resources are deleted without errors?</td>
                    <td>A. Add a DelelionPolicy attribute to the S3 bucket resource, with the value Delete forcing the bucket to be removed when the stack is<br>deleted.<br>B. Add a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the<br>Lambda function to delete all objects from the bucket when RequestType is Delete.<br>C. Identify the resource that was not deleted. Manually empty the S3 bucket and then delete it.<br>D. Replace the EC2 and S3 bucket resources with a single AWS OpsWorks Stacks resource. Dene a custom recipe for the stack to create<br>and delete the EC2 instance and the S3 bucket.</td>
                    <td>一个IT团队构建了一个AWS CloudFormation模板，以便公司中的其他人可以快速可靠地部署和终止应用程序。该模板创建了一个Amazon EC2实例，其中包含用户数据脚本来安装应用程序，以及一个Amazon S3存储桶，应用程序在运行时使用该存储桶来提供静态网页。当CloudFormation堆栈被删除时，所有资源都应该被移除。然而，团队观察到CloudFormation在堆栈删除过程中报告错误，堆栈创建的S3存储桶没有被删除。团队如何以最高效的方式解决这个错误，以确保所有资源都能无错误地删除？</td>
                    <td>A. 向S3存储桶资源添加DeletionPolicy属性，值为Delete，强制在删除堆栈时移除存储桶。这个选项存在拼写错误（DelelionPolicy应为DeletionPolicy），且即使设置了DeletionPolicy为Delete，如果S3存储桶中有对象，仍然无法删除存储桶，因为AWS不允许删除非空的S3存储桶。<br><br>B. 添加一个自定义资源，包含AWS Lambda函数，使用DependsOn属性指定S3存储桶，以及IAM角色。编写Lambda函数在RequestType为Delete时删除存储桶中的所有对象。这是正确的解决方案，因为它解决了根本问题：S3存储桶无法删除是因为其中包含对象，Lambda函数可以在删除堆栈时先清空存储桶。<br><br>C. 识别未删除的资源，手动清空S3存储桶然后删除它。虽然这可以解决当前问题，但不是最高效的方式，因为每次删除堆栈都需要手动干预，不符合自动化的DevOps原则。<br><br>D. 用单个AWS OpsWorks Stacks资源替换EC2和S3存储桶资源，为堆栈定义自定义配方来创建和删除EC2实例和S3存储桶。这种方法过于复杂，引入了不必要的复杂性，且OpsWorks主要用于应用程序管理而非基础设施管理。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>23</td>
                    <td>A company has an AWS CodePipeline pipeline that is congured with an Amazon S3 bucket in the eu-west-1 Region. The pipeline deploys an<br>AWS Lambda application to the same Region. The pipeline consists of an AWS CodeBuild project build action and an AWS CloudFormation<br>deploy action.<br>The CodeBuild project uses the aws cloudformation package AWS CLI command to build an artifact that contains the Lambda function code’s<br>.zip le and the CloudFormation template. The CloudFormation deploy action references the CloudFormation template from the output<br>artifact of the CodeBuild project’s build action.<br>The company wants to also deploy the Lambda application to the us-east-1 Region by using the pipeline in eu-west-1. A DevOps engineer has<br>already updated the CodeBuild project to use the aws cloudformation package command to produce an additional output artifact for us-east-<br>1.</td>
                    <td>A. Modify the CloudFormation template to include a parameter for the Lambda function code’s zip le location. Create a new<br>CloudFormation deploy action for us-east-1 in the pipeline. Congure the new deploy action to pass in the us-east-1 artifact location as a<br>parameter override.<br>B. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Congure the new deploy action to use the CloudFormation<br>template from the us-east-1 output artifact.<br>C. Create an S3 bucket in us-east-1. Congure the S3 bucket policy to allow CodePipeline to have read and write access.<br>D. Create an S3 bucket in us-east-1. Congure S3 Cross-Region Replication (CRR) from the S3 bucket in eu-west-1 to the S3 bucket in us-<br>east-1.<br>E. Modify the pipeline to include the S3 bucket for us-east-1 as an artifact store. Create a new CloudFormation deploy action for us-east-1<br>in the pipeline. Congure the new deploy action to use the CloudFormation template from the us-east-1 output artifact.</td>
                    <td>一家公司有一个AWS CodePipeline管道，配置了位于eu-west-1区域的Amazon S3存储桶。该管道将AWS Lambda应用程序部署到同一区域。管道包含一个AWS CodeBuild项目构建操作和一个AWS CloudFormation部署操作。<br><br>CodeBuild项目使用aws cloudformation package AWS CLI命令构建一个包含Lambda函数代码zip文件和CloudFormation模板的构件。CloudFormation部署操作引用来自CodeBuild项目构建操作输出构件的CloudFormation模板。<br><br>公司希望使用eu-west-1的管道将Lambda应用程序也部署到us-east-1区域。DevOps工程师已经更新了CodeBuild项目，使用aws cloudformation package命令为us-east-1生成额外的输出构件。</td>
                    <td>A. 修改CloudFormation模板以包含Lambda函数代码zip文件位置的参数。在管道中为us-east-1创建新的CloudFormation部署操作。配置新的部署操作以将us-east-1构件位置作为参数覆盖传入。这个选项是正确的，因为它通过参数化模板来处理不同区域的构件位置，允许同一个模板在不同区域使用不同的构件位置。<br><br>B. 在管道中为us-east-1创建新的CloudFormation部署操作。配置新的部署操作使用来自us-east-1输出构件的CloudFormation模板。这个选项也是正确的，因为CodeBuild已经为us-east-1生成了专门的输出构件，包含了适合该区域的CloudFormation模板。<br><br>C. 在us-east-1创建S3存储桶。配置S3存储桶策略以允许CodePipeline具有读写访问权限。这个选项不完整，仅创建存储桶和配置权限不足以实现跨区域部署。<br><br>D. 在us-east-1创建S3存储桶。配置从eu-west-1的S3存储桶到us-east-1的S3存储桶的S3跨区域复制(CRR)。这个选项不是最佳实践，跨区域复制不是CodePipeline跨区域部署的推荐方法。<br><br>E. 修改管道以包含us-east-1的S3存储桶作为构件存储。在管道中为us-east-1创建新的CloudFormation部署操作。配置新的部署操作使用来自us-east-1输出构件的CloudFormation模板。虽然这个选项提到了构件存储，但在CodePipeline中，跨区域部署通常不需要显式配置额外的构件存储。</td>
                    <td>AB</td>
                </tr>
                <tr>
                    <td>24</td>
                    <td>A company runs an application on one Amazon EC2 instance. Application metadata is stored in Amazon S3 and must be retrieved if the<br>instance is restarted. The instance must restart or relaunch automatically if the instance becomes unresponsive.</td>
                    <td>A. Create an Amazon CloudWatch alarm for the StatusCheckFailed metric. Use the recover action to stop and start the instance. Use an S3<br>event notication to push the metadata to the instance when the instance is back up and running.<br>B. Congure AWS OpsWorks, and use the auto healing feature to stop and start the instance. Use a lifecycle event in OpsWorks to pull the<br>metadata from Amazon S3 and update it on the instance.<br>C. Use EC2 Auto Recovery to automatically stop and start the instance in case of a failure. Use an S3 event notication to push the<br>metadata to the instance when the instance is back up and running.<br>D. Use AWS CloudFormation to create an EC2 instance that includes the UserData property for the EC2 resource. Add a command in<br>UserData to retrieve the application metadata from Amazon S3.</td>
                    <td>一家公司在一个Amazon EC2实例上运行应用程序。应用程序元数据存储在Amazon S3中，如果实例重启，必须检索这些元数据。如果实例变得无响应，实例必须自动重启或重新启动。</td>
                    <td>A. 为StatusCheckFailed指标创建Amazon CloudWatch告警，使用恢复操作来停止和启动实例，当实例重新运行时使用S3事件通知将元数据推送到实例。这个方案的问题在于S3事件通知无法直接推送数据到EC2实例，S3事件通知只能触发Lambda、SQS等服务，不能直接与EC2实例交互。<br><br>B. 配置AWS OpsWorks，使用自动修复功能来停止和启动实例，在OpsWorks中使用生命周期事件从Amazon S3拉取元数据并在实例上更新。OpsWorks提供了完整的自动修复机制和生命周期管理，可以在实例重启时自动执行脚本从S3获取元数据，这是一个完整可行的解决方案。<br><br>C. 使用EC2自动恢复在故障情况下自动停止和启动实例，当实例重新运行时使用S3事件通知将元数据推送到实例。与选项A类似，S3事件通知无法直接推送数据到EC2实例，这个方案在技术实现上存在问题。<br><br>D. 使用AWS CloudFormation创建包含UserData属性的EC2实例，在UserData中添加命令从Amazon S3检索应用程序元数据。虽然UserData可以在实例启动时执行脚本获取S3数据，但CloudFormation本身不提供自动故障检测和实例重启功能，无法满足自动重启的需求。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>25</td>
                    <td>A company has multiple AWS accounts. The company uses AWS IAM Identity Center (AWS Single Sign-On) that is integrated with AWS Toolkit<br>for Microsoft Azure DevOps. The attributes for access control feature is enabled in IAM Identity Center.<br>The attribute mapping list contains two entries. The department key is mapped to ${path:enterprise.department}. The costCenter key is<br>mapped to ${path:enterprise.costCenter}.<br>All existing Amazon EC2 instances have a department tag that corresponds to three company departments (d1, d2, d3). A DevOps engineer<br>must create policies based on the matching attributes. The policies must minimize administrative effort and must grant each Azure AD user<br>access to only the EC2 instances that are tagged with the user’s respective department name.</td>
                    <td>A. <br>B. <br>C. <br>D.</td>
                    <td>一家公司有多个AWS账户。该公司使用与Microsoft Azure DevOps的AWS工具包集成的AWS IAM Identity Center（AWS Single Sign-On）。IAM Identity Center中启用了访问控制属性功能。<br>属性映射列表包含两个条目。department键映射到${path:enterprise.department}。costCenter键映射到${path:enterprise.costCenter}。<br>所有现有的Amazon EC2实例都有一个department标签，对应于三个公司部门（d1、d2、d3）。DevOps工程师必须基于匹配属性创建策略。这些策略必须最小化管理工作量，并且必须授予每个Azure AD用户仅访问标记有用户各自部门名称的EC2实例。</td>
                    <td>由于题目中没有提供具体的选项A、B、C、D的内容，我无法对每个选项进行详细分析。但是基于题目描述，我可以分析解决方案的要求：<br><br>题目要求创建基于属性的访问控制策略，需要：<br>- 利用IAM Identity Center的属性映射功能<br>- 使用department属性来控制EC2实例访问<br>- 确保用户只能访问与其部门标签匹配的EC2实例<br>- 最小化管理开销<br><br>正确的解决方案应该包括：<br>- 创建基于条件的IAM策略，使用StringEquals条件操作符<br>- 在策略中使用${saml:department}或类似的属性引用<br>- 策略条件应该匹配EC2实例的department标签与用户的department属性<br>- 使用动态策略而不是为每个部门创建单独的静态策略</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>26</td>
                    <td>A company hosts a security auditing application in an AWS account. The auditing application uses an IAM role to access other AWS accounts.<br>All the accounts are in the same organization in AWS Organizations.<br>A recent security audit revealed that users in the audited AWS accounts could modify or delete the auditing application&#x27;s IAM role. The<br>company needs to prevent any modication to the auditing application&#x27;s IAM role by any entity other than a trusted administrator IAM role.</td>
                    <td>A. Create an SCP that includes a Deny statement for changes to the auditing application&#x27;s IAM role. Include a condition that allows the<br>trusted administrator IAM role to make changes. Attach the SCP to the root of the organization.<br>B. Create an SCP that includes an Allow statement for changes to the auditing application&#x27;s IAM role by the trusted administrator IAM role.<br>Include a Deny statement for changes by all other IAM principals. Attach the SCP to the IAM service in each AWS account where the<br>auditing application has an IAM role.<br>C. Create an IAM permissions boundary that includes a Deny statement for changes to the auditing application&#x27;s IAM role. Include a<br>condition that allows the trusted administrator IAM role to make changes. Attach the permissions boundary to the audited AWS accounts.<br>D. Create an IAM permissions boundary that includes a Deny statement for changes to the auditing application’s IAM role. Include a<br>condition that allows the trusted administrator IAM role to make changes. Attach the permissions boundary to the auditing application&#x27;s<br>IAM role in the AWS accounts.</td>
                    <td>一家公司在AWS账户中托管了一个安全审计应用程序。该审计应用程序使用IAM角色来访问其他AWS账户。所有账户都在AWS Organizations的同一个组织中。最近的安全审计发现，被审计AWS账户中的用户可以修改或删除审计应用程序的IAM角色。公司需要防止除受信任的管理员IAM角色之外的任何实体对审计应用程序的IAM角色进行任何修改。</td>
                    <td>选项A：创建一个包含拒绝更改审计应用程序IAM角色声明的SCP，包含允许受信任管理员IAM角色进行更改的条件，将SCP附加到组织根部。这个方案在技术上可行，SCP可以在组织级别控制权限，但将其附加到组织根部会影响所有账户，可能过于宽泛，且SCP主要用于限制整个账户的权限边界。<br><br>选项B：创建一个包含允许受信任管理员IAM角色更改审计应用程序IAM角色的Allow声明和拒绝所有其他IAM主体更改的Deny声明的SCP，将SCP附加到每个AWS账户中审计应用程序具有IAM角色的IAM服务。这个选项存在技术错误，SCP不能直接附加到IAM服务，只能附加到组织单位或账户。<br><br>选项C：创建一个包含拒绝更改审计应用程序IAM角色声明的IAM权限边界，包含允许受信任管理员IAM角色进行更改的条件，将权限边界附加到被审计的AWS账户。权限边界可以有效限制账户中用户和角色的最大权限，这是一个合理的方法来保护特定资源。<br><br>选项D：创建一个包含拒绝更改审计应用程序IAM角色声明的IAM权限边界，包含允许受信任管理员IAM角色进行更改的条件，将权限边界附加到AWS账户中审计应用程序的IAM角色。将权限边界附加到IAM角色本身在逻辑上不正确，权限边界应该附加到可能修改该角色的用户或角色上。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>27</td>
                    <td>A company has an on-premises application that is written in Go. A DevOps engineer must move the application to AWS. The company&#x27;s<br>development team wants to enable blue/green deployments and perform A/B testing.</td>
                    <td>A. Deploy the application on an Amazon EC2 instance, and create an AMI of the instance. Use the AMI to create an automatic scaling<br>launch conguration that is used in an Auto Scaling group. Use Elastic Load Balancing to distribute trac. When changes are made to the<br>application, a new AMI will be created, which will initiate an EC2 instance refresh.<br>B. Use Amazon Lightsail to deploy the application. Store the application in a zipped format in an Amazon S3 bucket. Use this zipped<br>version to deploy new versions of the application to Lightsail. Use Lightsail deployment options to manage the deployment.<br>C. Use AWS CodeArtifact to store the application code. Use AWS CodeDeploy to deploy the application to a eet of Amazon EC2 instances.<br>Use Elastic Load Balancing to distribute the trac to the EC2 instances. When making changes to the application, upload a new version to<br>CodeArtifact and create a new CodeDeploy deployment.<br>D. Use AWS Elastic Beanstalk to host the application. Store a zipped version of the application in Amazon S3. Use that location to deploy<br>new versions of the application. Use Elastic Beanstalk to manage the deployment options.</td>
                    <td>一家公司有一个用Go语言编写的本地应用程序。DevOps工程师必须将该应用程序迁移到AWS。公司的开发团队希望启用蓝/绿部署并执行A/B测试。</td>
                    <td>选项A：在Amazon EC2实例上部署应用程序，创建实例的AMI。使用AMI创建自动扩展启动配置，用于Auto Scaling组。使用Elastic Load Balancing分发流量。当应用程序发生更改时，将创建新的AMI，这将启动EC2实例刷新。这种方法虽然可以实现部署，但对于蓝/绿部署和A/B测试来说过于复杂，需要手动管理很多基础设施组件，不是最优解决方案。<br><br>选项B：使用Amazon Lightsail部署应用程序。将应用程序以压缩格式存储在Amazon S3存储桶中。使用此压缩版本将新版本的应用程序部署到Lightsail。使用Lightsail部署选项管理部署。Lightsail主要面向简单的Web应用程序，对于企业级的蓝/绿部署和A/B测试功能支持有限，不适合复杂的部署策略需求。<br><br>选项C：使用AWS CodeArtifact存储应用程序代码。使用AWS CodeDeploy将应用程序部署到Amazon EC2实例集群。使用Elastic Load Balancing将流量分发到EC2实例。当对应用程序进行更改时，将新版本上传到CodeArtifact并创建新的CodeDeploy部署。虽然CodeDeploy支持蓝/绿部署，但这种方案需要管理更多的基础设施组件，复杂度较高。<br><br>选项D：使用AWS Elastic Beanstalk托管应用程序。将应用程序的压缩版本存储在Amazon S3中。使用该位置部署应用程序的新版本。使用Elastic Beanstalk管理部署选项。Elastic Beanstalk原生支持多种部署策略，包括蓝/绿部署，并且可以轻松配置A/B测试，同时自动处理底层基础设施管理。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>28</td>
                    <td>A developer is maintaining a eet of 50 Amazon EC2 Linux servers. The servers are part of an Amazon EC2 Auto Scaling group, and also use<br>Elastic Load Balancing for load balancing.<br>Occasionally, some application servers are being terminated after failing ELB HTTP health checks. The developer would like to perform a root<br>cause analysis on the issue, but before being able to access application logs, the server is terminated.<br>How can log collection be automated?</td>
                    <td>A. Use Auto Scaling lifecycle hooks to put instances in a Pending:Wait state. Create an Amazon CloudWatch alarm for EC2 Instance<br>Terminate Successful and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon<br>S3, and complete the lifecycle action once logs are collected.<br>B. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an AWS Cong rule for EC2 Instance-terminate<br>Lifecycle Action and trigger a step function that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle<br>action once logs are collected.<br>C. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon CloudWatch subscription lter for EC2<br>Instance Terminate Successful and trigger a CloudWatch agent that invokes a script to collect logs, push them to Amazon S3, and<br>complete the lifecycle action once logs are collected.<br>D. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon EventBridge rule for EC2 Instance-<br>terminate Lifecycle Action and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to<br>Amazon S3, and complete the lifecycle action once logs are collected.</td>
                    <td>一名开发人员正在维护一个由50台Amazon EC2 Linux服务器组成的集群。这些服务器是Amazon EC2 Auto Scaling组的一部分，并且还使用Elastic Load Balancing进行负载均衡。<br>偶尔，一些应用服务器在ELB HTTP健康检查失败后被终止。开发人员希望对此问题进行根本原因分析，但在能够访问应用程序日志之前，服务器就被终止了。<br>如何自动化日志收集？</td>
                    <td>选项A：使用Auto Scaling生命周期钩子将实例置于Pending:Wait状态。这个选项的问题在于使用了错误的生命周期状态。当实例即将被终止时，应该使用Terminating:Wait状态，而不是Pending:Wait状态。Pending:Wait是用于实例启动过程中的等待状态，不适用于终止场景。<br><br>选项B：使用Auto Scaling生命周期钩子将实例置于Terminating:Wait状态，这是正确的。但是使用AWS Config规则来监听EC2实例终止生命周期动作是不正确的。AWS Config主要用于配置合规性检查，而不是实时事件处理。Step Functions虽然可以用于工作流编排，但在这种场景下不是最佳选择。<br><br>选项C：使用Auto Scaling生命周期钩子将实例置于Terminating:Wait状态是正确的。但是使用CloudWatch订阅过滤器来监听EC2实例终止成功事件是错误的。CloudWatch订阅过滤器主要用于日志流的实时处理，而不是系统事件。另外，CloudWatch代理不能直接被触发来执行脚本。<br><br>选项D：使用Auto Scaling生命周期钩子将实例置于Terminating:Wait状态是正确的。使用Amazon EventBridge规则来监听EC2实例终止生命周期动作是最合适的方法，因为EventBridge专门设计用于处理AWS服务事件。触发Lambda函数调用SSM Run Command脚本来收集日志并推送到S3，然后完成生命周期动作，这是一个完整且正确的解决方案。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>29</td>
                    <td>A company has an organization in AWS Organizations. The organization includes workload accounts that contain enterprise applications. The<br>company centrally manages users from an operations account. No users can be created in the workload accounts. The company recently<br>added an operations team and must provide the operations team members with administrator access to each workload account.</td>
                    <td>A. Create a SysAdmin role in the operations account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to<br>allow the sts:AssumeRole action from the workload accounts.<br>B. Create a SysAdmin role in each workload account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to<br>allow the sts:AssumeRole action from the operations account.<br>C. Create an Amazon Cognito identity pool in the operations account. Attach the SysAdmin role as an authenticated role.<br>D. In the operations account, create an IAM user for each operations team member.<br>E. In the operations account, create an IAM user group that is named SysAdmins. Add an IAM policy that allows the sts:AssumeRole action<br>for the SysAdmin role in each workload account. Add all operations team members to the group.</td>
                    <td>一家公司在AWS Organizations中有一个组织。该组织包含含有企业应用程序的工作负载账户。公司从运营账户集中管理用户。不能在工作负载账户中创建用户。公司最近新增了一个运营团队，必须为运营团队成员提供对每个工作负载账户的管理员访问权限。</td>
                    <td>A. 在运营账户中创建SysAdmin角色，附加AdministratorAccess策略到该角色，修改信任关系以允许来自工作负载账户的sts:AssumeRole操作。这个选项有问题，因为信任关系应该允许运营账户的用户承担工作负载账户中的角色，而不是相反。<br><br>B. 在每个工作负载账户中创建SysAdmin角色，附加AdministratorAccess策略到该角色，修改信任关系以允许来自运营账户的sts:AssumeRole操作。这是正确的，因为需要在目标账户（工作负载账户）中创建角色，并允许源账户（运营账户）的用户承担这些角色。<br><br>C. 在运营账户中创建Amazon Cognito身份池，将SysAdmin角色作为已认证角色附加。Cognito主要用于应用程序用户身份管理，不适合企业内部运营团队的跨账户访问场景。<br><br>D. 在运营账户中为每个运营团队成员创建IAM用户。这只是创建了用户，但没有提供跨账户访问工作负载账户的机制，不完整。<br><br>E. 在运营账户中创建名为SysAdmins的IAM用户组，添加允许对每个工作负载账户中SysAdmin角色执行sts:AssumeRole操作的IAM策略，将所有运营团队成员添加到该组。这是正确的，为运营账户中的用户提供了承担工作负载账户角色的权限。</td>
                    <td>BE</td>
                </tr>
                <tr>
                    <td>30</td>
                    <td>A company has multiple accounts in an organization in AWS Organizations. The company&#x27;s SecOps team needs to receive an Amazon Simple<br>Notication Service (Amazon SNS) notication if any account in the organization turns off the Block Public Access feature on an Amazon S3<br>bucket. A DevOps engineer must implement this change without affecting the operation of any AWS accounts. The implementation must<br>ensure that individual member accounts in the organization cannot turn off the notication.</td>
                    <td>A. Designate an account to be the delegated Amazon GuardDuty administrator account. Turn on GuardDuty for all accounts across the<br>organization. In the GuardDuty administrator account, create an SNS topic. Subscribe the SecOps team&#x27;s email address to the SNS topic. In<br>the same account, create an Amazon EventBridge rule that uses an event pattern for GuardDuty ndings and a target of the SNS topic.<br>B. Create an AWS CloudFormation template that creates an SNS topic and subscribes the SecOps team’s email address to the SNS topic.<br>In the template, include an Amazon EventBridge rule that uses an event pattern of CloudTrail activity for s3:PutBucketPublicAccessBlock<br>and a target of the SNS topic. Deploy the stack to every account in the organization by using CloudFormation StackSets.<br>C. Turn on AWS Cong across the organization. In the delegated administrator account, create an SNS topic. Subscribe the SecOps team&#x27;s<br>email address to the SNS topic. Deploy a conformance pack that uses the s3-bucket-level-public-access-prohibited AWS Cong managed<br>rule in each account and uses an AWS Systems Manager document to publish an event to the SNS topic to notify the SecOps team.<br>D. Turn on Amazon Inspector across the organization. In the Amazon Inspector delegated administrator account, create an SNS topic.<br>Subscribe the SecOps team’s email address to the SNS topic. In the same account, create an Amazon EventBridge rule that uses an event<br>pattern for public network exposure of the S3 bucket and publishes an event to the SNS topic to notify the SecOps team.</td>
                    <td>一家公司在AWS Organizations中有多个账户。公司的SecOps团队需要在组织中任何账户关闭Amazon S3存储桶的&quot;阻止公共访问&quot;功能时收到Amazon Simple Notification Service (Amazon SNS)通知。DevOps工程师必须在不影响任何AWS账户操作的情况下实施此更改。实施方案必须确保组织中的各个成员账户无法关闭通知功能。</td>
                    <td>选项A：指定一个账户作为委托的Amazon GuardDuty管理员账户，为组织中所有账户开启GuardDuty，创建SNS主题并设置EventBridge规则监控GuardDuty发现。但GuardDuty主要用于威胁检测，虽然可能检测到S3公共访问配置变更，但这不是其主要功能，且可能存在延迟和准确性问题。<br><br>选项B：创建CloudFormation模板，包含SNS主题和EventBridge规则，监控CloudTrail中的s3:PutBucketPublicAccessBlock活动，使用StackSets部署到所有账户。这个方案直接监控S3公共访问配置的API调用，能够实时准确地检测到配置变更，且通过StackSets统一管理确保成员账户无法单独关闭通知。<br><br>选项C：开启AWS Config并使用s3-bucket-level-public-access-prohibited规则。虽然Config可以监控合规性，但这个规则主要检查存储桶级别的公共访问设置，而题目要求的是监控&quot;阻止公共访问&quot;功能的开关状态，两者监控范围不完全匹配。<br><br>选项D：使用Amazon Inspector监控S3存储桶的公共网络暴露。Inspector主要用于安全漏洞评估，虽然可以检测公共暴露，但不是专门监控&quot;阻止公共访问&quot;功能开关状态的最佳工具，且响应时间可能不够及时。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>31</td>
                    <td>A company has migrated its container-based applications to Amazon EKS and want to establish automated email notications. The<br>notications sent to each email address are for specic activities related to EKS components. The solution will include Amazon SNS topics<br>and an AWS Lambda function to evaluate incoming log events and publish messages to the correct SNS topic.</td>
                    <td>A. Enable Amazon CloudWatch Logs to log the EKS components. Create a CloudWatch subscription lter for each component with Lambda<br>as the subscription feed destination.<br>B. Enable Amazon CloudWatch Logs to log the EKS components. Create CloudWatch Logs Insights queries linked to Amazon EventBridge<br>events that invoke Lambda.<br>C. Enable Amazon S3 logging for the EKS components. Congure an Amazon CloudWatch subscription lter for each component with<br>Lambda as the subscription feed destination.<br>D. Enable Amazon S3 logging for the EKS components. Congure S3 PUT Object event notications with AWS Lambda as the destination.</td>
                    <td>一家公司已将其基于容器的应用程序迁移到Amazon EKS，并希望建立自动化的电子邮件通知。发送到每个电子邮件地址的通知是针对与EKS组件相关的特定活动。该解决方案将包括Amazon SNS主题和AWS Lambda函数来评估传入的日志事件并将消息发布到正确的SNS主题。</td>
                    <td>A. 启用Amazon CloudWatch Logs来记录EKS组件日志。为每个组件创建CloudWatch订阅过滤器，以Lambda作为订阅源目标。这个选项在技术上是可行的，CloudWatch Logs可以收集EKS组件日志，订阅过滤器可以实时处理日志并触发Lambda函数。Lambda可以分析日志事件并发送到相应的SNS主题。这是一个标准的日志处理架构。<br><br>B. 启用Amazon CloudWatch Logs来记录EKS组件日志。创建链接到Amazon EventBridge事件的CloudWatch Logs Insights查询来调用Lambda。这个选项过于复杂，CloudWatch Logs Insights主要用于交互式查询分析，不是实时事件处理的最佳选择。EventBridge增加了不必要的复杂性。<br><br>C. 为EKS组件启用Amazon S3日志记录。为每个组件配置Amazon CloudWatch订阅过滤器，以Lambda作为订阅源目标。这个选项存在技术错误，因为CloudWatch订阅过滤器是用于CloudWatch Logs的功能，不能直接应用于S3中的日志文件。S3和CloudWatch订阅过滤器不能这样组合使用。<br><br>D. 为EKS组件启用Amazon S3日志记录。配置S3 PUT Object事件通知，以AWS Lambda作为目标。这个选项技术上可行，当日志文件上传到S3时会触发Lambda，但这种方式是批处理而非实时处理，不如CloudWatch Logs方案高效。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>32</td>
                    <td>A company is implementing an Amazon Elastic Container Service (Amazon ECS) cluster to run its workload. The company architecture will run<br>multiple ECS services on the cluster. The architecture includes an Application Load Balancer on the front end and uses multiple target groups<br>to route trac.<br>A DevOps engineer must collect application and access logs. The DevOps engineer then needs to send the logs to an Amazon S3 bucket for<br>near-real-time analysis.</td>
                    <td>A. Download the Amazon CloudWatch Logs container instance from AWS. Congure this instance as a task. Update the application service<br>denitions to include the logging task.<br>B. Then point the ALB directly to the logging S3 bucket.<br>C. Use Amazon EventBridge to schedule an AWS Lambda function that will run every 60 seconds and will run the Amazon CloudWatch<br>Logs create-export-task command. Then point the output to the logging S3 bucket.<br>D. Activate access logging on the AL<br>E. Activate access logging on the target groups that the ECS services use. Then send the logs directly to the logging S3 bucket.<br>F. Create an Amazon Kinesis Data Firehose delivery stream that has a destination of the logging S3 bucket. Then create an Amazon<br>CloudWatch Logs subscription lter for Kinesis Data Firehose.</td>
                    <td>一家公司正在实施Amazon弹性容器服务(Amazon ECS)集群来运行其工作负载。公司架构将在集群上运行多个ECS服务。该架构包括前端的应用负载均衡器，并使用多个目标组来路由流量。DevOps工程师必须收集应用程序和访问日志。然后DevOps工程师需要将日志发送到Amazon S3存储桶进行近实时分析。</td>
                    <td>选项A：从AWS下载Amazon CloudWatch Logs容器实例，将此实例配置为任务，更新应用程序服务定义以包含日志记录任务。这个描述不准确，CloudWatch Logs不是作为容器实例下载的，而且这种方法过于复杂且不是标准做法。<br><br>选项B：然后将ALB直接指向日志记录S3存储桶。这个选项描述不完整，但如果理解为激活ALB的访问日志记录功能并配置到S3存储桶，这是收集ALB访问日志的正确方法。<br><br>选项C：使用Amazon EventBridge调度AWS Lambda函数，每60秒运行一次并执行Amazon CloudWatch Logs create-export-task命令，然后将输出指向日志记录S3存储桶。这种方法可行但不是近实时的，60秒间隔加上导出任务的处理时间会有明显延迟。<br><br>选项D：激活ALB的访问日志记录。这是收集ALB访问日志的标准和推荐方法，可以直接将访问日志发送到S3存储桶。<br><br>选项E：激活ECS服务使用的目标组的访问日志记录，然后直接将日志发送到日志记录S3存储桶。目标组本身不直接提供访问日志记录功能，访问日志是在负载均衡器级别配置的。<br><br>选项F：创建一个以日志记录S3存储桶为目标的Amazon Kinesis Data Firehose传输流，然后为Kinesis Data Firehose创建Amazon CloudWatch Logs订阅过滤器。这是将CloudWatch Logs中的应用程序日志近实时传输到S3的标准方法。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>33</td>
                    <td>A company that uses electronic health records is running a eet of Amazon EC2 instances with an Amazon Linux operating system. As part of<br>patient privacy requirements, the company must ensure continuous compliance for patches for operating system and applications running on<br>the EC2 instances.<br>How can the deployments of the operating system and application patches be automated using a default and custom repository?</td>
                    <td>A. Use AWS Systems Manager to create a new patch baseline including the custom repository. Run the AWS-RunPatchBaseline document<br>using the run command to verify and install patches.<br>B. Use AWS Direct Connect to integrate the corporate repository and deploy the patches using Amazon CloudWatch scheduled events,<br>then use the CloudWatch dashboard to create reports.<br>C. Use yum-cong-manager to add the custom repository under /etc/yum.repos.d and run yum-cong-manager-enable to activate the<br>repository.<br>D. Use AWS Systems Manager to create a new patch baseline including the corporate repository. Run the AWS-<br>AmazonLinuxDefaultPatchBaseline document using the run command to verify and install patches.</td>
                    <td>一家使用电子健康记录的公司正在运行一组使用Amazon Linux操作系统的Amazon EC2实例。作为患者隐私要求的一部分，公司必须确保对EC2实例上运行的操作系统和应用程序的补丁持续合规。<br>如何使用默认和自定义存储库自动化操作系统和应用程序补丁的部署？</td>
                    <td>A. 使用AWS Systems Manager创建包含自定义存储库的新补丁基线。使用run command运行AWS-RunPatchBaseline文档来验证和安装补丁。这个选项是正确的，因为AWS Systems Manager Patch Manager可以创建自定义补丁基线，支持默认和自定义存储库，AWS-RunPatchBaseline是正确的文档名称，可以通过Run Command自动化执行补丁管理。<br><br>B. 使用AWS Direct Connect集成企业存储库，并使用Amazon CloudWatch计划事件部署补丁，然后使用CloudWatch仪表板创建报告。这个选项不正确，因为Direct Connect主要用于网络连接，不是补丁管理的最佳实践，CloudWatch主要用于监控而非补丁部署。<br><br>C. 使用yum-config-manager在/etc/yum.repos.d下添加自定义存储库，并运行yum-config-manager-enable激活存储库。这个选项虽然技术上可行，但不是自动化的最佳解决方案，需要手动在每个实例上配置，不符合大规模自动化部署的要求。<br><br>D. 使用AWS Systems Manager创建包含企业存储库的新补丁基线。使用run command运行AWS-AmazonLinuxDefaultPatchBaseline文档来验证和安装补丁。这个选项不正确，因为AWS-AmazonLinuxDefaultPatchBaseline不是正确的文档名称，正确的应该是AWS-RunPatchBaseline。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>34</td>
                    <td>A company is using AWS CodePipeline to automate its release pipeline. AWS CodeDeploy is being used in the pipeline to deploy an<br>application to Amazon Elastic Container Service (Amazon ECS) using the blue/green deployment model. The company wants to implement<br>scripts to test the green version of the application before shifting trac. These scripts will complete in 5 minutes or less. If errors are<br>discovered during these tests, the application must be rolled back.</td>
                    <td>A. Add a stage to the CodePipeline pipeline between the source and deploy stages. Use AWS CodeBuild to create a runtime environment<br>and build commands in the buildspec le to invoke test scripts. If errors are found, use the aws deploy stop-deployment command to stop<br>the deployment.<br>B. Add a stage to the CodePipeline pipeline between the source and deploy stages. Use this stage to invoke an AWS Lambda function that<br>will run the test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.<br>C. Add a hooks section to the CodeDeploy AppSpec le. Use the AfterAllowTestTrac lifecycle event to invoke an AWS Lambda function to<br>run the test scripts. If errors are found, exit the Lambda function with an error to initiate rollback.<br>D. Add a hooks section to the CodeDeploy AppSpec le. Use the AfterAllowTrac lifecycle event to invoke the test scripts. If errors are<br>found, use the aws deploy stop-deployment CLI command to stop the deployment.</td>
                    <td>一家公司正在使用AWS CodePipeline来自动化其发布管道。管道中使用AWS CodeDeploy将应用程序部署到Amazon弹性容器服务(Amazon ECS)，采用蓝/绿部署模型。公司希望实现脚本来测试应用程序的绿色版本，然后再切换流量。这些脚本将在5分钟或更短时间内完成。如果在测试期间发现错误，应用程序必须回滚。</td>
                    <td>选项A：在源代码阶段和部署阶段之间添加一个阶段，使用AWS CodeBuild创建运行时环境并在buildspec文件中构建命令来调用测试脚本。这个方案的问题是测试发生在部署之前，而不是在绿色环境部署完成后，无法测试实际部署的绿色版本。此外，使用aws deploy stop-deployment命令来停止部署的时机不正确。<br><br>选项B：在源代码阶段和部署阶段之间添加一个阶段，使用该阶段调用AWS Lambda函数来运行测试脚本。这个方案同样存在时机问题，测试是在部署之前进行的，而不是在绿色环境准备好接收测试流量之后。无法真正测试已部署的绿色版本应用程序。<br><br>选项C：在CodeDeploy AppSpec文件中添加hooks部分，使用AfterAllowTestTraffic生命周期事件调用AWS Lambda函数来运行测试脚本。如果发现错误，让Lambda函数以错误退出来启动回滚。这个方案正确地在绿色环境接收测试流量后进行测试，并且通过Lambda函数的错误退出机制可以自动触发CodeDeploy的回滚流程。<br><br>选项D：在CodeDeploy AppSpec文件中添加hooks部分，使用AfterAllowTraffic生命周期事件调用测试脚本。AfterAllowTraffic事件是在所有流量都切换到绿色环境之后触发的，这意味着如果测试失败，用户已经受到了影响。这不符合蓝/绿部署的最佳实践。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>35</td>
                    <td>A company uses AWS Storage Gateway in le gateway mode in front of an Amazon S3 bucket that is used by multiple resources. In the<br>morning when business begins, users do not see the objects processed by a third party the previous evening. When a DevOps engineer looks<br>directly at the S3 bucket, the data is there, but it is missing in Storage Gateway.</td>
                    <td>A. Congure a nightly Amazon EventBridge event to invoke an AWS Lambda function to run the RefreshCache command for Storage<br>Gateway.<br>B. Instruct the third party to put data into the S3 bucket using AWS Transfer for SFTP.<br>C. Modify Storage Gateway to run in volume gateway mode.<br>D. Use S3 Same-Region Replication to replicate any changes made directly in the S3 bucket to Storage Gateway.</td>
                    <td>一家公司在Amazon S3存储桶前使用AWS Storage Gateway的文件网关模式，该存储桶被多个资源使用。在早晨业务开始时，用户看不到第三方在前一天晚上处理的对象。当DevOps工程师直接查看S3存储桶时，数据是存在的，但在Storage Gateway中却缺失了。</td>
                    <td>A. 配置每夜的Amazon EventBridge事件来调用AWS Lambda函数，为Storage Gateway运行RefreshCache命令。这个选项是正确的，因为当第三方直接向S3存储桶写入数据时，Storage Gateway的本地缓存不会自动更新。RefreshCache命令可以强制Storage Gateway重新扫描S3存储桶并更新其缓存，使新添加的文件对用户可见。<br><br>B. 指示第三方使用AWS Transfer for SFTP将数据放入S3存储桶。这个选项不能解决根本问题，因为无论数据如何进入S3，只要不通过Storage Gateway，缓存同步问题仍然存在。<br><br>C. 修改Storage Gateway为卷网关模式。这个选项不合适，因为卷网关模式用于块存储，而不是文件存储，改变模式会完全改变架构和用途。<br><br>D. 使用S3同区域复制将直接在S3存储桶中的任何更改复制到Storage Gateway。这个选项是错误的，因为S3复制是在存储桶之间进行的，不能解决Storage Gateway缓存同步问题。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>36</td>
                    <td>A DevOps engineer needs to back up sensitive Amazon S3 objects that are stored within an S3 bucket with a private bucket policy using S3<br>cross-Region replication functionality. The objects need to be copied to a target bucket in a different AWS Region and account.</td>
                    <td>A. Create a replication IAM role in the source account<br>B. Create a replication I AM role in the target account.<br>C. Add statements to the source bucket policy allowing the replication IAM role to replicate objects.<br>D. Add statements to the target bucket policy allowing the replication IAM role to replicate objects.<br>E. Create a replication rule in the source bucket to enable the replication.</td>
                    <td>一位DevOps工程师需要备份存储在具有私有存储桶策略的S3存储桶中的敏感Amazon S3对象，使用S3跨区域复制功能。这些对象需要被复制到不同AWS区域和账户中的目标存储桶。</td>
                    <td>A. 在源账户中创建复制IAM角色 - 这是正确的。S3跨区域复制需要在源账户中创建一个IAM角色，该角色具有从源存储桶读取对象并将其写入目标存储桶的权限。这个角色是复制过程的执行者，必须在源账户中创建。<br><br>B. 在目标账户中创建复制IAM角色 - 这是错误的。复制IAM角色必须在源账户中创建，而不是目标账户。源账户的S3服务需要承担这个角色来执行复制操作。<br><br>C. 向源存储桶策略添加语句，允许复制IAM角色复制对象 - 这是错误的。由于复制IAM角色是在同一个源账户中创建的，通常不需要在源存储桶策略中明确授权该角色，因为角色本身的权限策略就足够了。<br><br>D. 向目标存储桶策略添加语句，允许复制IAM角色复制对象 - 这是正确的。由于目标存储桶在不同的账户中，必须在目标存储桶策略中明确授权源账户的复制IAM角色，允许其将对象写入目标存储桶。<br><br>E. 在源存储桶中创建复制规则以启用复制 - 这是正确的。必须在源存储桶上配置复制规则，指定要复制的对象、目标存储桶位置以及要使用的IAM角色。</td>
                    <td>ADE</td>
                </tr>
                <tr>
                    <td>37</td>
                    <td>A company has multiple member accounts that are part of an organization in AWS Organizations. The security team needs to review every<br>Amazon EC2 security group and their inbound and outbound rules. The security team wants to programmatically retrieve this information from<br>the member accounts using an AWS Lambda function in the management account of the organization.</td>
                    <td>A. Create a trust relationship that allows users in the member accounts to assume the management account IAM role.<br>B. Create a trust relationship that allows users in the management account to assume the IAM roles of the member accounts.<br>C. Create an IAM role in each member account that has access to the AmazonEC2ReadOnlyAccess managed policy.<br>D. Create an I AM role in each member account to allow the sts:AssumeRole action against the management account IAM role&#x27;s ARN.<br>E. Create an I AM role in the management account that allows the sts:AssumeRole action against the member account IAM role&#x27;s ARN.</td>
                    <td>一家公司有多个成员账户，这些账户是AWS Organizations中某个组织的一部分。安全团队需要审查每个Amazon EC2安全组及其入站和出站规则。安全团队希望使用组织管理账户中的AWS Lambda函数以编程方式从成员账户中检索这些信息。</td>
                    <td>A. 在成员账户中创建信任关系，允许成员账户中的用户承担管理账户的IAM角色。<br>这个选项方向错误。我们需要的是管理账户访问成员账户的资源，而不是成员账户访问管理账户。这种配置无法满足题目要求的从管理账户的Lambda函数访问成员账户EC2安全组信息的需求。<br><br>B. 创建信任关系，允许管理账户中的用户承担成员账户的IAM角色。<br>这个选项是正确的。通过建立这种信任关系，管理账户中的Lambda函数可以承担成员账户中的IAM角色，从而获得访问成员账户资源的权限。这是跨账户访问的标准做法。<br><br>C. 在每个成员账户中创建一个IAM角色，该角色具有AmazonEC2ReadOnlyAccess托管策略的访问权限。<br>这个选项是正确的。需要在成员账户中创建具有EC2只读权限的IAM角色，这样才能读取EC2安全组信息。AmazonEC2ReadOnlyAccess策略提供了查看EC2资源（包括安全组）所需的权限。<br><br>D. 在每个成员账户中创建IAM角色，允许对管理账户IAM角色的ARN执行sts:AssumeRole操作。<br>这个选项方向错误。应该是管理账户承担成员账户的角色，而不是成员账户承担管理账户的角色。这种配置不符合题目的访问模式要求。<br><br>E. 在管理账户中创建IAM角色，允许对成员账户IAM角色的ARN执行sts:AssumeRole操作。<br>这个选项是正确的。管理账户中的Lambda函数需要一个IAM角色，该角色具有承担成员账户中IAM角色的权限。这样Lambda函数就可以通过sts:AssumeRole操作获得访问成员账户资源的临时凭证。</td>
                    <td>BCE</td>
                </tr>
                <tr>
                    <td>38</td>
                    <td>A space exploration company receives telemetry data from multiple satellites. Small packets of data are received through Amazon API<br>Gateway and are placed directly into an Amazon Simple Queue Service (Amazon SQS) standard queue. A custom application is subscribed to<br>the queue and transforms the data into a standard format.<br>Because of inconsistencies in the data that the satellites produce, the application is occasionally unable to transform the data. In these<br>cases, the messages remain in the SQS queue. A DevOps engineer must develop a solution that retains the failed messages and makes them<br>available to scientists for review and future processing.</td>
                    <td>A. Congure AWS Lambda to poll the SQS queue and invoke a Lambda function to check whether the queue messages are valid. If<br>validation fails, send a copy of the data that is not valid to an Amazon S3 bucket so that the scientists can review and correct the data.<br>When the data is corrected, amend the message in the SQS queue by using a replay Lambda function with the corrected data.<br>B. Convert the SQS standard queue to an SQS FIFO queue. Congure AWS Lambda to poll the SQS queue every 10 minutes by using an<br>Amazon EventBridge schedule. Invoke the Lambda function to identify any messages with a SentTimestamp value that is older than 5<br>minutes, push the data to the same location as the application&#x27;s output location, and remove the messages from the queue.<br>C. Create an SQS dead-letter queue. Modify the existing queue by including a redrive policy that sets the Maximum Receives setting to 1<br>and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review<br>the data that is not valid. Reprocess this data at a later time.<br>D. Congure API Gateway to send messages to different SQS virtual queues that are named for each of the satellites. Update the<br>application to use a new virtual queue for any data that it cannot transform, and send the message to the new virtual queue. Instruct the<br>scientists to use the virtual queue to review the data that is not valid. Reprocess this data at a later time.</td>
                    <td>一家太空探索公司从多颗卫星接收遥测数据。小数据包通过Amazon API Gateway接收，并直接放入Amazon Simple Queue Service (Amazon SQS)标准队列中。一个自定义应用程序订阅该队列并将数据转换为标准格式。<br>由于卫星产生的数据存在不一致性，应用程序偶尔无法转换数据。在这些情况下，消息会保留在SQS队列中。DevOps工程师必须开发一个解决方案，保留失败的消息并使科学家能够审查和未来处理这些消息。</td>
                    <td>A. 配置AWS Lambda轮询SQS队列并调用Lambda函数检查队列消息是否有效。如果验证失败，将无效数据的副本发送到Amazon S3存储桶，以便科学家审查和纠正数据。当数据被纠正后，使用重放Lambda函数用纠正的数据修改SQS队列中的消息。这个方案提供了完整的数据验证、存储和重处理流程，能够有效处理失败消息并支持数据纠正后的重新处理。<br><br>B. 将SQS标准队列转换为SQS FIFO队列。配置AWS Lambda使用Amazon EventBridge调度每10分钟轮询SQS队列。调用Lambda函数识别SentTimestamp值超过5分钟的消息，将数据推送到应用程序输出位置，并从队列中删除消息。这个方案过于复杂，且基于时间戳的处理方式不能准确识别处理失败的消息，可能会误删正常等待处理的消息。<br><br>C. 创建SQS死信队列。修改现有队列，包含重驱动策略，将Maximum Receives设置为1，并将死信队列ARN设置为新创建队列的ARN。指导科学家使用死信队列审查无效数据，稍后重新处理。这个方案使用了AWS的标准死信队列机制，但将Maximum Receives设置为1过于严格，可能导致临时处理问题的消息也被立即移到死信队列。<br><br>D. 配置API Gateway将消息发送到以每颗卫星命名的不同SQS虚拟队列。更新应用程序为无法转换的数据使用新的虚拟队列，并将消息发送到新虚拟队列。指导科学家使用虚拟队列审查无效数据，稍后重新处理。这个方案增加了系统复杂性，需要修改API Gateway配置和应用程序逻辑，且虚拟队列的概念在AWS SQS中并不是标准功能。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>39</td>
                    <td>A company wants to use AWS CloudFormation for infrastructure deployment. The company has strict tagging and resource requirements and<br>wants to limit the deployment to two Regions. Developers will need to deploy multiple versions of the same application.</td>
                    <td>A. Create AWS Trusted Advisor checks to nd and remediate unapproved CloudFormation StackSets.<br>B. Create a Cloud Formation drift detection operation to nd and remediate unapproved CloudFormation StackSets.<br>C. Create CloudFormation StackSets with approved CloudFormation templates.<br>D. Create AWS Service Catalog products with approved CloudFormation templates.</td>
                    <td>一家公司希望使用AWS CloudFormation进行基础设施部署。该公司有严格的标签和资源要求，并希望将部署限制在两个区域内。开发人员需要部署同一应用程序的多个版本。</td>
                    <td>A. 创建AWS Trusted Advisor检查来发现和修复未经批准的CloudFormation StackSets - 这个选项不正确，因为Trusted Advisor主要用于成本优化、性能、安全性和容错性的建议，而不是用于控制和限制CloudFormation模板的部署。它无法提供模板治理和版本控制功能。<br><br>B. 创建CloudFormation漂移检测操作来发现和修复未经批准的CloudFormation StackSets - 这个选项也不合适，因为漂移检测主要用于检测已部署资源与模板定义之间的差异，而不是用于控制哪些模板可以被部署或强制执行标签要求。<br><br>C. 使用经过批准的CloudFormation模板创建CloudFormation StackSets - 虽然StackSets可以在多个区域部署，但它本身不提供模板治理、版本控制或强制执行标签策略的功能。开发人员仍然可能使用未经批准的模板。<br><br>D. 使用经过批准的CloudFormation模板创建AWS Service Catalog产品 - 这是最佳选择，因为Service Catalog提供了完整的治理框架，可以控制哪些模板可供使用，强制执行标签策略，限制部署区域，并支持多版本管理。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>40</td>
                    <td>A company requires that its internally facing web application be highly available. The architecture is made up of one Amazon EC2 web server<br>instance and one NAT instance that provides outbound internet access for updates and accessing public data.</td>
                    <td>A. Add the NAT instance to an EC2 Auto Scaling group that spans multiple Availability Zones. Update the route tables.<br>B. Create additional EC2 instances spanning multiple Availability Zones. Add an Application Load Balancer to split the load between<br>them.<br>C. Congure an Application Load Balancer in front of the EC2 instance. Congure Amazon CloudWatch alarms to recover the EC2 instance<br>upon host failure.<br>D. Replace the NAT instance with a NAT gateway in each Availability Zone. Update the route tables.<br>E. Replace the NAT instance with a NAT gateway that spans multiple Availability Zones. Update the route tables.</td>
                    <td>一家公司要求其内部面向的Web应用程序具有高可用性。该架构由一个Amazon EC2 Web服务器实例和一个NAT实例组成，NAT实例为更新和访问公共数据提供出站互联网访问。</td>
                    <td>A. 将NAT实例添加到跨多个可用区的EC2 Auto Scaling组中，并更新路由表。<br>这个选项只解决了NAT实例的高可用性问题，但没有解决Web服务器的高可用性。虽然NAT实例实现了跨AZ的冗余，但单个Web服务器仍然是单点故障。此选项不完整，无法满足整体高可用性要求。<br><br>B. 创建跨多个可用区的额外EC2实例，添加应用负载均衡器在它们之间分配负载。<br>这个选项直接解决了Web服务器层的高可用性问题。通过在多个AZ部署多个EC2实例并使用ALB进行负载分配，可以确保即使某个AZ或实例出现故障，应用程序仍然可用。这是实现Web层高可用性的标准做法。<br><br>C. 在EC2实例前配置应用负载均衡器，配置Amazon CloudWatch告警在主机故障时恢复EC2实例。<br>这个选项仍然依赖单个EC2实例，只是增加了自动恢复机制。虽然有一定的容错能力，但在实例恢复期间仍会有服务中断，无法提供真正的高可用性。<br><br>D. 用每个可用区中的NAT网关替换NAT实例，并更新路由表。<br>这个选项解决了NAT层的高可用性问题。NAT网关是AWS托管服务，具有内置的高可用性和更好的性能。在每个AZ部署NAT网关可以确保出站互联网连接的冗余性，避免NAT实例成为单点故障。<br><br>E. 用跨多个可用区的NAT网关替换NAT实例，并更新路由表。<br>这个选项在技术上是错误的。NAT网关是区域性资源，不能跨多个AZ部署。每个NAT网关只能部署在单个AZ中，需要在每个AZ分别部署NAT网关来实现高可用性。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>41</td>
                    <td>A DevOps engineer is building a multistage pipeline with AWS CodePipeline to build, verify, stage, test, and deploy an application. A manual<br>approval stage is required between the test stage and the deploy stage. The development team uses a custom chat tool with webhook<br>support that requires near-real-time notications.<br>How should the DevOps engineer congure status updates for pipeline activity and approval requests to post to the chat tool?</td>
                    <td>A. Create an Amazon CloudWatch Logs subscription that lters on CodePipeline Pipeline Execution State Change. Publish subscription<br>events to an Amazon Simple Notication Service (Amazon SNS) topic. Subscribe the chat webhook URL to the SNS topic, and complete the<br>subscription validation.<br>B. Create an AWS Lambda function that is invoked by AWS CloudTrail events. When a CodePipeline Pipeline Execution State Change event<br>is detected, send the event details to the chat webhook URL.<br>C. Create an Amazon EventBridge rule that lters on CodePipeline Pipeline Execution State Change. Publish the events to an Amazon<br>Simple Notication Service (Amazon SNS) topic. Create an AWS Lambda function that sends event details to the chat webhook URL.<br>Subscribe the function to the SNS topic.<br>D. Modify the pipeline code to send the event details to the chat webhook URL at the end of each stage. Parameterize the URL so that<br>each pipeline can send to a different URL based on the pipeline environment.</td>
                    <td>一名DevOps工程师正在使用AWS CodePipeline构建一个多阶段管道，用于构建、验证、暂存、测试和部署应用程序。在测试阶段和部署阶段之间需要一个手动批准阶段。开发团队使用一个支持webhook的自定义聊天工具，该工具需要近实时通知。<br>DevOps工程师应该如何配置管道活动和批准请求的状态更新，以便发布到聊天工具？</td>
                    <td>A. 创建一个Amazon CloudWatch Logs订阅，过滤CodePipeline管道执行状态变更。将订阅事件发布到Amazon SNS主题。将聊天webhook URL订阅到SNS主题，并完成订阅验证。这个方案存在问题，因为CloudWatch Logs主要用于日志监控，而不是事件监控。CodePipeline状态变更事件不是通过CloudWatch Logs来捕获的，而是通过EventBridge（原CloudWatch Events）。<br><br>B. 创建一个由AWS CloudTrail事件调用的Lambda函数。当检测到CodePipeline管道执行状态变更事件时，将事件详情发送到聊天webhook URL。虽然CloudTrail可以捕获API调用，但它主要用于审计目的，不是实时事件处理的最佳选择。CloudTrail通常有延迟，不适合近实时通知需求。<br><br>C. 创建一个Amazon EventBridge规则，过滤CodePipeline管道执行状态变更。将事件发布到Amazon SNS主题。创建一个AWS Lambda函数，将事件详情发送到聊天webhook URL。将该函数订阅到SNS主题。这是最佳方案，EventBridge专门用于实时事件处理，可以准确捕获CodePipeline状态变更，通过SNS和Lambda的组合可以实现可靠的webhook调用。<br><br>D. 修改管道代码，在每个阶段结束时将事件详情发送到聊天webhook URL。参数化URL，使每个管道可以根据管道环境发送到不同的URL。这种方法需要修改管道代码，增加了复杂性和维护成本，不是最优解决方案。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>42</td>
                    <td>A company&#x27;s application development team uses Linux-based Amazon EC2 instances as bastion hosts. Inbound SSH access to the bastion<br>hosts is restricted to specic IP addresses, as dened in the associated security groups. The company&#x27;s security team wants to receive a<br>notication if the security group rules are modied to allow SSH access from any IP address.</td>
                    <td>A. Create an Amazon EventBridge rule with a source of aws.cloudtrail and the event name AuthorizeSecurityGroupIngress. Dene an<br>Amazon Simple Notication Service (Amazon SNS) topic as the target.<br>B. Enable Amazon GuardDuty and check the ndings for security groups in AWS Security Hub. Congure an Amazon EventBridge rule with<br>a custom pattern that matches GuardDuty events with an output of NON_COMPLIANT. Dene an Amazon Simple Notication Service<br>(Amazon SNS) topic as the target.<br>C. Create an AWS Cong rule by using the restricted-ssh managed rule to check whether security groups disallow unrestricted incoming<br>SSH trac. Congure automatic remediation to publish a message to an Amazon Simple Notication Service (Amazon SNS) topic.<br>D. Enable Amazon Inspector. Include the Common Vulnerabilities and Exposures-1.1 rules package to check the security groups that are<br>associated with the bastion hosts. Congure Amazon Inspector to publish a message to an Amazon Simple Notication Service (Amazon<br>SNS) topic.</td>
                    <td>一家公司的应用程序开发团队使用基于Linux的Amazon EC2实例作为堡垒主机。对堡垒主机的入站SSH访问仅限于特定IP地址，这在相关的安全组中定义。公司的安全团队希望在安全组规则被修改为允许来自任何IP地址的SSH访问时收到通知。</td>
                    <td>选项A：创建一个Amazon EventBridge规则，源为aws.cloudtrail，事件名称为AuthorizeSecurityGroupIngress，定义Amazon SNS主题作为目标。这个方案可以监控安全组规则的修改，但它只是监控所有的安全组入站规则修改，无法特定检测是否允许了来自任何IP地址（0.0.0.0/0）的SSH访问，缺乏针对性的检测逻辑。<br><br>选项B：启用Amazon GuardDuty并在AWS Security Hub中检查安全组的发现结果，配置Amazon EventBridge规则匹配GuardDuty事件输出NON_COMPLIANT，定义Amazon SNS主题作为目标。GuardDuty主要用于威胁检测和安全监控，虽然它可能检测到一些安全组配置问题，但这不是其主要功能，且配置复杂。<br><br>选项C：使用restricted-ssh托管规则创建AWS Config规则，检查安全组是否禁止不受限制的入站SSH流量，配置自动修复以向Amazon SNS主题发布消息。AWS Config的restricted-ssh规则专门设计用于检测安全组是否允许来自0.0.0.0/0的SSH访问（端口22），这正好符合题目要求，能够持续监控并在检测到违规配置时发送通知。<br><br>选项D：启用Amazon Inspector，包含Common Vulnerabilities and Exposures-1.1规则包来检查与堡垒主机关联的安全组，配置Amazon Inspector向Amazon SNS主题发布消息。Amazon Inspector主要用于应用程序安全评估和漏洞扫描，不是专门用于监控安全组配置变更的服务。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>43</td>
                    <td>A DevOps team manages an API running on-premises that serves as a backend for an Amazon API Gateway endpoint. Customers have been<br>complaining about high response latencies, which the development team has veried using the API Gateway latency metrics in Amazon<br>CloudWatch. To identify the cause, the team needs to collect relevant data without introducing additional latency.</td>
                    <td>A. Install the CloudWatch agent server side and congure the agent to upload relevant logs to CloudWatch.<br>B. Enable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and upload those segments to X-Ray<br>during each request.<br>C. Enable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and use the X-Ray daemon to upload<br>segments to X-Ray.<br>D. Modify the on-premises application to send log information back to API Gateway with each request.<br>E. Modify the on-premises application to calculate and upload statistical data relevant to the API service requests to CloudWatch metrics.</td>
                    <td>一个DevOps团队管理着一个运行在本地的API，该API作为Amazon API Gateway端点的后端。客户一直在抱怨响应延迟过高，开发团队已经通过Amazon CloudWatch中的API Gateway延迟指标验证了这个问题。为了识别原因，团队需要收集相关数据而不引入额外的延迟。</td>
                    <td>A. 在服务器端安装CloudWatch代理并配置代理将相关日志上传到CloudWatch - 这是一个有效的解决方案。CloudWatch代理可以异步收集和上传日志数据，不会对API请求处理造成额外延迟。通过日志分析可以帮助识别性能瓶颈的根本原因。<br><br>B. 在API Gateway中启用AWS X-Ray跟踪，修改应用程序以捕获请求段，并在每个请求期间将这些段上传到X-Ray - 这个方案有问题。在每个请求处理过程中同步上传X-Ray段会引入额外的网络延迟，违反了题目要求的&quot;不引入额外延迟&quot;的条件。<br><br>C. 在API Gateway中启用AWS X-Ray跟踪，修改应用程序以捕获请求段，并使用X-Ray守护进程将段上传到X-Ray - 这是一个优秀的解决方案。X-Ray守护进程异步处理段数据的上传，不会阻塞主要的API请求处理流程，因此不会引入额外延迟。X-Ray提供详细的分布式跟踪信息，非常适合诊断延迟问题。<br><br>D. 修改本地应用程序以在每个请求中将日志信息发送回API Gateway - 这个方案不合理且不可行。API Gateway不是用来接收应用程序日志的，而且这种做法会显著增加请求延迟。<br><br>E. 修改本地应用程序以计算并上传与API服务请求相关的统计数据到CloudWatch指标 - 虽然CloudWatch指标有用，但仅有统计数据不足以进行详细的延迟问题诊断，需要更详细的跟踪信息。</td>
                    <td>AC</td>
                </tr>
                <tr>
                    <td>44</td>
                    <td>A company has an application that is using a MySQL-compatible Amazon Aurora Multi-AZ DB cluster as the database. A cross-Region read<br>replica has been created for disaster recovery purposes. A DevOps engineer wants to automate the promotion of the replica so it becomes the<br>primary database instance in the event of a failure.</td>
                    <td>A. Congure a latency-based Amazon Route 53 CNAME with health checks so it points to both the primary and replica endpoints.<br>Subscribe an Amazon SNS topic to Amazon RDS failure notications from AWS CloudTrail and use that topic to invoke an AWS Lambda<br>function that will promote the replica instance as the primary.<br>B. Create an Aurora custom endpoint to point to the primary database instance. Congure the application to use this endpoint. Congure<br>AWS CloudTrail to run an AWS Lambda function to promote the replica instance and modify the custom endpoint to point to the newly<br>promoted instance.<br>C. Create an AWS Lambda function to modify the application&#x27;s AWS CloudFormation template to promote the replica, apply the template to<br>update the stack, and point the application to the newly promoted instance. Create an Amazon CloudWatch alarm to invoke this Lambda<br>function after the failure event occurs.<br>D. Store the Aurora endpoint in AWS Systems Manager Parameter Store. Create an Amazon EventBridge event that detects the database<br>failure and runs an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Systems Manager<br>Parameter Store. Code the application to reload the endpoint from Parameter Store if a database connection fails.</td>
                    <td>一家公司有一个应用程序，使用与MySQL兼容的Amazon Aurora Multi-AZ数据库集群作为数据库。已经创建了一个跨区域只读副本用于灾难恢复目的。DevOps工程师希望自动化副本的提升过程，使其在发生故障时成为主数据库实例。</td>
                    <td>选项A：配置基于延迟的Amazon Route 53 CNAME记录并进行健康检查，使其指向主数据库和副本端点。订阅Amazon SNS主题以接收来自AWS CloudTrail的Amazon RDS故障通知，并使用该主题调用AWS Lambda函数来提升副本实例为主实例。这个方案存在问题：首先，CloudTrail主要用于记录API调用，不是监控数据库故障的最佳选择；其次，使用Route 53的延迟路由可能会导致应用程序连接到错误的数据库实例，因为只读副本不应该接收写操作。<br><br>选项B：创建Aurora自定义端点指向主数据库实例，配置应用程序使用此端点。配置AWS CloudTrail运行AWS Lambda函数来提升副本实例并修改自定义端点指向新提升的实例。这个方案的问题在于CloudTrail不是用于检测数据库故障的合适工具，它主要记录API调用历史，无法实时检测数据库故障事件。<br><br>选项C：创建AWS Lambda函数修改应用程序的AWS CloudFormation模板来提升副本，应用模板更新堆栈，并将应用程序指向新提升的实例。创建Amazon CloudWatch告警在故障事件发生后调用此Lambda函数。这个方案过于复杂，使用CloudFormation模板更新的方式会增加不必要的延迟和复杂性，不是最优的自动化方案。<br><br>选项D：将Aurora端点存储在AWS Systems Manager Parameter Store中。创建Amazon EventBridge事件来检测数据库故障并运行AWS Lambda函数提升副本实例，同时更新存储在AWS Systems Manager Parameter Store中的端点URL。编写应用程序代码，在数据库连接失败时从Parameter Store重新加载端点。这是最完整和实用的解决方案，使用EventBridge进行事件检测，Parameter Store进行配置管理，应用程序具备自动重连能力。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>45</td>
                    <td>A company hosts its staging website using an Amazon EC2 instance backed with Amazon EBS storage. The company wants to recover quickly<br>with minimal data losses in the event of network connectivity issues or power failures on the EC2 instance.</td>
                    <td>A. Add the instance to an EC2 Auto Scaling group with the minimum, maximum, and desired capacity set to 1.<br>B. Add the instance to an EC2 Auto Scaling group with a lifecycle hook to detach the EBS volume when the EC2 instance shuts down or<br>terminates.<br>C. Create an Amazon CloudWatch alarm for the StatusCheckFailed System metric and select the EC2 action to recover the instance.<br>D. Create an Amazon CloudWatch alarm for the StatusCheckFailed Instance metric and select the EC2 action to reboot the instance.</td>
                    <td>一家公司使用由Amazon EBS存储支持的Amazon EC2实例来托管其预发布网站。该公司希望在EC2实例出现网络连接问题或电源故障时能够快速恢复，并将数据损失降到最低。</td>
                    <td>A. 将实例添加到EC2 Auto Scaling组中，最小、最大和期望容量都设置为1。这个选项可以确保当实例出现故障时，Auto Scaling会自动启动一个新的实例来替换故障实例。由于EBS卷是持久化存储，新实例可以挂载相同的EBS卷，从而实现快速恢复且数据不会丢失。这是一个有效的高可用性解决方案。<br><br>B. 将实例添加到EC2 Auto Scaling组，并使用生命周期钩子在EC2实例关闭或终止时分离EBS卷。虽然这可以保护EBS卷不被意外删除，但仅仅分离卷并不能自动恢复服务，还需要手动干预来重新启动实例并挂载卷，不符合快速恢复的要求。<br><br>C. 为StatusCheckFailed System指标创建Amazon CloudWatch告警，并选择EC2操作来恢复实例。系统状态检查失败通常表示底层硬件问题，EC2恢复操作会将实例迁移到新的硬件上。这可以解决硬件故障问题，但对于某些类型的故障可能不够全面。<br><br>D. 为StatusCheckFailed Instance指标创建Amazon CloudWatch告警，并选择EC2操作来重启实例。实例状态检查失败通常表示操作系统级别的问题，重启可能有助于解决某些问题，但对于严重的系统故障或硬件问题，简单重启可能无效。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>46</td>
                    <td>A company wants to use AWS development tools to replace its current bash deployment scripts. The company currently deploys a LAMP<br>application to a group of Amazon EC2 instances behind an Application Load Balancer (ALB). During the deployments, the company unit tests<br>the committed application, stops and starts services, unregisters and re-registers instances with the load balancer, and updates le<br>permissions. The company wants to maintain the same deployment functionality through the shift to using AWS services.</td>
                    <td>A. Use AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy&#x27;s appspec.yml le to restart services, and<br>deregister and register instances with the AL<br>B. Update the<br>appspec.yml le to update le permissions without a custom script.<br>C. Use AWS CodePipeline to move the application source code from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy<br>to test the application. Use CodeDeploy&#x27;s appspec.yml le to restart services and update permissions without a custom script. Use AWS<br>CodeBuild to unregister and re-register instances with the AL<br>D. Use AWS CodePipeline to trigger AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy&#x27;s appspec.yml le<br>to restart services. Unregister and re-register the instances in the AWS CodeDeploy deployment group with the AL</td>
                    <td>一家公司希望使用AWS开发工具来替换其当前的bash部署脚本。该公司目前将LAMP应用程序部署到位于应用程序负载均衡器(ALB)后面的一组Amazon EC2实例上。在部署过程中，公司会对提交的应用程序进行单元测试，停止和启动服务，从负载均衡器注销和重新注册实例，以及更新文件权限。公司希望在转向使用AWS服务的过程中保持相同的部署功能。</td>
                    <td>选项A：使用AWS CodeBuild测试应用程序，使用AWS CodeDeploy的appspec.yml文件调用bash脚本来重启服务，注销和注册ALB实例，并更新appspec.yml文件以在不使用自定义脚本的情况下更新文件权限。这个选项缺少了完整的CI/CD流水线编排，没有提到CodePipeline来协调整个流程，而且描述有些混乱。<br><br>选项B：使用AWS CodePipeline将应用程序源代码从AWS CodeCommit存储库移动到AWS CodeDeploy，使用CodeDeploy测试应用程序，使用CodeDeploy的appspec.yml文件重启服务和更新权限而不使用自定义脚本，使用AWS CodeBuild注销和重新注册ALB实例。这个选项错误地将测试功能分配给了CodeDeploy，而CodeDeploy主要用于部署而非测试。<br><br>选项C：使用AWS CodePipeline触发AWS CodeBuild测试应用程序，使用AWS CodeDeploy的appspec.yml文件调用bash脚本重启服务，在AWS CodeDeploy部署组中注销和重新注册ALB实例。这个选项正确地使用了CodePipeline作为编排工具，CodeBuild进行测试，CodeDeploy进行部署，并且合理地使用了bash脚本和CodeDeploy的内置功能来处理负载均衡器的注册/注销。<br><br>选项D：内容与选项C相同，使用AWS CodePipeline触发AWS CodeBuild测试应用程序，使用AWS CodeDeploy的appspec.yml文件调用bash脚本重启服务，在AWS CodeDeploy部署组中注销和重新注册ALB实例。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>47</td>
                    <td>A company runs an application with an Amazon EC2 and on-premises conguration. A DevOps engineer needs to standardize patching across<br>both environments. Company policy dictates that patching only happens during non-business hours.</td>
                    <td>A. Add the physical machines into AWS Systems Manager using Systems Manager Hybrid Activations.<br>B. Attach an IAM role to the EC2 instances, allowing them to be managed by AWS Systems Manager.<br>C. Create IAM access keys for the on-premises machines to interact with AWS Systems Manager.<br>D. Run an AWS Systems Manager Automation document to patch the systems every hour<br>E. Use Amazon EventBridge scheduled events to schedule a patch window.<br>F. Use AWS Systems Manager Maintenance Windows to schedule a patch window.</td>
                    <td>一家公司运行着一个包含Amazon EC2和本地配置的应用程序。DevOps工程师需要在两个环境中标准化补丁管理。公司政策规定补丁只能在非工作时间进行。</td>
                    <td>A. 使用Systems Manager混合激活将物理机器添加到AWS Systems Manager中 - 这是正确的。混合激活功能允许本地服务器和虚拟机注册到Systems Manager，使其能够像EC2实例一样被管理，包括补丁管理。这是管理混合环境的标准做法。<br><br>B. 为EC2实例附加IAM角色，允许它们被AWS Systems Manager管理 - 这是正确的。EC2实例需要适当的IAM角色和权限才能与Systems Manager服务通信，这是启用补丁管理功能的必要步骤。<br><br>C. 为本地机器创建IAM访问密钥以与AWS Systems Manager交互 - 这是不正确的。虽然技术上可行，但这不是推荐的安全最佳实践。混合激活是更安全、更标准的方法。<br><br>D. 运行AWS Systems Manager自动化文档每小时修补系统 - 这是不正确的。题目明确要求只在非工作时间进行补丁，每小时执行违反了公司政策。<br><br>E. 使用Amazon EventBridge计划事件来安排补丁窗口 - 虽然EventBridge可以用于调度，但Systems Manager Maintenance Windows是专门为此类任务设计的更合适的服务。<br><br>F. 使用AWS Systems Manager维护窗口来安排补丁窗口 - 这是部分正确的，维护窗口确实是安排补丁的正确方法，但题目要求的是如何设置环境以支持标准化补丁管理。</td>
                    <td>AB</td>
                </tr>
                <tr>
                    <td>48</td>
                    <td>A company has chosen AWS to host a new application. The company needs to implement a multi-account strategy. A DevOps engineer creates<br>a new AWS account and an organization in AWS Organizations. The DevOps engineer also creates the OU structure for the organization and<br>sets up a landing zone by using AWS Control Tower.<br>The DevOps engineer must implement a solution that automatically deploys resources for new accounts that users create through AWS<br>Control Tower Account Factory. When a user creates a new account, the solution must apply AWS CloudFormation templates and SCPs that are<br>customized for the OU or the account to automatically deploy all the resources that are attached to the account. All the OUs are enrolled in<br>AWS Control Tower.</td>
                    <td>A. Use AWS Service Catalog with AWS Control Tower. Create portfolios and products in AWS Service Catalog. Grant granular permissions to<br>provision these resources. Deploy SCPs by using the AWS CLI and JSON documents.<br>B. Deploy CloudFormation stack sets by using the required templates. Enable automatic deployment. Deploy stack instances to the<br>required accounts. Deploy a CloudFormation stack set to the organization’s management account to deploy SCPs.<br>C. Create an Amazon EventBridge rule to detect the CreateManagedAccount event. Congure AWS Service Catalog as the target to deploy<br>resources to any new accounts. Deploy SCPs by using the AWS CLI and JSON documents.<br>D. Deploy the Customizations for AWS Control Tower (CfCT) solution. Use an AWS CodeCommit repository as the source. In the repository,<br>create a custom package that includes the CloudFormation templates and the SCP JSON documents.</td>
                    <td>一家公司选择AWS来托管新应用程序。该公司需要实施多账户策略。DevOps工程师创建了一个新的AWS账户和AWS Organizations中的组织。DevOps工程师还为组织创建了OU结构，并使用AWS Control Tower设置了着陆区。<br>DevOps工程师必须实施一个解决方案，自动为用户通过AWS Control Tower Account Factory创建的新账户部署资源。当用户创建新账户时，解决方案必须应用为OU或账户定制的AWS CloudFormation模板和SCP，以自动部署附加到账户的所有资源。所有OU都已注册到AWS Control Tower。</td>
                    <td>A. 使用AWS Service Catalog与AWS Control Tower结合。在AWS Service Catalog中创建产品组合和产品，授予精细权限来配置这些资源，通过AWS CLI和JSON文档部署SCP。这种方法虽然可以管理资源部署，但不是专门为Control Tower新账户自动化设计的，需要手动触发和管理，不能很好地与Account Factory集成。<br><br>B. 使用所需模板部署CloudFormation堆栈集，启用自动部署，将堆栈实例部署到所需账户，向组织管理账户部署CloudFormation堆栈集来部署SCP。这种方法可以工作，但缺乏与Control Tower Account Factory的原生集成，需要额外的配置和管理开销。<br><br>C. 创建Amazon EventBridge规则来检测CreateManagedAccount事件，配置AWS Service Catalog作为目标向新账户部署资源，通过AWS CLI和JSON文档部署SCP。这种方法有一定的自动化能力，但Service Catalog不是最佳的资源部署目标，且整体解决方案比较复杂。<br><br>D. 部署AWS Control Tower定制化解决方案(CfCT)，使用AWS CodeCommit存储库作为源，在存储库中创建包含CloudFormation模板和SCP JSON文档的自定义包。这是专门为Control Tower设计的官方解决方案，能够与Account Factory无缝集成，自动化程度最高。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>49</td>
                    <td>An online retail company based in the United States plans to expand its operations to Europe and Asia in the next six months. Its product<br>currently runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across<br>multiple Availability Zones. All data is stored in an Amazon Aurora database instance.<br>When the product is deployed in multiple regions, the company wants a single product catalog across all regions, but for compliance<br>purposes, its customer information and purchases must be kept in each region.<br>How should the company meet these requirements with the LEAST amount of application changes?</td>
                    <td>A. Use Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer information and purchases.<br>B. Use Amazon DynamoDB global tables for the product catalog and regional tables for the customer information and purchases.<br>C. Use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information<br>and purchases.<br>D. Use Aurora for the product catalog and Amazon DynamoDB global tables for the customer information and purchases.</td>
                    <td>一家位于美国的在线零售公司计划在未来六个月内将业务扩展到欧洲和亚洲。其产品目前运行在应用负载均衡器后面的Amazon EC2实例上。这些实例在跨多个可用区的Amazon EC2 Auto Scaling组中运行。所有数据都存储在Amazon Aurora数据库实例中。<br>当产品部署在多个区域时，公司希望在所有区域中拥有单一的产品目录，但出于合规目的，客户信息和购买记录必须保存在各自的区域中。<br>公司应该如何以最少的应用程序更改来满足这些要求？</td>
                    <td>A. 使用Amazon Redshift作为产品目录，使用Amazon DynamoDB表存储客户信息和购买记录。这个选项需要将现有的Aurora数据库迁移到完全不同的数据库系统（Redshift和DynamoDB），需要大量的应用程序代码修改，包括数据访问层的重写，不符合&quot;最少应用程序更改&quot;的要求。<br><br>B. 使用Amazon DynamoDB全球表作为产品目录，使用区域表存储客户信息和购买记录。这个选项同样需要将整个数据层从Aurora迁移到DynamoDB，这是一个NoSQL数据库，与关系型数据库Aurora在数据模型、查询语法和应用程序接口方面差异很大，需要大量的应用程序重构。<br><br>C. 使用Aurora读取副本作为产品目录，在每个区域使用额外的本地Aurora实例存储客户信息和购买记录。这个选项保持了现有的Aurora数据库技术栈，只需要配置跨区域读取副本和区域性实例，应用程序的数据访问代码基本不需要修改，只需要调整连接配置。<br><br>D. 使用Aurora作为产品目录，使用Amazon DynamoDB全球表存储客户信息和购买记录。这个选项需要将客户信息和购买记录从Aurora迁移到DynamoDB，仍然需要相当大的应用程序修改来适应不同的数据库系统。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>50</td>
                    <td>A company is implementing a well-architected design for its globally accessible API stack. The design needs to ensure both high reliability<br>and fast response times for users located in North America and Europe.<br>The API stack contains the following three tiers:<br>Amazon API Gateway -<br>AWS Lambda -<br>Amazon DynamoDB -</td>
                    <td>A. Congure Amazon Route 53 to point to API Gateway APIs in North America and Europe using health checks. Congure the APIs to<br>forward requests to a Lambda function in that Region. Congure the Lambda functions to retrieve and update the data in a DynamoDB<br>table in the same Region as the Lambda function.<br>B. Congure Amazon Route 53 to point to API Gateway APIs in North America and Europe using latency-based routing and health checks.<br>Congure the APIs to forward requests to a Lambda function in that Region. Congure the Lambda functions to retrieve and update the<br>data in a DynamoDB global table.<br>C. Congure Amazon Route 53 to point to API Gateway in North America, create a disaster recovery API in Europe, and congure both APIs<br>to forward requests to the Lambda functions in that Region. Retrieve the data from a DynamoDB global table. Deploy a Lambda function<br>to check the North America API health every 5 minutes. In the event of a failure, update Route 53 to point to the disaster recovery API.<br>D. Congure Amazon Route 53 to point to API Gateway API in North America using latency-based routing. Congure the API to forward<br>requests to the Lambda function in the Region nearest to the user. Congure the Lambda function to retrieve and update the data in a<br>DynamoDB table.</td>
                    <td>一家公司正在为其全球可访问的API堆栈实施良好架构设计。该设计需要确保位于北美和欧洲的用户都能获得高可靠性和快速响应时间。<br>API堆栈包含以下三层：<br>- Amazon API Gateway<br>- AWS Lambda  <br>- Amazon DynamoDB</td>
                    <td>选项A：配置Amazon Route 53使用健康检查指向北美和欧洲的API Gateway API。配置API将请求转发到该区域的Lambda函数。配置Lambda函数在与Lambda函数相同区域的DynamoDB表中检索和更新数据。这个方案的问题是没有使用基于延迟的路由来优化用户体验，而且使用独立的DynamoDB表而不是全局表会导致数据一致性问题。<br><br>选项B：配置Amazon Route 53使用基于延迟的路由和健康检查指向北美和欧洲的API Gateway API。配置API将请求转发到该区域的Lambda函数。配置Lambda函数从DynamoDB全局表中检索和更新数据。这个方案结合了基于延迟的路由来确保快速响应时间，健康检查来确保高可靠性，以及DynamoDB全局表来保证数据一致性。<br><br>选项C：配置Amazon Route 53指向北美的API Gateway，在欧洲创建灾难恢复API，并配置两个API将请求转发到该区域的Lambda函数。从DynamoDB全局表检索数据。部署Lambda函数每5分钟检查北美API健康状况。在故障情况下，更新Route 53指向灾难恢复API。这个方案将欧洲作为灾难恢复而不是主要服务区域，不能为欧洲用户提供最佳性能。<br><br>选项D：配置Amazon Route 53使用基于延迟的路由指向北美的API Gateway API。配置API将请求转发到距离用户最近区域的Lambda函数。配置Lambda函数在DynamoDB表中检索和更新数据。这个方案只在北美部署API Gateway，无法为欧洲用户提供最佳性能，而且没有使用全局表。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>51</td>
                    <td>A rapidly growing company wants to scale for developer demand for AWS development environments. Development environments are created<br>manually in the AWS Management Console. The networking team uses AWS CloudFormation to manage the networking infrastructure,<br>exporting stack output values for the Amazon VPC and all subnets. The development environments have common standards, such as<br>Application Load Balancers, Amazon EC2 Auto Scaling groups, security groups, and Amazon DynamoDB tables.<br>To keep up with demand, the DevOps engineer wants to automate the creation of development environments. Because the infrastructure<br>required to support the application is expected to grow, there must be a way to easily update the deployed infrastructure. CloudFormation will<br>be used to create a template for the development environments.</td>
                    <td>A. Use Fn::ImportValue intrinsic functions in the Resources section of the template to retrieve Virtual Private Cloud (VPC) and subnet<br>values. Use CloudFormation StackSets for the development environments, using the Count input parameter to indicate the number of<br>environments needed. Use the UpdateStackSet command to update existing development environments.<br>B. Use nested stacks to dene common infrastructure components. To access the exported values, use TemplateURL to reference the<br>networking team’s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the<br>Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development<br>environments.<br>C. Use nested stacks to dene common infrastructure components. Use Fn::ImportValue intrinsic functions with the resources of the<br>nested stack to retrieve Virtual Private Cloud (VPC) and subnet values. Use the CreateChangeSet and ExecuteChangeSet commands to<br>update existing development environments.<br>D. Use Fn::ImportValue intrinsic functions in the Parameters section of the root template to retrieve Virtual Private Cloud (VPC) and<br>subnet values. Dene the development resources in the order they need to be created in the CloudFormation nested stacks. Use the<br>CreateChangeSet. and ExecuteChangeSet commands to update existing development environments.</td>
                    <td>一家快速增长的公司希望扩展以满足开发人员对AWS开发环境的需求。开发环境目前在AWS管理控制台中手动创建。网络团队使用AWS CloudFormation来管理网络基础设施，导出Amazon VPC和所有子网的堆栈输出值。开发环境有通用标准，如应用程序负载均衡器、Amazon EC2自动扩展组、安全组和Amazon DynamoDB表。为了跟上需求，DevOps工程师希望自动化创建开发环境。由于支持应用程序所需的基础设施预计会增长，必须有一种方法来轻松更新已部署的基础设施。将使用CloudFormation为开发环境创建模板。</td>
                    <td>选项A：使用Fn::ImportValue内置函数在模板的Resources部分检索VPC和子网值，使用CloudFormation StackSets创建开发环境。但是StackSets主要用于跨多个账户和区域部署，不是为单个账户内的多个开发环境设计的。Count参数在CloudFormation中不存在，这是错误的概念。<br><br>选项B：使用嵌套堆栈定义通用基础设施组件，但错误地建议使用TemplateURL引用网络团队的模板来访问导出值，这不是正确的方法。另外，在Parameters部分使用Fn::ImportValue也是不正确的，应该在Resources部分使用。<br><br>选项C：正确使用嵌套堆栈来定义通用基础设施组件，这样可以实现代码重用和模块化。在嵌套堆栈的资源中使用Fn::ImportValue函数来检索网络团队导出的VPC和子网值是正确的做法。使用CreateChangeSet和ExecuteChangeSet命令来更新现有环境也是CloudFormation的最佳实践。<br><br>选项D：错误地在Parameters部分使用Fn::ImportValue函数，这个函数应该在Resources部分使用。虽然提到了嵌套堆栈和正确的更新命令，但核心的导入值使用方法是错误的。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>52</td>
                    <td>A company uses AWS Organizations to manage multiple accounts. Information security policies require that all unencrypted Amazon EBS<br>volumes be marked as non-compliant. A DevOps engineer needs to automatically deploy the solution and ensure that this compliance check is<br>always present.</td>
                    <td>A. Create an AWS CloudFormation template that denes an AWS Inspector rule to check whether EBS encryption is enabled. Save the<br>template to an Amazon S3 bucket that has been shared with all accounts within the company. Update the account creation script pointing<br>to the CloudFormation template in Amazon S3.<br>B. Create an AWS Cong organizational rule to check whether EBS encryption is enabled and deploy the rule using the AWS CLI. Create<br>and apply an SCP to prohibit stopping and deleting AWS Cong across the organization.<br>C. Create an SCP in Organizations. Set the policy to prevent the launch of Amazon EC2 instances without encryption on the EBS volumes<br>using a conditional expression. Apply the SCP to all AWS accounts. Use Amazon Athena to analyze the AWS CloudTrail output, looking for<br>events that deny an ec2:RunInstances action.<br>D. Deploy an IAM role to all accounts from a single trusted account. Build a pipeline with AWS CodePipeline with a stage in AWS Lambda<br>to assume the IAM role, and list all EBS volumes in the account. Publish a report to Amazon S3.</td>
                    <td>一家公司使用AWS Organizations来管理多个账户。信息安全策略要求所有未加密的Amazon EBS卷都被标记为不合规。DevOps工程师需要自动部署解决方案，并确保这种合规性检查始终存在。</td>
                    <td>选项A：创建AWS CloudFormation模板定义AWS Inspector规则来检查EBS加密是否启用。将模板保存到与公司内所有账户共享的Amazon S3存储桶中。更新账户创建脚本指向S3中的CloudFormation模板。这个方案存在问题，因为AWS Inspector主要用于安全漏洞评估，而不是配置合规性检查。此外，这种方法需要手动维护和更新，不能确保合规检查始终存在。<br><br>选项B：创建AWS Config组织规则来检查EBS加密是否启用，并使用AWS CLI部署规则。创建并应用SCP来禁止在整个组织中停止和删除AWS Config。这是一个完整的解决方案，AWS Config专门用于配置合规性检查，组织规则可以自动应用到所有账户，SCP确保Config服务不被意外删除，保证合规检查持续运行。<br><br>选项C：在Organizations中创建SCP策略，设置策略防止启动没有EBS卷加密的EC2实例，使用条件表达式。将SCP应用到所有AWS账户。使用Amazon Athena分析AWS CloudTrail输出，查找拒绝ec2:RunInstances操作的事件。这个方案是预防性的，但题目要求的是标记现有的不合规卷，而不是阻止创建新的不合规资源。<br><br>选项D：从单个受信任账户向所有账户部署IAM角色。使用AWS CodePipeline构建管道，在AWS Lambda阶段承担IAM角色，列出账户中的所有EBS卷。将报告发布到Amazon S3。这个方案可以检测不合规资源，但缺乏自动化的持续监控机制，需要手动触发管道运行。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>53</td>
                    <td>A company is performing vulnerability scanning for all Amazon EC2 instances across many accounts. The accounts are in an organization in<br>AWS Organizations. Each account&#x27;s VPCs are attached to a shared transit gateway. The VPCs send trac to the internet through a central<br>egress VP</td>
                    <td>C. Grant inspector:StartAssessmentRun permissions to the IAM role that the DevOps engineer is using.<br>A. Verify that AWS Systems Manager Agent is installed and is running on the EC2 instances that Amazon Inspector is not scanning.<br>B. Associate the target EC2 instances with security groups that allow outbound communication on port 443 to the AWS Systems Manager<br>service endpoint.<br>D. Congure EC2 Instance Connect for the EC2 instances that Amazon Inspector is not scanning.<br>E. Associate the target EC2 instances with instance proles that grant permissions to communicate with AWS Systems Manager.</td>
                    <td>一家公司正在对AWS Organizations组织中多个账户的所有Amazon EC2实例执行漏洞扫描。每个账户的VPC都连接到共享的传输网关。VPC通过中央出口VPC将流量发送到互联网。<br><br>题目似乎不完整，但从选项来看，这是关于Amazon Inspector漏洞扫描服务无法正常扫描某些EC2实例的故障排除问题。</td>
                    <td>选项A：验证AWS Systems Manager Agent已安装并在Amazon Inspector未扫描的EC2实例上运行。<br>这是正确的。Amazon Inspector依赖于AWS Systems Manager Agent (SSM Agent)来收集实例信息和执行漏洞评估。如果SSM Agent未安装或未运行，Inspector将无法扫描这些实例。这是Inspector正常工作的基本前提条件。<br><br>选项B：将目标EC2实例与允许在端口443上向AWS Systems Manager服务端点进行出站通信的安全组关联。<br>这是正确的。EC2实例需要通过HTTPS (端口443)与AWS Systems Manager服务端点通信，以便SSM Agent能够正常工作。如果安全组阻止了这种通信，Inspector将无法访问实例进行扫描。<br><br>选项C：向DevOps工程师使用的IAM角色授予inspector:StartAssessmentRun权限。<br>这是正确的。DevOps工程师需要适当的IAM权限来启动Inspector评估运行。没有这个权限，工程师无法启动或管理漏洞扫描任务。<br><br>选项D：为Amazon Inspector未扫描的EC2实例配置EC2 Instance Connect。<br>这是不正确的。EC2 Instance Connect主要用于SSH连接管理，与Amazon Inspector的漏洞扫描功能没有直接关系。配置Instance Connect不会解决Inspector扫描问题。<br><br>选项E：将目标EC2实例与授予与AWS Systems Manager通信权限的实例配置文件关联。<br>这是正确的但不在参考答案中。实例需要适当的IAM权限来与Systems Manager通信，这通过实例配置文件实现。</td>
                    <td>ABC</td>
                </tr>
                <tr>
                    <td>54</td>
                    <td>A development team uses AWS CodeCommit for version control for applications. The development team uses AWS CodePipeline, AWS<br>CodeBuild. and AWS CodeDeploy for CI/CD infrastructure. In CodeCommit, the development team recently merged pull requests that did not<br>pass long-running tests in the code base. The development team needed to perform rollbacks to branches in the codebase, resulting in lost<br>time and wasted effort.<br>A DevOps engineer must automate testing of pull requests in CodeCommit to ensure that reviewers more easily see the results of automated<br>tests as part of the pull request review.</td>
                    <td>A. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a<br>CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild<br>badge as a comment on the pull request so that developers will see the badge in their code review.<br>B. Create an Amazon EventBridge rule that reacts to the pullRequestCreated event. Create an AWS Lambda function that invokes a<br>CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild<br>test results as a comment on the pull request when the test results are complete.<br>C. Create an Amazon EventBridge rule that reacts to pullRequestCreated and pullRequestSourceBranchUpdated events. Create an AWS<br>Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the<br>Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code<br>review.<br>D. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a<br>CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild<br>test results as a comment on the pull request when the test results are complete.</td>
                    <td>一个开发团队使用AWS CodeCommit进行应用程序版本控制。开发团队使用AWS CodePipeline、AWS CodeBuild和AWS CodeDeploy构建CI/CD基础设施。在CodeCommit中，开发团队最近合并了一些没有通过代码库中长时间运行测试的拉取请求。开发团队需要对代码库中的分支执行回滚操作，导致时间损失和工作浪费。DevOps工程师必须自动化CodeCommit中拉取请求的测试，以确保审查者能够更容易地看到自动化测试的结果作为拉取请求审查的一部分。</td>
                    <td>选项A：创建一个响应pullRequestStatusChanged事件的EventBridge规则，创建Lambda函数调用CodePipeline管道运行测试，并将CodeBuild徽章作为评论发布到拉取请求中。这个方案的问题是使用了徽章而不是详细的测试结果，审查者无法看到具体的测试详情，只能看到一个状态徽章，不够详细。<br><br>选项B：创建一个响应pullRequestCreated事件的EventBridge规则，创建Lambda函数调用CodePipeline管道运行测试，并将CodeBuild测试结果作为评论发布。这个方案只监听拉取请求创建事件，但如果拉取请求的源分支后续有更新，不会触发新的测试，存在遗漏。<br><br>选项C：创建响应pullRequestCreated和pullRequestSourceBranchUpdated事件的EventBridge规则，创建Lambda函数调用CodePipeline管道运行测试，并将CodeBuild徽章发布为评论。虽然监听了正确的事件，但仍然只提供徽章而不是详细的测试结果。<br><br>选项D：创建一个响应pullRequestStatusChanged事件的EventBridge规则，创建Lambda函数调用CodePipeline管道运行测试，并在测试完成时将CodeBuild测试结果作为评论发布到拉取请求中。这个方案监听状态变化事件，能够捕获拉取请求的各种状态变化，并提供详细的测试结果。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>55</td>
                    <td>A company has deployed an application in a production VPC in a single AWS account. The application is popular and is experiencing heavy<br>usage. The company’s security team wants to add additional security, such as AWS WAF, to the application deployment. However, the<br>application&#x27;s product manager is concerned about cost and does not want to approve the change unless the security team can prove that<br>additional security is necessary.<br>The security team believes that some of the application&#x27;s demand might come from users that have IP addresses that are on a deny list. The<br>security team provides the deny list to a DevOps engineer. If any of the IP addresses on the deny list access the application, the security team<br>wants to receive automated notication in near real time so that the security team can document that the application needs additional<br>security. The DevOps engineer creates a VPC ow log for the production VP</td>
                    <td>C. Create an Amazon S3 bucket for log les. Congure the VPC ow log to capture accepted trac and to send the data to the S3 bucket.<br>Congure an Amazon OpenSearch Service cluster and domain for the log les. Create an AWS Lambda function to retrieve the logs from<br>the S3 bucket, format the logs, and load the logs into the OpenSearch Service cluster. Schedule the Lambda function to run every 5<br>minutes. Congure an alert and condition in OpenSearch Service to send alerts to the security team through an Amazon Simple<br>Notication Service (Amazon SNS) topic when access from the IP addresses on the deny list is detected.<br>A. Create a log group in Amazon CloudWatch Logs. Congure the VPC ow log to capture accepted trac and to send the data to the log<br>group. Create an Amazon CloudWatch metric lter for IP addresses on the deny list. Create a CloudWatch alarm with the metric lter as<br>input. Set the period to 5 minutes and the datapoints to alarm to 1. Use an Amazon Simple Notication Service (Amazon SNS) topic to<br>send alarm notices to the security team.<br>B. Create an Amazon S3 bucket for log les. Congure the VPC ow log to capture all trac and to send the data to the S3 bucket.<br>Congure Amazon Athena to return all log les in the S3 bucket for IP addresses on the deny list. Congure Amazon QuickSight to accept<br>data from Athena and to publish the data as a dashboard that the security team can access. Create a threshold alert of 1 for successful<br>access. Congure the alert to automatically notify the security team as frequently as possible when the alert threshold is met.<br>D. Create a log group in Amazon CloudWatch Logs. Create an Amazon S3 bucket to hold query results. Congure the VPC ow log to<br>capture all trac and to send the data to the log group. Deploy an Amazon Athena CloudWatch connector in AWS Lambda. Connect the<br>connector to the log group. Congure Athena to periodically query for all accepted trac from the IP addresses on the deny list and to<br>store the results in the S3 bucket. Congure an S3 event notication to automatically notify the security team through an Amazon Simple<br>Notication Service (Amazon SNS) topic when new objects are added to the S3 bucket.</td>
                    <td>一家公司在单个AWS账户的生产VPC中部署了一个应用程序。该应用程序很受欢迎，使用量很大。公司的安全团队希望为应用程序部署添加额外的安全措施，如AWS WAF。然而，应用程序的产品经理担心成本，除非安全团队能够证明额外的安全措施是必要的，否则不想批准这个变更。<br><br>安全团队认为应用程序的一些需求可能来自IP地址在拒绝列表上的用户。安全团队向DevOps工程师提供了拒绝列表。如果拒绝列表上的任何IP地址访问应用程序，安全团队希望接收近实时的自动通知，以便安全团队可以记录应用程序需要额外安全措施的证据。DevOps工程师为生产VPC创建了VPC流日志。</td>
                    <td>选项A：在Amazon CloudWatch Logs中创建日志组，配置VPC流日志捕获已接受的流量并发送到日志组。为拒绝列表上的IP地址创建CloudWatch指标过滤器，使用该过滤器创建CloudWatch告警，设置周期为5分钟，告警数据点为1。通过SNS主题向安全团队发送告警通知。这个方案简单高效，使用CloudWatch原生功能实现近实时监控和告警，成本相对较低，配置简单，能够满足需求。<br><br>选项B：创建S3存储桶存储日志文件，配置VPC流日志捕获所有流量并发送到S3。使用Athena查询拒绝列表IP地址的日志，通过QuickSight创建仪表板并设置阈值告警。这个方案过于复杂，QuickSight主要用于数据可视化而非实时告警，无法提供近实时通知功能，且成本较高。<br><br>选项C：创建S3存储桶，配置VPC流日志发送数据到S3，设置OpenSearch Service集群，使用Lambda函数每5分钟处理日志并加载到OpenSearch，配置告警通过SNS发送通知。这个方案架构复杂，涉及多个服务，成本高，5分钟的延迟也不够实时，过度设计。<br><br>选项D：创建CloudWatch Logs日志组和S3存储桶，配置VPC流日志捕获所有流量，部署Athena CloudWatch连接器，定期查询并存储结果到S3，通过S3事件通知发送告警。这个方案架构复杂，涉及不必要的组件，延迟较高，不够实时。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>56</td>
                    <td>A DevOps engineer has automated a web service deployment by using AWS CodePipeline with the following steps:<br>1. An AWS CodeBuild project compiles the deployment artifact and runs unit tests.<br>2. An AWS CodeDeploy deployment group deploys the web service to Amazon EC2 instances in the staging environment.<br>3. A CodeDeploy deployment group deploys the web service to EC2 instances in the production environment.<br>The quality assurance (QA) team requests permission to inspect the build artifact before the deployment to the production environment<br>occurs. The QA team wants to run an internal penetration testing tool to conduct manual tests. The tool will be invoked by a REST API call.</td>
                    <td>A. Insert a manual approval action between the test actions and deployment actions of the pipeline.<br>B. Modify the buildspec.yml le for the compilation stage to require manual approval before completion.<br>C. Update the CodeDeploy deployment groups so that they require manual approval to proceed.<br>D. Update the pipeline to directly call the REST API for the penetration testing tool.<br>E. Update the pipeline to invoke an AWS Lambda function that calls the REST API for the penetration testing tool.</td>
                    <td>一名DevOps工程师使用AWS CodePipeline自动化了Web服务部署，包含以下步骤：<br>1. AWS CodeBuild项目编译部署构件并运行单元测试<br>AWS CodeDeploy部署组将Web服务部署到暂存环境的Amazon EC2实例<br>3. CodeDeploy部署组将Web服务部署到生产环境的EC2实例<br>质量保证(QA)团队请求在部署到生产环境之前检查构建构件的权限。QA团队希望运行内部渗透测试工具来进行手动测试。该工具将通过REST API调用来触发。</td>
                    <td>A. 在管道的测试操作和部署操作之间插入手动批准操作 - 这是正确的。手动批准操作允许QA团队在生产部署前暂停管道，给他们时间检查构件并进行渗透测试。这是CodePipeline的标准功能，完全符合需求。<br><br>B. 修改编译阶段的buildspec.yml文件以要求完成前的手动批准 - 这是不正确的。buildspec.yml是CodeBuild的配置文件，不支持手动批准功能。手动批准是管道级别的操作，不是构建级别的。<br><br>C. 更新CodeDeploy部署组以要求手动批准才能继续 - 这是不正确的。CodeDeploy部署组本身不提供手动批准功能。批准应该在管道级别处理，而不是在部署组级别。<br><br>D. 更新管道以直接调用渗透测试工具的REST API - 这是不完整的解决方案。虽然可以调用API，但没有提供手动控制机制，QA团队无法在适当时机介入进行检查。<br><br>E. 更新管道以调用AWS Lambda函数来调用渗透测试工具的REST API - 这是正确的。Lambda函数可以作为管道中的操作，调用外部REST API来触发渗透测试工具，提供了自动化调用外部工具的能力。</td>
                    <td>AE</td>
                </tr>
                <tr>
                    <td>57</td>
                    <td>A company is hosting a web application in an AWS Region. For disaster recovery purposes, a second region is being used as a standby.<br>Disaster recovery requirements state that session data must be replicated between regions in near-real time and 1% of requests should route<br>to the secondary region to continuously verify system functionality. Additionally, if there is a disruption in service in the main region, trac<br>should be automatically routed to the secondary region, and the secondary region must be able to scale up to handle all trac.<br>How should a DevOps engineer meet these requirements?</td>
                    <td>A. In both regions, deploy the application on AWS Elastic Beanstalk and use Amazon DynamoDB global tables for session data. Use an<br>Amazon Route 53 weighted routing policy with health checks to distribute the trac across the regions.<br>B. In both regions, launch the application in Auto Scaling groups and use DynamoDB for session data. Use a Route 53 failover routing<br>policy with health checks to distribute the trac across the regions.<br>C. In both regions, deploy the application in AWS Lambda, exposed by Amazon API Gateway, and use Amazon RDS for PostgreSQL with<br>cross-region replication for session data. Deploy the web application with client-side logic to call the API Gateway directly.<br>D. In both regions, launch the application in Auto Scaling groups and use DynamoDB global tables for session data. Enable an Amazon<br>CloudFront weighted distribution across regions. Point the Amazon Route 53 DNS record at the CloudFront distribution.</td>
                    <td>一家公司在AWS区域中托管Web应用程序。出于灾难恢复目的，第二个区域被用作备用区域。灾难恢复要求规定会话数据必须在区域之间进行近实时复制，1%的请求应路由到辅助区域以持续验证系统功能。此外，如果主区域的服务出现中断，流量应自动路由到辅助区域，辅助区域必须能够扩展以处理所有流量。DevOps工程师应该如何满足这些要求？</td>
                    <td>选项A：在两个区域都使用AWS Elastic Beanstalk部署应用程序，使用Amazon DynamoDB全球表存储会话数据，使用Route 53加权路由策略和健康检查在区域间分配流量。这个方案能够满足所有要求：DynamoDB全球表提供近实时的跨区域会话数据复制，加权路由策略可以将1%流量路由到辅助区域进行持续验证，健康检查确保故障时自动切换，Elastic Beanstalk支持自动扩展处理所有流量。<br><br>选项B：使用Auto Scaling组和DynamoDB存储会话数据，采用Route 53故障转移路由策略。虽然Auto Scaling组能够扩展，但普通DynamoDB无法提供跨区域的近实时复制，需要手动配置复制。故障转移路由策略主要用于灾难恢复，但不适合将1%流量持续路由到辅助区域进行验证。<br><br>选项C：使用AWS Lambda和API Gateway部署应用程序，使用RDS PostgreSQL跨区域复制存储会话数据。Lambda可以自动扩展，但RDS跨区域复制通常是异步的，可能无法满足近实时要求。客户端直接调用API Gateway的架构复杂度较高，且缺乏有效的流量分配机制。<br><br>选项D：使用Auto Scaling组和DynamoDB全球表，通过CloudFront进行加权分配。虽然DynamoDB全球表满足会话数据复制要求，但CloudFront主要用于内容分发，不是最佳的跨区域流量分配解决方案，且配置复杂度较高。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>58</td>
                    <td>A company runs an application on Amazon EC2 instances. The company uses a series of AWS CloudFormation stacks to dene the application<br>resources. A developer performs updates by building and testing the application on a laptop and then uploading the build output and<br>CloudFormation stack templates to Amazon S3. The developer&#x27;s peers review the changes before the developer performs the CloudFormation<br>stack update and installs a new version of the application onto the EC2 instances.<br>The deployment process is prone to errors and is time-consuming when the developer updates each EC2 instance with the new application.<br>The company wants to automate as much of the application deployment process as possible while retaining a nal manual approval step<br>before the modication of the application or resources.<br>The company already has moved the source code for the application and the CloudFormation templates to AWS CodeCommit. The company<br>also has created an AWS CodeBuild project to build and test the application.</td>
                    <td>A. Create an application group and a deployment group in AWS CodeDeploy. Install the CodeDeploy agent on the EC2 instances.<br>B. Create an application revision and a deployment group in AWS CodeDeploy. Create an environment in CodeDeploy. Register the EC2<br>instances to the CodeDeploy environment.<br>C. Use AWS CodePipeline to invoke the CodeBuild job, run the CloudFormation update, and pause for a manual approval step. After<br>approval, start the AWS CodeDeploy deployment.<br>D. Use AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause<br>for a manual approval step. After approval, run the CloudFormation change sets and start the AWS CodeDeploy deployment.<br>E. Use AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause<br>for a manual approval step. After approval, start the AWS CodeDeploy deployment.</td>
                    <td>一家公司在Amazon EC2实例上运行应用程序。该公司使用一系列AWS CloudFormation堆栈来定义应用程序资源。开发人员通过在笔记本电脑上构建和测试应用程序，然后将构建输出和CloudFormation堆栈模板上传到Amazon S3来执行更新。开发人员的同事在开发人员执行CloudFormation堆栈更新并在EC2实例上安装新版本应用程序之前会审查这些更改。<br><br>当开发人员使用新应用程序更新每个EC2实例时，部署过程容易出错且耗时。公司希望尽可能自动化应用程序部署过程，同时在修改应用程序或资源之前保留最终的手动批准步骤。<br><br>公司已经将应用程序源代码和CloudFormation模板移动到AWS CodeCommit。公司还创建了一个AWS CodeBuild项目来构建和测试应用程序。</td>
                    <td>A. 在AWS CodeDeploy中创建应用程序组和部署组，在EC2实例上安装CodeDeploy代理。这个选项只涉及CodeDeploy的基本设置，没有提到完整的CI/CD流水线，也没有包含手动批准步骤和CloudFormation更新，不能满足题目要求的完整自动化部署流程。<br><br>B. 在AWS CodeDeploy中创建应用程序修订版本和部署组，在CodeDeploy中创建环境，将EC2实例注册到CodeDeploy环境。这个选项描述了CodeDeploy的正确配置方式，包括创建应用程序修订版本、部署组和环境，这是实现自动化部署的必要组件。<br><br>C. 使用AWS CodePipeline调用CodeBuild作业，运行CloudFormation更新，暂停进行手动批准步骤。批准后，启动AWS CodeDeploy部署。这个选项直接运行CloudFormation更新而不是创建变更集，这样无法在批准前预览将要进行的更改，不是最佳实践。<br><br>D. 使用AWS CodePipeline调用CodeBuild作业，为每个应用程序堆栈创建CloudFormation变更集，暂停进行手动批准步骤。批准后，运行CloudFormation变更集并启动AWS CodeDeploy部署。这个选项提供了完整的CI/CD流水线，包括构建、创建变更集供审查、手动批准和部署，是最佳实践。<br><br>E. 使用AWS CodePipeline调用CodeBuild作业，为每个应用程序堆栈创建CloudFormation变更集，暂停进行手动批准步骤。批准后，启动AWS CodeDeploy部署。这个选项创建了变更集但没有执行它们，这意味着基础设施更改不会被应用，只有应用程序部署会进行。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>59</td>
                    <td>A DevOps engineer manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances<br>run in an EC2 Auto Scaling group across multiple Availability Zones. The engineer needs to implement a deployment strategy that:<br>Launches a second eet of instances with the same capacity as the original eet.<br>Maintains the original eet unchanged while the second eet is launched.<br>Transitions trac to the second eet when the second eet is fully deployed.<br>Terminates the original eet automatically 1 hour after transition.</td>
                    <td>A. Use an AWS CloudFormation template with a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to reect<br>the new AL<br>B. Use two AWS Elastic Beanstalk environments to perform a blue/green deployment from the original environment to the new one. Create<br>an application version lifecycle policy to terminate the original environment in 1 hour.<br>C. Use AWS CodeDeploy with a deployment group congured with a blue/green deployment conguration Select the option Terminate the<br>original instances in the deployment group with a waiting period of 1 hour.<br>D. Use AWS Elastic Beanstalk with the conguration set to Immutable. Create an .ebextension using the Resources key that sets the<br>deletion policy of the ALB to 1 hour, and deploy the application.</td>
                    <td>一位DevOps工程师管理着一个运行在Amazon EC2实例上的Web应用程序，这些实例位于应用负载均衡器(ALB)后面。实例在跨多个可用区的EC2 Auto Scaling组中运行。工程师需要实施一个部署策略，该策略需要：<br>- 启动与原始实例组相同容量的第二个实例组<br>- 在启动第二个实例组时保持原始实例组不变<br>- 当第二个实例组完全部署后将流量转移到第二个实例组<br>- 在转移后1小时自动终止原始实例组</td>
                    <td>A. 使用AWS CloudFormation模板，为ALB设置1小时的保留策略，更新Amazon Route 53记录以反映新的ALB。这个选项没有提供完整的蓝绿部署解决方案，只是涉及DNS更新，无法满足题目要求的完整部署流程，特别是无法自动管理两个实例组的生命周期。<br><br>B. 使用两个AWS Elastic Beanstalk环境执行从原始环境到新环境的蓝绿部署，创建应用程序版本生命周期策略在1小时后终止原始环境。虽然Elastic Beanstalk支持蓝绿部署，但题目明确提到使用EC2 Auto Scaling组，而不是Elastic Beanstalk环境管理。<br><br>C. 使用AWS CodeDeploy配置部署组进行蓝绿部署配置，选择在1小时等待期后终止部署组中的原始实例。这个选项完全符合题目要求，CodeDeploy的蓝绿部署功能可以创建新的实例组，保持原实例组不变，完成部署后转移流量，并在指定时间后自动终止原实例组。<br><br>D. 使用AWS Elastic Beanstalk的不可变配置，创建.ebextension使用Resources键设置ALB的删除策略为1小时。这个选项同样涉及Elastic Beanstalk而非直接的EC2 Auto Scaling组管理，不符合题目场景。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>60</td>
                    <td>A video-sharing company stores its videos in Amazon S3. The company has observed a sudden increase in video access requests, but the<br>company does not know which videos are most popular. The company needs to identify the general access pattern for the video les. This<br>pattern includes the number of users who access a certain le on a given day, as well as the number of pull requests for certain les.<br>How can the company meet these requirements with the LEAST amount of effort?</td>
                    <td>A. Activate S3 server access logging. Import the access logs into an Amazon Aurora database. Use an Aurora SQL query to analyze the<br>access patterns.<br>B. Activate S3 server access logging. Use Amazon Athena to create an external table with the log les. Use Athena to create a SQL query<br>to analyze the access patterns.<br>C. Invoke an AWS Lambda function for every S3 object access event. Congure the Lambda function to write the le access information,<br>such as user. S3 bucket, and le key, to an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns.<br>D. Record an Amazon CloudWatch Logs log message for every S3 object access event. Congure a CloudWatch Logs log stream to write<br>the le access information, such as user, S3 bucket, and le key, to an Amazon Kinesis Data Analytics for SQL application. Perform a<br>sliding window analysis.</td>
                    <td>一家视频分享公司将其视频存储在Amazon S3中。该公司观察到视频访问请求突然增加，但公司不知道哪些视频最受欢迎。公司需要识别视频文件的一般访问模式。这种模式包括在给定日期访问某个文件的用户数量，以及某些文件的拉取请求数量。公司如何以最少的工作量满足这些要求？</td>
                    <td>A. 激活S3服务器访问日志记录。将访问日志导入Amazon Aurora数据库。使用Aurora SQL查询来分析访问模式。<br>这个选项需要额外的步骤来导入日志到Aurora数据库，增加了复杂性和成本。虽然可行，但不是最少工作量的解决方案。需要管理数据库实例，设置导入流程，维护成本较高。<br><br>B. 激活S3服务器访问日志记录。使用Amazon Athena创建带有日志文件的外部表。使用Athena创建SQL查询来分析访问模式。<br>这是一个无服务器解决方案，Athena可以直接查询存储在S3中的日志文件，无需额外的数据导入步骤。工作量最少，成本效益高，可以按查询付费，非常适合这种分析场景。<br><br>C. 为每个S3对象访问事件调用AWS Lambda函数。配置Lambda函数将文件访问信息（如用户、S3存储桶和文件键）写入Amazon Aurora数据库。使用Aurora SQL查询来分析访问模式。<br>这种方案过于复杂，需要为每个访问事件触发Lambda函数，会产生大量的Lambda调用费用，并且需要管理Aurora数据库，工作量和成本都很高。<br><br>D. 为每个S3对象访问事件记录Amazon CloudWatch Logs日志消息。配置CloudWatch Logs日志流将文件访问信息写入Amazon Kinesis Data Analytics for SQL应用程序。执行滑动窗口分析。<br>这个方案同样过于复杂，涉及多个服务的配置和管理，包括CloudWatch Logs和Kinesis Data Analytics，工作量大且成本高。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>61</td>
                    <td>A development team wants to use AWS CloudFormation stacks to deploy an application. However, the developer IAM role does not have the<br>required permissions to provision the resources that are specied in the AWS CloudFormation template. A DevOps engineer needs to<br>implement a solution that allows the developers to deploy the stacks. The solution must follow the principle of least privilege.</td>
                    <td>A. Create an IAM policy that allows the developers to provision the required resources. Attach the policy to the developer IAM role.<br>B. Create an IAM policy that allows full access to AWS CloudFormation. Attach the policy to the developer IAM role.<br>C. Create an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role a cloudformation:* action.<br>Use the new service role during stack deployments.<br>D. Create an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role the iam:PassRole<br>permission. Use the new service role during stack deployments.</td>
                    <td>一个开发团队想要使用AWS CloudFormation堆栈来部署应用程序。然而，开发者IAM角色没有所需的权限来配置AWS CloudFormation模板中指定的资源。DevOps工程师需要实现一个解决方案，允许开发者部署堆栈。该解决方案必须遵循最小权限原则。</td>
                    <td>A. 创建一个IAM策略，允许开发者配置所需的资源。将策略附加到开发者IAM角色。<br>这个选项直接给开发者角色添加资源配置权限，虽然可行，但可能违反最小权限原则，因为开发者可能获得超出CloudFormation部署范围的权限。<br><br>B. 创建一个IAM策略，允许完全访问AWS CloudFormation。将策略附加到开发者IAM角色。<br>这个选项给予开发者对CloudFormation的完全访问权限，这明显违反了最小权限原则。开发者将获得过多的权限，包括删除、修改任何CloudFormation堆栈的能力。<br><br>C. 创建一个具有所需权限的AWS CloudFormation服务角色。授予开发者IAM角色cloudformation:*操作权限。在堆栈部署期间使用新的服务角色。<br>这个选项创建了服务角色但给予开发者过多的CloudFormation权限（*通配符），仍然违反最小权限原则。<br><br>D. 创建一个具有所需权限的AWS CloudFormation服务角色。授予开发者IAM角色iam:PassRole权限。在堆栈部署期间使用新的服务角色。<br>这个选项最符合最小权限原则。通过创建专门的服务角色来执行资源配置，开发者只需要PassRole权限来使用该服务角色，而不需要直接的资源配置权限。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>62</td>
                    <td>A production account has a requirement that any Amazon EC2 instance that has been logged in to manually must be terminated within 24<br>hours. All applications in the production account are using Auto Scaling groups with the Amazon CloudWatch Logs agent congured.<br>How can this process be automated?</td>
                    <td>A. Create a CloudWatch Logs subscription to an AWS Step Functions application. Congure an AWS Lambda function to add a tag to the<br>EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a<br>second Lambda function once a day that will terminate all instances with this tag.<br>B. Create an Amazon CloudWatch alarm that will be invoked by the login event. Send the notication to an Amazon Simple Notication<br>Service (Amazon SNS) topic that the operations team is subscribed to, and have them terminate the EC2 instance within 24 hours.<br>C. Create an Amazon CloudWatch alarm that will be invoked by the login event. Congure the alarm to send to an Amazon Simple Queue<br>Service (Amazon SQS) queue. Use a group of worker instances to process messages from the queue, which then schedules an Amazon<br>EventBridge rule to be invoked.<br>D. Create a CloudWatch Logs subscription to an AWS Lambda function. Congure the function to add a tag to the EC2 instance that<br>produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a daily Lambda<br>function that terminates all instances with this tag.</td>
                    <td>一个生产账户有一个要求，任何被手动登录过的Amazon EC2实例必须在24小时内终止。生产账户中的所有应用程序都使用配置了Amazon CloudWatch Logs代理的Auto Scaling组。如何自动化这个过程？</td>
                    <td>选项A：创建CloudWatch Logs订阅到AWS Step Functions应用程序。配置AWS Lambda函数为产生登录事件的EC2实例添加标签并标记实例待停用。创建Amazon EventBridge规则每天调用第二个Lambda函数来终止所有带有此标签的实例。这个方案过于复杂，使用Step Functions增加了不必要的复杂性，而且Step Functions在这里没有明显的价值。<br><br>选项B：创建Amazon CloudWatch警报，由登录事件触发。将通知发送到运维团队订阅的Amazon SNS主题，让他们在24小时内终止EC2实例。这个方案依赖人工干预，不符合&quot;自动化&quot;的要求，违背了题目的核心需求。<br><br>选项C：创建Amazon CloudWatch警报，由登录事件触发。配置警报发送到Amazon SQS队列。使用一组工作实例处理队列中的消息，然后调度Amazon EventBridge规则被调用。这个架构过于复杂，使用了不必要的SQS队列和工作实例，增加了系统复杂性和成本。<br><br>选项D：创建CloudWatch Logs订阅到AWS Lambda函数。配置函数为产生登录事件的EC2实例添加标签并标记实例待停用。创建Amazon EventBridge规则调用每日Lambda函数来终止所有带有此标签的实例。这个方案简洁高效，直接从CloudWatch Logs获取登录事件，通过标签标记实例，然后定期清理，完全自动化且架构简单。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>63</td>
                    <td>A company has enabled all features for its organization in AWS Organizations. The organization contains 10 AWS accounts. The company has<br>turned on AWS CloudTrail in all the accounts. The company expects the number of AWS accounts in the organization to increase to 500 during<br>the next year. The company plans to use multiple OUs for these accounts.<br>The company has enabled AWS Cong in each existing AWS account in the organization. A DevOps engineer must implement a solution that<br>enables AWS Cong automatically for all future AWS accounts that are created in the organization.</td>
                    <td>A. In the organization&#x27;s management account, create an Amazon EventBridge rule that reacts to a CreateAccount API call. Congure the<br>rule to invoke an AWS Lambda function that enables trusted access to AWS Cong for the organization.<br>B. In the organization&#x27;s management account, create an AWS CloudFormation stack set to enable AWS Cong. Congure the stack set to<br>deploy automatically when an account is created through Organizations.<br>C. In the organization&#x27;s management account, create an SCP that allows the appropriate AWS Cong API calls to enable AWS Cong. Apply<br>the SCP to the root-level OU.<br>D. In the organization&#x27;s management account, create an Amazon EventBridge rule that reacts to a CreateAccount API call. Congure the<br>rule to invoke an AWS Systems Manager Automation runbook to enable AWS Cong for the account.</td>
                    <td>一家公司已为其AWS Organizations中的组织启用了所有功能。该组织包含10个AWS账户。公司已在所有账户中开启了AWS CloudTrail。公司预计在明年组织中的AWS账户数量将增加到500个。公司计划为这些账户使用多个组织单位(OU)。<br><br>公司已在组织中的每个现有AWS账户中启用了AWS Config。DevOps工程师必须实施一个解决方案，为组织中创建的所有未来AWS账户自动启用AWS Config。</td>
                    <td>选项A：在组织的管理账户中创建Amazon EventBridge规则来响应CreateAccount API调用，配置规则调用AWS Lambda函数为组织启用AWS Config的可信访问。这个方案只是启用了可信访问，但并没有在新创建的账户中实际配置AWS Config服务，不能满足自动为新账户启用AWS Config的需求。<br><br>选项B：在组织的管理账户中创建AWS CloudFormation堆栈集来启用AWS Config，配置堆栈集在通过Organizations创建账户时自动部署。CloudFormation堆栈集专门设计用于跨多个账户和区域部署资源，支持自动部署到新创建的账户，这正是题目要求的自动化解决方案。<br><br>选项C：在组织的管理账户中创建SCP(服务控制策略)允许适当的AWS Config API调用来启用AWS Config，将SCP应用到根级OU。SCP只是控制权限策略，允许或拒绝特定操作，但不会主动在新账户中配置或启用服务，无法实现自动化配置。<br><br>选项D：在组织的管理账户中创建Amazon EventBridge规则响应CreateAccount API调用，配置规则调用AWS Systems Manager自动化运行手册为账户启用AWS Config。虽然这个方案在技术上可行，但相比CloudFormation堆栈集，实现复杂度更高，需要额外的Lambda函数或自动化脚本开发。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>64</td>
                    <td>A company has many applications. Different teams in the company developed the applications by using multiple languages and frameworks.<br>The applications run on premises and on different servers with different operating systems. Each team has its own release protocol and<br>process. The company wants to reduce the complexity of the release and maintenance of these applications.<br>The company is migrating its technology stacks, including these applications, to AWS. The company wants centralized control of source code,<br>a consistent and automatic delivery pipeline, and as few maintenance tasks as possible on the underlying infrastructure.</td>
                    <td>A. Create one AWS CodeCommit repository for all applications. Put each application&#x27;s code in a different branch. Merge the branches, and<br>use AWS CodeBuild to build the applications. Use AWS CodeDeploy to deploy the applications to one centralized application server.<br>B. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time. Use<br>AWS CodeDeploy to deploy the applications to one centralized application server.<br>C. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time and to<br>create one AMI for each server. Use AWS CloudFormation StackSets to automatically provision and decommission Amazon EC2 eets by<br>using these AMIs.<br>D. Create one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build one Docker image for each<br>application in Amazon Elastic Container Registry (Amazon ECR). Use AWS CodeDeploy to deploy the applications to Amazon Elastic<br>Container Service (Amazon ECS) on infrastructure that AWS Fargate manages.</td>
                    <td>一家公司有许多应用程序。公司内不同的团队使用多种语言和框架开发了这些应用程序。这些应用程序运行在本地和不同操作系统的不同服务器上。每个团队都有自己的发布协议和流程。公司希望降低这些应用程序发布和维护的复杂性。公司正在将其技术栈（包括这些应用程序）迁移到AWS。公司希望实现源代码的集中控制、一致且自动化的交付管道，以及尽可能少的底层基础设施维护任务。</td>
                    <td>A. 为所有应用程序创建一个AWS CodeCommit存储库，将每个应用程序的代码放在不同分支中，合并分支并使用AWS CodeBuild构建应用程序，使用AWS CodeDeploy部署到一个集中的应用服务器。这种方案存在严重问题：将所有应用程序代码放在同一个存储库的不同分支中会导致版本管理混乱，不同语言和框架的应用程序难以统一构建和部署，且部署到单一服务器无法解决不同操作系统和运行环境的需求。<br><br>B. 为每个应用程序创建一个AWS CodeCommit存储库，使用AWS CodeBuild逐一构建应用程序，使用AWS CodeDeploy部署到一个集中的应用服务器。虽然分离了代码存储库，但仍然存在将不同语言、框架和操作系统需求的应用程序部署到同一服务器的问题，这无法解决应用程序的多样性和兼容性问题。<br><br>C. 为每个应用程序创建一个AWS CodeCommit存储库，使用AWS CodeBuild逐一构建应用程序并为每个服务器创建一个AMI，使用AWS CloudFormation StackSets自动配置和停用Amazon EC2集群。这种方案虽然解决了不同操作系统的问题，但需要管理多个AMI和EC2实例，增加了基础设施维护的复杂性，不符合&quot;尽可能少的维护任务&quot;的要求。<br><br>D. 为每个应用程序创建一个AWS CodeCommit存储库，使用AWS CodeBuild为每个应用程序构建一个Docker镜像并存储在Amazon ECR中，使用AWS CodeDeploy将应用程序部署到由AWS Fargate管理的Amazon ECS上。这种方案通过容器化解决了多语言、多框架、多操作系统的兼容性问题，Fargate提供无服务器容器管理，最大程度减少了基础设施维护工作。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>65</td>
                    <td>A company&#x27;s application is currently deployed to a single AWS Region. Recently, the company opened a new oce on a different continent.<br>The users in the new oce are experiencing high latency. The company&#x27;s application runs on Amazon EC2 instances behind an Application<br>Load Balancer (ALB) and uses Amazon DynamoDB as the database layer. The instances run in an EC2 Auto Scaling group across multiple<br>Availability Zones. A DevOps engineer is tasked with minimizing application response times and improving availability for users in both<br>Regions.</td>
                    <td>A. Create a new DynamoDB table in the new Region with cross-Region replication enabled.<br>B. F. Convert the DynamoDB table to a global table.<br>C. Create new ALB and Auto Scaling group resources in the new Region and congure the new ALB to direct trac to the new Auto Scaling<br>group.<br>D. Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the AL<br>E. Create Amazon Route 53 aliases, health checks, and failover routing policies to route to the AL</td>
                    <td>一家公司的应用程序目前部署在单个AWS区域中。最近，该公司在不同大陆开设了新办公室。新办公室的用户遇到了高延迟问题。该公司的应用程序运行在应用负载均衡器(ALB)后面的Amazon EC2实例上，并使用Amazon DynamoDB作为数据库层。这些实例在跨多个可用区的EC2自动扩展组中运行。DevOps工程师的任务是最小化应用程序响应时间并提高两个区域用户的可用性。</td>
                    <td>A. 在新区域创建新的DynamoDB表并启用跨区域复制 - 这个选项不是最佳实践，因为跨区域复制需要手动管理同步，而且会增加复杂性。DynamoDB全局表是更好的解决方案。<br><br>B. 将DynamoDB表转换为全局表 - 这是正确的选择。DynamoDB全局表提供了多区域、多主复制，可以在多个AWS区域之间自动同步数据，为不同区域的用户提供低延迟的数据访问。<br><br>C. 在新区域创建新的ALB和自动扩展组资源，并配置新的ALB将流量导向新的自动扩展组 - 这是必要的步骤。为了减少延迟，需要在新区域部署应用程序基础设施，包括负载均衡器和计算资源，这样用户可以访问地理位置更近的服务器。<br><br>D. 创建Amazon Route 53记录、健康检查和基于延迟的路由策略来路由到ALB - 这是正确的选择。基于延迟的路由可以自动将用户路由到响应时间最短的区域，提供最佳的用户体验。<br><br>E. 创建Amazon Route 53别名、健康检查和故障转移路由策略来路由到ALB - 故障转移路由主要用于灾难恢复场景，不是优化延迟的最佳选择。</td>
                    <td>BCD</td>
                </tr>
                <tr>
                    <td>66</td>
                    <td>A DevOps engineer needs to apply a core set of security controls to an existing set of AWS accounts. The accounts are in an organization in<br>AWS Organizations. Individual teams will administer individual accounts by using the AdministratorAccess AWS managed policy. For all<br>accounts. AWS CloudTrail and AWS Cong must be turned on in all available AWS Regions. Individual account administrators must not be able<br>to edit or delete any of the baseline resources. However, individual account administrators must be able to edit or delete their own CloudTrail<br>trails and AWS Cong rules.</td>
                    <td>A. Create an AWS CloudFormation template that denes the standard account resources. Deploy the template to all accounts from the<br>organization&#x27;s management account by using CloudFormation StackSets. Set the stack policy to deny Update:Delete actions.<br>B. Enable AWS Control Tower. Enroll the existing accounts in AWS Control Tower. Grant the individual account administrators access to<br>CloudTrail and AWS Cong.<br>C. Designate an AWS Cong management account. Create AWS Cong recorders in all accounts by using AWS CloudFormation StackSets.<br>Deploy AWS Cong rules to the organization by using the AWS Cong management account. Create a CloudTrail organization trail in the<br>organization’s management account. Deny modication or deletion of the AWS Cong recorders by using an SCP.<br>D. Create an AWS CloudFormation template that denes the standard account resources. Deploy the template to all accounts from the<br>organization&#x27;s management account by using Cloud Formation StackSets Create an SCP that prevents updates or deletions to CloudTrail<br>resources or AWS Cong resources unless the principal is an administrator of the organization&#x27;s management account.</td>
                    <td>一名DevOps工程师需要对AWS Organizations中现有的一组AWS账户应用一套核心安全控制措施。各个团队将使用AdministratorAccess AWS托管策略来管理各自的账户。对于所有账户，必须在所有可用的AWS区域中启用AWS CloudTrail和AWS Config。各个账户管理员不能编辑或删除任何基线资源。但是，各个账户管理员必须能够编辑或删除他们自己的CloudTrail跟踪和AWS Config规则。</td>
                    <td>A. 创建定义标准账户资源的AWS CloudFormation模板，使用CloudFormation StackSets从组织管理账户部署到所有账户，设置堆栈策略拒绝更新删除操作。这个方案的问题是堆栈策略会阻止所有更新删除操作，包括个人管理员对自己CloudTrail和Config资源的合法修改，不符合题目要求。<br><br>B. 启用AWS Control Tower，将现有账户注册到Control Tower中，授予个人账户管理员访问CloudTrail和Config的权限。Control Tower主要用于新账户的设置和治理，对现有账户的集成相对复杂，且无法精确控制基线资源的保护。<br><br>C. 指定AWS Config管理账户，使用CloudFormation StackSets在所有账户中创建Config记录器，通过Config管理账户向组织部署Config规则，在组织管理账户中创建CloudTrail组织跟踪，使用SCP拒绝对Config记录器的修改或删除。这个方案能够满足所有要求：保护基线资源不被删除，同时允许个人管理员管理自己的资源。<br><br>D. 创建CloudFormation模板定义标准资源，使用StackSets部署，创建SCP阻止对CloudTrail和Config资源的更新删除，除非主体是组织管理账户的管理员。这个方案过于严格，会阻止个人管理员对自己资源的合法操作。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>67</td>
                    <td>A company has its AWS accounts in an organization in AWS Organizations. AWS Cong is manually congured in each AWS account. The<br>company needs to implement a solution to centrally congure AWS Cong for all accounts in the organization The solution also must record<br>resource changes to a central account.</td>
                    <td>A. Congure a delegated administrator account for AWS Cong. Enable trusted access for AWS Cong in the organization.<br>B. Congure a delegated administrator account for AWS Cong. Create a service-linked role for AWS Cong in the organization’s<br>management account.<br>C. Create an AWS CloudFormation template to create an AWS Cong aggregator. Congure a CloudFormation stack set to deploy the<br>template to all accounts in the organization.<br>D. Create an AWS Cong organization aggregator in the organization&#x27;s management account. Congure data collection from all AWS<br>accounts in the organization and from all AWS Regions.<br>E. Create an AWS Cong organization aggregator in the delegated administrator account. Congure data collection from all AWS accounts<br>in the organization and from all AWS Regions.</td>
                    <td>一家公司在AWS Organizations中有多个AWS账户。AWS Config在每个AWS账户中都是手动配置的。公司需要实施一个解决方案来为组织中的所有账户集中配置AWS Config。该解决方案还必须将资源变更记录到一个中央账户。</td>
                    <td>A. 为AWS Config配置委托管理员账户。在组织中为AWS Config启用可信访问。<br>这个选项是正确的第一步。委托管理员账户允许非管理账户来管理AWS Config服务，而启用可信访问是AWS Organizations与AWS Config集成的必要条件。这样可以实现集中管理配置。<br><br>B. 为AWS Config配置委托管理员账户。在组织的管理账户中为AWS Config创建服务链接角色。<br>这个选项也是正确的。服务链接角色是AWS服务代表用户执行操作所需的IAM角色。在管理账户中创建AWS Config的服务链接角色是实现跨账户Config管理的必要步骤，配合委托管理员账户可以实现集中配置。<br><br>C. 创建AWS CloudFormation模板来创建AWS Config聚合器。配置CloudFormation堆栈集将模板部署到组织中的所有账户。<br>这个选项虽然技术上可行，但不是最佳实践。使用堆栈集部署到所有账户会增加复杂性，而且不如使用组织级聚合器来得直接和高效。<br><br>D. 在组织的管理账户中创建AWS Config组织聚合器。配置从组织中所有AWS账户和所有AWS区域收集数据。<br>这个选项不完全正确。虽然组织聚合器可以收集数据，但最佳实践是在委托管理员账户中创建聚合器，而不是在管理账户中。<br><br>E. 在委托管理员账户中创建AWS Config组织聚合器。配置从组织中所有AWS账户和所有AWS区域收集数据。<br>这个选项描述了数据收集的正确方式，但缺少了前置的配置步骤，如启用可信访问和设置服务链接角色。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>68</td>
                    <td>A company wants to migrate its content sharing web application hosted on Amazon EC2 to a serverless architecture. The company currently<br>deploys changes to its application by creating a new Auto Scaling group of EC2 instances and a new Elastic Load Balancer, and then shifting<br>the trac away using an Amazon Route 53 weighted routing policy.<br>For its new serverless application, the company is planning to use Amazon API Gateway and AWS Lambda. The company will need to update<br>its deployment processes to work with the new application. It will also need to retain the ability to test new features on a small number of<br>users before rolling the features out to the entire user base.</td>
                    <td>A. Use AWS CDK to deploy API Gateway and Lambda functions. When code needs to be changed, update the AWS CloudFormation stack<br>and deploy the new version of the APIs and Lambda functions. Use a Route 53 failover routing policy for the canary release strategy.<br>B. Use AWS CloudFormation to deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be<br>changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy.<br>Promote the new version when testing is complete.<br>C. Use AWS Elastic Beanstalk to deploy API Gateway and Lambda functions. When code needs to be changed, deploy a new version of the<br>API and Lambda functions. Shift trac gradually using an Elastic Beanstalk blue/green deployment.<br>D. Use AWS OpsWorks to deploy API Gateway in the service layer and Lambda functions in a custom layer. When code needs to be<br>changed, use OpsWorks to perform a blue/green deployment and shift trac gradually.</td>
                    <td>一家公司希望将其托管在Amazon EC2上的内容共享Web应用程序迁移到无服务器架构。该公司目前通过创建新的EC2实例Auto Scaling组和新的弹性负载均衡器来部署应用程序更改，然后使用Amazon Route 53加权路由策略转移流量。<br><br>对于新的无服务器应用程序，该公司计划使用Amazon API Gateway和AWS Lambda。公司需要更新其部署流程以适应新应用程序。它还需要保留在将功能推广到整个用户群之前，先在少数用户上测试新功能的能力。</td>
                    <td>选项A：使用AWS CDK部署API Gateway和Lambda函数。当需要更改代码时，更新AWS CloudFormation堆栈并部署新版本的API和Lambda函数。使用Route 53故障转移路由策略进行金丝雀发布策略。这个选项的问题在于Route 53故障转移路由策略主要用于灾难恢复场景，不适合金丝雀发布。故障转移策略是基于健康检查的主备切换，而不是渐进式流量分配。<br><br>选项B：使用AWS CloudFormation部署API Gateway和Lambda函数，使用Lambda函数版本。当需要更改代码时，使用新的Lambda代码更新CloudFormation堆栈，并使用金丝雀发布策略更新API版本。测试完成后提升新版本。这个选项正确利用了Lambda的版本控制功能和API Gateway的金丝雀部署能力，可以实现渐进式流量切换和测试。<br><br>选项C：使用AWS Elastic Beanstalk部署API Gateway和Lambda函数。当需要更改代码时，部署新版本的API和Lambda函数。使用Elastic Beanstalk蓝绿部署逐渐转移流量。问题是Elastic Beanstalk主要用于传统应用程序部署，不是为无服务器架构设计的，且不直接支持API Gateway和Lambda的部署。<br><br>选项D：使用AWS OpsWorks在服务层部署API Gateway，在自定义层部署Lambda函数。当需要更改代码时，使用OpsWorks执行蓝绿部署并逐渐转移流量。OpsWorks主要用于传统的基于实例的应用程序管理，不适合无服务器架构的部署和管理。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>69</td>
                    <td>A development team uses AWS CodeCommit, AWS CodePipeline, and AWS CodeBuild to develop and deploy an application. Changes to the<br>code are submitted by pull requests. The development team reviews and merges the pull requests, and then the pipeline builds and tests the<br>application.<br>Over time, the number of pull requests has increased. The pipeline is frequently blocked because of failing tests. To prevent this blockage, the<br>development team wants to run the unit and integration tests on each pull request before it is merged.</td>
                    <td>A. Create a CodeBuild project to run the unit and integration tests. Create a CodeCommit approval rule template. Congure the template<br>to require the successful invocation of the CodeBuild project. Attach the approval rule to the project&#x27;s CodeCommit repository.<br>B. Create an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit Create a CodeBuild project to run the unit<br>and integration tests. Congure the CodeBuild project as a target of the EventBridge rule that includes a custom event payload with the<br>CodeCommit repository and branch information from the event.<br>C. Create an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit. Modify the existing CodePipeline pipeline<br>to not run the deploy steps if the build is started from a pull request. Congure the EventBridge rule to run the pipeline with a custom<br>payload that contains the CodeCommit repository and branch information from the event.<br>D. Create a CodeBuild project to run the unit and integration tests. Create a CodeCommit notication rule that matches when a pull<br>request is created or updated. Congure the notication rule to invoke the CodeBuild project.</td>
                    <td>一个开发团队使用AWS CodeCommit、AWS CodePipeline和AWS CodeBuild来开发和部署应用程序。代码更改通过拉取请求（pull request）提交。开发团队审查并合并拉取请求，然后管道构建和测试应用程序。<br>随着时间推移，拉取请求的数量增加了。由于测试失败，管道经常被阻塞。为了防止这种阻塞，开发团队希望在每个拉取请求合并之前运行单元测试和集成测试。</td>
                    <td>A. 创建一个CodeBuild项目来运行单元和集成测试。创建一个CodeCommit审批规则模板。配置模板要求成功调用CodeBuild项目。将审批规则附加到项目的CodeCommit存储库。这个选项的问题是CodeCommit的审批规则模板主要用于设置审批流程，但不能直接触发CodeBuild项目的执行，也无法确保在合并前自动运行测试。<br><br>B. 创建一个Amazon EventBridge规则来匹配来自CodeCommit的pullRequestCreated事件。创建一个CodeBuild项目来运行单元和集成测试。将CodeBuild项目配置为EventBridge规则的目标，包含来自事件的CodeCommit存储库和分支信息的自定义事件负载。这个方案能够在创建拉取请求时自动触发测试，通过EventBridge实现事件驱动的自动化测试流程。<br><br>C. 创建一个Amazon EventBridge规则来匹配来自CodeCommit的pullRequestCreated事件。修改现有的CodePipeline管道，如果构建是从拉取请求启动的，则不运行部署步骤。配置EventBridge规则运行管道，包含来自事件的CodeCommit存储库和分支信息的自定义负载。这个方案过于复杂，需要修改现有管道，且可能影响正常的部署流程。<br><br>D. 创建一个CodeBuild项目来运行单元和集成测试。创建一个CodeCommit通知规则，匹配创建或更新拉取请求时。配置通知规则调用CodeBuild项目。CodeCommit通知规则主要用于发送通知到SNS或其他服务，不能直接调用CodeBuild项目。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>70</td>
                    <td>A company has an application that runs on a eet of Amazon EC2 instances. The application requires frequent restarts. The application logs<br>contain error messages when a restart is required. The application logs are published to a log group in Amazon CloudWatch Logs.<br>An Amazon CloudWatch alarm noties an application engineer through an Amazon Simple Notication Service (Amazon SNS) topic when the<br>logs contain a large number of restart-related error messages. The application engineer manually restarts the application on the instances<br>after the application engineer receives a notication from the SNS topic.<br>A DevOps engineer needs to implement a solution to automate the application restart on the instances without restarting the instances.</td>
                    <td>A. Congure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Congure the<br>SNS topic to invoke the runbook.<br>B. Create an AWS Lambda function that restarts the application on the instances. Congure the Lambda function as an event destination<br>of the SNS topic.<br>C. Congure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Create an AWS<br>Lambda function to invoke the runbook. Congure the Lambda function as an event destination of the SNS topic.<br>D. Congure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Congure an<br>Amazon EventBridge rule that reacts when the CloudWatch alarm enters ALARM state. Specify the runbook as a target of the rule.</td>
                    <td>一家公司有一个运行在Amazon EC2实例集群上的应用程序。该应用程序需要频繁重启。当需要重启时，应用程序日志会包含错误消息。应用程序日志发布到Amazon CloudWatch Logs中的日志组。当日志包含大量重启相关错误消息时，Amazon CloudWatch告警会通过Amazon Simple Notification Service (Amazon SNS)主题通知应用程序工程师。应用程序工程师在收到SNS主题通知后手动重启实例上的应用程序。DevOps工程师需要实现一个解决方案来自动化实例上的应用程序重启，而不重启实例本身。</td>
                    <td>选项A：配置AWS Systems Manager自动化运行手册来运行脚本重启实例上的应用程序，配置SNS主题直接调用运行手册。这个方案存在问题，因为SNS主题不能直接调用Systems Manager自动化运行手册。SNS主题只能向特定的目标发送消息，如Lambda函数、SQS队列、HTTP端点等，但不能直接触发Systems Manager运行手册。<br><br>选项B：创建一个AWS Lambda函数来重启实例上的应用程序，将Lambda函数配置为SNS主题的事件目标。这是一个可行的解决方案。Lambda函数可以作为SNS主题的订阅者，当SNS收到告警通知时，会触发Lambda函数执行。Lambda函数可以通过AWS SDK调用EC2实例上的命令来重启应用程序，比如使用Systems Manager的SendCommand API。<br><br>选项C：配置AWS Systems Manager自动化运行手册运行脚本重启应用程序，创建AWS Lambda函数调用运行手册，将Lambda函数配置为SNS主题的事件目标。虽然这个方案技术上可行，但增加了不必要的复杂性。既然Lambda函数已经可以直接重启应用程序，就没有必要再通过Systems Manager运行手册来实现。<br><br>选项D：配置AWS Systems Manager自动化运行手册运行脚本重启应用程序，配置Amazon EventBridge规则响应CloudWatch告警进入ALARM状态，指定运行手册作为规则的目标。这个方案改变了现有的架构流程，需要绕过SNS主题直接从CloudWatch告警触发EventBridge规则，虽然可行但不是最直接的解决方案。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>71</td>
                    <td>A DevOps engineer at a company is supporting an AWS environment in which all users use AWS IAM Identity Center (AWS Single Sign-On). The<br>company wants to immediately disable credentials of any new IAM user and wants the security team to receive a notication.</td>
                    <td>A. Create an Amazon EventBridge rule that reacts to an IAM CreateUser API call in AWS CloudTrail.<br>B. Create an Amazon EventBridge rule that reacts to an IAM GetLoginProle API call in AWS CloudTrail.<br>C. Create an AWS Lambda function that is a target of the EventBridge rule. Congure the Lambda function to disable any access keys and<br>delete the login proles that are associated with the IAM user.<br>D. Create an AWS Lambda function that is a target of the EventBridge rule. Congure the Lambda function to delete the login proles that<br>are associated with the IAM user.<br>E. Create an Amazon Simple Notication Service (Amazon SNS) topic that is a target of the EventBridge rule. Subscribe the security team&#x27;s<br>group email address to the topic.</td>
                    <td>一家公司的DevOps工程师正在支持一个AWS环境，该环境中所有用户都使用AWS IAM Identity Center（AWS Single Sign-On）。公司希望立即禁用任何新IAM用户的凭证，并希望安全团队收到通知。</td>
                    <td>A. 创建一个Amazon EventBridge规则，对AWS CloudTrail中的IAM CreateUser API调用做出反应。<br>这个选项是正确的。当创建新的IAM用户时，会触发CreateUser API调用，EventBridge可以监听这个事件并触发后续的自动化操作。这是整个解决方案的触发器部分。<br><br>B. 创建一个Amazon EventBridge规则，对AWS CloudTrail中的IAM GetLoginProfile API调用做出反应。<br>这个选项不正确。GetLoginProfile是用于获取用户登录配置文件信息的API调用，而不是创建用户时的事件。我们需要监听的是用户创建事件，而不是获取登录配置文件的事件。<br><br>C. 创建一个AWS Lambda函数作为EventBridge规则的目标。配置Lambda函数以禁用任何访问密钥并删除与IAM用户关联的登录配置文件。<br>这个选项是正确的。Lambda函数需要执行两个关键操作：禁用访问密钥（防止程序化访问）和删除登录配置文件（防止控制台访问）。这样可以全面禁用新创建用户的所有凭证。<br><br>D. 创建一个AWS Lambda函数作为EventBridge规则的目标。配置Lambda函数以删除与IAM用户关联的登录配置文件。<br>这个选项不完整。虽然删除登录配置文件可以防止控制台访问，但没有处理访问密钥，用户仍然可能通过API进行程序化访问。需要同时禁用访问密钥才能完全禁用凭证。<br><br>E. 创建一个Amazon Simple Notification Service（Amazon SNS）主题作为EventBridge规则的目标。将安全团队的群组邮箱地址订阅到该主题。<br>这个选项是正确的。题目要求安全团队收到通知，SNS是实现通知功能的标准服务。通过将安全团队邮箱订阅到SNS主题，可以确保他们及时收到新用户创建的通知。</td>
                    <td>ACE</td>
                </tr>
                <tr>
                    <td>72</td>
                    <td>A company wants to set up a continuous delivery pipeline. The company stores application code in a private GitHub repository. The company<br>needs to deploy the application components to Amazon Elastic Container Service (Amazon ECS). Amazon EC2, and AWS Lambda. The pipeline<br>must support manual approval actions.</td>
                    <td>A. Use AWS CodePipeline with Amazon ECS. Amazon EC2, and Lambda as deploy providers.<br>B. Use AWS CodePipeline with AWS CodeDeploy as the deploy provider.<br>C. Use AWS CodePipeline with AWS Elastic Beanstalk as the deploy provider.<br>D. Use AWS CodeDeploy with GitHub integration to deploy the application.</td>
                    <td>一家公司想要建立一个持续交付管道。该公司将应用程序代码存储在私有GitHub存储库中。公司需要将应用程序组件部署到Amazon Elastic Container Service (Amazon ECS)、Amazon EC2和AWS Lambda。该管道必须支持手动批准操作。</td>
                    <td>A. 使用AWS CodePipeline与Amazon ECS、Amazon EC2和Lambda作为部署提供商。<br>这个选项虽然提到了所有需要的部署目标，但直接将ECS、EC2和Lambda作为部署提供商是不准确的。CodePipeline需要通过特定的部署服务来管理这些资源的部署，而不是直接作为部署提供商。此外，这种方式难以统一管理多种不同类型的部署目标。<br><br>B. 使用AWS CodePipeline与AWS CodeDeploy作为部署提供商。<br>这是最佳选择。CodePipeline提供了完整的CI/CD管道功能，包括与GitHub的集成和手动批准步骤的支持。CodeDeploy作为部署服务，可以支持多种部署目标，包括EC2实例、ECS服务和Lambda函数。这种组合提供了统一的部署管理方式，并且完全满足题目要求。<br><br>C. 使用AWS CodePipeline与AWS Elastic Beanstalk作为部署提供商。<br>Elastic Beanstalk主要用于简化Web应用程序的部署和管理，但它不能直接部署到Lambda函数，也不适合复杂的多服务架构部署。对于需要同时部署到ECS、EC2和Lambda的场景，Beanstalk的功能过于局限。<br><br>D. 使用AWS CodeDeploy与GitHub集成来部署应用程序。<br>虽然CodeDeploy支持GitHub集成，但单独使用CodeDeploy无法提供完整的CI/CD管道功能，特别是缺少手动批准步骤的管理。CodeDeploy更多是一个部署工具，而不是完整的管道解决方案。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>73</td>
                    <td>A company has an application that runs on Amazon EC2 instances that are in an Auto Scaling group. When the application starts up. the<br>application needs to process data from an Amazon S3 bucket before the application can start to serve requests.<br>The size of the data that is stored in the S3 bucket is growing. When the Auto Scaling group adds new instances, the application now takes<br>several minutes to download and process the data before the application can serve requests. The company must reduce the time that elapses<br>before new EC2 instances are ready to serve requests.</td>
                    <td>A. Congure a warm pool for the Auto Scaling group with warmed EC2 instances in the Stopped state. Congure an<br>autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook<br>when the application is ready to serve requests.<br>B. Increase the maximum instance count of the Auto Scaling group. Congure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook<br>on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.<br>C. Congure a warm pool for the Auto Scaling group with warmed EC2 instances in the Running state. Congure an<br>autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook<br>when the application is ready to serve requests.<br>D. Increase the maximum instance count of the Auto Scaling group. Congure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook<br>on the Auto Scaling group. Modify the application to complete the lifecycle hook and to place the new instance in the Standby state when<br>the application is ready to serve requests.</td>
                    <td>一家公司有一个运行在Amazon EC2实例上的应用程序，这些实例位于Auto Scaling组中。当应用程序启动时，应用程序需要先处理Amazon S3存储桶中的数据，然后才能开始处理请求。存储在S3存储桶中的数据大小正在增长。当Auto Scaling组添加新实例时，应用程序现在需要几分钟来下载和处理数据，然后才能处理请求。公司必须减少新EC2实例准备好处理请求之前所需的时间。</td>
                    <td>A. 为Auto Scaling组配置warm pool，其中预热的EC2实例处于Stopped状态。在Auto Scaling组上配置autoscaling:EC2_INSTANCE_LAUNCHING生命周期钩子。修改应用程序以在应用程序准备好处理请求时完成生命周期钩子。这个选项的问题是实例处于Stopped状态，当需要启动时仍然需要时间来启动实例并处理数据，无法有效减少准备时间。<br><br>B. 增加Auto Scaling组的最大实例数。在Auto Scaling组上配置autoscaling:EC2_INSTANCE_LAUNCHING生命周期钩子。修改应用程序以在应用程序准备好处理请求时完成生命周期钩子。仅仅增加最大实例数并不能解决数据处理时间长的根本问题，新实例仍然需要花费几分钟来下载和处理S3数据。<br><br>C. 为Auto Scaling组配置warm pool，其中预热的EC2实例处于Running状态。在Auto Scaling组上配置autoscaling:EC2_INSTANCE_LAUNCHING生命周期钩子。修改应用程序以在应用程序准备好处理请求时完成生命周期钩子。这是最佳选择，因为预热的实例处于Running状态，可以预先下载和处理S3数据，当需要扩展时可以快速投入使用。<br><br>D. 增加Auto Scaling组的最大实例数。在Auto Scaling组上配置autoscaling:EC2_INSTANCE_LAUNCHING生命周期钩子。修改应用程序以在应用程序准备好处理请求时完成生命周期钩子，并在应用程序准备好处理请求时将新实例置于Standby状态。将实例置于Standby状态实际上是将其从服务中移除，这与目标相反。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>74</td>
                    <td>A company is using an AWS CodeBuild project to build and package an application. The packages are copied to a shared Amazon S3 bucket<br>before being deployed across multiple AWS accounts.<br>The buildspec.yml le contains the following:<br>The DevOps engineer has noticed that anybody with an AWS account is able to download the artifacts.</td>
                    <td>A. Modify the post_build command to use --acl public-read and congure a bucket policy that grants read access to the relevant AWS<br>accounts only.<br>B. Congure a default ACL for the S3 bucket that denes the set of authenticated users as the relevant AWS accounts only and grants<br>read-only access.<br>C. Create an S3 bucket policy that grants read access to the relevant AWS accounts and denies read access to the principal “*”.<br>D. Modify the post_build command to remove --acl authenticated-read and congure a bucket policy that allows read access to the<br>relevant AWS accounts only.</td>
                    <td>一家公司正在使用AWS CodeBuild项目来构建和打包应用程序。这些包在部署到多个AWS账户之前会被复制到共享的Amazon S3存储桶中。buildspec.yml文件包含以下内容：DevOps工程师注意到任何拥有AWS账户的人都能够下载这些构件。</td>
                    <td>A. 修改post_build命令使用--acl public-read并配置存储桶策略，仅向相关AWS账户授予读取访问权限。这个选项是错误的，因为public-read ACL会让所有人（包括匿名用户）都能访问对象，这比当前的authenticated-read更不安全，与题目要求限制访问的目标相反。<br><br>B. 为S3存储桶配置默认ACL，将认证用户集定义为仅相关AWS账户并授予只读访问权限。这个选项在技术上不可行，因为S3的默认ACL无法精确定义特定的AWS账户集合作为&quot;认证用户&quot;，ACL的粒度不够细致来实现这种控制。<br><br>C. 创建S3存储桶策略，向相关AWS账户授予读取访问权限并拒绝主体&quot;*&quot;的读取访问。这个选项存在逻辑冲突，因为拒绝&quot;*&quot;会阻止所有访问，包括被明确授权的账户，因为拒绝策略的优先级高于允许策略。<br><br>D. 修改post_build命令移除--acl authenticated-read并配置存储桶策略，仅允许相关AWS账户的读取访问。这是正确的方法，移除ACL后对象将继承存储桶的默认权限，然后通过存储桶策略精确控制哪些账户可以访问。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>75</td>
                    <td>A company has developed a serverless web application that is hosted on AWS. The application consists of Amazon S3. Amazon API Gateway,<br>several AWS Lambda functions, and an Amazon RDS for MySQL database. The company is using AWS CodeCommit to store the source code.<br>The source code is a combination of AWS Serverless Application Model (AWS SAM) templates and Python code.<br>A security audit and penetration test reveal that user names and passwords for authentication to the database are hardcoded within<br>CodeCommit repositories. A DevOps engineer must implement a solution to automatically detect and prevent hardcoded secrets.</td>
                    <td>A. Enable Amazon CodeGuru Proler. Decorate the handler function with @with_lambda_proler(). Manually review the recommendation<br>report. Write the secret to AWS Systems Manager Parameter Store as a secure string. Update the SAM templates and the Python code to<br>pull the secret from Parameter Store.<br>B. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations.<br>Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager.<br>C. Enable Amazon CodeGuru Proler. Decorate the handler function with @with_lambda_proler(). Manually review the recommendation<br>report. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets<br>Manager.<br>D. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations.<br>Write the secret to AWS Systems Manager Parameter Store as a string. Update the SAM templates and the Python code to pull the secret<br>from Parameter Store.</td>
                    <td>一家公司开发了一个托管在AWS上的无服务器Web应用程序。该应用程序由Amazon S3、Amazon API Gateway、多个AWS Lambda函数和Amazon RDS for MySQL数据库组成。公司使用AWS CodeCommit存储源代码。源代码是AWS无服务器应用程序模型(AWS SAM)模板和Python代码的组合。安全审计和渗透测试显示，用于数据库身份验证的用户名和密码在CodeCommit存储库中是硬编码的。DevOps工程师必须实施一个解决方案来自动检测和防止硬编码的机密信息。</td>
                    <td>选项A：使用Amazon CodeGuru Profiler来检测硬编码机密是不正确的。CodeGuru Profiler主要用于应用程序性能分析和优化，而不是代码安全审查。虽然建议使用Systems Manager Parameter Store存储机密是合理的，但Profiler无法检测硬编码机密问题。<br><br>选项B：将CodeCommit存储库与Amazon CodeGuru Reviewer关联是正确的方法。CodeGuru Reviewer专门用于自动化代码审查，能够检测包括硬编码机密在内的安全问题和代码质量问题。使用AWS Secrets Manager存储数据库凭证也是最佳实践，因为它专门为管理敏感信息而设计，提供自动轮换功能。<br><br>选项C：同选项A一样，使用CodeGuru Profiler来检测硬编码机密是错误的。Profiler不具备代码安全审查功能，无法识别硬编码的机密信息。虽然推荐使用Secrets Manager是正确的，但检测方法不当。<br><br>选项D：使用CodeGuru Reviewer是正确的，但将机密作为普通字符串存储在Systems Manager Parameter Store中是不安全的。应该使用SecureString类型或更好的选择是使用专门的AWS Secrets Manager来管理数据库凭证。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>76</td>
                    <td>A company is using Amazon S3 buckets to store important documents. The company discovers that some S3 buckets are not encrypted.<br>Currently, the company’s IAM users can create new S3 buckets without encryption. The company is implementing a new requirement that all<br>S3 buckets must be encrypted.<br>A DevOps engineer must implement a solution to ensure that server-side encryption is enabled on all existing S3 buckets and all new S3<br>buckets. The encryption must be enabled on new S3 buckets as soon as the S3 buckets are created. The default encryption type must be 256-<br>bit Advanced Encryption Standard (AES-256).</td>
                    <td>A. Create an AWS Lambda function that is invoked periodically by an Amazon EventBridge scheduled rule. Program the Lambda function<br>to scan all current S3 buckets for encryption status and to set AES-256 as the default encryption for any S3 bucket that does not have an<br>encryption conguration.<br>B. Set up and activate the s3-bucket-server-side-encryption-enabled AWS Cong managed rule. Congure the rule to use the AWS-<br>EnableS3BucketEncryption AWS Systems Manager Automation runbook as the remediation action. Manually run the re-evaluation process<br>to ensure that existing S3 buckets are compliant.<br>C. Create an AWS Lambda function that is invoked by an Amazon EventBridge event rule. Dene the rule with an event pattern that<br>matches the creation of new S3 buckets. Program the Lambda function to parse the EventBridge event, check the conguration of the S3<br>buckets from the event, and set AES-256 as the default encryption.<br>D. Congure an IAM policy that denies the s3:CreateBucket action if the s3:x-amz-server-side-encryption condition key has a value that is<br>not AES-256. Create an IAM group for all the company’s IAM users. Associate the IAM policy with the IAM group.</td>
                    <td>一家公司正在使用Amazon S3存储桶来存储重要文档。公司发现一些S3存储桶没有加密。目前，公司的IAM用户可以创建没有加密的新S3存储桶。公司正在实施一项新要求，即所有S3存储桶都必须加密。DevOps工程师必须实施一个解决方案，确保在所有现有S3存储桶和所有新S3存储桶上启用服务器端加密。加密必须在创建新S3存储桶时立即启用。默认加密类型必须是256位高级加密标准(AES-256)。</td>
                    <td>A. 创建一个AWS Lambda函数，由Amazon EventBridge定时规则定期调用。编程Lambda函数扫描所有当前S3存储桶的加密状态，并为任何没有加密配置的S3存储桶设置AES-256作为默认加密。这个方案可以处理现有存储桶，但对新创建的存储桶存在时间延迟，不能确保&quot;立即&quot;启用加密，且定期扫描效率较低。<br><br>B. 设置并激活s3-bucket-server-side-encryption-enabled AWS Config托管规则。配置规则使用AWS-EnableS3BucketEncryption AWS Systems Manager自动化运行手册作为修复操作。手动运行重新评估过程以确保现有S3存储桶合规。这个方案可以处理现有和新的存储桶，但仍然存在检测和修复的时间延迟，不能完全满足&quot;立即&quot;加密的要求。<br><br>C. 创建一个由Amazon EventBridge事件规则调用的AWS Lambda函数。定义规则，使用匹配新S3存储桶创建的事件模式。编程Lambda函数解析EventBridge事件，检查事件中S3存储桶的配置，并设置AES-256作为默认加密。这个方案只处理新创建的存储桶，不能解决现有未加密存储桶的问题。<br><br>D. 配置一个IAM策略，如果s3:x-amz-server-side-encryption条件键的值不是AES-256，则拒绝s3:CreateBucket操作。为公司所有IAM用户创建一个IAM组，并将IAM策略与IAM组关联。这个方案从源头预防问题，确保用户只能创建加密的存储桶，但不能处理现有未加密的存储桶。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>77</td>
                    <td>A DevOps engineer is architecting a continuous development strategy for a company’s software as a service (SaaS) web application running<br>on AWS. For application and security reasons, users subscribing to this application are distributed across multiple Application Load Balancers<br>(ALBs), each of which has a dedicated Auto Scaling group and eet of Amazon EC2 instances. The application does not require a build stage,<br>and when it is committed to AWS CodeCommit, the application must trigger a simultaneous deployment to all ALBs, Auto Scaling groups, and<br>EC2 eets.</td>
                    <td>A. Create a single AWS CodePipeline pipeline that deploys the application in parallel using unique AWS CodeDeploy applications and<br>deployment groups created for each ALB-Auto Scaling group pair.<br>B. Create a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single<br>deployment group.<br>C. Create a single AWS CodePipeline pipeline that deploys the application in parallel using a single AWS CodeDeploy application and<br>unique deployment group for each ALB-Auto Scaling group pair.<br>D. Create an AWS CodePipeline pipeline for each ALB-Auto Scaling group pair that deploys the application using an AWS CodeDeploy<br>application and deployment group created for the same ALB-Auto Scaling group pair.</td>
                    <td>一名DevOps工程师正在为公司运行在AWS上的软件即服务(SaaS) Web应用程序设计持续开发策略。出于应用程序和安全原因，订阅此应用程序的用户分布在多个应用程序负载均衡器(ALB)上，每个ALB都有专用的Auto Scaling组和EC2实例集群。该应用程序不需要构建阶段，当代码提交到AWS CodeCommit时，应用程序必须触发同时部署到所有ALB、Auto Scaling组和EC2集群。</td>
                    <td>选项A：创建单个AWS CodePipeline管道，使用为每个ALB-Auto Scaling组对创建的唯一AWS CodeDeploy应用程序和部署组并行部署应用程序。这种方案会创建多个CodeDeploy应用程序，增加了管理复杂性和资源开销，虽然能实现并行部署，但不是最优解决方案。<br><br>选项B：创建单个AWS CodePipeline管道，使用单个AWS CodeDeploy应用程序和单个部署组部署应用程序。这种方案无法满足题目要求的同时部署到多个不同的ALB-Auto Scaling组对，因为单个部署组无法覆盖多个独立的基础设施组合。<br><br>选项C：创建单个AWS CodePipeline管道，使用单个AWS CodeDeploy应用程序和为每个ALB-Auto Scaling组对创建的唯一部署组并行部署应用程序。这种方案既保持了管理的简洁性（单个应用程序），又通过多个部署组实现了对不同基础设施的精确控制和并行部署。<br><br>选项D：为每个ALB-Auto Scaling组对创建AWS CodePipeline管道，使用为相同ALB-Auto Scaling组对创建的AWS CodeDeploy应用程序和部署组部署应用程序。这种方案会创建多个管道，增加了管理复杂性，且可能导致部署时间不一致。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>78</td>
                    <td>A company is hosting a static website from an Amazon S3 bucket. The website is available to customers at example.com. The company uses<br>an Amazon Route 53 weighted routing policy with a TTL of 1 day. The company has decided to replace the existing static website with a<br>dynamic web application. The dynamic web application uses an Application Load Balancer (ALB) in front of a eet of Amazon EC2 instances.<br>On the day of production launch to customers, the company creates an additional Route 53 weighted DNS record entry that points to the ALB<br>with a weight of 255 and a TTL of 1 hour. Two days later, a DevOps engineer notices that the previous static website is displayed sometimes<br>when customers navigate to example.com.<br>How can the DevOps engineer ensure that the company serves only dynamic content for example.com?</td>
                    <td>A. Delete all objects, including previous versions, from the S3 bucket that contains the static website content.<br>B. <br>C. Congure webpage redirect requests on the S3 bucket with a hostname that redirects to the AL<br>D. Remove the weighted DNS record entry that points to the S3 bucket from the example.com hosted zone. Wait for DNS propagation to<br>become complete.</td>
                    <td>一家公司正在从Amazon S3存储桶托管静态网站。该网站通过example.com向客户提供服务。公司使用Amazon Route 53加权路由策略，TTL为1天。公司决定用动态Web应用程序替换现有的静态网站。动态Web应用程序在一组Amazon EC2实例前使用应用程序负载均衡器(ALB)。在向客户正式发布的当天，公司创建了一个额外的Route 53加权DNS记录条目，指向ALB，权重为255，TTL为1小时。两天后，DevOps工程师注意到当客户访问example.com时，有时仍然显示之前的静态网站。DevOps工程师如何确保公司只为example.com提供动态内容？</td>
                    <td>A. 删除S3存储桶中包含静态网站内容的所有对象，包括以前的版本。<br>这个选项不能解决问题的根本原因。问题在于DNS路由配置，而不是S3存储桶中的内容。即使删除了S3中的内容，Route 53仍然会将部分流量路由到S3存储桶，只是会返回错误页面而不是静态内容。<br><br>B. 选项内容缺失，无法分析。<br><br>C. 在S3存储桶上配置网页重定向请求，使用重定向到ALB的主机名。<br>这个方案可以作为临时解决方案，但不是最佳实践。它仍然需要DNS将流量路由到S3，然后再重定向到ALB，增加了不必要的延迟和复杂性。而且重定向可能会影响SEO和用户体验。<br><br>D. 从example.com托管区域中删除指向S3存储桶的加权DNS记录条目。等待DNS传播完成。<br>这是正确的解决方案。问题的根本原因是仍然存在指向S3存储桶的DNS记录，由于加权路由策略，部分流量仍然被路由到静态网站。删除这个记录后，所有流量都会路由到ALB。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>79</td>
                    <td>A company is implementing AWS CodePipeline to automate its testing process. The company wants to be notied when the execution state<br>fails and used the following custom event pattern in Amazon EventBridge:</td>
                    <td>A. Failed deploy and build actions across all the pipelines<br>B. All rejected or failed approval actions across all the pipelines<br>C. All the events across all pipelines<br>D. Approval actions across all the pipelines</td>
                    <td>一家公司正在实施AWS CodePipeline来自动化其测试流程。该公司希望在执行状态失败时收到通知，并在Amazon EventBridge中使用了以下自定义事件模式：<br><br>（注：题目中似乎缺少了具体的事件模式代码，但根据选项可以推断这是关于CodePipeline事件通知的配置问题）</td>
                    <td>A. 所有管道中失败的部署和构建操作 - 这个选项只涵盖了部署和构建操作的失败，但没有包括审批操作的失败或拒绝情况。在DevOps流程中，审批环节的失败同样重要，因此这个选项覆盖范围不够全面。<br><br>B. 所有管道中被拒绝或失败的审批操作 - 这个选项专门针对审批操作的异常情况，包括被拒绝和失败两种状态。在自动化测试流程中，审批环节是关键的质量控制点，当审批被拒绝或失败时，需要及时通知相关人员进行处理。这符合题目中提到的&quot;执行状态失败&quot;的通知需求。<br><br>C. 所有管道中的所有事件 - 这个选项范围过于宽泛，会产生大量不必要的通知，包括成功的操作也会触发通知，这不符合题目要求的只在失败时通知的需求，会造成通知噪音。<br><br>D. 所有管道中的审批操作 - 这个选项包括了所有审批操作，不仅仅是失败的情况，还包括成功的审批，这同样不符合题目中只在失败时通知的具体要求。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>80</td>
                    <td>An application running on a set of Amazon EC2 instances in an Auto Scaling group requires a conguration le to operate. The instances are<br>created and maintained with AWS CloudFormation. A DevOps engineer wants the instances to have the latest conguration le when<br>launched, and wants changes to the conguration le to be reected on all the instances with a minimal delay when the CloudFormation<br>template is updated. Company policy requires that application conguration les be maintained along with AWS infrastructure conguration<br>les in source control.</td>
                    <td>A. In the CloudFormation template, add an AWS Cong rule. Place the conguration le content in the rule’s InputParameters property, and<br>set the Scope property to the EC2 Auto Scaling group. Add an AWS Systems Manager Resource Data Sync resource to the template to poll<br>for updates to the conguration.<br>B. In the CloudFormation template, add an EC2 launch template resource. Place the conguration le content in the launch template.<br>Congure the cfn-init script to run when the instance is launched, and congure the cfn-hup script to poll for updates to the conguration.<br>C. In the CloudFormation template, add an EC2 launch template resource. Place the conguration le content in the launch template. Add<br>an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the conguration.<br>D. In the CloudFormation template, add CloudFormation init metadata. Place the conguration le content in the metadata. Congure the<br>cfn-init script to run when the instance is launched, and congure the cfn-hup script to poll for updates to the conguration.</td>
                    <td>一个运行在Auto Scaling组中的Amazon EC2实例集合上的应用程序需要一个配置文件来运行。这些实例通过AWS CloudFormation创建和维护。DevOps工程师希望实例在启动时拥有最新的配置文件，并且当CloudFormation模板更新时，配置文件的更改能够以最小的延迟反映到所有实例上。公司政策要求应用程序配置文件与AWS基础设施配置文件一起在源代码控制中维护。</td>
                    <td>选项A：使用AWS Config规则来管理配置文件内容是不正确的方法。AWS Config主要用于监控和评估AWS资源的配置合规性，而不是用于分发应用程序配置文件。Systems Manager Resource Data Sync也不是用于轮询配置更新的正确工具，它主要用于数据同步到S3存储桶。<br><br>选项B：在CloudFormation模板中添加EC2启动模板资源，将配置文件内容放在启动模板中。配置cfn-init脚本在实例启动时运行，配置cfn-hup脚本轮询配置更新。这是正确的方法，因为cfn-init可以在实例启动时获取最新配置，cfn-hup可以持续监控CloudFormation元数据的变化并自动更新配置。<br><br>选项C：虽然使用启动模板是正确的，但Systems Manager Resource Data Sync不是用于轮询配置更新的合适工具。这个服务主要用于将数据同步到S3，而不是实时监控配置变化。<br><br>选项D：使用CloudFormation init元数据是一个可行的方法，但将配置放在启动模板中比放在元数据中更加灵活和易于管理。启动模板提供了更好的版本控制和管理能力。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>81</td>
                    <td>A company manages an application that stores logs in Amazon CloudWatch Logs. The company wants to archive the logs to an Amazon S3<br>bucket. Logs are rarely accessed after 90 days and must be retained for 10 years.</td>
                    <td>A. Congure a CloudWatch Logs subscription lter to use AWS Glue to transfer all logs to an S3 bucket.<br>B. Congure a CloudWatch Logs subscription lter to use Amazon Kinesis Data Firehose to stream all logs to an S3 bucket.<br>C. Congure a CloudWatch Logs subscription lter to stream all logs to an S3 bucket.<br>D. Congure the S3 bucket lifecycle policy to transition logs to S3 Glacier after 90 days and to expire logs after 3.650 days.<br>E. Congure the S3 bucket lifecycle policy to transition logs to Reduced Redundancy after 90 days and to expire logs after 3.650 days.</td>
                    <td>一家公司管理一个将日志存储在Amazon CloudWatch Logs中的应用程序。该公司希望将日志归档到Amazon S3存储桶中。日志在90天后很少被访问，并且必须保留10年。</td>
                    <td>A. 配置CloudWatch Logs订阅过滤器使用AWS Glue将所有日志传输到S3存储桶 - 这个选项在技术上是可行的，但AWS Glue主要用于数据转换和ETL作业，对于简单的日志流传输来说过于复杂且成本较高，不是最佳实践。<br><br>B. 配置CloudWatch Logs订阅过滤器使用Amazon Kinesis Data Firehose将所有日志流式传输到S3存储桶 - 这是正确的选项。Kinesis Data Firehose是专门设计用于将流数据可靠地传输到S3的服务，支持数据压缩、格式转换，并且可以自动处理传输失败的重试，是将CloudWatch Logs数据传输到S3的推荐方式。<br><br>C. 配置CloudWatch Logs订阅过滤器直接将所有日志流式传输到S3存储桶 - 这个选项不正确，因为CloudWatch Logs订阅过滤器不能直接流式传输到S3，需要通过中间服务如Kinesis Data Firehose或Lambda函数。<br><br>D. 配置S3存储桶生命周期策略，在90天后将日志转换到S3 Glacier，并在3650天后使日志过期 - 这是正确的选项。根据需求，日志在90天后很少访问，转换到成本更低的Glacier存储类别是合理的，10年（3650天）后过期符合保留要求。<br><br>E. 配置S3存储桶生命周期策略，在90天后将日志转换到降低冗余存储，并在3650天后使日志过期 - 这个选项不正确，因为Reduced Redundancy Storage (RRS)已被AWS弃用，且其成本实际上比标准存储更高，不适合长期归档。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>82</td>
                    <td>A company is developing a new application. The application uses AWS Lambda functions for its compute tier. The company must use a canary<br>deployment for any changes to the Lambda functions. Automated rollback must occur if any failures are reported.<br>The company’s DevOps team needs to create the infrastructure as code (IaC) and the CI/CD pipeline for this solution.</td>
                    <td>A. Create an AWS CloudFormation template for the application. Dene each Lambda function in the template by using the<br>AWS::Lambda::Function resource type. In the template, include a version for the Lambda function by using the AWS::Lambda::Version<br>resource type. Declare the CodeSha256 property. Congure an AWS::Lambda::Alias resource that references the latest version of the<br>Lambda function.<br>B. Create an AWS Serverless Application Model (AWS SAM) template for the application. Dene each Lambda function in the template by<br>using the AWS::Serverless::Function resource type. For each function, include congurations for the AutoPublishAlias property and the<br>DeploymentPreference property. Congure the deployment conguration type to LambdaCanary10Percent10Minutes.<br>C. Create an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage<br>that starts the pipeline. Create an AWS CodeBuild project to deploy the AWS Serverless Application Model (AWS SAM) template. Upload<br>the template and source code to the CodeCommit repository. In the CodeCommit repository, create a buildspec.yml le that includes the<br>commands to build and deploy the SAM application.<br>D. Create an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage<br>that starts the pipeline. Create an AWS CodeDeploy deployment group that is congured for canary deployments with a<br>DeploymentPreference type of Canary10Percent10Minutes. Upload the AWS CloudFormation template and source code to the<br>CodeCommit repository. In the CodeCommit repository, create an appspec.yml le that includes the commands to deploy the<br>CloudFormation template.<br>E. Create an Amazon CloudWatch composite alarm for all the Lambda functions. Congure an evaluation period and dimensions for<br>Lambda. Congure the alarm to enter the ALARM state if any errors are detected or if there is insucient data.<br>F. Create an Amazon CloudWatch alarm for each Lambda function. Congure the alarms to enter the ALARM state if any errors are<br>detected. Congure an evaluation period, dimensions for each Lambda function and version, and the namespace as AWS/Lambda on the<br>Errors metric.</td>
                    <td>一家公司正在开发一个新应用程序。该应用程序使用AWS Lambda函数作为其计算层。公司必须对Lambda函数的任何更改使用金丝雀部署。如果报告任何故障，必须进行自动回滚。公司的DevOps团队需要为此解决方案创建基础设施即代码(IaC)和CI/CD管道。</td>
                    <td>选项A：创建AWS CloudFormation模板，使用AWS::Lambda::Function资源类型定义Lambda函数，包含版本和别名配置。这个选项只提供了基础的Lambda函数定义，但没有提供金丝雀部署的具体配置，也没有自动回滚机制，不能满足题目要求。<br><br>选项B：创建AWS SAM模板，使用AWS::Serverless::Function资源类型定义Lambda函数，配置AutoPublishAlias属性和DeploymentPreference属性，设置部署配置类型为LambdaCanary10Percent10Minutes。这个选项完美地满足了金丝雀部署的要求，SAM的DeploymentPreference功能内置了自动回滚机制。<br><br>选项C：创建CodeCommit仓库和CodePipeline管道，使用CodeBuild项目部署SAM模板，创建buildspec.yml文件。这个选项提供了完整的CI/CD管道配置，与选项B的SAM模板配合使用，可以实现自动化的金丝雀部署流程。<br><br>选项D：使用CodeDeploy部署组配置金丝雀部署。虽然CodeDeploy支持金丝雀部署，但它主要用于EC2和本地服务器部署，对于Lambda函数部署，SAM是更合适的选择。<br><br>选项E：创建复合CloudWatch告警。虽然监控很重要，但复合告警可能过于复杂，且&quot;数据不足&quot;状态可能导致误报。<br><br>选项F：为每个Lambda函数创建CloudWatch告警，配置错误检测。这个选项提供了精确的监控配置，对于自动回滚机制是必要的，可以准确检测函数错误并触发回滚。</td>
                    <td>BC</td>
                </tr>
                <tr>
                    <td>83</td>
                    <td>A DevOps engineer is deploying a new version of a company’s application in an AWS CodeDeploy deployment group associated with its<br>Amazon EC2 instances. After some time, the deployment fails. The engineer realizes that all the events associated with the specic<br>deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group.</td>
                    <td>A. The networking conguration does not allow the EC2 instances to reach the internet via a NAT gateway or internet gateway, and the<br>CodeDeploy endpoint cannot be reached.<br>B. The IAM user who triggered the application deployment does not have permission to interact with the CodeDeploy endpoint.<br>C. The target EC2 instances were not properly registered with the CodeDeploy endpoint.<br>D. An instance prole with proper permissions was not attached to the target EC2 instances.<br>E. The appspec.yml le was not included in the application revision.</td>
                    <td>一名DevOps工程师正在使用AWS CodeDeploy部署组为公司应用程序部署新版本，该部署组与Amazon EC2实例相关联。一段时间后，部署失败了。工程师意识到与特定部署ID相关的所有事件都处于&quot;跳过&quot;状态，代码没有部署到与部署组关联的实例上。</td>
                    <td>A. 网络配置不允许EC2实例通过NAT网关或互联网网关访问互联网，无法到达CodeDeploy端点。这是一个合理的原因，因为EC2实例需要能够访问CodeDeploy服务端点来接收部署指令和下载应用程序包。如果网络连接有问题，实例无法与CodeDeploy通信，会导致部署事件被跳过。<br><br>B. 触发应用程序部署的IAM用户没有与CodeDeploy端点交互的权限。这个选项不太可能是正确答案，因为如果IAM用户没有权限，部署根本不会开始，而不是显示为&quot;跳过&quot;状态。<br><br>C. 目标EC2实例没有正确注册到CodeDeploy端点。这个说法不准确，EC2实例不需要&quot;注册&quot;到CodeDeploy端点，而是通过部署组进行管理。<br><br>D. 没有为目标EC2实例附加具有适当权限的实例配置文件。这是一个重要原因，EC2实例需要IAM实例配置文件来访问CodeDeploy服务、下载S3中的应用程序包等操作。没有适当的权限会导致部署被跳过。<br><br>E. 应用程序修订版本中没有包含appspec.yml文件。虽然这会导致部署失败，但通常会产生错误消息而不是跳过状态。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>84</td>
                    <td>A company has a guideline that every Amazon EC2 instance must be launched from an AMI that the company’s security team produces. Every<br>month, the security team sends an email message with the latest approved AMIs to all the development teams.<br>The development teams use AWS CloudFormation to deploy their applications. When developers launch a new service, they have to search<br>their email for the latest AMIs that the security department sent. A DevOps engineer wants to automate the process that the security team<br>uses to provide the AMI IDs to the development teams.</td>
                    <td>A. Direct the security team to use CloudFormation to create new versions of the AMIs and to list the AMI ARNs in an encrypted Amazon S3<br>object as part of the stack’s Outputs section. Instruct the developers to use a cross-stack reference to load the encrypted S3 object and<br>obtain the most recent AMI ARNs.<br>B. Direct the security team to use a CloudFormation stack to create an AWS CodePipeline pipeline that builds new AMIs and places the<br>latest AMI ARNs in an encrypted Amazon S3 object as part of the pipeline output. Instruct the developers to use a cross-stack reference<br>within their own CloudFormation template to obtain the S3 object location and the most recent AMI ARNs.<br>C. Direct the security team to use Amazon EC2 Image Builder to create new AMIs and to place the AMI ARNs as parameters in AWS<br>Systems Manager Parameter Store. Instruct the developers to specify a parameter of type SSM in their CloudFormation stack to obtain<br>the most recent AMI ARNs from Parameter Store.<br>D. Direct the security team to use Amazon EC2 Image Builder to create new AMIs and to create an Amazon Simple Notication Service<br>(Amazon SNS) topic so that every development team can receive notications. When the development teams receive a notication,<br>instruct them to write an AWS Lambda function that will update their CloudFormation stack with the most recent AMI ARNs.</td>
                    <td>一家公司有一个准则，即每个Amazon EC2实例都必须从公司安全团队制作的AMI启动。每个月，安全团队会向所有开发团队发送包含最新批准AMI的电子邮件。开发团队使用AWS CloudFormation来部署他们的应用程序。当开发人员启动新服务时，他们必须在邮件中搜索安全部门发送的最新AMI。一名DevOps工程师希望自动化安全团队向开发团队提供AMI ID的流程。</td>
                    <td>选项A：指导安全团队使用CloudFormation创建新版本的AMI，并将AMI ARN列在加密的Amazon S3对象中作为堆栈输出部分的一部分。指导开发人员使用跨堆栈引用来加载加密的S3对象并获取最新的AMI ARN。这个方案存在问题，因为CloudFormation本身不是用来创建AMI的最佳工具，而且跨堆栈引用S3对象的方式比较复杂，不够直接。<br><br>选项B：指导安全团队使用CloudFormation堆栈创建AWS CodePipeline管道，该管道构建新的AMI并将最新的AMI ARN放在加密的Amazon S3对象中作为管道输出。指导开发人员在自己的CloudFormation模板中使用跨堆栈引用来获取S3对象位置和最新的AMI ARN。虽然使用CodePipeline构建AMI是可行的，但通过S3对象和跨堆栈引用的方式仍然比较复杂。<br><br>选项C：指导安全团队使用Amazon EC2 Image Builder创建新的AMI，并将AMI ARN作为参数放在AWS Systems Manager Parameter Store中。指导开发人员在CloudFormation堆栈中指定SSM类型的参数，从Parameter Store获取最新的AMI ARN。这是最优雅的解决方案，EC2 Image Builder专门用于创建和管理AMI，Parameter Store提供了集中化的参数管理，CloudFormation原生支持SSM参数类型。<br><br>选项D：指导安全团队使用Amazon EC2 Image Builder创建新的AMI，并创建Amazon SNS主题，以便每个开发团队都能收到通知。当开发团队收到通知时，指导他们编写AWS Lambda函数来更新CloudFormation堆栈中的最新AMI ARN。这种方案增加了不必要的复杂性，需要额外的Lambda函数和手动干预。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>85</td>
                    <td>An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). A DevOps engineer is using AWS CodeDeploy to<br>release a new version. The deployment fails during the AllowTrac lifecycle event, but a cause for the failure is not indicated in the<br>deployment logs.</td>
                    <td>A. The appspec.yml le contains an invalid script that runs in the AllowTrac lifecycle hook.<br>B. <br>C. The health checks specied for the ALB target group are miscongured.<br>D. The CodeDeploy agent was not installed in the EC2 instances that are part of the ALB target group.</td>
                    <td>一个应用程序运行在应用负载均衡器(ALB)后面的Amazon EC2实例上。DevOps工程师正在使用AWS CodeDeploy发布新版本。部署在AllowTraffic生命周期事件期间失败，但部署日志中没有显示失败的原因。</td>
                    <td>A. appspec.yml文件包含在AllowTraffic生命周期钩子中运行的无效脚本。<br>这个选项不太可能，因为如果appspec.yml文件中的脚本有问题，通常会在部署日志中显示具体的错误信息，比如脚本执行失败的详细原因。题目明确提到日志中没有显示失败原因，所以这个选项不符合情况。<br><br>B. 选项B缺失内容，无法进行分析。<br><br>C. 为ALB目标组指定的健康检查配置错误。<br>这是最可能的原因。在CodeDeploy的AllowTraffic阶段，系统会将流量重新路由到新部署的实例。如果ALB的健康检查配置不正确（比如健康检查路径错误、端口配置错误、超时时间设置不当等），ALB会认为新实例不健康，导致AllowTraffic事件失败。由于这是ALB层面的问题，CodeDeploy的部署日志可能不会显示具体的失败原因。<br><br>D. CodeDeploy代理没有安装在作为ALB目标组一部分的EC2实例上。<br>如果CodeDeploy代理没有安装，部署会在更早的阶段失败，而不是在AllowTraffic阶段。而且这种情况下，部署日志通常会明确显示代理连接问题或通信失败的错误信息。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>86</td>
                    <td>A company has 20 service teams. Each service team is responsible for its own microservice. Each service team uses a separate AWS account<br>for its microservice and a VPC with the 192.168.0.0/22 CIDR block. The company manages the AWS accounts with AWS Organizations.<br>Each service team hosts its microservice on multiple Amazon EC2 instances behind an Application Load Balancer. The microservices<br>communicate with each other across the public internet. The company’s security team has issued a new guideline that all communication<br>between microservices must use HTTPS over private network connections and cannot traverse the public internet.<br>A DevOps engineer must implement a solution that fullls these obligations and minimizes the number of changes for each service team.</td>
                    <td>A. Create a new AWS account in AWS Organizations. Create a VPC in this account, and use AWS Resource Access Manager to share the<br>private subnets of this VPC with the organization. Instruct the service teams to launch a new Network Load Balancer (NLB) and EC2<br>instances that use the shared private subnets. Use the NLB DNS names for communication between microservices.<br>B. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use AWS PrivateLink to create VPC endpoints in each AWS<br>account for the NLBs. Create subscriptions to each VPC endpoint in each of the other AWS accounts. Use the VPC endpoint DNS names<br>for communication between microservices.<br>C. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Create VPC peering connections between each of the<br>microservice VPCs. Update the route tables for each VPC to use the peering links. Use the NLB DNS names for communication between<br>microservices.<br>D. Create a new AWS account in AWS Organizations. Create a transit gateway in this account, and use AWS Resource Access Manager to<br>share the transit gateway with the organization. In each of the microservice VPCs, create a transit gateway attachment to the shared<br>transit gateway. Update the route tables of each VPC to use the transit gateway. Create a Network Load Balancer (NLB) in each of the<br>microservice VPCs. Use the NLB DNS names for communication between microservices.</td>
                    <td>一家公司有20个服务团队。每个服务团队负责自己的微服务。每个服务团队为其微服务使用单独的AWS账户和一个使用19168.0.0/22 CIDR块的VPC。公司使用AWS Organizations管理AWS账户。每个服务团队在应用负载均衡器后面的多个Amazon EC2实例上托管其微服务。微服务之间通过公共互联网相互通信。公司的安全团队发布了新的指导原则，要求微服务之间的所有通信必须在私有网络连接上使用HTTPS，不能通过公共互联网传输。DevOps工程师必须实施一个满足这些要求并最小化每个服务团队更改数量的解决方案。</td>
                    <td>选项A：在AWS Organizations中创建新账户，创建VPC并使用AWS Resource Access Manager共享私有子网，要求服务团队启动新的网络负载均衡器和EC2实例。这种方案需要服务团队重新部署基础设施，变更较大，不符合最小化更改的要求。<br><br>选项B：在每个微服务VPC中创建网络负载均衡器，使用AWS PrivateLink为NLB创建VPC端点，在其他账户中创建对每个VPC端点的订阅。这种方案通过PrivateLink实现私有连接，无需重新部署现有基础设施，只需添加NLB和VPC端点，变更最小。<br><br>选项C：在每个微服务VPC中创建网络负载均衡器，在微服务VPC之间创建VPC对等连接。由于有20个服务团队，需要创建大量的对等连接（最多190个连接），管理复杂度极高，不现实。<br><br>选项D：创建新账户和传输网关，使用Resource Access Manager共享，在每个VPC中创建传输网关附件。虽然技术可行，但需要额外的账户和传输网关成本，且配置相对复杂。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>87</td>
                    <td>An Amazon EC2 instance is running in a VPC and needs to download an object from a restricted Amazon S3 bucket. When the DevOps<br>engineer tries to download the object, an AccessDenied error is received.</td>
                    <td>A. The S3 bucket default encryption is enabled.<br>B. There is an error in the S3 bucket policy.<br>C. The object has been moved to S3 Glacier.<br>D. There is an error in the IAM role conguration.<br>E. S3 Versioning is enabled.</td>
                    <td>一个Amazon EC2实例运行在VPC中，需要从一个受限制的Amazon S3存储桶下载对象。当DevOps工程师尝试下载对象时，收到了AccessDenied错误。</td>
                    <td>A. S3存储桶默认加密已启用 - 这个选项不正确。S3存储桶启用默认加密不会导致AccessDenied错误。加密是在服务器端透明处理的，如果有适当的权限，加密的对象仍然可以正常访问和下载。加密功能主要影响数据的存储安全性，而不是访问权限。<br><br>B. S3存储桶策略存在错误 - 这个选项是正确的。S3存储桶策略是控制对存储桶和其中对象访问权限的重要机制。如果存储桶策略配置错误，比如明确拒绝了某些操作或者没有正确授予读取权限，就会导致AccessDenied错误。存储桶策略可能包含错误的条件、错误的主体或者过于严格的限制。<br><br>C. 对象已被移动到S3 Glacier - 这个选项不正确。当对象存储在Glacier中时，通常不会返回AccessDenied错误，而是会返回其他类型的错误，比如需要先恢复对象的提示。Glacier存储类别的对象需要先进行恢复操作才能访问，但这不会产生AccessDenied错误。<br><br>D. IAM角色配置存在错误 - 这个选项是正确的。EC2实例通常通过附加的IAM角色来获取访问S3的权限。如果IAM角色没有正确配置，比如缺少必要的S3权限（如s3:GetObject），或者角色的信任策略配置错误，就会导致AccessDenied错误。这是导致此类错误的最常见原因之一。<br><br>E. S3版本控制已启用 - 这个选项不正确。启用S3版本控制本身不会导致AccessDenied错误。版本控制是S3的一个功能特性，用于保存对象的多个版本，它不会影响基本的访问权限。即使启用了版本控制，只要权限配置正确，仍然可以正常访问对象。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>88</td>
                    <td>A company wants to use a grid system for a proprietary enterprise in-memory data store on top of AWS. This system can run in multiple server<br>nodes in any Linux-based distribution. The system must be able to recongure the entire cluster every time a node is added or removed.<br>When adding or removing nodes, an /etc/cluster/nodes.cong le must be updated, listing the IP addresses of the current node members of<br>that cluster.<br>The company wants to automate the task of adding new nodes to a cluster.</td>
                    <td>A. Use AWS OpsWorks Stacks to layer the server nodes of that cluster. Create a Chef recipe that populates the content of the<br>/etc/cluster/nodes.cong le and restarts the service by using the current members of the layer. Assign that recipe to the Congure<br>lifecycle event.<br>B. Put the le nodes.cong in version control. Create an AWS CodeDeploy deployment conguration and deployment group based on an<br>Amazon EC2 tag value for the cluster nodes. When adding a new node to the cluster, update the le with all tagged instances, and make a<br>commit in version control. Deploy the new le and restart the services.<br>C. Create an Amazon S3 bucket and upload a version of the /etc/cluster/nodes.cong le. Create a crontab script that will poll for that S3<br>le and download it frequently. Use a process manager, such as Monit or systemd, to restart the cluster services when it detects that the<br>new le was modied. When adding a node to the cluster, edit the le’s most recent members. Upload the new le to the S3 bucket.<br>D. Create a user data script that lists all members of the current security group of the cluster and automatically updates the<br>/etc/cluster/nodes.cong le whenever a new instance is added to the cluster.</td>
                    <td>一家公司希望在AWS上为专有企业内存数据存储使用网格系统。该系统可以在任何基于Linux的发行版的多个服务器节点上运行。系统必须能够在每次添加或删除节点时重新配置整个集群。当添加或删除节点时，必须更新/etc/cluster/nodes.config文件，列出该集群当前节点成员的IP地址。公司希望自动化向集群添加新节点的任务。</td>
                    <td>选项A：使用AWS OpsWorks Stacks对集群的服务器节点进行分层。创建一个Chef配方，使用层的当前成员填充/etc/cluster/nodes.config文件的内容并重启服务。将该配方分配给Configure生命周期事件。这个方案非常适合，因为OpsWorks可以自动检测层中节点的变化，Configure事件会在节点添加或删除时自动触发，Chef配方可以动态获取当前层中所有实例的信息并更新配置文件。<br><br>选项B：将nodes.config文件放入版本控制。基于集群节点的Amazon EC2标签值创建AWS CodeDeploy部署配置和部署组。向集群添加新节点时，用所有标记的实例更新文件，并在版本控制中提交。部署新文件并重启服务。这个方案需要手动更新文件和提交，不够自动化，每次添加节点都需要人工干预。<br><br>选项C：创建Amazon S3存储桶并上传/etc/cluster/nodes.config文件的版本。创建crontab脚本定期轮询和下载S3文件。使用进程管理器如Monit或systemd在检测到新文件被修改时重启集群服务。添加节点时编辑文件的最新成员并上传到S3。这个方案依赖轮询机制，实时性差，且需要手动更新S3中的文件。<br><br>选项D：创建用户数据脚本，列出集群当前安全组的所有成员，并在新实例添加到集群时自动更新/etc/cluster/nodes.config文件。这个方案有一定的自动化程度，但用户数据脚本只在实例启动时执行一次，无法持续监控集群变化。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>89</td>
                    <td>A DevOps engineer is working on a data archival project that requires the migration of on-premises data to an Amazon S3 bucket. The DevOps<br>engineer develops a script that incrementally archives on-premises data that is older than 1 month to Amazon S3. Data that is transferred to<br>Amazon S3 is deleted from the on-premises location. The script uses the S3 PutObject operation.<br>During a code review, the DevOps engineer notices that the script does not verify whether the data was successfully copied to Amazon S3. The<br>DevOps engineer must update the script to ensure that data is not corrupted during transmission. The script must use MD5 checksums to<br>verify data integrity before the on-premises data is deleted.</td>
                    <td>A. Check the returned response for the VersionId. Compare the returned VersionId against the MD5 checksum.<br>B. Include the MD5 checksum within the Content-MD5 parameter. Check the operation call’s return status to nd out if an error was<br>returned.<br>C. Include the checksum digest within the tagging parameter as a URL query parameter.<br>D. Check the returned response for the ETag. Compare the returned ETag against the MD5 checksum.<br>E. Include the checksum digest within the Metadata parameter as a name-value pair. After upload, use the S3 HeadObject operation to<br>retrieve metadata from the object.</td>
                    <td>一名DevOps工程师正在进行数据归档项目，需要将本地数据迁移到Amazon S3存储桶。该DevOps工程师开发了一个脚本，用于增量归档超过1个月的本地数据到Amazon S3。传输到Amazon S3的数据会从本地位置删除。该脚本使用S3 PutObject操作。<br>在代码审查期间，DevOps工程师注意到脚本没有验证数据是否成功复制到Amazon S3。DevOps工程师必须更新脚本以确保数据在传输过程中不会损坏。脚本必须使用MD5校验和来验证数据完整性，然后才能删除本地数据。</td>
                    <td>A. 检查返回响应中的VersionId，将返回的VersionId与MD5校验和进行比较。这个选项是错误的，因为VersionId是用于版本控制的标识符，与MD5校验和没有直接关系，无法用于验证数据完整性。<br><br>B. 在Content-MD5参数中包含MD5校验和，检查操作调用的返回状态以确定是否返回错误。这是正确的方法，Content-MD5参数允许客户端提供预期的MD5校验和，S3会在接收数据后验证校验和是否匹配，如果不匹配会返回错误。<br><br>C. 将校验和摘要作为URL查询参数包含在标签参数中。这个选项是错误的，标签参数不是用于数据完整性验证的，而且将校验和放在标签中也无法实现自动验证功能。<br><br>D. 检查返回响应中的ETag，将返回的ETag与MD5校验和进行比较。这是正确的方法，对于简单上传（非多部分上传），ETag通常就是对象的MD5校验和，可以用来验证数据完整性。<br><br>E. 将校验和摘要作为名称-值对包含在Metadata参数中，上传后使用S3 HeadObject操作从对象中检索元数据。这个方法虽然可以存储校验和，但需要额外的API调用，而且不能提供传输过程中的自动验证。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>90</td>
                    <td>A company deploys updates to its Amazon API Gateway API several times a week by using an AWS CodePipeline pipeline. As part of the<br>update process, the company exports the JavaScript SDK for the API from the API Gateway console and uploads the SDK to an Amazon S3<br>bucket.<br>The company has congured an Amazon CloudFront distribution that uses the S3 bucket as an origin. Web clients then download the SDK by<br>using the CloudFront distribution’s endpoint. A DevOps engineer needs to implement a solution to make the new SDK available automatically<br>during new API deployments.</td>
                    <td>A. Create a CodePipeline action immediately after the deployment stage of the API. Congure the action to invoke an AWS Lambda<br>function. Congure the Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and create a<br>CloudFront invalidation for the SDK path.<br>B. Create a CodePipeline action immediately after the deployment stage of the API. Congure the action to use the CodePipeline<br>integration with API Gateway to export the SDK to Amazon S3. Create another action that uses the CodePipeline integration with Amazon<br>S3 to invalidate the cache for the SDK path.<br>C. Create an Amazon EventBridge rule that reacts to UpdateStage events from aws.apigateway. Congure the rule to invoke an AWS<br>Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the CloudFront API to create an<br>invalidation for the SDK path.<br>D. Create an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Congure the rule to invoke an AWS<br>Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the S3 API to invalidate the cache for<br>the SDK path.</td>
                    <td>一家公司每周多次使用AWS CodePipeline管道向其Amazon API Gateway API部署更新。作为更新过程的一部分，公司从API Gateway控制台导出API的JavaScript SDK，并将SDK上传到Amazon S3存储桶。<br><br>公司已配置了一个Amazon CloudFront分发，该分发使用S3存储桶作为源。然后Web客户端通过CloudFront分发的端点下载SDK。DevOps工程师需要实现一个解决方案，在新的API部署期间自动使新的SDK可用。</td>
                    <td>A. 在API部署阶段之后立即创建一个CodePipeline操作。配置该操作调用AWS Lambda函数。配置Lambda函数从API Gateway下载SDK，将SDK上传到S3存储桶，并为SDK路径创建CloudFront失效。这个选项是正确的，因为它直接集成在CodePipeline中，确保在API部署后立即执行SDK更新，并且正确地处理了CloudFront缓存失效。<br><br>B. 在API部署阶段之后立即创建一个CodePipeline操作。配置该操作使用CodePipeline与API Gateway的集成将SDK导出到Amazon S3。创建另一个使用CodePipeline与Amazon S3集成的操作来使SDK路径的缓存失效。这个选项有问题，因为CodePipeline没有直接的API Gateway SDK导出集成，也没有S3缓存失效集成。<br><br>C. 创建一个Amazon EventBridge规则，响应来自aws.apigateway的UpdateStage事件。配置规则调用AWS Lambda函数从API Gateway下载SDK，将SDK上传到S3存储桶，并调用CloudFront API为SDK路径创建失效。这个选项使用了错误的事件类型，UpdateStage不是部署完成的正确指示。<br><br>D. 创建一个Amazon EventBridge规则，响应来自aws.apigateway的CreateDeployment事件。配置规则调用AWS Lambda函数从API Gateway下载SDK，将SDK上传到S3存储桶，并调用S3 API使SDK路径的缓存失效。这个选项有两个问题：使用了错误的API（S3 API不能使CloudFront缓存失效），且EventBridge方法不如直接集成在CodePipeline中可靠。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>91</td>
                    <td>A company has developed an AWS Lambda function that handles orders received through an API. The company is using AWS CodeDeploy to<br>deploy the Lambda function as the nal stage of a CI/CD pipeline.<br>A DevOps engineer has noticed there are intermittent failures of the ordering API for a few seconds after deployment. After some<br>investigation, the DevOps engineer believes the failures are due to database changes not having fully propagated before the Lambda function<br>is invoked.<br>How should the DevOps engineer overcome this?</td>
                    <td>A. Add a BeforeAllowTrac hook to the AppSpec le that tests and waits for any necessary database changes before trac can ow to the<br>new version of the Lambda function.<br>B. Add an AfterAllowTrac hook to the AppSpec le that forces trac to wait for any pending database changes before allowing the new<br>version of the Lambda function to respond.<br>C. Add a BeforeInstall hook to the AppSpec le that tests and waits for any necessary database changes before deploying the new version<br>of the Lambda function.<br>D. Add a ValidateService hook to the AppSpec le that inspects incoming trac and rejects the payload if dependent services, such as the<br>database, are not yet ready.</td>
                    <td>一家公司开发了一个AWS Lambda函数来处理通过API接收的订单。该公司使用AWS CodeDeploy在CI/CD管道的最后阶段部署Lambda函数。<br>一名DevOps工程师注意到在部署后的几秒钟内，订单API会出现间歇性故障。经过一些调查，DevOps工程师认为故障是由于在Lambda函数被调用之前，数据库更改尚未完全传播导致的。<br>DevOps工程师应该如何解决这个问题？</td>
                    <td>A. 在AppSpec文件中添加BeforeAllowTraffic钩子，该钩子测试并等待任何必要的数据库更改完成，然后才允许流量流向Lambda函数的新版本。这个选项是正确的，因为BeforeAllowTraffic钩子会在允许流量路由到新版本之前执行，可以确保数据库更改已经完全传播，从而避免间歇性故障。<br><br>B. 在AppSpec文件中添加AfterAllowTraffic钩子，强制流量等待任何待处理的数据库更改，然后才允许Lambda函数的新版本响应。这个选项不正确，因为AfterAllowTraffic钩子在流量已经被允许后执行，此时如果数据库还未准备好，仍然会出现故障。<br><br>C. 在AppSpec文件中添加BeforeInstall钩子，在部署Lambda函数新版本之前测试并等待任何必要的数据库更改。这个选项不够理想，因为BeforeInstall钩子在安装阶段执行，但Lambda函数可能在安装完成后立即接收流量，仍可能出现数据库未同步的问题。<br><br>D. 在AppSpec文件中添加ValidateService钩子，检查传入的流量并在依赖服务（如数据库）尚未准备就绪时拒绝负载。这个选项不正确，因为ValidateService钩子主要用于验证服务健康状况，而不是等待数据库同步完成。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>92</td>
                    <td>A company uses a single AWS account to test applications on Amazon EC2 instances. The company has turned on AWS Cong in the AWS<br>account and has activated the restricted-ssh AWS Cong managed rule.<br>The company needs an automated monitoring solution that will provide a customized notication in real time if any security group in the<br>account is not compliant with the restricted-ssh rule. The customized notication must contain the name and ID of the noncompliant security<br>group.<br>A DevOps engineer creates an Amazon Simple Notication Service (Amazon SNS) topic in the account and subscribes the appropriate<br>personnel to the topic.</td>
                    <td>A. Create an Amazon EventBridge rule that matches an AWS Cong evaluation result of NON_COMPLIANT for the restricted-ssh rule.<br>Congure an input transformer for the EventBridge rule. Congure the EventBridge rule to publish a notication to the SNS topic.<br>B. Congure AWS Cong to send all evaluation results for the restricted-ssh rule to the SNS topic. Congure a lter policy on the SNS topic<br>to send only notications that contain the text of NON_COMPLIANT in the notication to subscribers.<br>C. Create an Amazon EventBridge rule that matches an AWS Cong evaluation result of NON_COMPLIANT for the restricted-ssh rule.<br>Congure the EventBridge rule to invoke AWS Systems Manager Run Command on the SNS topic to customize a notication and to publish<br>the notication to the SNS topic.<br>D. Create an Amazon EventBridge rule that matches all AWS Cong evaluation results of NON_COMPLIANT. Congure an input transformer<br>for the restricted-ssh rule. Congure the EventBridge rule to publish a notication to the SNS topic.</td>
                    <td>一家公司使用单个AWS账户在Amazon EC2实例上测试应用程序。该公司已在AWS账户中启用了AWS Config，并激活了restricted-ssh AWS Config托管规则。<br>该公司需要一个自动化监控解决方案，当账户中任何安全组不符合restricted-ssh规则时，能够实时提供定制化通知。定制化通知必须包含不合规安全组的名称和ID。<br>一名DevOps工程师在账户中创建了Amazon Simple Notification Service (Amazon SNS)主题，并让相关人员订阅了该主题。</td>
                    <td>选项A：创建Amazon EventBridge规则匹配restricted-ssh规则的NON_COMPLIANT评估结果。为EventBridge规则配置输入转换器。配置EventBridge规则向SNS主题发布通知。这个方案完全符合要求，EventBridge可以精确匹配特定规则的不合规结果，输入转换器可以定制通知内容包含安全组名称和ID，然后直接发布到SNS主题。<br><br>选项B：配置AWS Config将restricted-ssh规则的所有评估结果发送到SNS主题。在SNS主题上配置过滤策略，仅向订阅者发送包含NON_COMPLIANT文本的通知。这个方案会发送所有评估结果（包括合规的），虽然有过滤但效率较低，且无法很好地定制通知内容格式。<br><br>选项C：创建EventBridge规则匹配restricted-ssh规则的NON_COMPLIANT评估结果。配置EventBridge规则调用AWS Systems Manager Run Command在SNS主题上定制通知并发布到SNS主题。这个方案过于复杂，使用Run Command是不必要的，且可能存在权限和执行复杂性问题。<br><br>选项D：创建EventBridge规则匹配所有NON_COMPLIANT的AWS Config评估结果。为restricted-ssh规则配置输入转换器。配置EventBridge规则向SNS主题发布通知。这个方案会匹配所有规则的不合规结果，不仅仅是restricted-ssh规则，不符合题目要求的精确性。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>93</td>
                    <td>A company requires an RPO of 2 hours and an RTO of 10 minutes for its data and application at all times. An application uses a MySQL<br>database and Amazon EC2 web servers. The development team needs a strategy for failover and disaster recovery.</td>
                    <td>A. Create an Amazon Aurora cluster in one Availability Zone across multiple Regions as the data store. Use Aurora’s automatic recovery<br>capabilities in the event of a disaster.<br>B. Create an Amazon Aurora global database in two Regions as the data store. In the event of a failure, promote the secondary Region as<br>the primary for the application.<br>C. Create an Amazon Aurora multi-master cluster across multiple Regions as the data store. Use a Network Load Balancer to balance the<br>database trac in different Regions.<br>D. Set up the application in two Regions and use Amazon Route 53 failover-based routing that points to the Application Load Balancers in<br>both Regions. Use health checks to determine the availability in a given Region. Use Auto Scaling groups in each Region to adjust capacity<br>based on demand.<br>E. Set up the application in two Regions and use a multi-Region Auto Scaling group behind Application Load Balancers to manage the<br>capacity based on demand. In the event of a disaster, adjust the Auto Scaling group’s desired instance count to increase baseline capacity<br>in the failover Region.</td>
                    <td>一家公司要求其数据和应用程序在任何时候都要达到2小时的RPO（恢复点目标）和10分钟的RTO（恢复时间目标）。应用程序使用MySQL数据库和Amazon EC2 Web服务器。开发团队需要一个故障转移和灾难恢复策略。</td>
                    <td>选项A：在多个区域的单个可用区中创建Amazon Aurora集群作为数据存储，使用Aurora的自动恢复能力应对灾难。这个选项存在问题，因为在单个可用区中创建跨多个区域的集群在技术上是不可行的，而且单个可用区无法满足灾难恢复的要求。Aurora的自动恢复主要针对单区域内的故障，无法满足跨区域的灾难恢复需求。<br><br>选项B：在两个区域创建Amazon Aurora全球数据库作为数据存储，在发生故障时将次要区域提升为应用程序的主要区域。这是一个很好的数据库层面的解决方案，Aurora全球数据库可以提供跨区域的数据复制，通常可以满足2小时RPO的要求，并且故障转移时间相对较短，有助于满足10分钟的RTO要求。<br><br>选项C：跨多个区域创建Amazon Aurora多主集群作为数据存储，使用网络负载均衡器平衡不同区域的数据库流量。虽然多主集群提供了高可用性，但Aurora多主集群目前主要在单个区域内工作，跨区域的多主配置复杂且可能无法满足性能要求。<br><br>选项D：在两个区域设置应用程序，使用Amazon Route 53基于故障转移的路由指向两个区域的应用程序负载均衡器，使用健康检查确定给定区域的可用性，在每个区域使用Auto Scaling组根据需求调整容量。这个选项很好地解决了应用层的高可用性和故障转移问题，可以满足RTO要求。<br><br>选项E：在两个区域设置应用程序，使用应用程序负载均衡器后的多区域Auto Scaling组根据需求管理容量，在灾难发生时调整Auto Scaling组的期望实例数量以增加故障转移区域的基线容量。多区域Auto Scaling组的概念在AWS中并不存在，Auto Scaling组是区域特定的服务。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>94</td>
                    <td>A business has an application that consists of ve independent AWS Lambda functions.<br>The DevOps engineer has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each<br>Lambda function in sequence. The pipeline uses an Amazon EventBridge rule to ensure the pipeline starts as quickly as possible after a<br>change is made to the application source code.<br>After working with the pipeline for a few months, the DevOps engineer has noticed the pipeline takes too long to complete.</td>
                    <td>A. Modify the CodeBuild projects within the pipeline to use a compute type with more available network throughput.<br>B. Create a custom CodeBuild execution environment that includes a symmetric multiprocessing conguration to run the builds in parallel.<br>C. Modify the CodePipeline conguration to run actions for each Lambda function in parallel by specifying the same runOrder.<br>D. Modify each CodeBuild project to run within a VPC and use dedicated instances to increase throughput.</td>
                    <td>一个企业有一个由五个独立的AWS Lambda函数组成的应用程序。DevOps工程师使用AWS CodePipeline和AWS CodeBuild构建了一个CI/CD管道，该管道按顺序构建、测试、打包和部署每个Lambda函数。该管道使用Amazon EventBridge规则来确保在对应用程序源代码进行更改后尽快启动管道。在使用该管道几个月后，DevOps工程师注意到管道完成时间过长。</td>
                    <td>A. 修改管道中的CodeBuild项目以使用具有更多可用网络吞吐量的计算类型。这个选项主要解决网络带宽问题，但题目中的性能瓶颈更可能是由于串行执行导致的时间累积，而不是网络吞吐量不足。增加网络吞吐量对于Lambda函数的构建和部署过程改善有限。<br><br>B. 创建一个包含对称多处理配置的自定义CodeBuild执行环境，以并行运行构建。虽然这可以在单个CodeBuild项目内实现并行处理，但这种方法复杂度较高，需要自定义环境配置，而且不如直接在管道级别实现并行化来得直接有效。<br><br>C. 修改CodePipeline配置，通过指定相同的runOrder来并行运行每个Lambda函数的操作。这是最直接有效的解决方案。由于五个Lambda函数是独立的，它们可以并行构建和部署，通过设置相同的runOrder值，可以让这些操作同时执行，大大减少总体执行时间。<br><br>D. 修改每个CodeBuild项目在VPC内运行并使用专用实例来增加吞吐量。将CodeBuild放在VPC中实际上可能会增加网络延迟，而专用实例主要解决的是资源隔离问题，对于构建速度的提升效果有限，且成本较高。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>95</td>
                    <td>A company uses AWS CloudFormation stacks to deploy updates to its application. The stacks consist of different resources. The resources<br>include AWS Auto Scaling groups, Amazon EC2 instances, Application Load Balancers (ALBs), and other resources that are necessary to<br>launch and maintain independent stacks. Changes to application resources outside of CloudFormation stack updates are not allowed.<br>The company recently attempted to update the application stack by using the AWS CLI. The stack failed to update and produced the following<br>error message: “ERROR: both the deployment and the CloudFormation stack rollback failed. The deployment failed because the following<br>resource(s) failed to update: [AutoScalingGroup].”<br>The stack remains in a status of UPDATE_ROLLBACK_FAILE</td>
                    <td>D. Delete the Auto Scaling group resource. Run the aws cloudformation rollback-stack AWS CLI command.<br>A. Update the subnet mappings that are congured for the ALBs. Run the aws cloudformation update-stack-set AWS CLI command.<br>B. Update the IAM role by providing the necessary permissions to update the stack. Run the aws cloudformation continue-update-rollback<br>AWS CLI command.<br>C. Submit a request for a quota increase for the number of EC2 instances for the account. Run the aws cloudformation cancel-update-<br>stack AWS CLI command.</td>
                    <td>一家公司使用AWS CloudFormation堆栈来部署应用程序更新。这些堆栈由不同的资源组成，包括AWS Auto Scaling组、Amazon EC2实例、应用程序负载均衡器(ALB)以及启动和维护独立堆栈所需的其他资源。不允许在CloudFormation堆栈更新之外对应用程序资源进行更改。<br><br>该公司最近尝试使用AWS CLI更新应用程序堆栈。堆栈更新失败并产生以下错误消息：&quot;错误：部署和CloudFormation堆栈回滚都失败了。部署失败是因为以下资源更新失败：[AutoScalingGroup]。&quot;<br><br>堆栈保持在UPDATE_ROLLBACK_FAILED状态。</td>
                    <td>选项A：更新为ALB配置的子网映射，运行aws cloudformation update-stack-set AWS CLI命令。这个选项不正确，因为错误信息明确指出是AutoScalingGroup资源更新失败，而不是ALB的子网映射问题。此外，update-stack-set命令用于堆栈集，不适用于单个堆栈的回滚恢复场景。<br><br>选项B：通过提供必要的权限来更新IAM角色，运行aws cloudformation continue-update-rollback AWS CLI命令。这是正确的解决方案。当堆栈处于UPDATE_ROLLBACK_FAILED状态时，continue-update-rollback命令专门用于从失败的回滚中恢复。权限问题经常导致资源更新失败，更新IAM角色权限可以解决这个问题。<br><br>选项C：为账户的EC2实例数量提交配额增加请求，运行aws cloudformation cancel-update-stack AWS CLI命令。虽然配额限制可能导致Auto Scaling组更新失败，但cancel-update-stack命令无法从UPDATE_ROLLBACK_FAILED状态中恢复堆栈。<br><br>选项D：删除Auto Scaling组资源，运行aws cloudformation rollback-stack AWS CLI命令。手动删除资源违反了题目中&quot;不允许在CloudFormation堆栈更新之外对应用程序资源进行更改&quot;的要求，且rollback-stack命令不存在。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>96</td>
                    <td>A company is deploying a new application that uses Amazon EC2 instances. The company needs a solution to query application logs and AWS<br>account API activity.</td>
                    <td>A. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Congure AWS CloudTrail to<br>deliver the API logs to Amazon S3. Use CloudWatch to query both sets of logs.<br>B. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Congure AWS CloudTrail to<br>deliver the API logs to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs.<br>C. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon Kinesis. Congure AWS CloudTrail to deliver the API<br>logs to Kinesis. Use Kinesis to load the data into Amazon Redshift. Use Amazon Redshift to query both sets of logs.<br>D. Use the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon S3. Use AWS CloudTrail to deliver the API logs to<br>Amazon S3. Use Amazon Athena to query both sets of logs in Amazon S3.</td>
                    <td>一家公司正在部署一个使用Amazon EC2实例的新应用程序。该公司需要一个解决方案来查询应用程序日志和AWS账户API活动。</td>
                    <td>选项A：使用Amazon CloudWatch代理将EC2实例的日志发送到Amazon CloudWatch Logs。配置AWS CloudTrail将API日志传送到Amazon S3。使用CloudWatch查询两组日志。<br>这个选项的问题在于CloudWatch本身不能直接查询存储在S3中的CloudTrail日志。CloudWatch主要用于监控指标，而不是查询存储在S3中的日志文件。这种架构会导致日志分散在不同位置，查询复杂且不统一。<br><br>选项B：使用Amazon CloudWatch代理将EC2实例的日志发送到Amazon CloudWatch Logs。配置AWS CloudTrail将API日志传送到CloudWatch Logs。使用CloudWatch Logs Insights查询两组日志。<br>这是最佳选择。CloudWatch Logs Insights是专门设计用来查询和分析CloudWatch Logs中日志数据的服务。通过将应用程序日志和CloudTrail API日志都发送到CloudWatch Logs，可以使用统一的查询界面和强大的查询语言来分析两种类型的日志，实现集中化的日志管理和查询。<br><br>选项C：使用Amazon CloudWatch代理将EC2实例的日志发送到Amazon Kinesis。配置AWS CloudTrail将API日志传送到Kinesis。使用Kinesis将数据加载到Amazon Redshift。使用Amazon Redshift查询两组日志。<br>这个方案过于复杂，涉及多个服务（Kinesis、Redshift），增加了架构复杂性和成本。Redshift主要用于数据仓库和大规模分析，对于简单的日志查询来说是过度设计。<br><br>选项D：使用Amazon CloudWatch代理将EC2实例的日志发送到Amazon S3。配置AWS CloudTrail将API日志传送到Amazon S3。使用Amazon Athena查询S3中的两组日志。<br>虽然技术上可行，但CloudWatch代理通常不直接发送日志到S3，而是发送到CloudWatch Logs。这种配置不是标准做法，且需要额外的配置复杂性。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>97</td>
                    <td>A company wants to ensure that their EC2 instances are secure. They want to be notied if any new vulnerabilities are discovered on their<br>instances, and they also want an audit trail of all login activities on the instances.</td>
                    <td>A. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Amazon Kinesis Agent to capture system logs and<br>deliver them to Amazon S3.<br>B. Use AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture system logs<br>and view login activity in the CloudTrail console.<br>C. Congure Amazon CloudWatch to detect vulnerabilities on the EC2 instances. Install the AWS Cong daemon to capture system logs<br>and view them in the AWS Cong console.<br>D. Congure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system<br>logs and record them via Amazon CloudWatch Logs.</td>
                    <td>一家公司希望确保他们的EC2实例是安全的。他们希望在实例上发现任何新漏洞时收到通知，同时也希望获得实例上所有登录活动的审计跟踪。</td>
                    <td>A. 使用AWS Systems Manager检测EC2实例上的漏洞。安装Amazon Kinesis Agent来捕获系统日志并将其传送到Amazon S3。<br>分析：AWS Systems Manager主要用于实例管理和补丁管理，虽然有一些安全功能，但不是专门的漏洞检测服务。Kinesis Agent可以收集日志，但这个组合不是最佳的漏洞检测解决方案。<br><br>B. 使用AWS Systems Manager检测EC2实例上的漏洞。安装Systems Manager Agent来捕获系统日志并在CloudTrail控制台中查看登录活动。<br>分析：同样，Systems Manager不是专门的漏洞检测工具。另外，CloudTrail主要记录AWS API调用，而不是EC2实例内部的登录活动，这里存在概念混淆。<br><br>C. 配置Amazon CloudWatch来检测EC2实例上的漏洞。安装AWS Config守护进程来捕获系统日志并在AWS Config控制台中查看。<br>分析：CloudWatch是监控服务，不具备漏洞检测功能。AWS Config主要用于配置合规性检查，不是用来收集系统日志的工具。这个选项在技术上是不正确的。<br><br>D. 配置Amazon Inspector来检测EC2实例上的漏洞。安装Amazon CloudWatch Agent来捕获系统日志并通过Amazon CloudWatch Logs记录。<br>分析：Amazon Inspector是AWS专门的安全评估服务，专门用于检测EC2实例和容器镜像中的漏洞。CloudWatch Agent可以有效收集系统日志，包括登录活动，并将其发送到CloudWatch Logs进行存储和分析。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>98</td>
                    <td>A company is running an application on Amazon EC2 instances in an Auto Scaling group. Recently, an issue occurred that prevented EC2<br>instances from launching successfully, and it took several hours for the support team to discover the issue. The support team wants to be<br>notied by email whenever an EC2 instance does not start successfully.</td>
                    <td>A. Add a health check to the Auto Scaling group to invoke an AWS Lambda function whenever an instance status is impaired.<br>B. Congure the Auto Scaling group to send a notication to an Amazon SNS topic whenever a failed instance launch occurs.<br>C. Create an Amazon CloudWatch alarm that invokes an AWS Lambda function when a failed AttachInstances Auto Scaling API call is<br>made.<br>D. Create a status check alarm on Amazon EC2 to send a notication to an Amazon SNS topic whenever a status check fail occurs.</td>
                    <td>一家公司在Auto Scaling组中的Amazon EC2实例上运行应用程序。最近发生了一个问题，导致EC2实例无法成功启动，支持团队花了几个小时才发现这个问题。支持团队希望在EC2实例启动失败时能够通过电子邮件收到通知。</td>
                    <td>A. 向Auto Scaling组添加健康检查，以便在实例状态受损时调用AWS Lambda函数。这个选项不正确，因为健康检查主要用于检测已运行实例的健康状态，而不是检测实例启动失败的情况。健康检查是在实例已经启动后进行的，无法捕获启动阶段的失败。<br><br>B. 配置Auto Scaling组在实例启动失败时向Amazon SNS主题发送通知。这是正确的选项。Auto Scaling组原生支持向SNS主题发送通知，当发生实例启动失败等事件时会自动触发通知。SNS可以配置电子邮件订阅，完美满足需求。<br><br>C. 创建Amazon CloudWatch警报，当AttachInstances Auto Scaling API调用失败时调用AWS Lambda函数。这个选项不准确，因为AttachInstances API主要用于将现有实例附加到Auto Scaling组，而不是用于监控新实例的启动失败。<br><br>D. 在Amazon EC2上创建状态检查警报，当状态检查失败时向Amazon SNS主题发送通知。这个选项虽然可以发送通知，但状态检查是针对已运行实例的健康监控，无法检测到实例启动失败的情况，因为启动失败的实例根本不会进入运行状态。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>99</td>
                    <td>A company is using AWS Organizations to centrally manage its AWS accounts. The company has turned on AWS Cong in each member<br>account by using AWS CloudFormation StackSets. The company has congured trusted access in Organizations for AWS Cong and has<br>congured a member account as a delegated administrator account for AWS Cong.<br>A DevOps engineer needs to implement a new security policy. The policy must require all current and future AWS member accounts to use a<br>common baseline of AWS Cong rules that contain remediation actions that are managed from a central account. Non-administrator users<br>who can access member accounts must not be able to modify this common baseline of AWS Cong rules that are deployed into each member<br>account.</td>
                    <td>A. Create a CloudFormation template that contains the AWS Cong rules and remediation actions. Deploy the template from the<br>Organizations management account by using CloudFormation StackSets.<br>B. Create an AWS Cong conformance pack that contains the AWS Cong rules and remediation actions. Deploy the pack from the<br>Organizations management account by using CloudFormation StackSets.<br>C. Create a CloudFormation template that contains the AWS Cong rules and remediation actions. Deploy the template from the<br>delegated administrator account by using AWS Cong.<br>D. Create an AWS Cong conformance pack that contains the AWS Cong rules and remediation actions. Deploy the pack from the<br>delegated administrator account by using AWS Cong.</td>
                    <td>一家公司正在使用AWS Organizations来集中管理其AWS账户。该公司已通过AWS CloudFormation StackSets在每个成员账户中启用了AWS Config。公司已在Organizations中为AWS Config配置了可信访问，并将一个成员账户配置为AWS Config的委托管理员账户。<br><br>DevOps工程师需要实施一项新的安全策略。该策略必须要求所有当前和未来的AWS成员账户使用一个通用的AWS Config规则基线，该基线包含从中央账户管理的修复操作。能够访问成员账户的非管理员用户不得修改部署到每个成员账户中的这个通用AWS Config规则基线。</td>
                    <td>A. 创建包含AWS Config规则和修复操作的CloudFormation模板，从Organizations管理账户使用CloudFormation StackSets部署模板。这个选项虽然可以实现集中部署，但使用CloudFormation模板而不是Config conformance pack不是最佳实践，且从管理账户部署而不是委托管理员账户部署不符合题目中已配置委托管理员的设置。<br><br>B. 创建包含AWS Config规则和修复操作的AWS Config conformance pack，从Organizations管理账户使用CloudFormation StackSets部署。虽然使用了conformance pack这个正确的工具，但从管理账户部署而不是委托管理员账户部署，没有充分利用已配置的委托管理员设置。<br><br>C. 创建包含AWS Config规则和修复操作的CloudFormation模板，从委托管理员账户使用AWS Config部署。虽然使用了正确的委托管理员账户，但使用CloudFormation模板而不是专门的conformance pack不是最佳选择。<br><br>D. 创建包含AWS Config规则和修复操作的AWS Config conformance pack，从委托管理员账户使用AWS Config部署。这个选项使用了最适合的工具（conformance pack）来管理Config规则，并且从已配置的委托管理员账户进行部署，完全符合题目要求。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>100</td>
                    <td>A DevOps engineer manages a large commercial website that runs on Amazon EC2. The website uses Amazon Kinesis Data Streams to collect<br>and process web logs. The DevOps engineer manages the Kinesis consumer application, which also runs on Amazon EC2.<br>Sudden increases of data cause the Kinesis consumer application to fall behind, and the Kinesis data streams drop records before the records<br>can be processed. The DevOps engineer must implement a solution to improve stream handling.</td>
                    <td>A. Modify the Kinesis consumer application to store the logs durably in Amazon S3. Use Amazon EMR to process the data directly on<br>Amazon S3 to derive customer insights. Store the results in Amazon S3.<br>B. Horizontally scale the Kinesis consumer application by adding more EC2 instances based on the Amazon CloudWatch<br>GetRecords.IteratorAgeMilliseconds metric. Increase the retention period of the Kinesis data streams.<br>C. Convert the Kinesis consumer application to run as an AWS Lambda function. Congure the Kinesis data streams as the event source<br>for the Lambda function to process the data streams.<br>D. Increase the number of shards in the Kinesis data streams to increase the overall throughput so that the consumer application<br>processes the data faster.</td>
                    <td>一名DevOps工程师管理着一个运行在Amazon EC2上的大型商业网站。该网站使用Amazon Kinesis Data Streams来收集和处理Web日志。DevOps工程师管理着同样运行在Amazon EC2上的Kinesis消费者应用程序。数据的突然增加导致Kinesis消费者应用程序处理滞后，Kinesis数据流在记录被处理之前就丢弃了记录。DevOps工程师必须实施一个解决方案来改善流处理。</td>
                    <td>A. 修改Kinesis消费者应用程序将日志持久存储在Amazon S3中，使用Amazon EMR直接在Amazon S3上处理数据以获得客户洞察，将结果存储在Amazon S3中。这个方案虽然可以解决数据丢失问题，但改变了整个架构，从实时流处理变成了批处理模式，不符合原有的实时处理需求，且实施复杂度较高。<br><br>B. 基于Amazon CloudWatch的GetRecords.IteratorAgeMilliseconds指标水平扩展Kinesis消费者应用程序，通过添加更多EC2实例来处理负载，同时增加Kinesis数据流的保留期。这个方案直接针对问题根源：处理能力不足和数据保留时间过短，通过扩展消费者实例提高处理能力，延长保留期防止数据丢失。<br><br>C. 将Kinesis消费者应用程序转换为AWS Lambda函数，配置Kinesis数据流作为Lambda函数的事件源来处理数据流。Lambda虽然可以自动扩展，但有执行时间限制（最长15分钟），且对于大量持续的流数据处理可能不是最佳选择，还可能产生较高的成本。<br><br>D. 增加Kinesis数据流中的分片数量以增加整体吞吐量，使消费者应用程序更快地处理数据。虽然增加分片可以提高吞吐量，但如果消费者应用程序本身处理能力不足，仅增加分片并不能根本解决问题。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>101</td>
                    <td>A company recently created a new AWS Control Tower landing zone in a new organization in AWS Organizations. The landing zone must be<br>able to demonstrate compliance with the Center for Internet Security (CIS) Benchmarks for AWS Foundations.<br>The company’s security team wants to use AWS Security Hub to view compliance across all accounts. Only the security team can be allowed to<br>view aggregated Security Hub ndings. In addition, specic users must be able to view ndings from their own accounts within the<br>organization. All accounts must be enrolled in Security Hub after the accounts are created.</td>
                    <td>A. Turn on trusted access for Security Hub in the organization’s management account. Create a new security account by using AWS Control<br>Tower. Congure the new security account as the delegated administrator account for Security Hub. In the new security account, provide<br>Security Hub with the CIS Benchmarks for AWS Foundations standards.<br>B. Turn on trusted access for Security Hub in the organization’s management account. From the management account, provide Security<br>Hub with the CIS Benchmarks for AWS Foundations standards.<br>C. Create an AWS IAM Identity Center (AWS Single Sign-On) permission set that includes the required permissions. Use the<br>CreateAccountAssignment API operation to associate the security team users with the permission set and with the delegated security<br>account.<br>D. Create an SCP that explicitly denies any user who is not on the security team from accessing Security Hub.<br>E. In Security Hub, turn on automatic enablement.<br>F. In the organization’s management account, create an Amazon EventBridge rule that reacts to the CreateManagedAccount event. Create<br>an AWS Lambda function that uses the Security Hub CreateMembers API operation to add new accounts to Security Hub. Congure the<br>EventBridge rule to invoke the Lambda function.</td>
                    <td>一家公司最近在AWS Organizations的新组织中创建了一个新的AWS Control Tower着陆区。该着陆区必须能够证明符合AWS基础设施的互联网安全中心(CIS)基准。<br>公司的安全团队希望使用AWS Security Hub来查看所有账户的合规性。只有安全团队被允许查看聚合的Security Hub发现结果。此外，特定用户必须能够查看组织内自己账户的发现结果。所有账户必须在创建后注册到Security Hub。</td>
                    <td>A. 在组织的管理账户中为Security Hub开启可信访问。使用AWS Control Tower创建新的安全账户。将新安全账户配置为Security Hub的委托管理员账户。在新安全账户中，为Security Hub提供AWS基础设施CIS基准标准。这是正确的，因为委托管理员账户可以集中管理所有成员账户的Security Hub，并且安全团队可以通过这个专门的安全账户查看聚合结果。<br><br>B. 在组织的管理账户中为Security Hub开启可信访问。从管理账户为Security Hub提供AWS基础设施CIS基准标准。这不是最佳实践，因为应该使用专门的安全账户作为委托管理员，而不是直接使用管理账户。<br><br>C. 创建包含所需权限的AWS IAM Identity Center权限集。使用CreateAccountAssignment API操作将安全团队用户与权限集和委托安全账户关联。这是正确的，因为它确保只有安全团队成员能够访问聚合的Security Hub发现结果。<br><br>D. 创建SCP明确拒绝非安全团队用户访问Security Hub。这过于严格，会阻止用户查看自己账户的发现结果，不符合要求。<br><br>E. 在Security Hub中开启自动启用功能。这是正确的，确保所有新创建的账户都会自动注册到Security Hub。<br><br>F. 在组织管理账户中创建EventBridge规则响应CreateManagedAccount事件，创建Lambda函数使用Security Hub CreateMembers API添加新账户。这是不必要的，因为选项E的自动启用功能已经能够处理新账户的注册。</td>
                    <td>ACE</td>
                </tr>
                <tr>
                    <td>102</td>
                    <td>A company runs applications in AWS accounts that are in an organization in AWS Organizations. The applications use Amazon EC2 instances<br>and Amazon S3.<br>The company wants to detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity in its existing AWS<br>accounts and in any AWS accounts that the company creates in the future. When the company detects one of these events, the company<br>wants to use an existing Amazon Simple Notication Service (Amazon SNS) topic to send a notication to its operational support team for<br>investigation and remediation.</td>
                    <td>A. In the organization’s management account, congure an AWS account as the Amazon GuardDuty administrator account. In the<br>GuardDuty administrator account, add the company’s existing AWS accounts to GuardDuty as members. In the GuardDuty administrator<br>account, create an Amazon EventBridge rule with an event pattern to match GuardDuty events and to forward matching events to the SNS<br>topic.<br>B. In the organization’s management account, congure Amazon GuardDuty to add newly created AWS accounts by invitation and to send<br>invitations to the existing AWS accounts. Create an AWS CloudFormation stack set that accepts the GuardDuty invitation and creates an<br>Amazon EventBridge rule. Congure the rule with an event pattern to match GuardDuty events and to forward matching events to the SNS<br>topic. Congure the CloudFormation stack set to deploy into all AWS accounts in the organization.<br>C. In the organization’s management account, create an AWS CloudTrail organization trail. Activate the organization trail in all AWS<br>accounts in the organization. Create an SCP that enables VPC Flow Logs in each account in the organization. Congure AWS Security Hub<br>for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching<br>events to the SNS topic.<br>D. In the organization’s management account, congure an AWS account as the AWS CloudTrail administrator account. In the CloudTrail<br>administrator account, create a CloudTrail organization trail. Add the company’s existing AWS accounts to the organization trail. Create an<br>SCP that enables VPC Flow Logs in each account in the organization. Congure AWS Security Hub for the organization. Create an Amazon<br>EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic.</td>
                    <td>一家公司在AWS Organizations组织中的AWS账户中运行应用程序。这些应用程序使用Amazon EC2实例和Amazon S3。<br>该公司希望在其现有的AWS账户以及将来创建的任何AWS账户中检测可能被入侵的EC2实例、可疑的网络活动和异常的API活动。当公司检测到这些事件之一时，公司希望使用现有的Amazon Simple Notification Service (Amazon SNS)主题向其运营支持团队发送通知，以便进行调查和修复。</td>
                    <td>选项A：在组织的管理账户中，配置一个AWS账户作为Amazon GuardDuty管理员账户。在GuardDuty管理员账户中，将公司现有的AWS账户作为成员添加到GuardDuty。在GuardDuty管理员账户中，创建一个Amazon EventBridge规则，使用事件模式匹配GuardDuty事件并将匹配的事件转发到SNS主题。这个方案可以检测安全威胁，但没有自动处理未来新创建账户的机制，需要手动添加新账户。<br><br>选项B：在组织的管理账户中，配置Amazon GuardDuty通过邀请添加新创建的AWS账户，并向现有AWS账户发送邀请。创建一个AWS CloudFormation堆栈集，接受GuardDuty邀请并创建Amazon EventBridge规则。配置规则使用事件模式匹配GuardDuty事件并将匹配的事件转发到SNS主题。配置CloudFormation堆栈集部署到组织中的所有AWS账户。这个方案通过堆栈集实现了自动化管理，可以处理现有和未来的账户。<br><br>选项C：在组织的管理账户中，创建AWS CloudTrail组织跟踪。在组织中的所有AWS账户中激活组织跟踪。创建一个SCP启用每个账户中的VPC Flow Logs。为组织配置AWS Security Hub。创建Amazon EventBridge规则匹配Security Hub事件并转发到SNS主题。虽然提供了安全监控，但CloudTrail和Security Hub不能直接检测被入侵的EC2实例，这需要GuardDuty的威胁检测能力。<br><br>选项D：在组织的管理账户中，配置一个AWS账户作为AWS CloudTrail管理员账户。在CloudTrail管理员账户中，创建CloudTrail组织跟踪。将公司现有的AWS账户添加到组织跟踪。创建SCP在组织中的每个账户中启用VPC Flow Logs。为组织配置AWS Security Hub。创建Amazon EventBridge规则匹配Security Hub事件并转发到SNS主题。与选项C类似，缺乏GuardDuty的威胁检测功能，无法有效检测被入侵的实例。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>103</td>
                    <td>A company’s DevOps engineer is working in a multi-account environment. The company uses AWS Transit Gateway to route all outbound trac<br>through a network operations account. In the network operations account, all account trac passes through a rewall appliance for inspection<br>before the trac goes to an internet gateway.<br>The rewall appliance sends logs to Amazon CloudWatch Logs and includes event severities of CRITICAL, HIGH, MEDIUM, LOW, and INFO. The<br>security team wants to receive an alert if any CRITICAL events occur.</td>
                    <td>A. Create an Amazon CloudWatch Synthetics canary to monitor the rewall state. If the rewall reaches a CRITICAL state or logs a<br>CRITICAL event, use a CloudWatch alarm to publish a notication to an Amazon Simple Notication Service (Amazon SNS) topic. Subscribe<br>the security team’s email address to the topic.<br>B. Create an Amazon CloudWatch metric lter by using a search for CRITICAL events. Publish a custom metric for the nding. Use a<br>CloudWatch alarm based on the custom metric to publish a notication to an Amazon Simple Notication Service (Amazon SNS) topic.<br>Subscribe the security team’s email address to the topic.<br>C. Enable Amazon GuardDuty in the network operations account. Congure GuardDuty to monitor ow logs. Create an Amazon EventBridge<br>event rule that is invoked by GuardDuty events that are CRITICAL. Dene an Amazon Simple Notication Service (Amazon SNS) topic as a<br>target. Subscribe the security team’s email address to the topic.<br>D. Use AWS Firewall Manager to apply consistent policies across all accounts. Create an Amazon EventBridge event rule that is invoked by<br>Firewall Manager events that are CRITICAL. Dene an Amazon Simple Notication Service (Amazon SNS) topic as a target. Subscribe the<br>security team’s email address to the topic.</td>
                    <td>一家公司的DevOps工程师在多账户环境中工作。该公司使用AWS Transit Gateway将所有出站流量通过网络运营账户进行路由。在网络运营账户中，所有账户流量在到达互联网网关之前都会通过防火墙设备进行检查。<br><br>防火墙设备将日志发送到Amazon CloudWatch Logs，包含CRITICAL、HIGH、MEDIUM、LOW和INFO等事件严重级别。安全团队希望在发生任何CRITICAL事件时收到警报。</td>
                    <td>A. 创建Amazon CloudWatch Synthetics canary来监控防火墙状态。如果防火墙达到CRITICAL状态或记录CRITICAL事件，使用CloudWatch警报发布通知到Amazon SNS主题。这个选项不正确，因为CloudWatch Synthetics主要用于监控应用程序端点和API，而不是用于分析日志内容中的特定事件级别。<br><br>B. 使用搜索CRITICAL事件创建Amazon CloudWatch指标过滤器。为发现的内容发布自定义指标。基于自定义指标使用CloudWatch警报发布通知到Amazon SNS主题。这个选项正确，CloudWatch指标过滤器可以扫描日志数据中的特定模式（如CRITICAL），创建自定义指标，然后基于该指标设置警报。<br><br>C. 在网络运营账户中启用Amazon GuardDuty。配置GuardDuty监控流日志。创建由CRITICAL级别GuardDuty事件调用的Amazon EventBridge事件规则。这个选项不正确，因为GuardDuty主要用于威胁检测，而不是分析防火墙设备的自定义日志格式。<br><br>D. 使用AWS Firewall Manager在所有账户中应用一致的策略。创建由CRITICAL级别Firewall Manager事件调用的Amazon EventBridge事件规则。这个选项不正确，因为Firewall Manager用于管理防火墙策略，而不是分析现有防火墙设备的日志内容。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>104</td>
                    <td>A company is divided into teams. Each team has an AWS account, and all the accounts are in an organization in AWS Organizations. Each<br>team must retain full administrative rights to its AWS account. Each team also must be allowed to access only AWS services that the company<br>approves for use. AWS services must gain approval through a request and approval process.<br>How should a DevOps engineer congure the accounts to meet these requirements?</td>
                    <td>A. Use AWS CloudFormation StackSets to provision IAM policies in each account to deny access to restricted AWS services. In each<br>account, congure AWS Cong rules that ensure that the policies are attached to IAM principals in the account.<br>B. Use AWS Control Tower to provision the accounts into OUs within the organization. Congure AWS Control Tower to enable AWS IAM<br>Identity Center (AWS Single Sign-On). Congure IAM Identity Center to provide administrative access. Include deny policies on user roles<br>for restricted AWS services.<br>C. Place all the accounts under a new top-level OU within the organization. Create an SCP that denies access to restricted AWS services.<br>Attach the SCP to the OU.<br>D. Create an SCP that allows access to only approved AWS services. Attach the SCP to the root OU of the organization. Remove the<br>FullAWSAccess SCP from the root OU of the organization.</td>
                    <td>一家公司分为多个团队。每个团队都有一个AWS账户，所有账户都在AWS Organizations的组织中。每个团队必须保留对其AWS账户的完全管理权限。每个团队也必须只能访问公司批准使用的AWS服务。AWS服务必须通过请求和批准流程获得批准。<br>DevOps工程师应该如何配置账户以满足这些要求？</td>
                    <td>选项A：使用AWS CloudFormation StackSets在每个账户中配置IAM策略来拒绝访问受限制的AWS服务。在每个账户中，配置AWS Config规则以确保策略附加到账户中的IAM主体。这种方法的问题是需要在每个账户中单独管理IAM策略，管理复杂度高，而且团队仍然拥有完全管理权限，可能会修改或删除这些限制策略，无法有效防止绕过限制。<br><br>选项B：使用AWS Control Tower将账户配置到组织内的OU中。配置AWS Control Tower启用AWS IAM Identity Center。配置IAM Identity Center提供管理访问权限，在用户角色上包含对受限AWS服务的拒绝策略。这种方法主要关注身份管理和访问控制，但不能从根本上限制账户级别的服务访问，团队仍可能通过其他方式访问受限服务。<br><br>选项C：将所有账户放在组织内的新顶级OU下。创建一个SCP来拒绝访问受限制的AWS服务。将SCP附加到OU。这种方法使用服务控制策略(SCP)在组织级别强制执行服务访问限制，即使团队拥有完全管理权限，也无法绕过SCP的限制，能够有效满足要求。<br><br>选项D：创建一个SCP，只允许访问已批准的AWS服务。将SCP附加到组织的根OU。从组织的根OU中删除FullAWSAccess SCP。这种方法会影响组织中的所有账户，包括可能需要更广泛权限的管理账户，过于严格且可能影响正常运营。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>105</td>
                    <td>A DevOps engineer used an AWS CloudFormation custom resource to set up AD Connector. The AWS Lambda function ran and created AD<br>Connector, but CloudFormation is not transitioning from CREATE_IN_PROGRESS to CREATE_COMPLET</td>
                    <td>E. Which action should the engineer take to resolve this issue?<br>A. Ensure the Lambda function code has exited successfully.<br>B. Ensure the Lambda function code returns a response to the pre-signed URL.<br>C. Ensure the Lambda function IAM role has cloudformation:UpdateStack permissions for the stack ARN.<br>D. Ensure the Lambda function IAM role has ds:ConnectDirectory permissions for the AWS account.</td>
                    <td>一位DevOps工程师使用AWS CloudFormation自定义资源来设置AD Connector。AWS Lambda函数运行并创建了AD Connector，但CloudFormation没有从CREATE_IN_PROGRESS状态转换到CREATE_COMPLETE状态。工程师应该采取哪个操作来解决这个问题？</td>
                    <td>A. 确保Lambda函数代码已成功退出 - 这个选项不正确。题目明确提到Lambda函数已经运行并成功创建了AD Connector，说明函数本身执行是成功的。仅仅是函数成功退出并不能解决CloudFormation状态转换的问题，因为CloudFormation需要接收到明确的响应才能知道自定义资源的操作状态。<br><br>B. 确保Lambda函数代码向预签名URL返回响应 - 这是正确答案。在CloudFormation自定义资源中，Lambda函数必须向CloudFormation提供的预签名URL发送响应，告知操作的成功或失败状态。如果Lambda函数没有发送这个响应，CloudFormation就会一直等待，导致堆栈停留在CREATE_IN_PROGRESS状态。这正是题目描述的问题症状。<br><br>C. 确保Lambda函数IAM角色具有cloudformation:UpdateStack权限 - 这个选项不正确。自定义资源的Lambda函数不需要UpdateStack权限来更新CloudFormation堆栈状态。CloudFormation通过预签名URL机制来接收Lambda函数的响应，而不是通过直接的API调用来更新堆栈。<br><br>D. 确保Lambda函数IAM角色具有ds:ConnectDirectory权限 - 这个选项不正确。虽然创建AD Connector确实需要相应的Directory Service权限，但题目已经说明AD Connector已经成功创建，这表明权限是足够的。问题在于CloudFormation状态转换，而不是资源创建权限。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>106</td>
                    <td>A company uses AWS CodeCommit for source code control. Developers apply their changes to various feature branches and create pull<br>requests to move those changes to the main branch when the changes are ready for production.<br>The developers should not be able to push changes directly to the main branch. The company applied the AWSCodeCommitPowerUser<br>managed policy to the developers’ IAM role, and now these developers can push changes to the main branch directly on every repository in<br>the AWS account.</td>
                    <td>A. Create an additional policy to include a Deny rule for the GitPush and PutFile actions. Include a restriction for the specic repositories<br>in the policy statement with a condition that references the main branch.<br>B. Remove the IAM policy, and add an AWSCodeCommitReadOnly managed policy. Add an Allow rule for the GitPush and PutFile actions<br>for the specic repositories in the policy statement with a condition that references the main branch.<br>C. Modify the IAM policy. Include a Deny rule for the GitPush and PutFile actions for the specic repositories in the policy statement with a<br>condition that references the main branch.<br>D. Create an additional policy to include an Allow rule for the GitPush and PutFile actions. Include a restriction for the specic repositories<br>in the policy statement with a condition that references the feature branches.</td>
                    <td>一家公司使用AWS CodeCommit进行源代码控制。开发人员将他们的更改应用到各种功能分支，并在更改准备好投入生产时创建拉取请求将这些更改移动到主分支。开发人员不应该能够直接将更改推送到主分支。公司将AWSCodeCommitPowerUser托管策略应用到开发人员的IAM角色，现在这些开发人员可以在AWS账户中的每个存储库上直接将更改推送到主分支。</td>
                    <td>A. 创建一个额外的策略，包含对GitPush和PutFile操作的拒绝规则。在策略声明中包含对特定存储库的限制，并添加引用主分支的条件。这个选项是正确的，因为它通过添加额外的拒绝策略来限制对主分支的直接推送，同时保持现有的AWSCodeCommitPowerUser权限不变。拒绝规则会覆盖允许规则，有效阻止开发人员直接推送到主分支。<br><br>B. 移除IAM策略，添加AWSCodeCommitReadOnly托管策略。为特定存储库的GitPush和PutFile操作添加允许规则，并在策略声明中添加引用主分支的条件。这个选项是错误的，因为它给予了对主分支的推送权限，这与需求相反。而且将权限降级为只读会影响开发人员对功能分支的正常操作。<br><br>C. 修改IAM策略。在策略声明中为特定存储库的GitPush和PutFile操作包含拒绝规则，并添加引用主分支的条件。这个选项虽然逻辑正确，但修改托管策略是不可能的，因为AWS托管策略是由AWS维护的，用户无法直接修改。<br><br>D. 创建一个额外的策略，包含对GitPush和PutFile操作的允许规则。在策略声明中包含对特定存储库的限制，并添加引用功能分支的条件。这个选项是错误的，因为它只是添加了对功能分支的明确允许，但没有阻止对主分支的访问，AWSCodeCommitPowerUser策略仍然允许推送到主分支。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>107</td>
                    <td>A company manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run<br>in an Auto Scaling group across multiple Availability Zones. The application uses an Amazon RDS for MySQL DB instance to store the data.<br>The company has congured Amazon Route 53 with an alias record that points to the AL</td>
                    <td>B. Launch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new<br>Region, and congure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health<br>check to congure a latency routing policy.<br>A. Launch a replica environment of everything except Amazon RDS in a different Availability Zone. Create an RDS read replica in the new<br>Availability Zone, and congure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using<br>a health check to congure a failover routing policy.<br>C. Launch a replica environment of everything except Amazon RDS in a different AWS Region. In the event of an outage, copy and restore<br>the latest RDS snapshot from the primary Region to the DR Region. Adjust the Route 53 record set to point to the ALB in the DR Region.<br>D. Launch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new<br>Region, and congure the new environment to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a<br>health check to congure a failover routing policy. In the event of an outage, promote the read replica to primary.</td>
                    <td>一家公司管理着一个运行在Amazon EC2实例上的Web应用程序，这些实例位于应用程序负载均衡器(ALB)后面。EC2实例在跨多个可用区的Auto Scaling组中运行。应用程序使用Amazon RDS for MySQL数据库实例来存储数据。公司已配置Amazon Route 53，其中包含指向ALB的别名记录。</td>
                    <td>选项A：在不同的可用区启动除Amazon RDS之外所有组件的副本环境。在新可用区创建RDS只读副本，并配置新堆栈指向本地RDS数据库实例。使用健康检查将新堆栈添加到Route 53记录集，配置故障转移路由策略。这个方案的问题是只在不同可用区部署，无法应对整个区域级别的故障，灾难恢复能力有限。<br><br>选项B：在不同AWS区域启动除Amazon RDS之外所有组件的副本环境。在新区域创建RDS只读副本，并配置新堆栈指向本地RDS数据库实例。使用健康检查将新堆栈添加到Route 53记录集，配置延迟路由策略。这个方案使用延迟路由策略而不是故障转移策略，不适合灾难恢复场景，延迟路由主要用于性能优化而非灾难恢复。<br><br>选项C：在不同AWS区域启动除Amazon RDS之外所有组件的副本环境。在发生故障时，从主区域复制并恢复最新的RDS快照到灾难恢复区域。调整Route 53记录集指向灾难恢复区域的ALB。这个方案的问题是恢复时间较长，需要手动复制和恢复快照，可能导致数据丢失和较长的停机时间。<br><br>选项D：在不同AWS区域启动除Amazon RDS之外所有组件的副本环境。在新区域创建RDS只读副本，并配置新环境指向本地RDS数据库实例。使用健康检查将新堆栈添加到Route 53记录集，配置故障转移路由策略。在发生故障时，将只读副本提升为主数据库。这是最完整的灾难恢复方案，提供了跨区域冗余、自动故障转移和最小的数据丢失。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>108</td>
                    <td>A large enterprise is deploying a web application on AWS. The application runs on Amazon EC2 instances behind an Application Load<br>Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The application stores data in an Amazon RDS for<br>Oracle DB instance and Amazon DynamoD</td>
                    <td>B. Launch the EC2 instances with an EC2 IAM role to access AWS services. Retrieve the database credentials from AWS Secrets Manager.<br>A. Retrieve an access key from an AWS Systems Manager SecureString parameter to access AWS services. Retrieve the database<br>credentials from a Systems Manager SecureString parameter.<br>C. Retrieve an access key from an AWS Systems Manager plaintext parameter to access AWS services. Retrieve the database credentials<br>from a Systems Manager SecureString parameter.<br>D. Launch the EC2 instances with an EC2 IAM role to access AWS services. Store the database passwords in an encrypted cong le with<br>the application artifacts.</td>
                    <td>一家大型企业正在AWS上部署Web应用程序。该应用程序运行在应用负载均衡器后面的Amazon EC2实例上。这些实例在Auto Scaling组中跨多个可用区运行。应用程序将数据存储在Amazon RDS for Oracle数据库实例和Amazon DynamoDB中。</td>
                    <td>选项A：从AWS Systems Manager SecureString参数检索访问密钥来访问AWS服务，从Systems Manager SecureString参数检索数据库凭证。这种方法虽然使用了SecureString来保护凭证，但仍然使用访问密钥来访问AWS服务，这不是最佳实践。访问密钥需要在EC2实例上存储和管理，存在安全风险，且难以轮换和管理。<br><br>选项B：使用EC2 IAM角色启动EC2实例来访问AWS服务，从AWS Secrets Manager检索数据库凭证。这是最佳实践方案。IAM角色提供临时凭证，无需在实例上存储长期访问密钥，更加安全。AWS Secrets Manager专门用于管理敏感信息如数据库凭证，支持自动轮换，提供更好的安全性和管理能力。<br><br>选项C：从AWS Systems Manager明文参数检索访问密钥来访问AWS服务，从Systems Manager SecureString参数检索数据库凭证。这种方法存在严重安全问题，将访问密钥以明文形式存储在Systems Manager中是极不安全的做法，容易导致凭证泄露。<br><br>选项D：使用EC2 IAM角色启动EC2实例来访问AWS服务，将数据库密码存储在加密配置文件中与应用程序工件一起。虽然使用了IAM角色这是正确的，但将数据库密码存储在配置文件中不是最佳实践，即使加密也难以进行凭证轮换和集中管理。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>109</td>
                    <td>The security team depends on AWS CloudTrail to detect sensitive security issues in the company’s AWS account. The DevOps engineer needs a<br>solution to auto-remediate CloudTrail being turned off in an AWS account.</td>
                    <td>A. Create an Amazon EventBridge rule for the CloudTrail StopLogging event. Create an AWS Lambda function that uses the AWS SDK to<br>call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the<br>EventBridge rule.<br>B. Deploy the AWS-managed CloudTrail-enabled AWS Cong rule, set with a periodic interval of 1 hour. Create an Amazon EventBridge rule<br>for AWS Cong rules compliance change. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the<br>resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule.<br>C. Create an Amazon EventBridge rule for a scheduled event every 5 minutes. Create an AWS Lambda function that uses the AWS SDK to<br>call StartLogging on a CloudTrail trail in the AWS account. Add the Lambda function ARN as a target to the EventBridge rule.<br>D. Launch a t2.nano instance with a script running every 5 minutes that uses the AWS SDK to query CloudTrail in the current account. If the<br>CloudTrail trail is disabled, have the script re-enable the trail.</td>
                    <td>安全团队依赖AWS CloudTrail来检测公司AWS账户中的敏感安全问题。DevOps工程师需要一个解决方案来自动修复AWS账户中CloudTrail被关闭的情况。</td>
                    <td>选项A：创建一个Amazon EventBridge规则来监听CloudTrail StopLogging事件。创建一个AWS Lambda函数，使用AWS SDK在调用StopLogging的资源ARN上调用StartLogging。将Lambda函数ARN作为目标添加到EventBridge规则中。这个方案是事件驱动的，能够实时响应CloudTrail被关闭的事件，立即触发自动修复。这是最高效和准确的解决方案，因为它直接响应具体的停止日志记录事件。<br><br>选项B：部署AWS托管的CloudTrail-enabled AWS Config规则，设置为1小时的周期性间隔。为AWS Config规则合规性变更创建Amazon EventBridge规则。创建AWS Lambda函数使用AWS SDK调用StartLogging。这个方案虽然可行，但响应时间较慢（最多1小时延迟），而且增加了不必要的复杂性，需要额外的AWS Config服务。<br><br>选项C：创建一个Amazon EventBridge规则用于每5分钟的计划事件。创建AWS Lambda函数使用AWS SDK在AWS账户中的CloudTrail轨迹上调用StartLogging。这个方案是基于时间调度的，会定期执行，但可能会在CloudTrail正常运行时也执行不必要的操作，效率较低且可能产生额外成本。<br><br>选项D：启动一个t2.nano实例，运行每5分钟执行的脚本，使用AWS SDK查询当前账户中的CloudTrail。如果CloudTrail轨迹被禁用，脚本会重新启用轨迹。这个方案需要维护EC2实例，增加了基础设施成本和管理复杂性，不如无服务器解决方案高效。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>110</td>
                    <td>A company uses AWS CodeArtifact to centrally store Python packages. The CodeArtifact repository is congured with the following repository<br>policy:<br>A development team is building a new project in an account that is in an organization in AWS Organizations. The development team wants to<br>use a Python library that has already been stored in the CodeArtifact repository in the organization. The development team uses AWS<br>CodePipeline and AWS CodeBuild to build the new application. The CodeBuild job that the development team uses to build the application is<br>congured to run in a VP</td>
                    <td>C. Share the CodeArtifact repository with the organization by using AWS Resource Access Manager (AWS RAM).<br>A. Create an Amazon S3 gateway endpoint. Update the route tables for the subnets that are running the CodeBuild job.<br>B. Update the repository policy’s Principal statement to include the ARN of the role that the CodeBuild project uses.<br>D. Update the role that the CodeBuild project uses so that the role has sucient permissions to use the CodeArtifact repository.<br>E. Specify the account that hosts the repository as the delegated administrator for CodeArtifact in the organization.</td>
                    <td>一家公司使用AWS CodeArtifact来集中存储Python包。CodeArtifact存储库配置了以下存储库策略：<br>开发团队正在AWS Organizations组织中的一个账户中构建新项目。开发团队想要使用已经存储在组织中CodeArtifact存储库中的Python库。开发团队使用AWS CodePipeline和AWS CodeBuild来构建新应用程序。开发团队用于构建应用程序的CodeBuild作业配置为在VPC中运行。</td>
                    <td>A. 创建Amazon S3网关端点。更新运行CodeBuild作业的子网的路由表。<br>这个选项不正确。S3网关端点与CodeArtifact访问无关，CodeArtifact有自己的VPC端点，而且题目的核心问题是权限配置，不是网络连接问题。<br><br>B. 更新存储库策略的Principal语句，包含CodeBuild项目使用的角色ARN。<br>这个选项正确。CodeArtifact存储库策略需要明确授权哪些主体可以访问存储库。通过在Principal语句中添加CodeBuild角色的ARN，可以允许该角色访问存储库中的Python包。<br><br>C. 使用AWS Resource Access Manager (AWS RAM)与组织共享CodeArtifact存储库。<br>这个选项不正确。CodeArtifact不支持通过AWS RAM进行跨账户共享。CodeArtifact的跨账户访问需要通过存储库策略和适当的IAM权限来实现。<br><br>D. 更新CodeBuild项目使用的角色，使该角色具有使用CodeArtifact存储库的足够权限。<br>这个选项正确。除了存储库策略允许访问外，CodeBuild角色本身也需要具有访问CodeArtifact的IAM权限，如codeartifact:GetAuthorizationToken、codeartifact:ReadFromRepository等权限。<br><br>E. 指定托管存储库的账户作为组织中CodeArtifact的委托管理员。<br>这个选项不正确。委托管理员主要用于集中管理组织级别的服务，但对于解决当前的访问权限问题不是必需的，通过存储库策略和IAM权限就可以解决。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>111</td>
                    <td>A company uses a series of individual Amazon CloudFormation templates to deploy its multi-Region applications. These templates must be<br>deployed in a specic order. The company is making more changes to the templates than previously expected and wants to deploy new<br>templates more eciently. Additionally, the data engineering team must be notied of all changes to the templates.</td>
                    <td>A. Create an AWS Lambda function to deploy the CloudFormation templates in the required order. Use stack policies to alert the data<br>engineering team.<br>B. Host the CloudFormation templates in Amazon S3. Use Amazon S3 events to directly trigger CloudFormation updates and Amazon SNS<br>notications.<br>C. Implement CloudFormation StackSets and use drift detection to trigger update alerts to the data engineering team.<br>D. Leverage CloudFormation nested stacks and stack sets for deployments. Use Amazon SNS to notify the data engineering team.</td>
                    <td>一家公司使用一系列独立的Amazon CloudFormation模板来部署其多区域应用程序。这些模板必须按特定顺序部署。公司对模板的更改比预期的更频繁，希望更高效地部署新模板。此外，数据工程团队必须收到所有模板更改的通知。</td>
                    <td>A. 创建AWS Lambda函数按所需顺序部署CloudFormation模板。使用堆栈策略来警告数据工程团队。<br>这个选项有一定可行性，Lambda可以控制部署顺序，但堆栈策略主要用于防止意外更新，而不是用于通知。堆栈策略不能直接发送警报给团队，这个方案在通知机制上存在问题。<br><br>B. 将CloudFormation模板托管在Amazon S3中。使用Amazon S3事件直接触发CloudFormation更新和Amazon SNS通知。<br>这个选项提供了自动化的解决方案。当模板文件在S3中更新时，S3事件可以触发CloudFormation的更新，同时通过SNS发送通知给数据工程团队。这种方案实现了高效部署和通知需求，但需要确保部署顺序的控制。<br><br>C. 实施CloudFormation StackSets并使用漂移检测来触发更新警报给数据工程团队。<br>StackSets适合多区域部署，但漂移检测主要用于检测配置偏差，不是用于模板更改通知。这个方案没有很好地解决通知需求和部署效率问题。<br><br>D. 利用CloudFormation嵌套堆栈和堆栈集进行部署。使用Amazon SNS通知数据工程团队。<br>嵌套堆栈可以管理复杂的依赖关系和部署顺序，StackSets适合多区域部署，SNS提供通知功能。这个方案综合性较好，能够满足所有需求。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>112</td>
                    <td>A DevOps engineer has implemented a CI/CD pipeline to deploy an AWS CloudFormation template that provisions a web application. The web<br>application consists of an Application Load Balancer (ALB), a target group, a launch template that uses an Amazon Linux 2 AMI, an Auto<br>Scaling group of Amazon EC2 instances, a security group, and an Amazon RDS for MySQL database. The launch template includes user data<br>that species a script to install and start the application.<br>The initial deployment of the application was successful. The DevOps engineer made changes to update the version of the application with<br>the user data. The CI/CD pipeline has deployed a new version of the template. However, the health checks on the ALB are now failing. The<br>health checks have marked all targets as unhealthy.<br>During investigation, the DevOps engineer notices that the CloudFormation stack has a status of UPDATE_COMPLET</td>
                    <td>E. However, when the<br>DevOps engineer connects to one of the EC2 instances and checks /var/log/messages, the DevOps engineer notices that the Apache web<br>server failed to start successfully because of a conguration error.<br>How can the DevOps engineer ensure that the CloudFormation deployment will fail if the user data fails to successfully nish running?<br>A. Use the cfn-signal helper script to signal success or failure to CloudFormation. Use the WaitOnResourceSignals update policy within the<br>CloudFormation template. Set an appropriate timeout for the update policy.<br>B. Create an Amazon CloudWatch alarm for the UnhealthyHostCount metric. Include an appropriate alarm threshold for the target group.<br>Create an Amazon Simple Notication Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation.<br>C. Create a lifecycle hook on the Auto Scaling group by using the AWS::AutoScaling::LifecycleHook resource. Create an Amazon Simple<br>Notication Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation. Set an appropriate timeout on the<br>lifecycle hook.<br>D. Use the Amazon CloudWatch agent to stream the cloud-init logs. Create a subscription lter that includes an AWS Lambda function with<br>an appropriate invocation timeout. Congure the Lambda function to use the SignalResource API operation to signal success or failure to<br>CloudFormation.</td>
                    <td>一名DevOps工程师实施了一个CI/CD流水线来部署AWS CloudFormation模板，该模板用于配置一个Web应用程序。该Web应用程序包含一个应用负载均衡器(ALB)、一个目标组、一个使用Amazon Linux 2 AMI的启动模板、一个Amazon EC2实例的Auto Scaling组、一个安全组，以及一个Amazon RDS for MySQL数据库。启动模板包含用户数据，指定了安装和启动应用程序的脚本。<br><br>应用程序的初始部署是成功的。DevOps工程师通过用户数据进行了更改以更新应用程序版本。CI/CD流水线已部署了新版本的模板。然而，ALB上的健康检查现在失败了。健康检查将所有目标标记为不健康。<br><br>在调查过程中，DevOps工程师注意到CloudFormation堆栈的状态为UPDATE_COMPLETE。然而，当DevOps工程师连接到其中一个EC2实例并检查/var/log/messages时，发现Apache Web服务器由于配置错误而未能成功启动。<br><br>DevOps工程师如何确保当用户数据未能成功完成运行时，CloudFormation部署会失败？</td>
                    <td>A. 使用cfn-signal辅助脚本向CloudFormation发送成功或失败信号。在CloudFormation模板中使用WaitOnResourceSignals更新策略。为更新策略设置适当的超时时间。<br>这是正确的方法。cfn-signal是专门为此目的设计的CloudFormation辅助脚本，可以在用户数据脚本执行完成后向CloudFormation发送信号。WaitOnResourceSignals策略会等待接收到成功信号后才认为资源创建成功，如果在超时时间内未收到信号或收到失败信号，部署会失败。<br><br>B. 为UnhealthyHostCount指标创建Amazon CloudWatch告警。为目标组包含适当的告警阈值。创建Amazon Simple Notification Service (Amazon SNS)主题作为目标，向CloudFormation发送成功或失败信号。<br>这种方法不正确。CloudWatch告警无法直接向CloudFormation发送信号来影响部署状态，而且这是在部署完成后的监控，无法在部署过程中阻止失败的部署。<br><br>C. 通过使用AWS::AutoScaling::LifecycleHook资源在Auto Scaling组上创建生命周期钩子。创建Amazon SNS主题作为目标，向CloudFormation发送成功或失败信号。为生命周期钩子设置适当的超时时间。<br>生命周期钩子主要用于在实例启动或终止过程中执行自定义操作，但它们不能直接向CloudFormation发送部署成功或失败的信号。<br><br>D. 使用Amazon CloudWatch代理流式传输cloud-init日志。创建包含AWS Lambda函数的订阅过滤器，设置适当的调用超时。配置Lambda函数使用SignalResource API操作向CloudFormation发送成功或失败信号。<br>虽然技术上可行，但这种方法过于复杂，需要额外的组件和配置。相比之下，cfn-signal是更直接和标准的解决方案。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>113</td>
                    <td>A company has a data ingestion application that runs across multiple AWS accounts. The accounts are in an organization in AWS<br>Organizations. The company needs to monitor the application and consolidate access to the application. Currently, the company is running<br>the application on Amazon EC2 instances from several Auto Scaling groups. The EC2 instances have no access to the internet because the<br>data is sensitive. Engineers have deployed the necessary VPC endpoints. The EC2 instances run a custom AMI that is built specically for the<br>application.<br>To maintain and troubleshoot the application, system administrators need the ability to log in to the EC2 instances. This access must be<br>automated and controlled centrally. The company’s security team must receive a notication whenever the instances are accessed.</td>
                    <td>A. Create an Amazon EventBridge rule to send notications to the security team whenever a user logs in to an EC2 instance. Use EC2<br>Instance Connect to log in to the instances. Deploy Auto Scaling groups by using AWS CloudFormation. Use the cfn-init helper script to<br>deploy appropriate VPC routes for external access. Rebuild the custom AMI so that the custom AMI includes AWS Systems Manager Agent.<br>B. Deploy a NAT gateway and a bastion host that has internet access. Create a security group that allows incoming trac on all the EC2<br>instances from the bastion host. Install AWS Systems Manager Agent on all the EC2 instances. Use Auto Scaling group lifecycle hooks for<br>monitoring and auditing access. Use Systems Manager Session Manager to log in to the instances. Send logs to a log group in Amazon<br>CloudWatch Logs. Export data to Amazon S3 for auditing. Send notications to the security team by using S3 event notications.<br>C. Use EC2 Image Builder to rebuild the custom AMI. Include the most recent version of AWS Systems Manager Agent in the image.<br>Congure the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to all the EC2 instances. Use Systems Manager<br>Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notication for new le<br>uploads to send a message to the security team through an Amazon Simple Notication Service (Amazon SNS) topic.<br>D. Use AWS Systems Manager Automation to build Systems Manager Agent into the custom AMI. Congure AWS Cong to attach an SCP to<br>the root organization account to allow the EC2 instances to connect to Systems Manager. Use Systems Manager Session Manager to log<br>in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notication for new le uploads to send a<br>message to the security team through an Amazon Simple Notication Service (Amazon SNS) topic.</td>
                    <td>一家公司有一个跨多个AWS账户运行的数据摄取应用程序。这些账户位于AWS Organizations中的一个组织内。公司需要监控应用程序并整合对应用程序的访问。目前，公司在来自多个Auto Scaling组的Amazon EC2实例上运行应用程序。由于数据敏感，EC2实例无法访问互联网。工程师已部署了必要的VPC端点。EC2实例运行专门为应用程序构建的自定义AMI。<br><br>为了维护和故障排除应用程序，系统管理员需要能够登录到EC2实例。此访问必须是自动化的并集中控制的。公司的安全团队必须在实例被访问时收到通知。</td>
                    <td>选项A：使用EventBridge规则和EC2 Instance Connect的方案存在问题。EC2 Instance Connect需要互联网访问来工作，但题目明确说明实例没有互联网访问。使用cfn-init部署外部访问的VPC路由与安全要求矛盾，因为数据敏感不应有外部访问。<br><br>选项B：部署NAT网关和堡垒主机会增加复杂性和成本，且与题目要求的无互联网访问相矛盾。虽然使用了Systems Manager Session Manager，但通过堡垒主机的方式不是最佳实践，增加了安全风险点。<br><br>选项C：使用EC2 Image Builder重建自定义AMI并包含最新版本的Systems Manager Agent是正确的方法。配置Auto Scaling组附加AmazonSSMManagedInstanceCore角色确保实例有适当权限。使用Session Manager登录无需互联网访问，启用S3日志记录和SNS通知满足审计和通知要求。<br><br>选项D：使用AWS Config附加SCP到根组织账户的描述不正确。AWS Config不用于附加SCP，且SCP是在组织级别管理的策略。Systems Manager Automation构建AMI的方法不如EC2 Image Builder标准和高效。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>114</td>
                    <td>A company uses Amazon S3 to store proprietary information. The development team creates buckets for new projects on a daily basis. The<br>security team wants to ensure that all existing and future buckets have encryption, logging, and versioning enabled. Additionally, no buckets<br>should ever be publicly read or write accessible.</td>
                    <td>A. Enable AWS CloudTrail and congure automatic remediation using AWS Lambda.<br>B. Enable AWS Cong rules and congure automatic remediation using AWS Systems Manager documents.<br>C. Enable AWS Trusted Advisor and congure automatic remediation using Amazon EventBridge.<br>D. Enable AWS Systems Manager and congure automatic remediation using Systems Manager documents.</td>
                    <td>一家公司使用Amazon S3存储专有信息。开发团队每天为新项目创建存储桶。安全团队希望确保所有现有和未来的存储桶都启用加密、日志记录和版本控制。此外，任何存储桶都不应该允许公共读取或写入访问。</td>
                    <td>A. 启用AWS CloudTrail并使用AWS Lambda配置自动修复：CloudTrail主要用于记录API调用和审计，虽然可以检测到S3存储桶的配置变更，但它本身不是专门用于合规性检查的服务。使用Lambda进行自动修复需要自定义开发大量代码来检查和修复存储桶配置，实现复杂且维护成本高。<br><br>B. 启用AWS Config规则并使用AWS Systems Manager文档配置自动修复：AWS Config专门设计用于监控和评估AWS资源的配置合规性。它提供预构建的规则来检查S3存储桶的加密、版本控制、公共访问等配置。Systems Manager文档可以定义标准化的修复操作，当Config检测到不合规时自动执行修复。这是最适合的解决方案。<br><br>C. 启用AWS Trusted Advisor并使用Amazon EventBridge配置自动修复：Trusted Advisor主要提供成本优化、性能和安全建议，但不是实时的合规性监控工具，且其检查频率有限。EventBridge虽然可以处理事件，但Trusted Advisor不是最佳的合规性检查源。<br><br>D. 启用AWS Systems Manager并使用Systems Manager文档配置自动修复：虽然Systems Manager可以执行修复操作，但缺少持续监控和合规性检查的组件。需要配合其他服务才能实现完整的解决方案。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>115</td>
                    <td>A DevOps engineer is researching the least expensive way to implement an image batch processing cluster on AWS. The application cannot<br>run in Docker containers and must run on Amazon EC2. The batch job stores checkpoint data on an NFS volume and can tolerate interruptions.<br>Conguring the cluster software from a generic EC2 Linux image takes 30 minutes.</td>
                    <td>A. Use Amazon EFS for checkpoint data. To complete the job, use an EC2 Auto Scaling group and an On-Demand pricing model to provision<br>EC2 instances temporarily.<br>B. Use GlusterFS on EC2 instances for checkpoint data. To run the batch job, congure EC2 instances manually. When the job completes,<br>shut down the instances manually.<br>C. Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances, and utilize user data to congure the EC2 Linux<br>instance on startup.<br>D. Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances. Create a custom AMI for the cluster and use the<br>latest AMI when creating instances.</td>
                    <td>一位DevOps工程师正在研究在AWS上实现图像批处理集群的最经济方式。该应用程序无法在Docker容器中运行，必须在Amazon EC2上运行。批处理作业将检查点数据存储在NFS卷上，并且可以容忍中断。从通用EC2 Linux镜像配置集群软件需要30分钟。</td>
                    <td>A. 使用Amazon EFS存储检查点数据，使用EC2 Auto Scaling组和按需定价模型临时配置EC2实例来完成作业。这个方案使用按需实例成本较高，虽然EFS是合适的NFS解决方案，但不是最经济的选择。<br><br>B. 在EC2实例上使用GlusterFS存储检查点数据，手动配置EC2实例运行批处理作业，作业完成后手动关闭实例。手动管理增加了运维复杂度，GlusterFS需要额外配置和管理，不如托管的EFS服务便捷，且没有利用Spot实例降低成本。<br><br>C. 使用Amazon EFS存储检查点数据，使用EC2 Fleet启动EC2 Spot实例，利用用户数据在启动时配置EC2 Linux实例。这个方案结合了EFS的NFS兼容性、Spot实例的低成本优势，以及EC2 Fleet的自动化管理能力，虽然每次启动需要30分钟配置时间，但成本最低。<br><br>D. 使用Amazon EFS存储检查点数据，使用EC2 Fleet启动EC2 Spot实例，创建自定义AMI用于集群并在创建实例时使用最新AMI。虽然自定义AMI可以减少启动配置时间，但创建和维护AMI会增加额外成本和管理复杂度。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>116</td>
                    <td>A company recently migrated its legacy application from on-premises to AWS. The application is hosted on Amazon EC2 instances behind an<br>Application Load Balancer, which is behind Amazon API Gateway. The company wants to ensure users experience minimal disruptions during<br>any deployment of a new version of the application. The company also wants to ensure it can quickly roll back updates if there is an issue.</td>
                    <td>A. Introduce changes as a separate environment parallel to the existing one. Congure API Gateway to use a canary release deployment<br>to send a small subset of user trac to the new environment.<br>B. Introduce changes as a separate environment parallel to the existing one. Update the application’s DNS alias records to point to the<br>new environment.<br>C. Introduce changes as a separate target group behind the existing Application Load Balancer. Congure API Gateway to route user<br>trac to the new target group in steps.<br>D. Introduce changes as a separate target group behind the existing Application Load Balancer. Congure API Gateway to route all trac<br>to the Application Load Balancer, which then sends the trac to the new target group.</td>
                    <td>一家公司最近将其传统应用程序从本地迁移到AWS。该应用程序托管在Amazon EC2实例上，位于应用负载均衡器后面，而应用负载均衡器又位于Amazon API Gateway后面。公司希望确保用户在部署新版本应用程序时体验到最小的中断。公司还希望确保在出现问题时能够快速回滚更新。</td>
                    <td>选项A：引入更改作为与现有环境并行的独立环境。配置API Gateway使用金丝雀发布部署，将一小部分用户流量发送到新环境。这种方法需要创建完全独立的环境，包括新的EC2实例和负载均衡器，成本较高。虽然API Gateway支持金丝雀部署，但这种架构会增加复杂性和资源消耗。<br><br>选项B：引入更改作为与现有环境并行的独立环境。更新应用程序的DNS别名记录以指向新环境。这种方法存在重大缺陷，因为DNS更改需要时间传播，无法实现渐进式流量切换，也难以快速回滚。DNS切换是全有或全无的方式，不符合最小中断的要求。<br><br>选项C：引入更改作为现有应用负载均衡器后面的独立目标组。配置API Gateway分步骤将用户流量路由到新目标组。这种方法利用了ALB的加权路由功能，可以逐步增加到新目标组的流量比例，实现平滑的蓝绿部署或金丝雀部署，成本效益高且易于管理。<br><br>选项D：引入更改作为现有应用负载均衡器后面的独立目标组。配置API Gateway将所有流量路由到应用负载均衡器，然后由负载均衡器将流量发送到新目标组。这种方法没有提供渐进式部署机制，会将所有流量立即切换到新版本，不符合最小中断的要求。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>117</td>
                    <td>A company is storing 100 GB of log data in .csv format in an Amazon S3 bucket. SQL developers want to query this data and generate graphs<br>to visualize it. The SQL developers also need an ecient, automated way to store metadata from the .csv le.</td>
                    <td>A. Filter the data through AWS X-Ray to visualize the data.<br>B. Filter the data through Amazon QuickSight to visualize the data.<br>C. Query the data with Amazon Athena.<br>D. Query the data with Amazon Redshift.<br>E. Use the AWS Glue Data Catalog as the persistent metadata store.</td>
                    <td>一家公司在Amazon S3存储桶中存储了100 GB的.csv格式日志数据。SQL开发人员希望查询这些数据并生成图表来可视化展示。SQL开发人员还需要一种高效、自动化的方式来存储.csv文件的元数据。</td>
                    <td>A. 通过AWS X-Ray过滤数据以可视化数据 - 这个选项不正确。AWS X-Ray是一个分布式跟踪服务，主要用于分析和调试分布式应用程序的性能，而不是用于查询和可视化存储在S3中的CSV数据。X-Ray不适合处理静态的日志文件数据分析需求。<br><br>B. 通过Amazon QuickSight过滤数据以可视化数据 - 这个选项正确。Amazon QuickSight是AWS的商业智能服务，专门用于数据可视化和生成图表。它可以直接连接到S3中的数据源，支持CSV格式，并能够创建各种类型的图表和仪表板，完全满足题目中生成图表可视化的需求。<br><br>C. 使用Amazon Athena查询数据 - 这个选项正确。Amazon Athena是一个无服务器的交互式查询服务，可以直接对存储在S3中的数据使用标准SQL进行查询。它支持CSV格式，非常适合SQL开发人员查询S3中的结构化数据，无需预先加载数据。<br><br>D. 使用Amazon Redshift查询数据 - 这个选项在技术上可行但不是最佳选择。Redshift是一个数据仓库服务，需要先将数据加载到Redshift集群中才能查询。对于直接查询S3中的数据，Athena更加高效和经济，不需要额外的数据加载步骤和持续的集群成本。<br><br>E. 使用AWS Glue Data Catalog作为持久化元数据存储 - 这个选项正确。AWS Glue Data Catalog是一个完全托管的元数据存储库，可以自动发现、分类和存储数据的元数据信息。它可以自动爬取S3中的CSV文件并提取schema信息，完美满足题目中自动化存储元数据的需求。</td>
                    <td>BCE</td>
                </tr>
                <tr>
                    <td>118</td>
                    <td>A company deploys its corporate infrastructure on AWS across multiple AWS Regions and Availability Zones. The infrastructure is deployed on<br>Amazon EC2 instances and connects with AWS IoT Greengrass devices. The company deploys additional resources on on-premises servers<br>that are located in the corporate headquarters.<br>The company wants to reduce the overhead involved in maintaining and updating its resources. The company’s DevOps team plans to use<br>AWS Systems Manager to implement automated management and application of patches. The DevOps team conrms that Systems Manager<br>is available in the Regions that the resources are deployed in. Systems Manager also is available in a Region near the corporate<br>headquarters.</td>
                    <td>A. Apply tags to all the EC2 instances, AWS IoT Greengrass devices, and on-premises servers. Use Systems Manager Session Manager to<br>push patches to all the tagged devices.<br>B. Use Systems Manager Run Command to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises<br>servers.<br>C. Use Systems Manager Patch Manager to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises<br>servers as a Systems Manager maintenance window task.<br>D. Congure Amazon EventBridge to monitor Systems Manager Patch Manager for updates to patch baselines. Associate Systems<br>Manager Run Command with the event to initiate a patch action for all EC2 instances, AWS IoT Greengrass devices, and on-premises<br>servers.<br>E. Create an IAM instance prole for Systems Manager. Attach the instance prole to all the EC2 instances in the AWS account. For the<br>AWS IoT Greengrass devices and on-premises servers, create an IAM service role for Systems Manager.</td>
                    <td>一家公司在AWS的多个区域和可用区部署其企业基础设施。基础设施部署在Amazon EC2实例上，并与AWS IoT Greengrass设备连接。该公司还在位于企业总部的本地服务器上部署了额外资源。<br><br>该公司希望减少维护和更新资源的开销。公司的DevOps团队计划使用AWS Systems Manager来实现补丁的自动化管理和应用。DevOps团队确认Systems Manager在资源部署的区域中可用，在企业总部附近的区域中也可用。</td>
                    <td>A. 为所有EC2实例、AWS IoT Greengrass设备和本地服务器应用标签。使用Systems Manager Session Manager向所有标记的设备推送补丁。<br>这个选项不正确。Session Manager主要用于安全的shell访问，而不是补丁管理。虽然标签是好的实践，但Session Manager不是推送补丁的正确工具。<br><br>B. 使用Systems Manager Run Command为EC2实例、AWS IoT Greengrass设备和本地服务器安排补丁。<br>这个选项正确。Run Command可以在所有这些类型的设备上执行补丁命令，包括EC2实例、IoT Greengrass设备和本地服务器，只要它们安装了SSM Agent。<br><br>C. 使用Systems Manager Patch Manager作为Systems Manager维护窗口任务，为EC2实例、AWS IoT Greengrass设备和本地服务器安排补丁。<br>这个选项正确。Patch Manager是专门设计用于补丁管理的服务，可以通过维护窗口进行调度，支持所有提到的设备类型。<br><br>D. 配置Amazon EventBridge监控Systems Manager Patch Manager的补丁基线更新。将Systems Manager Run Command与事件关联，为所有EC2实例、AWS IoT Greengrass设备和本地服务器启动补丁操作。<br>这个选项过于复杂，不是实现自动化补丁管理的最直接方法。虽然技术上可行，但增加了不必要的复杂性。<br><br>E. 为Systems Manager创建IAM实例配置文件。将实例配置文件附加到AWS账户中的所有EC2实例。对于AWS IoT Greengrass设备和本地服务器，创建Systems Manager的IAM服务角色。<br>这个选项描述的是权限配置，这是必要的先决条件，但不是实际的补丁管理解决方案。</td>
                    <td>BC</td>
                </tr>
                <tr>
                    <td>119</td>
                    <td>A company is testing a web application that runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto<br>Scaling group across multiple Availability Zones. The company uses a blue/green deployment process with immutable instances when<br>deploying new software.<br>During testing, users are being automatically logged out of the application at random times. Testers also report that, when a new version of<br>the application is deployed, all users are logged out. The development team needs a solution to ensure users remain logged in across scaling<br>events and application deployments.</td>
                    <td>A. Enable smart sessions on the load balancer and modify the application to check for an existing session.<br>B. Enable session sharing on the load balancer and modify the application to read from the session store.<br>C. Store user session information in an Amazon S3 bucket and modify the application to read session information from the bucket.<br>D. Modify the application to store user session information in an Amazon ElastiCache cluster.</td>
                    <td>一家公司正在测试运行在Application Load Balancer后面Amazon EC2实例上的Web应用程序。这些实例在Auto Scaling组中跨多个可用区运行。公司在部署新软件时使用蓝/绿部署流程和不可变实例。<br>在测试期间，用户会在随机时间被自动登出应用程序。测试人员还报告说，当部署新版本的应用程序时，所有用户都会被登出。开发团队需要一个解决方案来确保用户在扩缩容事件和应用程序部署过程中保持登录状态。</td>
                    <td>A. 在负载均衡器上启用智能会话并修改应用程序以检查现有会话 - 这个选项存在问题，因为Application Load Balancer本身没有&quot;智能会话&quot;这样的功能。ALB只有粘性会话(sticky sessions)功能，但这无法解决蓝/绿部署时实例完全替换导致的会话丢失问题。<br><br>B. 在负载均衡器上启用会话共享并修改应用程序从会话存储中读取 - 这个选项也不准确，因为ALB本身不提供会话共享功能。负载均衡器主要负责流量分发，不是会话存储的解决方案。<br><br>C. 将用户会话信息存储在Amazon S3存储桶中，并修改应用程序从存储桶读取会话信息 - 虽然S3可以存储数据，但它不适合作为会话存储，因为S3的访问延迟较高，不适合频繁的会话读写操作，会影响应用程序性能。<br><br>D. 修改应用程序将用户会话信息存储在Amazon ElastiCache集群中 - 这是最佳解决方案。ElastiCache是专门为缓存设计的内存数据库服务，具有低延迟、高性能的特点，非常适合存储会话数据。它独立于EC2实例存在，因此在Auto Scaling或蓝/绿部署时会话数据不会丢失。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>120</td>
                    <td>A DevOps engineer needs to congure a blue/green deployment for an existing three-tier application. The application runs on Amazon EC2<br>instances and uses an Amazon RDS database. The EC2 instances run behind an Application Load Balancer (ALB) and are in an Auto Scaling<br>group.<br>The DevOps engineer has created a launch template and an Auto Scaling group for the blue environment. The DevOps engineer also has<br>created a launch template and an Auto Scaling group for the green environment. Each Auto Scaling group deploys to a matching blue or green<br>target group. The target group also species which software, blue or green, gets loaded on the EC2 instances. The ALB can be congured to<br>send trac to the blue environment’s target group or the green environment’s target group. An Amazon Route 53 record for www.example.com<br>points to the AL</td>
                    <td>B. <br>A. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment’s EC2<br>instances. When the rolling restart is complete, use an AWS CLI command to update the ALB to send trac to the green environment’s<br>target group.<br>C. Update the launch template to deploy the green environment’s software on the blue environment’s EC2 instances. Keep the target<br>groups and Auto Scaling groups unchanged in both environments. Perform a rolling restart of the blue environment’s EC2 instances.<br>D. Start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment’s EC2<br>instances. When the rolling restart is complete, update the Route 53 DNS to point to the green environment’s endpoint on the AL</td>
                    <td>一名DevOps工程师需要为现有的三层应用程序配置蓝/绿部署。该应用程序运行在Amazon EC2实例上，并使用Amazon RDS数据库。EC2实例运行在应用负载均衡器(ALB)后面，并位于Auto Scaling组中。<br><br>DevOps工程师已经为蓝色环境创建了启动模板和Auto Scaling组。DevOps工程师还为绿色环境创建了启动模板和Auto Scaling组。每个Auto Scaling组部署到匹配的蓝色或绿色目标组。目标组还指定在EC2实例上加载哪个软件，蓝色或绿色。ALB可以配置为将流量发送到蓝色环境的目标组或绿色环境的目标组。Amazon Route 53记录www.example.com指向ALB。<br><br>选项：<br>A. [选项A内容缺失]<br>B. 启动绿色环境Auto Scaling组的滚动重启，在绿色环境的EC2实例上部署新软件。当滚动重启完成后，使用AWS CLI命令更新ALB将流量发送到绿色环境的目标组。<br>C. 更新启动模板，在蓝色环境的EC2实例上部署绿色环境的软件。保持两个环境中的目标组和Auto Scaling组不变。对蓝色环境的EC2实例执行滚动重启。<br>D. 启动绿色环境Auto Scaling组的滚动重启，在绿色环境的EC2实例上部署新软件。当滚动重启完成后，更新Route 53 DNS指向ALB上绿色环境的端点。</td>
                    <td>选项B：这个方案通过滚动重启绿色环境来部署新软件，然后通过ALB切换流量到绿色环境。这是标准的蓝/绿部署方式，能够实现快速切换，如果出现问题可以快速回滚到蓝色环境。滚动重启确保了服务的连续性，通过ALB进行流量切换是即时的，符合蓝/绿部署的最佳实践。<br><br>选项C：这个方案破坏了蓝/绿部署的核心概念。它试图在蓝色环境中部署绿色软件，这样就失去了两个独立环境的优势。如果部署失败，就没有可以快速回滚的环境，违背了蓝/绿部署的基本原则。<br><br>选项D：虽然部署新软件到绿色环境的方法是正确的，但通过修改Route 53 DNS记录来切换流量存在问题。DNS更改需要时间传播，可能需要几分钟到几小时，这不符合蓝/绿部署快速切换的要求，而且回滚也会很慢。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>121</td>
                    <td>A company is building a new pipeline by using AWS CodePipeline and AWS CodeBuild in a build account. The pipeline consists of two stages.<br>The rst stage is a CodeBuild job to build and package an AWS Lambda function. The second stage consists of deployment actions that<br>operate on two different AWS accounts: a development environment account and a production environment account. The deployment stages<br>use the AWS CloudFormation action that CodePipeline invokes to deploy the infrastructure that the Lambda function requires.<br>A DevOps engineer creates the CodePipeline pipeline and congures the pipeline to encrypt build artifacts by using the AWS Key<br>Management Service (AWS KMS) AWS managed key for Amazon S3 (the aws/s3 key). The artifacts are stored in an S3 bucket. When the<br>pipeline runs, the CloudFormation actions fail with an access denied error.</td>
                    <td>A. Create an S3 bucket in each AWS account for the artifacts. Allow the pipeline to write to the S3 buckets. Create a CodePipeline S3<br>action to copy the artifacts to the S3 bucket in each AWS account. Update the CloudFormation actions to reference the artifacts S3 bucket<br>in the production account.<br>B. Create a customer managed KMS key. Congure the KMS key policy to allow the IAM roles used by the CloudFormation action to<br>perform decrypt operations. Modify the pipeline to use the customer managed KMS key to encrypt artifacts.<br>C. Create an AWS managed KMS key. Congure the KMS key policy to allow the development account and the production account to<br>perform decrypt operations. Modify the pipeline to use the KMS key to encrypt artifacts.<br>D. In the development account and in the production account, create an IAM role for CodePipeline. Congure the roles with permissions to<br>perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline<br>account, congure the CodePipeline CloudFormation action to use the roles.<br>E. In the development account and in the production account, create an IAM role for CodePipeline. Congure the roles with permissions to<br>perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline<br>account, modify the artifacts S3 bucket policy to allow the roles access. Congure the CodePipeline CloudFormation action to use the<br>roles.</td>
                    <td>一家公司正在使用AWS CodePipeline和AWS CodeBuild在构建账户中构建新的管道。该管道包含两个阶段。<br>第一阶段是CodeBuild作业，用于构建和打包AWS Lambda函数。第二阶段包含在两个不同AWS账户上运行的部署操作：开发环境账户和生产环境账户。部署阶段使用CodePipeline调用的AWS CloudFormation操作来部署Lambda函数所需的基础设施。<br>DevOps工程师创建了CodePipeline管道，并配置管道使用AWS密钥管理服务(AWS KMS)的AWS托管密钥(aws/s3密钥)来加密构建工件。工件存储在S3存储桶中。当管道运行时，CloudFormation操作失败并出现访问拒绝错误。</td>
                    <td>A. 在每个AWS账户中创建S3存储桶用于工件，允许管道写入S3存储桶，创建CodePipeline S3操作将工件复制到每个账户的S3存储桶，更新CloudFormation操作以引用生产账户中的工件S3存储桶。这种方法虽然可以解决跨账户访问问题，但增加了复杂性且不是最佳实践，因为它需要额外的存储桶和复制操作。<br><br>B. 创建客户托管的KMS密钥，配置KMS密钥策略以允许CloudFormation操作使用的IAM角色执行解密操作，修改管道以使用客户托管的KMS密钥加密工件。这是正确的解决方案，因为AWS托管的aws/s3密钥无法跨账户使用，而客户托管密钥可以配置跨账户权限。<br><br>C. 创建AWS托管的KMS密钥并配置密钥策略。这是错误的，因为AWS托管密钥的策略无法修改，用户无法为其配置跨账户权限。<br><br>D. 在开发和生产账户中创建CodePipeline的IAM角色，配置角色具有CloudFormation操作权限和从工件S3存储桶检索和解密对象的权限，在CodePipeline账户中配置CloudFormation操作使用这些角色。这是正确的跨账户角色配置方法。<br><br>E. 与D类似但还需要修改工件S3存储桶策略。虽然这也可能有效，但通常通过IAM角色权限就足够了，不需要额外的存储桶策略修改。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>122</td>
                    <td>A company is using an organization in AWS Organizations to manage multiple AWS accounts. The company’s development team wants to use<br>AWS Lambda functions to meet resiliency requirements and is rewriting all applications to work with Lambda functions that are deployed in a<br>VP</td>
                    <td>C. Create a new EFS le system in Account<br>B. <br>A. <br>D. Update the Lambda execution roles with permission to access the VPC and the EFS le system.<br>E. Create a VPC peering connection to connect Account A to Account</td>
                    <td>一家公司正在使用AWS Organizations中的组织来管理多个AWS账户。公司的开发团队希望使用AWS Lambda函数来满足弹性要求，并正在重写所有应用程序以使用部署在VPC中的Lambda函数。</td>
                    <td>由于题目内容不完整，选项A、B、E的具体内容缺失，只能看到部分选项内容：<br>- 选项A：内容缺失，无法分析具体要求<br>- 选项B：内容缺失，无法分析具体要求  <br>- 选项C：在账户B中创建一个新的EFS文件系统。这个选项涉及创建EFS文件系统，EFS可以为Lambda函数提供持久化存储，支持多个Lambda函数同时访问，有助于满足弹性要求<br>- 选项D：更新Lambda执行角色，授予访问VPC和EFS文件系统的权限。这是必要的配置，因为Lambda函数需要适当的IAM权限才能访问VPC资源和EFS文件系统<br>- 选项E：创建VPC对等连接来连接账户A到账户（内容不完整）。VPC对等连接可以实现跨账户的网络连接，允许不同账户中的资源相互通信</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>123</td>
                    <td>A media company has several thousand Amazon EC2 instances in an AWS account. The company is using Slack and a shared email inbox for<br>team communications and important updates. A DevOps engineer needs to send all AWS-scheduled EC2 maintenance notications to the<br>Slack channel and the shared inbox. The solution must include the instances’ Name and Owner tags.</td>
                    <td>A. Integrate AWS Trusted Advisor with AWS Cong. Congure a custom AWS Cong rule to invoke an AWS Lambda function to publish<br>notications to an Amazon Simple Notication Service (Amazon SNS) topic. Subscribe a Slack channel endpoint and the shared inbox to<br>the topic.<br>B. Use Amazon EventBridge to monitor for AWS Health events. Congure the maintenance events to target an Amazon Simple Notication<br>Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to send notications to the Slack channel and the<br>shared inbox.<br>C. Create an AWS Lambda function that sends EC2 maintenance notications to the Slack channel and the shared inbox. Monitor EC2<br>health events by using Amazon CloudWatch metrics. Congure a CloudWatch alarm that invokes the Lambda function when a<br>maintenance notication is received.<br>D. Congure AWS Support integration with AWS CloudTrail. Create a CloudTrail lookup event to invoke an AWS Lambda function to pass<br>EC2 maintenance notications to Amazon Simple Notication Service (Amazon SNS). Congure Amazon SNS to target the Slack channel<br>and the shared inbox.</td>
                    <td>一家媒体公司在AWS账户中有数千个Amazon EC2实例。该公司使用Slack和共享邮箱进行团队沟通和重要更新。DevOps工程师需要将所有AWS计划的EC2维护通知发送到Slack频道和共享邮箱。解决方案必须包含实例的Name和Owner标签。</td>
                    <td>选项A：使用AWS Trusted Advisor与AWS Config集成，配置自定义AWS Config规则来调用Lambda函数发布通知到SNS主题。这个方案存在问题，因为Trusted Advisor主要用于成本优化、性能、安全性和容错性建议，而不是用于监控EC2维护事件。AWS Config主要用于资源配置合规性检查，不是监控维护事件的正确工具。<br><br>选项B：使用Amazon EventBridge监控AWS Health事件，配置维护事件目标为SNS主题，订阅Lambda函数到SNS主题来发送通知到Slack频道和共享邮箱。这是正确的架构，因为AWS Health事件包含EC2维护通知，EventBridge可以有效捕获这些事件，Lambda函数可以处理实例标签信息并发送到多个目标。<br><br>选项C：创建Lambda函数发送EC2维护通知，使用CloudWatch指标监控EC2健康事件。这个方案有问题，因为CloudWatch指标不能直接监控到AWS计划的维护事件，维护通知不是通过CloudWatch指标传递的。<br><br>选项D：配置AWS Support与CloudTrail集成，创建CloudTrail查找事件调用Lambda函数。这个方案不正确，因为CloudTrail主要记录API调用活动，不是监控AWS Health维护事件的合适工具，且AWS Support集成方式不符合需求。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>124</td>
                    <td>An AWS CodePipeline pipeline has implemented a code release process. The pipeline is integrated with AWS CodeDeploy to deploy versions<br>of an application to multiple Amazon EC2 instances for each CodePipeline stage.<br>During a recent deployment, the pipeline failed due to a CodeDeploy issue. The DevOps team wants to improve monitoring and notications<br>during deployment to decrease resolution times.</td>
                    <td>A. Implement Amazon CloudWatch Logs for CodePipeline and CodeDeploy, create an AWS Cong rule to evaluate code deployment issues,<br>and create an Amazon Simple Notication Service (Amazon SNS) topic to notify stakeholders of deployment issues.<br>B. Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an AWS Lambda function to evaluate code deployment<br>issues, and create an Amazon Simple Notication Service (Amazon SNS) topic to notify stakeholders of deployment issues.<br>C. Implement AWS CloudTrail to record CodePipeline and CodeDeploy API call information, create an AWS Lambda function to evaluate<br>code deployment issues, and create an Amazon Simple Notication Service (Amazon SNS) topic to notify stakeholders of deployment<br>issues.<br>D. Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an Amazon Inspector assessment target to evaluate code<br>deployment issues, and create an Amazon Simple Notication Service (Amazon SNS) topic to notify stakeholders of deployment issues.</td>
                    <td>一个AWS CodePipeline管道已经实现了代码发布流程。该管道与AWS CodeDeploy集成，为每个CodePipeline阶段将应用程序版本部署到多个Amazon EC2实例。在最近的一次部署中，管道由于CodeDeploy问题而失败。DevOps团队希望改进部署期间的监控和通知，以减少解决时间。</td>
                    <td>A. 为CodePipeline和CodeDeploy实施Amazon CloudWatch Logs，创建AWS Config规则来评估代码部署问题，并创建Amazon SNS主题来通知利益相关者部署问题。这个选项的问题在于AWS Config主要用于配置合规性检查，而不是实时监控部署事件。CloudWatch Logs虽然可以记录日志，但不是最佳的事件驱动解决方案。<br><br>B. 为CodePipeline和CodeDeploy实施Amazon EventBridge，创建AWS Lambda函数来评估代码部署问题，并创建Amazon SNS主题来通知利益相关者部署问题。这是最佳选择，因为EventBridge专门设计用于捕获AWS服务的状态变化事件，Lambda可以处理复杂的业务逻辑来评估部署问题，SNS提供可靠的通知机制。<br><br>C. 实施AWS CloudTrail来记录CodePipeline和CodeDeploy API调用信息，创建AWS Lambda函数来评估代码部署问题，并创建Amazon SNS主题来通知利益相关者部署问题。CloudTrail主要用于审计API调用，不是实时监控部署状态变化的最佳工具，响应时间较慢。<br><br>D. 为CodePipeline和CodeDeploy实施Amazon EventBridge，创建Amazon Inspector评估目标来评估代码部署问题，并创建Amazon SNS主题来通知利益相关者部署问题。Amazon Inspector主要用于安全漏洞评估，不适合评估部署问题。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>125</td>
                    <td>A global company manages multiple AWS accounts by using AWS Control Tower. The company hosts internal applications and public<br>applications.<br>Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in<br>AWS Organizations. One of the AWS Control Tower member accounts serves as a centralized DevOps account with CI/CD pipelines that<br>application teams use to deploy applications to their respective target AWS accounts. An IAM role for deployment exists in the centralized<br>DevOps account.<br>An application team is attempting to deploy its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in an application<br>AWS account. An IAM role for deployment exists in the application AWS account. The deployment is through an AWS CodeBuild project that is<br>set up in the centralized DevOps account. The CodeBuild project uses an IAM service role for CodeBuild. The deployment is failing with an<br>Unauthorized error during attempts to connect to the cross-account EKS cluster from CodeBuild.</td>
                    <td>A. Congure the application account’s deployment IAM role to have a trust relationship with the centralized DevOps account. Congure<br>the trust relationship to allow the sts:AssumeRole action. Congure the application account’s deployment IAM role to have the required<br>access to the EKS cluster. Congure the EKS cluster aws-auth CongMap to map the role to the appropriate system permissions.<br>B. Congure the centralized DevOps account’s deployment IAM role to have a trust relationship with the application account. Congure<br>the trust relationship to allow the sts:AssumeRole action. Congure the centralized DevOps account’s deployment IAM role to allow the<br>required access to CodeBuild.<br>C. Congure the centralized DevOps account’s deployment IAM role to have a trust relationship with the application account. Congure<br>the trust relationship to allow the sts:AssumeRoleWithSAML action. Congure the centralized DevOps account’s deployment IAM role to<br>allow the required access to CodeBuild.<br>D. Congure the application account’s deployment IAM role to have a trust relationship with the AWS Control Tower management account.<br>Congure the trust relationship to allow the sts:AssumeRole action. Congure the application account’s deployment IAM role to have the<br>required access to the EKS cluster. Congure the EKS cluster aws-auth CongMap to map the role to the appropriate system permissions.</td>
                    <td>一家全球公司使用AWS Control Tower管理多个AWS账户。该公司托管内部应用程序和公共应用程序。<br>公司中的每个应用程序团队都有自己的AWS账户用于应用程序托管。这些账户在AWS Organizations的组织中进行整合。其中一个AWS Control Tower成员账户作为集中式DevOps账户，具有CI/CD管道，应用程序团队使用这些管道将应用程序部署到各自的目标AWS账户。集中式DevOps账户中存在用于部署的IAM角色。<br>一个应用程序团队正在尝试将其应用程序部署到应用程序AWS账户中的Amazon Elastic Kubernetes Service (Amazon EKS)集群。应用程序AWS账户中存在用于部署的IAM角色。部署是通过在集中式DevOps账户中设置的AWS CodeBuild项目进行的。CodeBuild项目使用CodeBuild的IAM服务角色。在尝试从CodeBuild连接到跨账户EKS集群时，部署失败并出现未授权错误。</td>
                    <td>A. 配置应用程序账户的部署IAM角色与集中式DevOps账户建立信任关系。配置信任关系以允许sts:AssumeRole操作。配置应用程序账户的部署IAM角色具有对EKS集群的必需访问权限。配置EKS集群aws-auth ConfigMap将角色映射到适当的系统权限。这个选项的信任关系方向是错误的，应该是DevOps账户的角色去assume应用程序账户的角色，而不是相反。<br><br>B. 配置集中式DevOps账户的部署IAM角色与应用程序账户建立信任关系。配置信任关系以允许sts:AssumeRole操作。配置集中式DevOps账户的部署IAM角色允许对CodeBuild的必需访问权限。这个选项正确地设置了信任关系的方向，DevOps账户的角色可以assume应用程序账户的角色，从而获得访问EKS集群的权限。<br><br>C. 配置集中式DevOps账户的部署IAM角色与应用程序账户建立信任关系。配置信任关系以允许sts:AssumeRoleWithSAML操作。配置集中式DevOps账户的部署IAM角色允许对CodeBuild的必需访问权限。这个选项使用了错误的操作类型，sts:AssumeRoleWithSAML用于SAML联合身份验证，不适用于跨账户角色假设。<br><br>D. 配置应用程序账户的部署IAM角色与AWS Control Tower管理账户建立信任关系。配置信任关系以允许sts:AssumeRole操作。配置应用程序账户的部署IAM角色具有对EKS集群的必需访问权限。配置EKS集群aws-auth ConfigMap将角色映射到适当的系统权限。这个选项错误地涉及了Control Tower管理账户，而实际问题是DevOps账户和应用程序账户之间的跨账户访问。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>126</td>
                    <td>A highly regulated company has a policy that DevOps engineers should not log in to their Amazon EC2 instances except in emergencies. If a<br>DevOps engineer does log in, the security team must be notied within 15 minutes of the occurrence.</td>
                    <td>A. Install the Amazon Inspector agent on each EC2 instance. Subscribe to Amazon EventBridge notications. Invoke an AWS Lambda<br>function to check if a message is about user logins. If it is, send a notication to the security team using Amazon SNS.<br>B. Install the Amazon CloudWatch agent on each EC2 instance. Congure the agent to push all logs to Amazon CloudWatch Logs and set<br>up a CloudWatch metric lter that searches for user logins. If a login is found, send a notication to the security team using Amazon SNS.<br>C. Set up AWS CloudTrail with Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Attach AWS Lambda to Kinesis<br>to parse and determine if a log contains a user login. If it does, send a notication to the security team using Amazon SNS.<br>D. Set up a script on each Amazon EC2 instance to push all logs to Amazon S3. Set up an S3 event to invoke an AWS Lambda function,<br>which invokes an Amazon Athena query to run. The Athena query checks for logins and sends the output to the security team using<br>Amazon SNS.</td>
                    <td>一家受到严格监管的公司有一项政策，规定DevOps工程师除非在紧急情况下，否则不应登录到他们的Amazon EC2实例。如果DevOps工程师确实登录了，安全团队必须在事件发生后15分钟内收到通知。</td>
                    <td>A. 在每个EC2实例上安装Amazon Inspector代理。订阅Amazon EventBridge通知。调用AWS Lambda函数检查消息是否与用户登录有关。如果是，使用Amazon SNS向安全团队发送通知。<br>这个选项不正确。Amazon Inspector主要用于安全漏洞评估和应用程序安全分析，而不是用于监控用户登录活动。Inspector代理不会捕获或报告用户登录事件，因此无法满足监控登录活动的需求。<br><br>B. 在每个EC2实例上安装Amazon CloudWatch代理。配置代理将所有日志推送到Amazon CloudWatch Logs，并设置CloudWatch指标过滤器搜索用户登录。如果发现登录，使用Amazon SNS向安全团队发送通知。<br>这个选项是正确的。CloudWatch代理可以收集系统日志（如/var/log/auth.log或/var/log/secure），这些日志包含SSH登录信息。通过指标过滤器可以实时监控登录事件，并通过CloudWatch告警触发SNS通知，能够满足15分钟内通知的要求。<br><br>C. 设置AWS CloudTrail与Amazon CloudWatch Logs。将CloudWatch Logs订阅到Amazon Kinesis。将AWS Lambda附加到Kinesis以解析和确定日志是否包含用户登录。如果包含，使用Amazon SNS向安全团队发送通知。<br>这个选项不正确。CloudTrail主要记录AWS API调用，而不是EC2实例内部的SSH登录活动。CloudTrail无法捕获到用户通过SSH登录到EC2实例的事件，因此无法检测到实际的登录行为。<br><br>D. 在每个Amazon EC2实例上设置脚本将所有日志推送到Amazon S3。设置S3事件调用AWS Lambda函数，该函数调用Amazon Athena查询运行。Athena查询检查登录并使用Amazon SNS将输出发送给安全团队。<br>这个选项不正确。虽然技术上可行，但这种方法过于复杂且不够实时。将日志推送到S3然后使用Athena查询会引入延迟，无法满足15分钟内通知的严格要求，而且成本较高。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>127</td>
                    <td>A company updated the AWS CloudFormation template for a critical business application. The stack update process failed due to an error in<br>the updated template, and AWS CloudFormation automatically began the stack rollback process. Later, a DevOps engineer discovered that the<br>application was still unavailable and that the stack was in the UPDATE_ROLLBACK_FAILED state.</td>
                    <td>A. Attach the AWSCloudFormationFullAccess IAM policy to the AWS CloudFormation role.<br>B. Automatically recover the stack resources by using AWS CloudFormation drift detection.<br>C. Issue a ContinueUpdateRollback command from the AWS CloudFormation console or the AWS CLI.<br>D. Manually adjust the resources to match the expectations of the stack.<br>E. Update the existing AWS CloudFormation stack by using the original template.</td>
                    <td>一家公司更新了关键业务应用程序的AWS CloudFormation模板。由于更新模板中的错误，堆栈更新过程失败，AWS CloudFormation自动开始了堆栈回滚过程。后来，DevOps工程师发现应用程序仍然不可用，堆栈处于UPDATE_ROLLBACK_FAILED状态。</td>
                    <td>A. 将AWSCloudFormationFullAccess IAM策略附加到AWS CloudFormation角色 - 这个选项不正确。UPDATE_ROLLBACK_FAILED状态通常不是由权限问题引起的，而是由于资源状态不一致或其他技术问题导致的。仅仅添加更多权限不会解决根本问题。<br><br>B. 使用AWS CloudFormation漂移检测自动恢复堆栈资源 - 这个选项不正确。漂移检测只是用来识别资源配置与模板定义之间的差异，它不能自动修复UPDATE_ROLLBACK_FAILED状态。漂移检测是诊断工具，不是修复工具。<br><br>C. 从AWS CloudFormation控制台或AWS CLI发出ContinueUpdateRollback命令 - 这个选项正确。当堆栈处于UPDATE_ROLLBACK_FAILED状态时，ContinueUpdateRollback是专门设计用来处理这种情况的命令，它可以继续完成失败的回滚操作。<br><br>D. 手动调整资源以匹配堆栈的期望 - 这个选项正确。当自动回滚失败时，通常是因为某些资源处于不一致状态。手动调整这些资源状态，使其与堆栈期望的状态匹配，然后再执行ContinueUpdateRollback命令是常见的解决方案。<br><br>E. 使用原始模板更新现有的AWS CloudFormation堆栈 - 这个选项不正确。当堆栈处于UPDATE_ROLLBACK_FAILED状态时，不能直接进行新的更新操作，必须先解决当前的失败状态。</td>
                    <td>CD</td>
                </tr>
                <tr>
                    <td>128</td>
                    <td>A development team manually builds an artifact locally and then places it in an Amazon S3 bucket. The application has a local cache that<br>must be cleared when a deployment occurs. The team runs a command to do this, downloads the artifact from Amazon S3, and unzips the<br>artifact to complete the deployment.<br>A DevOps team wants to migrate to a CI/CD process and build in checks to stop and roll back the deployment when a failure occurs. This<br>requires the team to track the progression of the deployment.</td>
                    <td>A. Allow developers to check the code into a code repository. Using Amazon EventBridge, on every pull into the main branch, invoke an<br>AWS Lambda function to build the artifact and store it in Amazon S3.<br>B. Create a custom script to clear the cache. Specify the script in the BeforeInstall lifecycle hook in the AppSpec le.<br>C. Create user data for each Amazon EC2 instance that contains the clear cache script. Once deployed, test the application. If it is not<br>successful, deploy it again.<br>D. Set up AWS CodePipeline to deploy the application. Allow developers to check the code into a code repository as a source for the<br>pipeline.<br>E. Use AWS CodeBuild to build the artifact and place it in Amazon S3. Use AWS CodeDeploy to deploy the artifact to Amazon EC2<br>instances.</td>
                    <td>一个开发团队手动在本地构建工件，然后将其放置在Amazon S3存储桶中。应用程序有一个本地缓存，在部署发生时必须清除。团队运行命令来执行此操作，从Amazon S3下载工件，并解压工件以完成部署。<br>DevOps团队希望迁移到CI/CD流程，并构建检查机制来在发生故障时停止和回滚部署。这要求团队跟踪部署的进度。</td>
                    <td>A. 允许开发人员将代码检入代码仓库。使用Amazon EventBridge，在每次拉取到主分支时，调用AWS Lambda函数来构建工件并存储在Amazon S3中。这个选项提供了自动化构建机制，但缺少部署和回滚功能，不能完全满足CI/CD需求。<br><br>B. 创建自定义脚本来清除缓存。在AppSpec文件的BeforeInstall生命周期钩子中指定该脚本。这是正确的做法，AppSpec文件是AWS CodeDeploy的配置文件，BeforeInstall钩子可以在安装前执行清除缓存操作，满足了清除本地缓存的需求。<br><br>C. 为每个Amazon EC2实例创建包含清除缓存脚本的用户数据。部署后测试应用程序，如果不成功则重新部署。这种方法过于简单粗暴，缺乏自动化的失败检测和回滚机制，不符合现代CI/CD最佳实践。<br><br>D. 设置AWS CodePipeline来部署应用程序。允许开发人员将代码检入代码仓库作为管道的源。这是正确的选择，CodePipeline提供了完整的CI/CD流水线功能，包括自动化部署、失败检测和回滚机制，能够跟踪部署进度。<br><br>E. 使用AWS CodeBuild构建工件并将其放置在Amazon S3中。使用AWS CodeDeploy将工件部署到Amazon EC2实例。这是完整CI/CD解决方案的关键组件，CodeBuild负责自动化构建，CodeDeploy提供部署和回滚功能，完全满足需求。</td>
                    <td>BDE</td>
                </tr>
                <tr>
                    <td>129</td>
                    <td>A DevOps engineer is working on a project that is hosted on Amazon Linux and has failed a security review. The DevOps manager has been<br>asked to review the company buildspec.yaml le for an AWS CodeBuild project and provide recommendations. The buildspec.yaml le is<br>congured as follows:</td>
                    <td>A. Add a post-build command to remove the temporary les from the container before termination to ensure they cannot be seen by other<br>CodeBuild users.<br>B. Update the CodeBuild project role with the necessary permissions and then remove the AWS credentials from the environment variable.<br>C. Store the DB_PASSWORD as a SecureString value in AWS Systems Manager Parameter Store and then remove the DB_PASSWORD from<br>the environment variables.<br>D. Move the environment variables to the ‘db-deploy-bucket’ Amazon S3 bucket, add a prebuild stage to download, then export the<br>variables.<br>E. Use AWS Systems Manager run command versus scp and ssh commands directly to the instance.</td>
                    <td>一名DevOps工程师正在处理一个托管在Amazon Linux上的项目，该项目未通过安全审查。DevOps经理被要求审查公司AWS CodeBuild项目的buildspec.yaml文件并提供建议。buildspec.yaml文件配置如下：</td>
                    <td>A. 添加构建后命令，在容器终止前从容器中删除临时文件，以确保其他CodeBuild用户无法看到这些文件。<br>这个选项有一定的安全价值，但不是最关键的安全问题。CodeBuild容器在构建完成后会被销毁，临时文件通常不会被其他用户访问到。虽然清理临时文件是好的实践，但相比其他选项，这不是最紧迫的安全问题。<br><br>B. 更新CodeBuild项目角色的必要权限，然后从环境变量中删除AWS凭证。<br>这是一个重要的安全建议。在环境变量中硬编码AWS凭证是严重的安全风险。应该使用IAM角色来授予CodeBuild项目必要的权限，而不是在代码或环境变量中暴露凭证。这符合AWS安全最佳实践。<br><br>C. 将DB_PASSWORD作为SecureString值存储在AWS Systems Manager Parameter Store中，然后从环境变量中删除DB_PASSWORD。<br>这是另一个关键的安全改进。数据库密码不应该以明文形式存储在环境变量中，这会造成严重的安全风险。使用Parameter Store的SecureString功能可以加密存储敏感信息，并在运行时安全地检索。<br><br>D. 将环境变量移动到&#x27;db-deploy-bucket&#x27; Amazon S3存储桶中，添加预构建阶段来下载，然后导出变量。<br>这个选项实际上可能会增加安全风险。将敏感信息存储在S3存储桶中，即使有适当的权限控制，也不如使用专门的密钥管理服务安全。这不是处理敏感环境变量的最佳方式。<br><br>E. 使用AWS Systems Manager运行命令，而不是直接使用scp和ssh命令连接到实例。<br>这是一个很好的安全实践建议。Systems Manager Session Manager提供了更安全的实例访问方式，无需开放SSH端口，并且提供了更好的审计和日志记录功能。这可以减少安全攻击面。</td>
                    <td>BCE</td>
                </tr>
                <tr>
                    <td>130</td>
                    <td>A company has a legacy application. A DevOps engineer needs to automate the process of building the deployable artifact for the legacy<br>application. The solution must store the deployable artifact in an existing Amazon S3 bucket for future deployments to reference.</td>
                    <td>A. Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new<br>Amazon Elastic Container Registry (Amazon ECR) repository. Congure a new AWS CodeBuild project to use the custom Docker image to<br>build the deployable artifact and to save the artifact to the S3 bucket.<br>B. Launch a new Amazon EC2 instance. Install all the dependencies for the legacy application on the EC2 instance. Use the EC2 instance<br>to build the deployable artifact and to save the artifact to the S3 bucket.<br>C. Create a custom EC2 Image Builder image. Install all the dependencies for the legacy application on the image. Launch a new Amazon<br>EC2 instance from the image. Use the new EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.<br>D. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with an AWS Fargate prole that runs in multiple Availability Zones.<br>Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new<br>Amazon Elastic Container Registry (Amazon ECR) repository. Use the custom Docker image inside the EKS cluster to build the deployable<br>artifact and to save the artifact to the S3 bucket.</td>
                    <td>一家公司有一个遗留应用程序。DevOps工程师需要自动化构建该遗留应用程序可部署工件的过程。解决方案必须将可部署工件存储在现有的Amazon S3存储桶中，以供未来部署时引用。</td>
                    <td>选项A：创建一个包含遗留应用程序所有依赖项的自定义Docker镜像。将自定义Docker镜像存储在新的Amazon Elastic Container Registry (Amazon ECR)仓库中。配置一个新的AWS CodeBuild项目，使用自定义Docker镜像来构建可部署工件并将工件保存到S3存储桶。这个方案使用了AWS原生的CI/CD服务CodeBuild，具有良好的可扩展性和自动化能力，符合DevOps最佳实践。CodeBuild是专门为构建任务设计的托管服务，可以自动处理资源分配和释放。<br><br>选项B：启动一个新的Amazon EC2实例。在EC2实例上安装遗留应用程序的所有依赖项。使用EC2实例构建可部署工件并将工件保存到S3存储桶。这种方案需要手动管理EC2实例，缺乏自动化程度，不符合DevOps自动化的要求。同时需要持续维护实例，成本较高且管理复杂。<br><br>选项C：创建一个自定义EC2 Image Builder镜像。在镜像上安装遗留应用程序的所有依赖项。从该镜像启动新的Amazon EC2实例。使用新EC2实例构建可部署工件并将工件保存到S3存储桶。虽然使用了Image Builder来标准化环境，但仍然需要手动管理EC2实例的生命周期，自动化程度不够高。<br><br>选项D：创建一个Amazon Elastic Kubernetes Service (Amazon EKS)集群，配置在多个可用区运行的AWS Fargate配置文件。创建包含遗留应用程序所有依赖项的自定义Docker镜像。将自定义Docker镜像存储在新的Amazon Elastic Container Registry (Amazon ECR)仓库中。在EKS集群内使用自定义Docker镜像构建可部署工件并将工件保存到S3存储桶。这个方案过于复杂，引入了不必要的Kubernetes复杂性，对于简单的构建任务来说是过度设计。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>131</td>
                    <td>A company builds a container image in an AWS CodeBuild project by running Docker commands. After the container image is built, the<br>CodeBuild project uploads the container image to an Amazon S3 bucket. The CodeBuild project has an IAM service role that has permissions<br>to access the S3 bucket.<br>A DevOps engineer needs to replace the S3 bucket with an Amazon Elastic Container Registry (Amazon ECR) repository to store the container<br>images. The DevOps engineer creates an ECR private image repository in the same AWS Region of the CodeBuild project. The DevOps<br>engineer adjusts the IAM service role with the permissions that are necessary to work with the new ECR repository. The DevOps engineer also<br>places new repository information into the docker build command and the docker push command that are used in the buildspec.yml le.<br>When the CodeBuild project runs a build job, the job fails when the job tries to access the ECR repository.</td>
                    <td>A. Update the buildspec.yml le to log in to the ECR repository by using the aws ecr get-login-password AWS CLI command to obtain an<br>authentication token. Update the docker login command to use the authentication token to access the ECR repository.<br>B. Add an environment variable of type SECRETS_MANAGER to the CodeBuild project. In the environment variable, include the ARN of the<br>CodeBuild project&#x27;s IAM service role. Update the buildspec.yml le to use the new environment variable to log in with the docker login<br>command to access the ECR repository.<br>C. Update the ECR repository to be a public image repository. Add an ECR repository policy that allows the IAM service role to have access.<br>D. Update the buildspec.yml le to use the AWS CLI to assume the IAM service role for ECR operations. Add an ECR repository policy that<br>allows the IAM service role to have access.</td>
                    <td>一家公司在AWS CodeBuild项目中通过运行Docker命令构建容器镜像。容器镜像构建完成后，CodeBuild项目将容器镜像上传到Amazon S3存储桶。CodeBuild项目有一个IAM服务角色，该角色具有访问S3存储桶的权限。<br><br>DevOps工程师需要用Amazon Elastic Container Registry (Amazon ECR)仓库替换S3存储桶来存储容器镜像。DevOps工程师在CodeBuild项目的同一AWS区域创建了一个ECR私有镜像仓库。DevOps工程师调整了IAM服务角色，添加了使用新ECR仓库所需的权限。DevOps工程师还在buildspec.yml文件中使用的docker build命令和docker push命令中放置了新的仓库信息。<br><br>当CodeBuild项目运行构建作业时，作业在尝试访问ECR仓库时失败。</td>
                    <td>A. 更新buildspec.yml文件，使用aws ecr get-login-password AWS CLI命令获取认证令牌来登录ECR仓库。更新docker login命令使用认证令牌访问ECR仓库。这是正确的方法，因为ECR需要身份验证，即使IAM角色有权限，也需要先获取认证令牌并登录到ECR。<br><br>B. 向CodeBuild项目添加SECRETS_MANAGER类型的环境变量，在环境变量中包含CodeBuild项目IAM服务角色的ARN。更新buildspec.yml文件使用新环境变量通过docker login命令登录访问ECR仓库。这种方法不正确，因为IAM角色ARN不是用于docker登录的凭据。<br><br>C. 将ECR仓库更新为公共镜像仓库，添加允许IAM服务角色访问的ECR仓库策略。虽然这可能解决访问问题，但将仓库设为公共并不是最佳实践，且题目要求使用私有仓库。<br><br>D. 更新buildspec.yml文件使用AWS CLI为ECR操作承担IAM服务角色，添加允许IAM服务角色访问的ECR仓库策略。这种方法复杂且不必要，CodeBuild已经在使用服务角色运行。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>132</td>
                    <td>A company manually provisions IAM access for its employees. The company wants to replace the manual process with an automated process.<br>The company has an existing Active Directory system congured with an external SAML 2.0 identity provider (IdP).<br>The company wants employees to use their existing corporate credentials to access AWS. The groups from the existing Active Directory<br>system must be available for permission management in AWS Identity and Access Management (IAM). A DevOps engineer has completed the<br>initial conguration of AWS IAM Identity Center (AWS Single Sign-On) in the company’s AWS account.</td>
                    <td>A. Congure an external IdP as an identity source. Congure automatic provisioning of users and groups by using the SCIM protocol.<br>B. Congure AWS Directory Service as an identity source. Congure automatic provisioning of users and groups by using the SAML<br>protocol.<br>C. Congure an AD Connector as an identity source. Congure automatic provisioning of users and groups by using the SCIM protocol.<br>D. Congure an external IdP as an identity source Congure automatic provisioning of users and groups by using the SAML protocol.</td>
                    <td>一家公司手动为其员工配置IAM访问权限。该公司希望用自动化流程替换手动流程。<br>该公司有一个现有的Active Directory系统，配置了外部SAML 0身份提供商(IdP)。<br>该公司希望员工使用其现有的企业凭据来访问AWS。现有Active Directory系统中的组必须在AWS身份和访问管理(IAM)中可用于权限管理。DevOps工程师已经在公司的AWS账户中完成了AWS IAM Identity Center(AWS Single Sign-On)的初始配置。</td>
                    <td>A. 配置外部IdP作为身份源。使用SCIM协议配置用户和组的自动配置。<br>这个选项正确。由于公司已经有配置了SAML 2.0的外部IdP，应该直接使用外部IdP作为身份源。SCIM(System for Cross-domain Identity Management)协议专门用于自动化用户和组的配置、更新和删除，能够实现从外部IdP到AWS IAM Identity Center的自动同步。<br><br>B. 配置AWS Directory Service作为身份源。使用SAML协议配置用户和组的自动配置。<br>这个选项不正确。AWS Directory Service是AWS托管的目录服务，但题目明确说明公司已经有现有的Active Directory系统配置了外部SAML 2.0 IdP，不需要创建新的AWS Directory Service。另外，SAML协议主要用于身份验证，而不是用户和组的自动配置。<br><br>C. 配置AD Connector作为身份源。使用SCIM协议配置用户和组的自动配置。<br>这个选项部分正确但不是最佳选择。AD Connector可以连接到现有的Active Directory，但题目强调公司已经配置了外部SAML 2.0 IdP，直接使用外部IdP更加直接和高效。<br><br>D. 配置外部IdP作为身份源。使用SAML协议配置用户和组的自动配置。<br>这个选项不正确。虽然使用外部IdP作为身份源是正确的，但SAML协议主要用于身份验证和授权，不是用于自动配置用户和组的协议。自动配置需要使用SCIM协议。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>133</td>
                    <td>A company is using AWS to run digital workloads. Each application team in the company has its own AWS account for application hosting. The<br>accounts are consolidated in an organization in AWS Organizations.<br>The company wants to enforce security standards across the entire organization. To avoid noncompliance because of security<br>misconguration, the company has enforced the use of AWS CloudFormation. A production support team can modify resources in the<br>production environment by using the AWS Management Console to troubleshoot and resolve application-related issues.<br>A DevOps engineer must implement a solution to identify in near real time any AWS service misconguration that results in noncompliance.<br>The solution must automatically remediate the issue within 15 minutes of identication. The solution also must track noncompliant resources<br>and events in a centralized dashboard with accurate timestamps.</td>
                    <td>A. Use CloudFormation drift detection to identify noncompliant resources. Use drift detection events from CloudFormation to invoke an<br>AWS Lambda function for remediation. Congure the Lambda function to publish logs to an Amazon CloudWatch Logs log group.<br>Congure an Amazon CloudWatch dashboard to use the log group for tracking.<br>B. Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon Athena to identify noncompliant resources. Use<br>AWS Step Functions to track query results on Athena for drift detection and to invoke an AWS Lambda function for remediation. For<br>tracking, set up an Amazon QuickSight dashboard that uses Athena as the data source.<br>C. Turn on the conguration recorder in AWS Cong in all the AWS accounts to identify noncompliant resources. Enable AWS Security Hub<br>with the --no-enable-default-standards option in all the AWS accounts. Set up AWS Cong managed rules and custom rules. Set up<br>automatic remediation by using AWS Cong conformance packs. For tracking, set up a dashboard on Security Hub in a designated Security<br>Hub administrator account.<br>D. Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon CloudWatch Logs to identify noncompliant<br>resources. Use CloudWatch Logs lters for drift detection. Use Amazon EventBridge to invoke the Lambda function for remediation.<br>Stream ltered CloudWatch logs to Amazon OpenSearch Service. Set up a dashboard on OpenSearch Service for tracking.</td>
                    <td>一家公司正在使用AWS运行数字化工作负载。公司中的每个应用团队都有自己的AWS账户来托管应用程序。这些账户在AWS Organizations中的一个组织中进行整合。<br>公司希望在整个组织中强制执行安全标准。为了避免因安全配置错误而导致的不合规，公司已强制使用AWS CloudFormation。生产支持团队可以通过使用AWS管理控制台修改生产环境中的资源，以排查和解决与应用程序相关的问题。<br>DevOps工程师必须实施一个解决方案，以近实时识别任何导致不合规的AWS服务配置错误。该解决方案必须在识别后15分钟内自动修复问题。该解决方案还必须在集中式仪表板中跟踪不合规资源和事件，并提供准确的时间戳。</td>
                    <td>选项A：使用CloudFormation漂移检测来识别不合规资源。使用CloudFormation的漂移检测事件来调用AWS Lambda函数进行修复。配置Lambda函数将日志发布到Amazon CloudWatch Logs日志组。配置Amazon CloudWatch仪表板使用日志组进行跟踪。这个方案的问题是CloudFormation漂移检测不是实时的，需要手动触发，无法满足近实时监控的要求。而且CloudFormation漂移检测只能检测通过CloudFormation部署的资源的变化，对于直接通过控制台修改的资源可能无法全面覆盖。<br><br>选项B：在AWS账户中开启AWS CloudTrail。使用Amazon Athena分析CloudTrail日志来识别不合规资源。使用AWS Step Functions跟踪Athena上的查询结果进行漂移检测，并调用AWS Lambda函数进行修复。设置使用Athena作为数据源的Amazon QuickSight仪表板进行跟踪。这个方案过于复杂，需要自定义大量逻辑来分析CloudTrail日志，而且Athena查询不是实时的，无法满足15分钟内自动修复的要求。<br><br>选项C：在所有AWS账户中开启AWS Config的配置记录器来识别不合规资源。在所有AWS账户中启用AWS Security Hub并使用--no-enable-default-standards选项。设置AWS Config托管规则和自定义规则。使用AWS Config一致性包设置自动修复。在指定的Security Hub管理员账户中设置Security Hub仪表板进行跟踪。这是最合适的方案，AWS Config可以实时监控资源配置变化，一致性包提供自动修复功能，Security Hub提供集中式的合规性仪表板。<br><br>选项D：在AWS账户中开启AWS CloudTrail。使用Amazon CloudWatch Logs分析CloudTrail日志来识别不合规资源。使用CloudWatch Logs过滤器进行漂移检测。使用Amazon EventBridge调用Lambda函数进行修复。将过滤的CloudWatch日志流式传输到Amazon OpenSearch Service。在OpenSearch Service上设置仪表板进行跟踪。这个方案同样过于复杂，需要大量自定义开发，而且CloudTrail日志分析无法直接判断资源配置的合规性。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>134</td>
                    <td>A company uses AWS Organizations to manage its AWS accounts. The organization root has an OU that is named Environments. The<br>Environments OU has two child OUs that are named Development and Production, respectively.<br>The Environments OU and the child OUs have the default FullAWSAccess policy in place. A DevOps engineer plans to remove the<br>FullAWSAccess policy from the Development OU and replace the policy with a policy that allows all actions on Amazon EC2 resources.</td>
                    <td>A. All users in the Development OU will be allowed all API actions on all resources.<br>B. All users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied.<br>C. All users in the Development OU will be denied all API actions on all resources.<br>D. All users in the Development OU will be denied all API actions on EC2 resources. All other API actions will be allowed.</td>
                    <td>一家公司使用AWS Organizations来管理其AWS账户。组织根有一个名为Environments的OU（组织单位）。Environments OU有两个子OU，分别名为Development和Production。Environments OU和子OU都配置了默认的FullAWSAccess策略。一名DevOps工程师计划从Development OU中移除FullAWSAccess策略，并用一个允许对Amazon EC2资源执行所有操作的策略来替换该策略。</td>
                    <td>A. Development OU中的所有用户将被允许对所有资源执行所有API操作 - 这个选项不正确。如果移除了FullAWSAccess策略并只添加EC2相关的策略，用户就不会再拥有对所有AWS资源的完全访问权限，而只能访问EC2资源。<br><br>B. Development OU中的所有用户将被允许对EC2资源执行所有API操作，所有其他API操作将被拒绝 - 这个选项是正确的。当移除FullAWSAccess策略并替换为仅允许EC2操作的策略时，用户将只能对EC2资源执行操作，而对其他AWS服务的访问将被拒绝。<br><br>C. Development OU中的所有用户将被拒绝对所有资源执行所有API操作 - 这个选项不正确。虽然移除了FullAWSAccess策略，但是添加了新的EC2策略，所以用户仍然可以访问EC2资源。<br><br>D. Development OU中的所有用户将被拒绝对EC2资源执行所有API操作，所有其他API操作将被允许 - 这个选项完全相反，不正确。新策略明确允许EC2操作，而不是拒绝EC2操作。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>135</td>
                    <td>A company is examining its disaster recovery capability and wants the ability to switch over its daily operations to a secondary AWS Region.<br>The company uses AWS CodeCommit as a source control tool in the primary Region.<br>A DevOps engineer must provide the capability for the company to develop code in the secondary Region. If the company needs to use the<br>secondary Region, developers can add an additional remote URL to their local Git conguration.</td>
                    <td>A. Create a CodeCommit repository in the secondary Region. Create an AWS CodeBuild project to perform a Git mirror operation of the<br>primary Region&#x27;s CodeCommit repository to the secondary Region&#x27;s CodeCommit repository. Create an AWS Lambda function that invokes<br>the CodeBuild project. Create an Amazon EventBridge rule that reacts to merge events in the primary Region&#x27;s CodeCommit repository.<br>Congure the EventBridge rule to invoke the Lambda function.<br>B. Create an Amazon S3 bucket in the secondary Region. Create an AWS Fargate task to perform a Git mirror operation of the primary<br>Region&#x27;s CodeCommit repository and copy the result to the S3 bucket. Create an AWS Lambda function that initiates the Fargate task.<br>Create an Amazon EventBridge rule that reacts to merge events in the CodeCommit repository. Congure the EventBridge rule to invoke<br>the Lambda function.<br>C. Create an AWS CodeArtifact repository in the secondary Region. Create an AWS CodePipeline pipeline that uses the primary Region’s<br>CodeCommit repository for the source action. Create a cross-Region stage in the pipeline that packages the CodeCommit repository<br>contents and stores the contents in the CodeArtifact repository when a pull request is merged into the CodeCommit repository.<br>D. Create an AWS Cloud9 environment and a CodeCommit repository in the secondary Region. Congure the primary Region&#x27;s CodeCommit<br>repository as a remote repository in the AWS Cloud9 environment. Connect the secondary Region&#x27;s CodeCommit repository to the AWS<br>Cloud9 environment.</td>
                    <td>一家公司正在检查其灾难恢复能力，希望能够将其日常运营切换到辅助AWS区域。该公司在主区域使用AWS CodeCommit作为源代码控制工具。DevOps工程师必须为公司提供在辅助区域开发代码的能力。如果公司需要使用辅助区域，开发人员可以在其本地Git配置中添加额外的远程URL。</td>
                    <td>选项A：在辅助区域创建CodeCommit存储库，使用CodeBuild项目执行Git镜像操作，通过Lambda函数和EventBridge规则自动同步。这个方案技术上可行，能够实现代码库的自动同步，但CodeBuild主要用于构建任务，用来做Git镜像操作不是最佳实践。而且这种方案相对复杂，需要多个服务协调工作。<br><br>选项B：在辅助区域创建S3存储桶，使用Fargate任务执行Git镜像操作并将结果复制到S3桶中，通过Lambda和EventBridge实现自动触发。这个方案使用Fargate容器来处理Git操作更加灵活和可控，S3作为存储介质也更加经济实用。Fargate可以运行自定义的Git同步脚本，提供更好的控制能力。<br><br>选项C：使用CodeArtifact存储库和CodePipeline管道来处理代码同步。但CodeArtifact主要用于包管理而不是源代码管理，这不符合题目要求的Git远程URL配置需求。开发人员无法直接将CodeArtifact作为Git远程仓库使用。<br><br>选项D：创建Cloud9环境和CodeCommit存储库，配置主区域的CodeCommit作为远程仓库。这个方案没有提供自动同步机制，需要手动操作，不符合灾难恢复的自动化要求。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>136</td>
                    <td>A DevOps team is merging code revisions for an application that uses an Amazon RDS Multi-AZ DB cluster for its production database. The<br>DevOps team uses continuous integration to periodically verify that the application works. The DevOps team needs to test the changes before<br>the changes are deployed to the production database.</td>
                    <td>A. Use a buildspec le in AWS CodeBuild to restore the DB cluster from a snapshot of the production database, run integration tests, and<br>drop the restored database after verication.<br>B. Deploy the application to production. Congure an audit log of data control language (DCL) operations to capture database activities to<br>perform if verication fails.<br>C. Create a snapshot of the DB cluster before deploying the application. Use the Update requires:Replacement property on the DB<br>instance in AWS CloudFormation to deploy the application and apply the changes.<br>D. Ensure that the DB cluster is a Multi-AZ deployment. Deploy the application with the updates. Fail over to the standby instance if<br>verication fails.</td>
                    <td>一个DevOps团队正在为一个使用Amazon RDS Multi-AZ数据库集群作为生产数据库的应用程序合并代码修订。DevOps团队使用持续集成来定期验证应用程序是否正常工作。DevOps团队需要在将更改部署到生产数据库之前测试这些更改。</td>
                    <td>A. 在AWS CodeBuild中使用buildspec文件从生产数据库的快照恢复数据库集群，运行集成测试，并在验证后删除恢复的数据库。这个选项提供了一个相对安全的测试方法，通过创建独立的测试环境来验证更改，但会产生额外的成本和时间开销，且需要管理临时数据库实例的生命周期。<br><br>B. 将应用程序部署到生产环境，配置数据控制语言(DCL)操作的审计日志来捕获数据库活动，以便在验证失败时执行回滚操作。这个选项风险很高，因为直接在生产环境中测试可能会影响正在运行的服务和数据完整性。<br><br>C. 在部署应用程序之前创建数据库集群的快照，在AWS CloudFormation中使用数据库实例的Update requires:Replacement属性来部署应用程序并应用更改。这个方法涉及替换整个数据库实例，会导致服务中断，不适合生产环境的持续运行需求。<br><br>D. 确保数据库集群是Multi-AZ部署，部署带有更新的应用程序，如果验证失败则故障转移到备用实例。这个选项利用了Multi-AZ的高可用性特性，可以在主实例出现问题时快速切换到备用实例，最小化服务中断时间。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>137</td>
                    <td>A company manages a multi-tenant environment in its VPC and has congured Amazon GuardDuty for the corresponding AWS account. The<br>company sends all GuardDuty ndings to AWS Security Hub.<br>Trac from suspicious sources is generating a large number of ndings. A DevOps engineer needs to implement a solution to automatically<br>deny trac across the entire VPC when GuardDuty discovers a new suspicious source.</td>
                    <td>A. Create a GuardDuty threat list. Congure GuardDuty to reference the list. Create an AWS Lambda function that will update the threat<br>list. Congure the Lambda function to run in response to new Security Hub ndings that come from GuardDuty.<br>B. Congure an AWS WAF web ACL that includes a custom rule group. Create an AWS Lambda function that will create a block rule in the<br>custom rule group. Congure the Lambda function to run in response to new Security Hub ndings that come from GuardDuty.<br>C. Congure a rewall in AWS Network Firewall. Create an AWS Lambda function that will create a Drop action rule in the rewall policy.<br>Congure the Lambda function to run in response to new Security Hub ndings that come from GuardDuty.<br>D. Create an AWS Lambda function that will create a GuardDuty suppression rule. Congure the Lambda function to run in response to new<br>Security Hub ndings that come from GuardDuty.</td>
                    <td>一家公司在其VPC中管理多租户环境，并为相应的AWS账户配置了Amazon GuardDuty。该公司将所有GuardDuty发现的问题发送到AWS Security Hub。来自可疑来源的流量产生了大量发现。DevOps工程师需要实施一个解决方案，当GuardDuty发现新的可疑来源时，自动拒绝整个VPC中的流量。</td>
                    <td>A. 创建GuardDuty威胁列表并配置GuardDuty引用该列表，创建AWS Lambda函数更新威胁列表，配置Lambda函数响应来自GuardDuty的新Security Hub发现。这个选项的问题是GuardDuty威胁列表主要用于检测威胁，而不是阻止流量。威胁列表只是帮助GuardDuty识别已知的恶意IP地址，但不会实际阻止网络流量通过VPC。<br><br>B. 配置包含自定义规则组的AWS WAF web ACL，创建AWS Lambda函数在自定义规则组中创建阻止规则，配置Lambda函数响应来自GuardDuty的新Security Hub发现。AWS WAF可以有效阻止来自特定IP地址的HTTP/HTTPS流量，Lambda函数可以动态添加阻止规则，这是一个可行的自动化解决方案。<br><br>C. 在AWS Network Firewall中配置防火墙，创建AWS Lambda函数在防火墙策略中创建Drop动作规则，配置Lambda函数响应来自GuardDuty的新Security Hub发现。Network Firewall可以在网络层阻止流量，但题目强调的是web应用流量，WAF更适合处理应用层的威胁。<br><br>D. 创建AWS Lambda函数创建GuardDuty抑制规则，配置Lambda函数响应来自GuardDuty的新Security Hub发现。抑制规则只是阻止GuardDuty报告某些发现，并不会实际阻止网络流量，这不符合题目要求。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>138</td>
                    <td>A company uses AWS Secrets Manager to store a set of sensitive API keys that an AWS Lambda function uses. When the Lambda function is<br>invoked the Lambda function retrieves the API keys and makes an API call to an external service. The Secrets Manager secret is encrypted<br>with the default AWS Key Management Service (AWS KMS) key.<br>A DevOps engineer needs to update the infrastructure to ensure that only the Lambda function’s execution role can access the values in<br>Secrets Manager. The solution must apply the principle of least privilege.</td>
                    <td>A. Update the default KMS key for Secrets Manager to allow only the Lambda function’s execution role to decrypt<br>B. Create a KMS customer managed key that trusts Secrets Manager and allows the Lambda function&#x27;s execution role to decrypt. Update<br>Secrets Manager to use the new customer managed key<br>C. Create a KMS customer managed key that trusts Secrets Manager and allows the account&#x27;s root principal to decrypt. Update Secrets<br>Manager to use the new customer managed key<br>D. Ensure that the Lambda function’s execution role has the KMS permissions scoped on the resource level. Congure the permissions so<br>that the KMS key can encrypt the Secrets Manager secret<br>E. Remove all KMS permissions from the Lambda function’s execution role</td>
                    <td>一家公司使用AWS Secrets Manager存储一组敏感的API密钥，这些密钥被AWS Lambda函数使用。当Lambda函数被调用时，Lambda函数会检索API密钥并向外部服务发起API调用。Secrets Manager密钥使用默认的AWS密钥管理服务(AWS KMS)密钥进行加密。<br>DevOps工程师需要更新基础设施，以确保只有Lambda函数的执行角色可以访问Secrets Manager中的值。解决方案必须应用最小权限原则。</td>
                    <td>A. 更新Secrets Manager的默认KMS密钥，只允许Lambda函数的执行角色解密 - 这个选项不正确，因为默认的AWS托管密钥无法修改其密钥策略，用户无法直接控制谁可以使用默认密钥。<br><br>B. 创建一个信任Secrets Manager并允许Lambda函数执行角色解密的KMS客户托管密钥。更新Secrets Manager使用新的客户托管密钥 - 这个选项部分正确，创建客户托管密钥可以提供更精细的访问控制，但仅允许Lambda执行角色可能过于限制性，可能影响其他必要的AWS服务操作。<br><br>C. 创建一个信任Secrets Manager并允许账户根主体解密的KMS客户托管密钥。更新Secrets Manager使用新的客户托管密钥 - 这个选项正确，根主体拥有完整的管理权限，同时可以通过IAM策略进一步限制实际的访问权限。<br><br>D. 确保Lambda函数的执行角色具有资源级别的KMS权限。配置权限使KMS密钥可以加密Secrets Manager密钥 - 这个选项描述不清晰且不完整，没有解决核心的访问控制问题。<br><br>E. 从Lambda函数的执行角色中移除所有KMS权限 - 这个选项正确，因为当使用客户托管密钥时，可以通过密钥策略而不是IAM权限来控制访问，符合最小权限原则。</td>
                    <td>CE</td>
                </tr>
                <tr>
                    <td>139</td>
                    <td>A company&#x27;s DevOps engineer is creating an AWS Lambda function to process notications from an Amazon Simple Notication Service<br>(Amazon SNS) topic. The Lambda function will process the notication messages and will write the contents of the notication messages to<br>an Amazon RDS Multi-AZ DB instance.<br>During testing, a database administrator accidentally shut down the DB instance. While the database was down the company lost several of<br>the SNS notication messages that were delivered during that time.<br>The DevOps engineer needs to prevent the loss of notication messages in the future.</td>
                    <td>A. Replace the RDS Multi-AZ DB instance with an Amazon DynamoDB table.<br>B. Congure an Amazon Simple Queue Service (Amazon SQS) queue as a destination of the Lambda function.<br>C. Congure an Amazon Simple Queue Service (Amazon SQS) dead-letter queue for the SNS topic.<br>D. Subscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Congure the Lambda function to process<br>messages from the SQS queue.<br>E. Replace the SNS topic with an Amazon EventBridge event bus. Congure an EventBridge rule on the new event bus to invoke the<br>Lambda function for each event.</td>
                    <td>一家公司的DevOps工程师正在创建一个AWS Lambda函数来处理来自Amazon Simple Notification Service (Amazon SNS)主题的通知。Lambda函数将处理通知消息并将通知消息的内容写入Amazon RDS Multi-AZ数据库实例。<br>在测试期间，数据库管理员意外关闭了数据库实例。当数据库停机时，公司丢失了在那段时间内传递的几条SNS通知消息。<br>DevOps工程师需要防止将来丢失通知消息。</td>
                    <td>A. 用Amazon DynamoDB表替换RDS Multi-AZ数据库实例 - 这个选项有一定价值，因为DynamoDB是完全托管的NoSQL数据库服务，具有更高的可用性和自动扩展能力，相比RDS更不容易出现停机问题。但这主要解决的是数据库层面的可用性问题，而不是消息丢失的根本原因。<br><br>B. 配置Amazon Simple Queue Service (Amazon SQS)队列作为Lambda函数的目标 - 这个选项配置错误。SQS队列应该作为Lambda函数的事件源，而不是目标。Lambda函数处理完消息后通常写入数据库或其他存储服务。<br><br>C. 为SNS主题配置Amazon Simple Queue Service (Amazon SQS)死信队列 - 死信队列主要用于处理无法成功处理的消息，但在这个场景中，问题不是消息处理失败，而是数据库不可用导致的消息丢失。死信队列无法解决这个问题。<br><br>D. 将Amazon Simple Queue Service (Amazon SQS)队列订阅到SNS主题，配置Lambda函数从SQS队列处理消息 - 这是一个很好的解决方案。SQS提供消息持久化和重试机制，当Lambda函数因为数据库不可用而处理失败时，消息会保留在队列中并可以重试处理。<br><br>E. 用Amazon EventBridge事件总线替换SNS主题，在新的事件总线上配置EventBridge规则来为每个事件调用Lambda函数 - EventBridge虽然功能强大，但在这个场景中并不能解决消息丢失的问题，因为它仍然是直接调用Lambda函数，没有提供消息持久化机制。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>140</td>
                    <td>A company has an application that runs on Amazon EC2 instances. The company uses an AWS CodePipeline pipeline to deploy the application<br>into multiple AWS Regions. The pipeline is congured with a stage for each Region. Each stage contains an AWS CloudFormation action for<br>each Region.<br>When the pipeline deploys the application to a Region, the company wants to conrm that the application is in a healthy state before the<br>pipeline moves on to the next Region. Amazon Route 53 record sets are congured for the application in each Region. A DevOps engineer<br>creates a Route 53 health check that is based on an Amazon CloudWatch alarm for each Region where the application is deployed.</td>
                    <td>A. Create an AWS Step Functions workow to check the state of the CloudWatch alarm. Congure the Step Functions workow to exit with<br>an error if the alarm is in the ALARM state. Create a new stage in the pipeline between each Region deployment stage. In each new stage,<br>include an action to invoke the Step Functions workow.<br>B. Congure an AWS CodeDeploy application to deploy a CloudFormation template with automatic rollback. Congure the CloudWatch<br>alarm as the instance health check for the CodeDeploy application. Remove the CloudFormation actions from the pipeline. Create a<br>CodeDeploy action in the pipeline stage for each Region.<br>C. Create a new pipeline stage for each Region where the application is deployed. Congure a CloudWatch alarm action for the new stage<br>to check the state of the CloudWatch alarm and to exit with an error if the alarm is in the ALARM state<br>D. Congure the CloudWatch agent on the EC2 instances to report the application status to the Route 53 health check. Create a new<br>pipeline stage for each Region where the application is deployed. Congure a CloudWatch alarm action to exit with an error if the<br>CloudWatch alarm is in the ALARM state.</td>
                    <td>一家公司有一个运行在Amazon EC2实例上的应用程序。该公司使用AWS CodePipeline管道将应用程序部署到多个AWS区域。管道配置了每个区域的阶段。每个阶段包含每个区域的AWS CloudFormation操作。<br>当管道将应用程序部署到一个区域时，公司希望在管道继续到下一个区域之前确认应用程序处于健康状态。Amazon Route 53记录集为每个区域的应用程序配置。DevOps工程师为部署应用程序的每个区域创建了基于Amazon CloudWatch告警的Route 53健康检查。</td>
                    <td>选项A：创建AWS Step Functions工作流来检查CloudWatch告警状态，如果告警处于ALARM状态则退出并报错，在每个区域部署阶段之间创建新阶段来调用Step Functions工作流。这个方案技术上可行，但增加了系统复杂性，需要额外的Step Functions服务，而且没有直接利用现有的Route 53健康检查机制。<br><br>选项B：配置AWS CodeDeploy应用程序来部署CloudFormation模板并自动回滚，将CloudWatch告警配置为CodeDeploy应用程序的实例健康检查，移除管道中的CloudFormation操作并创建CodeDeploy操作。这个方案改变了现有的部署架构，需要重新配置整个部署流程，复杂度较高。<br><br>选项C：为每个部署应用程序的区域创建新的管道阶段，为新阶段配置CloudWatch告警操作来检查告警状态，如果告警处于ALARM状态则退出并报错。这个方案相对简单，但没有充分利用已经配置的Route 53健康检查。<br><br>选项D：在EC2实例上配置CloudWatch代理来向Route 53健康检查报告应用程序状态，为每个部署应用程序的区域创建新的管道阶段，配置CloudWatch告警操作在告警处于ALARM状态时退出并报错。这个方案充分利用了现有的Route 53健康检查机制，通过CloudWatch代理实现应用程序状态的实时监控。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>141</td>
                    <td>A company plans to use Amazon CloudWatch to monitor its Amazon EC2 instances. The company needs to stop EC2 instances when the<br>average of the NetworkPacketsIn metric is less than 5 for at least 3 hours in a 12-hour time window. The company must evaluate the metric<br>every hour. The EC2 instances must continue to run if there is missing data for the NetworkPacketsIn metric during the evaluation period.<br>A DevOps engineer creates a CloudWatch alarm for the NetworkPacketsIn metric. The DevOps engineer congures a threshold value of 5 and<br>an evaluation period of 1 hour.</td>
                    <td>A. Congure the Datapoints to Alarm value to be 3 out of 12. Congure the alarm to treat missing data as breaching the threshold. Add an<br>AWS Systems Manager action to stop the instance when the alarm enters the ALARM state.<br>B. Congure the Datapoints to Alarm value to be 3 out of 12. Congure the alarm to treat missing data as not breaching the threshold.<br>Add an EC2 action to stop the instance when the alarm enters the ALARM state.<br>C. Congure the Datapoints to Alarm value to be 9 out of 12. Congure the alarm to treat missing data as breaching the threshold. Add an<br>EC2 action to stop the instance when the alarm enters the ALARM state.<br>D. Congure the Datapoints to Alarm value to be 9 out of 12. Congure the alarm to treat missing data as not breaching the threshold.<br>Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state.</td>
                    <td>一家公司计划使用Amazon CloudWatch来监控其Amazon EC2实例。公司需要在12小时时间窗口内，当NetworkPacketsIn指标的平均值连续至少3小时小于5时停止EC2实例。公司必须每小时评估一次该指标。如果在评估期间NetworkPacketsIn指标存在缺失数据，EC2实例必须继续运行。一位DevOps工程师为NetworkPacketsIn指标创建了CloudWatch告警。该DevOps工程师配置了阈值为5，评估周期为1小时。</td>
                    <td>选项A：配置数据点告警值为12个中的3个，将缺失数据视为违反阈值，使用Systems Manager操作停止实例。这个配置错误，因为&quot;3 out of 12&quot;意味着只需要3个数据点违反阈值就触发告警，但题目要求连续3小时（即连续3个数据点）违反阈值。另外，将缺失数据视为违反阈值与题目要求相反。<br><br>选项B：配置数据点告警值为12个中的3个，将缺失数据视为不违反阈值，使用EC2操作停止实例。虽然缺失数据处理正确，但&quot;3 out of 12&quot;的配置仍然错误，因为这允许非连续的3个数据点触发告警，而不是连续的3小时。<br><br>选项C：配置数据点告警值为12个中的9个，将缺失数据视为违反阈值，使用EC2操作停止实例。&quot;9 out of 12&quot;意味着需要9个数据点不违反阈值才保持正常状态，换句话说，需要连续3个数据点违反阈值才触发告警（12-9=3）。但缺失数据处理设置错误。<br><br>选项D：配置数据点告警值为12个中的9个，将缺失数据视为不违反阈值，使用Systems Manager操作停止实例。数据点配置正确，缺失数据处理也正确，但应该使用EC2操作而不是Systems Manager操作来停止实例。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>142</td>
                    <td>A company manages 500 AWS accounts that are in an organization in AWS Organizations. The company discovers many unattached Amazon<br>Elastic Block Store (Amazon EBS) volumes in all the accounts. The company wants to automatically tag the unattached EBS volumes for<br>investigation.<br>A DevOps engineer needs to deploy an AWS Lambda function to all the AWS accounts. The Lambda function must run every 30 minutes to tag<br>all the EBS volumes that have been unattached for a period of 7 days or more.</td>
                    <td>A. Congure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda<br>function. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the<br>member accounts in the organization. Create an Amazon EventBridge event bus in the delegated administrator account to invoke the<br>Lambda function in each member account every 30 minutes.<br>B. Create a cross-account IAM role in the organization&#x27;s member accounts. Attach the AWSLambda_FullAccess policy and the<br>AWSCloudFormationFullAccess policy to the role. Create an AWS CloudFormation template that contains the Lambda function and an<br>Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Create a custom script in the organization’s<br>management account that assumes the role and deploys the CloudFormation template to the member accounts.<br>C. Congure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda<br>function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Use CloudFormation StackSets to<br>deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization<br>D. Create a cross-account IAM role in the organization&#x27;s member accounts. Attach the AmazonS3FullAccess policy and the<br>AWSCodeDeployDeployerAccess policy to the role. Use AWS CodeDeploy to assume the role to deploy the Lambda function from the<br>organization&#x27;s management account. Congure an Amazon EventBridge scheduled rule in the member accounts to invoke the Lambda<br>function every 30 minutes.</td>
                    <td>一家公司管理着500个AWS账户，这些账户都在AWS Organizations的组织中。公司发现所有账户中都有许多未附加的Amazon弹性块存储（Amazon EBS）卷。公司希望自动为这些未附加的EBS卷打标签以便调查。<br>DevOps工程师需要将AWS Lambda函数部署到所有AWS账户中。Lambda函数必须每30分钟运行一次，为所有未附加7天或更长时间的EBS卷打标签。</td>
                    <td>选项A：配置组织的委托管理员账户，创建包含Lambda函数的CloudFormation模板，使用CloudFormation StackSets从委托管理员账户部署到所有成员账户，在委托管理员账户中创建EventBridge事件总线每30分钟调用各成员账户中的Lambda函数。这个方案使用了正确的服务组合，但EventBridge事件总线的跨账户调用配置相对复杂。<br><br>选项B：在成员账户中创建跨账户IAM角色，附加Lambda和CloudFormation的完全访问策略，创建包含Lambda函数和EventBridge定时规则的CloudFormation模板，在管理账户中创建自定义脚本来承担角色并部署模板。这个方案可行但需要自定义脚本，不如使用AWS原生服务优雅。<br><br>选项C：配置委托管理员账户，创建包含Lambda函数和EventBridge定时规则的CloudFormation模板，使用CloudFormation StackSets从委托管理员账户部署到所有成员账户。这个方案最为简洁，使用AWS原生服务，每个账户都有自己的定时规则和Lambda函数。<br><br>选项D：使用CodeDeploy部署Lambda函数，但CodeDeploy主要用于EC2和本地服务器的应用程序部署，不是Lambda函数部署的最佳选择，且所需的IAM策略也不正确。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>143</td>
                    <td>A company&#x27;s production environment uses an AWS CodeDeploy blue/green deployment to deploy an application. The deployment incudes<br>Amazon EC2 Auto Scaling groups that launch instances that run Amazon Linux 2.<br>A working appspec.yml le exists in the code repository and contains the following text:<br>A DevOps engineer needs to ensure that a script downloads and installs a license le onto the instances before the replacement instances<br>start to handle request trac. The DevOps engineer adds a hooks section to the appspec.yml le.</td>
                    <td>A. AfterBlockTrac<br>B. BeforeBlockTrac<br>C. BeforeInstall<br>D. DownloadBundle</td>
                    <td>一家公司的生产环境使用AWS CodeDeploy蓝/绿部署来部署应用程序。该部署包括Amazon EC2 Auto Scaling组，这些组启动运行Amazon Linux 2的实例。<br>代码仓库中存在一个可用的appspec.yml文件并包含以下文本：<br>DevOps工程师需要确保在替换实例开始处理请求流量之前，有一个脚本下载并安装许可证文件到实例上。DevOps工程师向appspec.yml文件添加了一个hooks部分。</td>
                    <td>A. AfterBlockTraffic - 这个钩子在流量被阻止之后执行，此时实例已经不再接收新的请求。这个阶段主要用于清理工作，不适合安装许可证文件，因为此时实例可能即将被终止。<br><br>B. BeforeBlockTraffic - 这个钩子在阻止流量之前执行，通常用于在停止服务前进行一些准备工作。但这个阶段是针对旧实例的，不是新实例，所以不适合在新实例上安装许可证文件。<br><br>C. BeforeInstall - 这个钩子在安装应用程序之前执行，可以用于准备实例环境。虽然可以在此阶段下载许可证文件，但通常用于清理和准备工作，不是最佳选择。<br><br>D. DownloadBundle - 这个钩子在下载应用程序包期间执行，是部署生命周期的早期阶段。在此阶段下载和安装许可证文件可以确保在应用程序启动前所有必需的组件都已就位。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>144</td>
                    <td>A company has an application that includes AWS Lambda functions. The Lambda functions run Python code that is stored in an AWS<br>CodeCommit repository. The company has recently experienced failures in the production environment because of an error in the Python<br>code. An engineer has written unit tests for the Lambda functions to help avoid releasing any future defects into the production environment.<br>The company&#x27;s DevOps team needs to implement a solution to integrate the unit tests into an existing AWS CodePipeline pipeline. The<br>solution must produce reports about the unit tests for the company to view.</td>
                    <td>A. Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Create a new AWS CodeBuild project. In the CodePipeline<br>pipeline, congure a test stage that uses the new CodeBuild project. Create a buildspec.yml le in the CodeCommit repository. In the<br>buildspec yml le, dene the actions to run a CodeGuru review.<br>B. Create a new AWS CodeBuild project. In the CodePipeline pipeline, congure a test stage that uses the new CodeBuild project. Create a<br>CodeBuild report group. Create a buildspec.yml le in the CodeCommit repository. In the buildspec.yml le, dene the actions to run the<br>unit tests with an output of JUNITXML in the build phase section. Congure the test reports to be uploaded to the new CodeBuild report<br>group.<br>C. Create a new AWS CodeArtifact repository. Create a new AWS CodeBuild project. In the CodePipeline pipeline, congure a test stage<br>that uses the new CodeBuild project. Create an appspec.yml le in the original CodeCommit repository. In the appspec.yml le, dene the<br>actions to run the unit tests with an output of CUCUMBERJSON in the build phase section. Congure the tests reports to be sent to the<br>new CodeArtifact repository.<br>D. Create a new AWS CodeBuild project. In the CodePipeline pipeline, congure a test stage that uses the new CodeBuild project. Create a<br>new Amazon S3 bucket. Create a buildspec.yml le in the CodeCommit repository. In the buildspec yml le, dene the actions to run the<br>unit tests with an output of HTML in the phases section. In the reports section, upload the test reports to the S3 bucket.</td>
                    <td>一家公司有一个包含AWS Lambda函数的应用程序。Lambda函数运行存储在AWS CodeCommit存储库中的Python代码。该公司最近在生产环境中遇到了由于Python代码错误导致的故障。一名工程师为Lambda函数编写了单元测试，以帮助避免将任何未来的缺陷发布到生产环境中。公司的DevOps团队需要实施一个解决方案，将单元测试集成到现有的AWS CodePipeline管道中。该解决方案必须生成关于单元测试的报告供公司查看。</td>
                    <td>选项A：将CodeCommit存储库与Amazon CodeGuru Reviewer关联，创建新的CodeBuild项目，在CodePipeline中配置测试阶段，创建buildspec.yml文件来运行CodeGuru审查。这个选项的问题是CodeGuru Reviewer主要用于代码审查和静态分析，而不是运行单元测试。它不能执行工程师编写的单元测试，也不会产生单元测试报告。<br><br>选项B：创建新的CodeBuild项目，在CodePipeline中配置测试阶段，创建CodeBuild报告组，在buildspec.yml文件中定义运行单元测试的操作，输出JUNITXML格式，并将测试报告上传到CodeBuild报告组。这是正确的方法，因为CodeBuild可以执行单元测试，JUNITXML是标准的测试报告格式，CodeBuild报告组专门用于存储和展示测试报告。<br><br>选项C：创建CodeArtifact存储库，创建CodeBuild项目，使用appspec.yml文件定义单元测试操作，输出CUCUMBERJSON格式，将报告发送到CodeArtifact。这个选项有多个问题：appspec.yml是用于CodeDeploy的配置文件，不适用于CodeBuild；CodeArtifact是包管理服务，不是用于存储测试报告的；CUCUMBERJSON通常用于行为驱动开发测试，不适合Python单元测试。<br><br>选项D：创建CodeBuild项目，创建S3存储桶，在buildspec.yml中定义单元测试操作，输出HTML格式，将报告上传到S3。虽然技术上可行，但HTML格式不是标准的测试报告格式，且S3不提供专门的测试报告查看功能，不如CodeBuild报告组方便。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>145</td>
                    <td>A company manages multiple AWS accounts in AWS Organizations. The company’s security policy states that AWS account root user<br>credentials for member accounts must not be used. The company monitors access to the root user credentials.<br>A recent alert shows that the root user in a member account launched an Amazon EC2 instance. A DevOps engineer must create an SCP at the<br>organization&#x27;s root level that will prevent the root user in member accounts from making any AWS service API calls.</td>
                    <td>A. <br>B. <br>C. <br>D.</td>
                    <td>一家公司在AWS Organizations中管理多个AWS账户。公司的安全策略规定不得使用成员账户的AWS账户根用户凭证。公司监控对根用户凭证的访问。<br>最近的一个警报显示，成员账户中的根用户启动了一个Amazon EC2实例。DevOps工程师必须在组织的根级别创建一个SCP（服务控制策略），以防止成员账户中的根用户进行任何AWS服务API调用。</td>
                    <td>由于题目中没有提供具体的选项A、B、C、D的内容，我无法对每个选项进行详细分析。但是基于题目要求，正确的SCP策略应该包含以下要素：<br>- 使用&quot;Deny&quot;效果来阻止根用户的API调用<br>- 在Principal中指定根用户（通常使用&quot;AWS&quot;: &quot;*&quot;或类似的根用户标识符）<br>- 在Condition中使用适当的条件来识别根用户身份<br>- Action应该设置为&quot;*&quot;以阻止所有AWS服务API调用<br>- 策略应该应用于成员账户而不影响组织管理账户的根用户权限</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>146</td>
                    <td>A company uses AWS and has a VPC that contains critical compute infrastructure with predictable trac patterns. The company has<br>congured VPC ow logs that are published to a log group in Amazon CloudWatch Logs.<br>The company&#x27;s DevOps team needs to congure a monitoring solution for the VPC ow logs to identify anomalies in network trac to the VPC<br>over time. If the monitoring solution detects an anomaly, the company needs the ability to initiate a response to the anomaly.<br>How should the DevOps team congure the monitoring solution to meet these requirements?</td>
                    <td>A. Create an Amazon Kinesis data stream. Subscribe the log group to the data stream. Congure Amazon Kinesis Data Analytics to detect<br>log anomalies in the data stream. Create an AWS Lambda function to use as the output of the data stream. Congure the Lambda<br>function to write to the default Amazon EventBridge event bus in the event of an anomaly nding.<br>B. Create an Amazon Kinesis Data Firehose delivery stream that delivers events to an Amazon S3 bucket. Subscribe the log group to the<br>delivery stream. Congure Amazon Lookout for Metrics to monitor the data in the S3 bucket for anomalies. Create an AWS Lambda<br>function to run in response to Lookout for Metrics anomaly ndings. Congure the Lambda function to publish to the default Amazon<br>EventBridge event bus.<br>C. Create an AWS Lambda function to detect anomalies. Congure the Lambda function to publish an event to the default Amazon<br>EventBridge event bus if the Lambda function detects an anomaly. Subscribe the Lambda function to the log group.<br>D. Create an Amazon Kinesis data stream. Subscribe the log group to the data stream. Create an AWS Lambda function to detect log<br>anomalies. Congure the Lambda function to write to the default Amazon EventBridge event bus if the Lambda function detects an<br>anomaly. Set the Lambda function as the processor for the data stream.</td>
                    <td>一家公司使用AWS，拥有一个包含关键计算基础设施的VPC，该基础设施具有可预测的流量模式。公司已配置VPC流日志，这些日志发布到Amazon CloudWatch Logs中的日志组。<br>公司的DevOps团队需要为VPC流日志配置监控解决方案，以识别随时间变化的VPC网络流量异常。如果监控解决方案检测到异常，公司需要能够对异常启动响应。<br>DevOps团队应该如何配置监控解决方案来满足这些要求？</td>
                    <td>选项A：创建Amazon Kinesis数据流，将日志组订阅到数据流，配置Amazon Kinesis Data Analytics检测数据流中的日志异常，创建AWS Lambda函数作为数据流的输出，配置Lambda函数在发现异常时写入默认的Amazon EventBridge事件总线。这个方案使用了专门的流分析服务Kinesis Data Analytics来检测异常，这是AWS提供的专业异常检测服务，能够有效处理实时流数据并识别模式异常。整个架构合理且使用了AWS托管服务。<br><br>选项B：创建Amazon Kinesis Data Firehose传输流将事件传输到S3存储桶，将日志组订阅到传输流，配置Amazon Lookout for Metrics监控S3存储桶中的数据异常，创建Lambda函数响应Lookout for Metrics异常发现并发布到EventBridge。虽然Lookout for Metrics是专业的异常检测服务，但这个方案增加了不必要的S3存储步骤，使架构变得复杂，且可能引入延迟。<br><br>选项C：创建AWS Lambda函数检测异常，配置Lambda函数在检测到异常时发布事件到EventBridge，将Lambda函数订阅到日志组。这个方案要求在Lambda函数中自行实现异常检测逻辑，增加了开发复杂性和维护成本，没有利用AWS的专业异常检测服务。<br><br>选项D：创建Kinesis数据流，将日志组订阅到数据流，创建Lambda函数检测日志异常，配置Lambda函数在检测到异常时写入EventBridge，将Lambda函数设置为数据流的处理器。与选项C类似，需要自行实现异常检测逻辑，没有充分利用AWS托管的异常检测能力。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>147</td>
                    <td>AnyCompany is using AWS Organizations to create and manage multiple AWS accounts. AnyCompany recently acquired a smaller company,<br>Example Corp. During the acquisition process, Example Corp&#x27;s single AWS account joined AnyCompany&#x27;s management account through an<br>Organizations invitation. AnyCompany moved the new member account under an OU that is dedicated to Example Corp.<br>AnyCompany&#x27;s DevOps engineer has an IAM user that assumes a role that is named OrganizationAccountAccessRole to access member<br>accounts. This role is congured with a full access policy. When the DevOps engineer tries to use the AWS Management Console to assume<br>the role in Example Corp&#x27;s new member account, the DevOps engineer receives the following error message: &quot;Invalid information in one or<br>more elds. Check your information or contact your administrator.&quot;</td>
                    <td>A. In the management account, grant the DevOps engineer&#x27;s IAM user permission to assume the OrganizationAccountAccessRole IAM role<br>in the new member account.<br>B. In the management account, create a new SCP. In the SCP, grant the DevOps engineer&#x27;s IAM user full access to all resources in the new<br>member account. Attach the SCP to the OU that contains the new member account.<br>C. In the new member account, create a new IAM role that is named OrganizationAccountAccessRole. Attach the AdministratorAccess AWS<br>managed policy to the role. In the role&#x27;s trust policy, grant the management account permission to assume the role.<br>D. In the new member account, edit the trust policy for the OrganizationAccountAccessRole IAM role. Grant the management account<br>permission to assume the role.</td>
                    <td>AnyCompany正在使用AWS Organizations来创建和管理多个AWS账户。AnyCompany最近收购了一家较小的公司Example Corp。在收购过程中，Example Corp的单个AWS账户通过Organizations邀请加入了AnyCompany的管理账户。AnyCompany将新的成员账户移动到专门为Example Corp设立的OU下。<br><br>AnyCompany的DevOps工程师有一个IAM用户，该用户承担名为OrganizationAccountAccessRole的角色来访问成员账户。该角色配置了完全访问策略。当DevOps工程师尝试使用AWS管理控制台在Example Corp的新成员账户中承担该角色时，DevOps工程师收到以下错误消息：&quot;一个或多个字段中的信息无效。请检查您的信息或联系您的管理员。&quot;</td>
                    <td>A. 在管理账户中，授予DevOps工程师的IAM用户在新成员账户中承担OrganizationAccountAccessRole IAM角色的权限。这个选项不正确，因为问题不在于管理账户中的权限设置，而是新成员账户中角色的信任策略问题。<br><br>B. 在管理账户中创建新的SCP，在SCP中授予DevOps工程师的IAM用户对新成员账户中所有资源的完全访问权限，并将SCP附加到包含新成员账户的OU。这个选项不正确，因为SCP（服务控制策略）是用于限制权限的，不能用于授予权限，而且这不能解决角色承担的问题。<br><br>C. 在新成员账户中创建名为OrganizationAccountAccessRole的新IAM角色，将AdministratorAccess AWS托管策略附加到该角色，在角色的信任策略中授予管理账户承担该角色的权限。这个选项可能有效，但不是最佳解决方案，因为角色可能已经存在，只是信任策略配置不正确。<br><br>D. 在新成员账户中编辑OrganizationAccountAccessRole IAM角色的信任策略，授予管理账户承担该角色的权限。这是正确的选项，因为当账户通过邀请加入Organizations时，现有的角色信任策略可能没有正确配置来允许管理账户承担该角色。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>148</td>
                    <td>A DevOps engineer is designing an application that integrates with a legacy REST API. The application has an AWS Lambda function that<br>reads records from an Amazon Kinesis data stream. The Lambda function sends the records to the legacy REST API.<br>Approximately 10% of the records that the Lambda function sends from the Kinesis data stream have data errors and must be processed<br>manually. The Lambda function event source conguration has an Amazon Simple Queue Service (Amazon SQS) dead-letter queue as an on-<br>failure destination. The DevOps engineer has congured the Lambda function to process records in batches and has implemented retries in<br>case of failure.<br>During testing, the DevOps engineer notices that the dead-letter queue contains many records that have no data errors and that already have<br>been processed by the legacy REST API. The DevOps engineer needs to congure the Lambda function&#x27;s event source options to reduce the<br>number of errorless records that are sent to the dead-letter queue.</td>
                    <td>A. Increase the retry attempts.<br>B. Congure the setting to split the batch when an error occurs.<br>C. Increase the concurrent batches per shard.<br>D. Decrease the maximum age of record.</td>
                    <td>一位DevOps工程师正在设计一个与传统REST API集成的应用程序。该应用程序有一个AWS Lambda函数，用于从Amazon Kinesis数据流中读取记录。Lambda函数将这些记录发送到传统REST API。<br>Lambda函数从Kinesis数据流发送的记录中，大约10%存在数据错误，必须手动处理。Lambda函数事件源配置有一个Amazon Simple Queue Service (Amazon SQS)死信队列作为失败时的目标。DevOps工程师已配置Lambda函数批量处理记录，并在失败时实施重试机制。<br>在测试过程中，DevOps工程师注意到死信队列包含许多没有数据错误且已经被传统REST API处理过的记录。DevOps工程师需要配置Lambda函数的事件源选项，以减少发送到死信队列的无错误记录数量。</td>
                    <td>A. 增加重试次数：这个选项不会解决问题的根本原因。增加重试次数可能会让更多无错误的记录被重复处理，反而可能增加死信队列中的无错误记录数量。当批次中只有部分记录有错误时，整个批次会被重试，导致已成功处理的记录也被重新发送。<br><br>B. 配置在发生错误时拆分批次的设置：这是正确的解决方案。当启用批次拆分功能时，如果批次中的某些记录处理失败，Lambda会将批次拆分成更小的批次，只重试失败的记录，而不是重试整个批次。这样可以避免将已成功处理的记录发送到死信队列。<br><br>C. 增加每个分片的并发批次数：这个选项主要影响处理吞吐量和并发性，但不会解决批次中部分记录失败导致整个批次被发送到死信队列的问题。增加并发批次可能会提高处理速度，但不会减少死信队列中的无错误记录。<br><br>D. 减少记录的最大存活时间：这个选项控制记录在流中保留的时间，与死信队列中出现无错误记录的问题无关。减少最大存活时间不会解决批次处理中的错误处理逻辑问题。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>149</td>
                    <td>A company has microservices running in AWS Lambda that read data from Amazon DynamoD</td>
                    <td>B. Create an AWS CodeBuild conguration that triggers when the test code is pushed. Use AWS CloudFormation to trigger an AWS<br>CodePipeline conguration that deploys the new Lambda versions and species the trac shift percentage and interval.<br>A. Create an AWS CodePipeline conguration and set up a post-commit hook to trigger the pipeline after tests have passed. Use AWS<br>CodeDeploy and create a Canary deployment conguration that species the percentage of trac and interval.<br>C. Create an AWS CodePipeline conguration and set up the source code step to trigger when code is pushed. Set up the build step to use<br>AWS CodeBuild to run the tests. Set up an AWS CodeDeploy conguration to deploy, then select the<br>CodeDeployDefault.LambdaLinear10PercentEvery3Minutes option.<br>D. Use the AWS CLI to set up a post-commit hook that uploads the code to an Amazon S3 bucket after tests have passed Set up an S3<br>event trigger that runs a Lambda function that deploys the new version. Use an interval in the Lambda function to deploy the code over<br>time at the required percentage.</td>
                    <td>一家公司有运行在AWS Lambda上的微服务，这些服务从Amazon DynamoDB读取数据。[题目似乎被截断了，但从选项可以推断这是关于Lambda函数的持续集成/持续部署(CI/CD)和渐进式部署的问题]</td>
                    <td>选项A：创建AWS CodePipeline配置并设置提交后钩子，在测试通过后触发管道。使用AWS CodeDeploy创建金丝雀部署配置，指定流量百分比和间隔。这是一个完整且标准的CI/CD解决方案，CodePipeline负责编排整个流程，CodeDeploy专门处理Lambda函数的渐进式部署，金丝雀部署是Lambda函数部署的最佳实践之一。<br><br>选项B：创建AWS CodeBuild配置，在测试代码推送时触发。使用AWS CloudFormation触发CodePipeline配置来部署新的Lambda版本并指定流量转移百分比和间隔。这个方案的问题在于CloudFormation主要用于基础设施即代码，不是专门用于应用程序部署和流量管理的最佳工具。<br><br>选项C：创建CodePipeline配置，设置源代码步骤在代码推送时触发，构建步骤使用CodeBuild运行测试，设置CodeDeploy配置进行部署，然后选择CodeDeployDefault.LambdaLinear10PercentEvery3Minutes选项。这是一个技术上可行的方案，但相比选项A缺少了灵活性，因为它硬编码了特定的部署策略。<br><br>选项D：使用AWS CLI设置提交后钩子，在测试通过后将代码上传到S3存储桶。设置S3事件触发器运行Lambda函数来部署新版本，在Lambda函数中使用间隔来按要求的百分比逐步部署代码。这是一个过于复杂的自定义解决方案，不如使用AWS原生的CI/CD服务。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>150</td>
                    <td>A company is building a web and mobile application that uses a serverless architecture powered by AWS Lambda and Amazon API Gateway.<br>The company wants to fully automate the backend Lambda deployment based on code that is pushed to the appropriate environment branch<br>in an AWS CodeCommit repository.<br>The deployment must have the following:<br>• Separate environment pipelines for testing and production<br>• Automatic deployment that occurs for test environments only</td>
                    <td>A. Congure a new AWS CodePipeline service. Create a CodeCommit repository for each environment. Set up CodePipeline to retrieve the<br>source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.<br>B. Create two AWS CodePipeline congurations for test and production environments. Congure the production pipeline to have a manual<br>approval step. Create a CodeCommit repository for each environment. Set up each CodePipeline to retrieve the source code from the<br>appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.<br>C. Create two AWS CodePipeline congurations for test and production environments. Congure the production pipeline to have a manual<br>approval step. Create one CodeCommit repository with a branch for each environment. Set up each CodePipeline to retrieve the source<br>code from the appropriate branch in the repository. Set up the deployment step to deploy the Lambda functions with AWS<br>CloudFormation.<br>D. Create an AWS CodeBuild conguration for test and production environments. Congure the production pipeline to have a manual<br>approval step. Create one CodeCommit repository with a branch for each environment. Push the Lambda function code to an Amazon S3<br>bucket. Set up the deployment step to deploy the Lambda functions from the S3 bucket.</td>
                    <td>一家公司正在构建一个使用由AWS Lambda和Amazon API Gateway驱动的无服务器架构的Web和移动应用程序。该公司希望基于推送到AWS CodeCommit存储库中相应环境分支的代码，完全自动化后端Lambda部署。部署必须具备以下条件：<br>• 为测试和生产环境分别设置环境管道<br>• 仅在测试环境中进行自动部署</td>
                    <td>选项A：配置新的AWS CodePipeline服务，为每个环境创建CodeCommit存储库，设置CodePipeline从相应存储库检索源代码，使用AWS CloudFormation部署Lambda函数。这个方案的问题是为每个环境创建单独的存储库会增加管理复杂性，且没有提到生产环境的手动批准步骤，不符合&quot;仅测试环境自动部署&quot;的要求。<br><br>选项B：为测试和生产环境创建两个AWS CodePipeline配置，生产管道配置手动批准步骤，为每个环境创建CodeCommit存储库。虽然满足了手动批准的要求，但为每个环境创建单独存储库的做法不是最佳实践，会导致代码管理分散和同步问题。<br><br>选项C：为测试和生产环境创建两个AWS CodePipeline配置，生产管道配置手动批准步骤，创建一个CodeCommit存储库，每个环境使用不同分支，每个CodePipeline从存储库的相应分支检索源代码，使用AWS CloudFormation部署Lambda函数。这个方案完美满足所有要求：分离的环境管道、生产环境手动批准、统一的代码管理。<br><br>选项D：为测试和生产环境创建AWS CodeBuild配置，但题目要求的是完整的部署管道，CodeBuild主要用于构建，不是完整的CI/CD解决方案。将Lambda代码推送到S3桶的方式也不是最佳实践。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>151</td>
                    <td>A DevOps engineer wants to nd a solution to migrate an application from on premises to AWS. The application is running on Linux and needs<br>to run on specic versions of Apache Tomcat, HAProxy, and Varnish Cache to function properly. The application&#x27;s operating system-level<br>parameters require tuning. The solution must include a way to automate the deployment of new application versions. The infrastructure<br>should be scalable and faulty servers should be replaced automatically.</td>
                    <td>A. Upload the application as a Docker image that contains all the necessary software to Amazon ECR. Create an Amazon ECS cluster using<br>an AWS Fargate launch type and an Auto Scaling group. Create an AWS CodePipeline pipeline that uses Amazon ECR as a source and<br>Amazon ECS as a deployment provider.<br>B. Upload the application code to an AWS CodeCommit repository with a saved conguration le to congure and install the software.<br>Create an AWS Elastic Beanstalk web server tier and a load balanced-type environment that uses the Tomcat solution stack. Create an<br>AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.<br>C. Upload the application code to an AWS CodeCommit repository with a set of .ebextensions les to congure and install the software.<br>Create an AWS Elastic Beanstalk worker tier environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that<br>uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.<br>D. Upload the application code to an AWS CodeCommit repository with an appspec.yml le to congure and install the necessary software.<br>Create an AWS CodeDeploy deployment group associated with an Amazon EC2 Auto Scaling group. Create an AWS CodePipeline pipeline<br>that uses CodeCommit as a source and CodeDeploy as a deployment provider.</td>
                    <td>一位DevOps工程师想要找到一个解决方案，将应用程序从本地迁移到AWS。该应用程序运行在Linux上，需要运行在特定版本的Apache Tomcat、HAProxy和Varnish Cache上才能正常工作。应用程序的操作系统级参数需要调优。解决方案必须包括自动化部署新应用程序版本的方法。基础设施应该是可扩展的，故障服务器应该自动替换。</td>
                    <td>选项A：将应用程序作为包含所有必要软件的Docker镜像上传到Amazon ECR，创建使用AWS Fargate启动类型和Auto Scaling组的Amazon ECS集群。虽然这个方案提供了容器化和自动扩展功能，但Fargate对操作系统级参数调优的支持有限，无法满足题目中对特定软件版本和OS级参数调优的严格要求。<br><br>选项B：将应用程序代码上传到AWS CodeCommit，使用配置文件来配置和安装软件，创建使用Tomcat解决方案堆栈的Elastic Beanstalk web服务器层。虽然Elastic Beanstalk支持Tomcat，但它对HAProxy和Varnish Cache等特定软件版本的支持有限，且对操作系统级参数的自定义能力不足。<br><br>选项C：类似选项B，但使用worker tier环境。Worker tier主要用于后台处理任务，不适合需要HAProxy和Varnish Cache的web应用程序架构，且同样存在软件版本控制和OS参数调优的限制。<br><br>选项D：将应用程序代码上传到CodeCommit，使用appspec.yml文件配置和安装必要软件，创建与EC2 Auto Scaling组关联的CodeDeploy部署组。这个方案提供了对EC2实例的完全控制，可以安装特定版本的软件，进行操作系统级参数调优，同时通过Auto Scaling实现自动扩展和故障替换。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>152</td>
                    <td>A DevOps engineer is using AWS CodeDeploy across a eet of Amazon EC2 instances in an EC2 Auto Scaling group. The associated<br>CodeDeploy deployment group, which is integrated with EC2 Auto Scaling, is congured to perform in-place deployments with<br>CodeDeployDefault.OneAtATime. During an ongoing new deployment, the engineer discovers that, although the overall deployment nished<br>successfully, two out of ve instances have the previous application revision deployed. The other three instances have the newest application<br>revision.</td>
                    <td>A. The two affected instances failed to fetch the new deployment.<br>B. A failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances.<br>C. The CodeDeploy agent was not installed in two affected instances.<br>D. EC2 Auto Scaling launched two new instances while the new deployment had not yet nished, causing the previous version to be<br>deployed on the affected instances.</td>
                    <td>一名DevOps工程师正在使用AWS CodeDeploy在EC2 Auto Scaling组中的一组Amazon EC2实例上进行部署。与EC2 Auto Scaling集成的相关CodeDeploy部署组配置为使用CodeDeployDefault.OneAtATime执行就地部署。在正在进行的新部署过程中，工程师发现尽管整体部署成功完成，但五个实例中有两个仍然部署着之前的应用程序版本。其他三个实例有最新的应用程序版本。</td>
                    <td>A. 两个受影响的实例未能获取新部署。<br>这个选项是合理的。在CodeDeploy部署过程中，如果某些实例由于网络问题、权限问题或其他技术故障无法成功获取新的部署包，它们会保持原有版本。由于使用OneAtATime策略，部署会继续进行到其他实例，导致版本不一致的情况。<br><br>B. 失败的AfterInstall生命周期事件钩子导致CodeDeploy代理在受影响的实例上回滚到之前的版本。<br>这个选项不太可能。如果AfterInstall钩子失败，通常会导致整个部署在该实例上失败，而不是静默回滚到之前版本。CodeDeploy会明确报告这种失败。<br><br>C. CodeDeploy代理未安装在两个受影响的实例中。<br>这个选项不正确。如果CodeDeploy代理未安装，这些实例根本不会参与部署过程，也不会在部署组中被识别为成功部署的目标。<br><br>D. EC2 Auto Scaling在新部署尚未完成时启动了两个新实例，导致在受影响的实例上部署了之前的版本。<br>这个选项不符合CodeDeploy与Auto Scaling的集成机制。当部署正在进行时，新启动的实例通常会等待当前部署完成或接收最新的部署版本。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>153</td>
                    <td>A security team is concerned that a developer can unintentionally attach an Elastic IP address to an Amazon EC2 instance in production. No<br>developer should be allowed to attach an Elastic IP address to an instance. The security team must be notied if any production server has an<br>Elastic IP address at any time.<br>How can this task be automated?</td>
                    <td>A. Use Amazon Athena to query AWS CloudTrail logs to check for any associate-address attempts. Create an AWS Lambda function to<br>disassociate the Elastic IP address from the instance, and alert the security team.<br>B. Attach an IAM policy to the developers&#x27; IAM group to deny associate-address permissions. Create a custom AWS Cong rule to check<br>whether an Elastic IP address is associated with any instance tagged as production, and alert the security team.<br>C. Ensure that all IAM groups associated with developers do not have associate-address permissions. Create a scheduled AWS Lambda<br>function to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team if an<br>instance has an Elastic IP address associated with it.<br>D. Create an AWS Cong rule to check that all production instances have EC2 IAM roles that include deny associate-address permissions.<br>Verify whether there is an Elastic IP address associated with any instance, and alert the security team if an instance has an Elastic IP<br>address associated with it.</td>
                    <td>安全团队担心开发人员可能会无意中将弹性IP地址附加到生产环境中的Amazon EC2实例上。不应允许任何开发人员将弹性IP地址附加到实例。如果任何生产服务器在任何时候都有弹性IP地址，必须通知安全团队。<br>如何自动化这个任务？</td>
                    <td>A. 使用Amazon Athena查询AWS CloudTrail日志来检查任何associate-address尝试。创建AWS Lambda函数来取消弹性IP地址与实例的关联，并警告安全团队。这个方案只是被动地检查日志，无法预防开发人员执行associate-address操作，而且使用Athena查询CloudTrail日志的实时性较差，不能及时发现问题。<br><br>B. 将IAM策略附加到开发人员的IAM组以拒绝associate-address权限。创建自定义AWS Config规则来检查是否有弹性IP地址与任何标记为生产的实例关联，并警告安全团队。这个方案既通过IAM策略预防了开发人员执行不当操作，又通过Config规则实现了持续监控和告警，是最全面的解决方案。<br><br>C. 确保与开发人员关联的所有IAM组都没有associate-address权限。创建定时的AWS Lambda函数来检查是否有弹性IP地址与任何标记为生产的实例关联，如果实例有关联的弹性IP地址则警告安全团队。虽然预防措施正确，但使用定时Lambda函数进行检查不如Config规则的实时性和可靠性好。<br><br>D. 创建AWS Config规则来检查所有生产实例都有包含拒绝associate-address权限的EC2 IAM角色。验证是否有弹性IP地址与任何实例关联，如果实例有关联的弹性IP地址则警告安全团队。这个方案试图通过EC2实例角色来控制权限，但这不是控制associate-address操作的正确方法，因为这个操作是由用户执行的，不是由实例角色执行的。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>154</td>
                    <td>A company is using AWS Organizations to create separate AWS accounts for each of its departments. The company needs to automate the<br>following tasks:<br>• Update the Linux AMIs with new patches periodically and generate a golden image<br>• Install a new version of Chef agents in the golden image, if available<br>• Provide the newly generated AMIs to the department&#x27;s accounts</td>
                    <td>A. Write a script to launch an Amazon EC2 instance from the previous golden image. Apply the patch updates. Install the new version of<br>the Chef agent, generate a new golden image, and then modify the AMI permissions to share only the new image with the department&#x27;s<br>accounts.<br>B. Use Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef<br>agent. Use AWS Resource Access Manager to share EC2 Image Builder images with the department&#x27;s accounts.<br>C. Use an AWS Systems Manager Automation runbook to update the Linux AMI by using the previous image. Provide the URL for the script<br>that will update the Chef agent. Use AWS Organizations to replace the previous golden image in the department&#x27;s accounts.<br>D. Use Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef<br>agent. Create a parameter in AWS Systems Manager Parameter Store to store the new AMI ID that can be referenced by the department&#x27;s<br>accounts.</td>
                    <td>一家公司正在使用AWS Organizations为其各个部门创建独立的AWS账户。该公司需要自动化以下任务：<br>• 定期使用新补丁更新Linux AMI并生成黄金镜像<br>• 如果可用，在黄金镜像中安装新版本的Chef代理<br>• 将新生成的AMI提供给各部门的账户</td>
                    <td>A. 编写脚本从之前的黄金镜像启动Amazon EC2实例，应用补丁更新，安装新版本的Chef代理，生成新的黄金镜像，然后修改AMI权限以仅与部门账户共享新镜像。这个方案虽然可行，但需要手动编写和维护脚本，自动化程度不高，且需要手动管理AMI权限共享，运维复杂度较高。<br><br>B. 使用Amazon EC2 Image Builder创建包含基础Linux AMI和Chef代理安装组件的镜像管道，使用AWS Resource Access Manager与部门账户共享EC2 Image Builder镜像。这是一个很好的现代化解决方案，EC2 Image Builder专门用于自动化镜像构建，RAM可以有效管理跨账户资源共享。<br><br>C. 使用AWS Systems Manager自动化运行手册通过之前的镜像更新Linux AMI，提供更新Chef代理的脚本URL，使用AWS Organizations在部门账户中替换之前的黄金镜像。这个方案利用了Systems Manager的自动化能力，并且通过Organizations可以统一管理多个账户的镜像更新。<br><br>D. 使用Amazon EC2 Image Builder创建包含基础Linux AMI和Chef代理安装组件的镜像管道，在AWS Systems Manager Parameter Store中创建参数存储新的AMI ID供部门账户引用。虽然使用了EC2 Image Builder，但仅通过Parameter Store共享AMI ID不如直接共享镜像资源有效。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>155</td>
                    <td>A company has a mission-critical application on AWS that uses automatic scaling. The company wants the deployment lifecycle to meet the<br>following parameters:<br>• The application must be deployed one instance at a time to ensure the remaining eet continues to serve trac.<br>• The application is CPU intensive and must be closely monitored.<br>• The deployment must automatically roll back if the CPU utilization of the deployment instance exceeds 85%.</td>
                    <td>A. Use AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move to one instance at a<br>time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto<br>Scaling group using the heartbeat timeout.<br>B. Use AWS CodeDeploy with Amazon EC2 Auto Scaling Congure an alarm tied to the CPU utilization metric. Use the CodeDeployDefault<br>OneAtAtime conguration as a deployment strategy. Congure automatic rollbacks within the deployment group to roll back the<br>deployment if the alarm thresholds are breached.<br>C. Use AWS Elastic Beanstalk for load balancing and AWS Auto Scaling. Congure an alarm tied to the CPU utilization metric. Congure<br>rolling deployments with a xed batch size of one instance. Enable enhanced health to monitor the status of the deployment and roll back<br>based on the alarm previously created.<br>D. Use AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Congure an alarm tied to the CPU<br>utilization metric. Deploy updates one at a time. Congure automatic rollbacks within the Auto Scaling group to roll back the deployment<br>if the alarm thresholds are breached.</td>
                    <td>一家公司在AWS上有一个关键任务应用程序，使用自动扩展。公司希望部署生命周期满足以下参数：<br>• 应用程序必须一次部署一个实例，以确保剩余的实例群继续为流量提供服务。<br>• 应用程序是CPU密集型的，必须被密切监控。<br>• 如果部署实例的CPU利用率超过85%，部署必须自动回滚。</td>
                    <td>选项A：使用AWS CloudFormation创建AWS Step Functions状态机和Auto Scaling生命周期钩子，一次将一个实例移动到等待状态。使用AWS Systems Manager自动化来部署更新到每个实例，并使用心跳超时将其移回Auto Scaling组。这个方案过于复杂，需要多个服务协调工作，而且Systems Manager自动化并不是专门为应用程序部署设计的，缺乏内置的回滚机制和CPU监控集成。<br><br>选项B：使用AWS CodeDeploy与Amazon EC2 Auto Scaling。配置与CPU利用率指标绑定的警报。使用CodeDeployDefault.OneAtATime配置作为部署策略。在部署组内配置自动回滚，当警报阈值被突破时回滚部署。这个方案完美满足所有要求：CodeDeploy专门用于应用程序部署，OneAtATime策略确保一次只部署一个实例，CloudWatch警报可以监控CPU利用率，并且CodeDeploy有内置的自动回滚功能。<br><br>选项C：使用AWS Elastic Beanstalk进行负载均衡和AWS Auto Scaling。配置与CPU利用率指标绑定的警报。配置固定批次大小为一个实例的滚动部署。启用增强健康监控来监控部署状态，并基于之前创建的警报进行回滚。虽然Elastic Beanstalk支持滚动部署和健康监控，但它主要是一个平台即服务解决方案，对于已有的关键任务应用程序来说可能过于重量级，且与现有架构的集成可能复杂。<br><br>选项D：使用AWS Systems Manager执行蓝绿部署与Amazon EC2 Auto Scaling。配置与CPU利用率指标绑定的警报。一次部署一个更新。在Auto Scaling组内配置自动回滚，当警报阈值被突破时回滚部署。Systems Manager主要用于系统管理和补丁管理，不是专门的应用程序部署工具，且Auto Scaling组本身没有基于CPU警报的自动回滚功能。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>156</td>
                    <td>A company has a single developer writing code for an automated deployment pipeline. The developer is storing source code in an Amazon S3<br>bucket for each project. The company wants to add more developers to the team but is concerned about code conicts and lost work. The<br>company also wants to build a test environment to deploy newer versions of code for testing and allow developers to automatically deploy to<br>both environments when code is changed in the repository.</td>
                    <td>A. Create an AWS CodeCommit repository for each project, use the main branch for production code, and create a testing branch for code<br>deployed to testing. Use feature branches to develop new features and pull requests to merge code to testing and main branches.<br>B. Create another S3 bucket for each project for testing code, and use an AWS Lambda function to promote code changes between testing<br>and production buckets. Enable versioning on all buckets to prevent code conicts.<br>C. Create an AWS CodeCommit repository for each project, and use the main branch for production and test code with different<br>deployment pipelines for each environment. Use feature branches to develop new features.<br>D. Enable versioning and branching on each S3 bucket, use the main branch for production code, and create a testing branch for code<br>deployed to testing. Have developers use each branch for developing in each environment.</td>
                    <td>一家公司有一个开发人员为自动化部署管道编写代码。该开发人员将每个项目的源代码存储在Amazon S3存储桶中。公司希望向团队添加更多开发人员，但担心代码冲突和工作丢失。公司还希望构建一个测试环境来部署较新版本的代码进行测试，并允许开发人员在代码库中的代码发生更改时自动部署到两个环境。</td>
                    <td>A. 为每个项目创建AWS CodeCommit存储库，使用主分支用于生产代码，创建测试分支用于部署到测试环境的代码。使用功能分支开发新功能，使用拉取请求将代码合并到测试和主分支。这是最佳实践，CodeCommit是专门的源代码管理服务，支持Git工作流，能很好地处理多开发人员协作、代码冲突和版本控制。<br><br>B. 为每个项目创建另一个S3存储桶用于测试代码，使用AWS Lambda函数在测试和生产存储桶之间推广代码更改。在所有存储桶上启用版本控制以防止代码冲突。虽然S3支持版本控制，但它不是为源代码管理设计的，缺乏分支、合并、冲突解决等功能。<br><br>C. 为每个项目创建AWS CodeCommit存储库，使用主分支用于生产和测试代码，为每个环境使用不同的部署管道。使用功能分支开发新功能。这种方法的问题是生产和测试代码在同一分支上，无法独立管理不同环境的代码版本。<br><br>D. 在每个S3存储桶上启用版本控制和分支，使用主分支用于生产代码，创建测试分支用于部署到测试的代码。让开发人员使用每个分支在每个环境中开发。S3本身不支持Git风格的分支功能，这个选项在技术上不可行。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>157</td>
                    <td>A DevOps engineer notices that all Amazon EC2 instances running behind an Application Load Balancer in an Auto Scaling group are failing to<br>respond to user requests. The EC2 instances are also failing target group HTTP health checks.<br>Upon inspection, the engineer notices the application process was not running in any EC2 instances. There are a signicant number of out of<br>memory messages in the system logs. The engineer needs to improve the resilience of the application to cope with a potential application<br>memory leak. Monitoring and notications should be enabled to alert when there is an issue.</td>
                    <td>A. Change the Auto Scaling conguration to replace the instances when they fail the load balancer&#x27;s health checks.<br>B. Change the target group health check HealthCheckIntervalSeconds parameter to reduce the interval between health checks.<br>C. Change the target group health checks from HTTP to TCP to check if the port where the application is listening is reachable.<br>D. Enable the available memory consumption metric within the Amazon CloudWatch dashboard for the entire Auto Scaling group. Create<br>an alarm when the memory utilization is high. Associate an Amazon SNS topic to the alarm to receive notications when the alarm goes<br>off.<br>E. Use the Amazon CloudWatch agent to collect the memory utilization of the EC2 instances in the Auto Scaling group. Create an alarm<br>when the memory utilization is high and associate an Amazon SNS topic to receive a notication.</td>
                    <td>一名DevOps工程师注意到在Auto Scaling组中运行在Application Load Balancer后面的所有Amazon EC2实例都无法响应用户请求。这些EC2实例也无法通过目标组的HTTP健康检查。经过检查，工程师发现应用程序进程在任何EC2实例中都没有运行。系统日志中有大量的内存不足消息。工程师需要提高应用程序的弹性以应对潜在的应用程序内存泄漏。应该启用监控和通知功能，以便在出现问题时发出警报。</td>
                    <td>A. 更改Auto Scaling配置，当实例未通过负载均衡器的健康检查时替换实例。这个选项可以解决实例故障后的自动替换问题，但没有解决根本的监控和预警需求，也没有提供内存泄漏的早期检测机制。<br><br>B. 更改目标组健康检查的HealthCheckIntervalSeconds参数以减少健康检查之间的间隔。这只是缩短了检查频率，并不能解决内存泄漏问题或提供预警机制，治标不治本。<br><br>C. 将目标组健康检查从HTTP更改为TCP，以检查应用程序监听的端口是否可达。这是一个有效的改进，因为当应用程序因内存不足而崩溃时，TCP检查可以更快地检测到端口不可达的情况，从而触发实例替换。<br><br>D. 在Amazon CloudWatch仪表板中为整个Auto Scaling组启用可用内存消耗指标。当内存利用率高时创建警报，并关联Amazon SNS主题以在警报触发时接收通知。这个选项存在技术错误，因为CloudWatch默认不提供内存指标，需要额外配置。<br><br>E. 使用Amazon CloudWatch代理收集Auto Scaling组中EC2实例的内存利用率。当内存利用率高时创建警报并关联Amazon SNS主题以接收通知。这是正确的监控解决方案，CloudWatch代理可以收集自定义指标包括内存使用率，并提供预警机制。</td>
                    <td>AE</td>
                </tr>
                <tr>
                    <td>158</td>
                    <td>An ecommerce company uses a large number of Amazon Elastic Block Store (Amazon EBS) backed Amazon EC2 instances. To decrease<br>manual work across all the instances, a DevOps engineer is tasked with automating restart actions when EC2 instance retirement events are<br>scheduled.<br>How can this be accomplished?</td>
                    <td>A. Create a scheduled Amazon EventBridge rule to run an AWS Systems Manager Automation runbook that checks if any EC2 instances are<br>scheduled for retirement once a week. If the instance is scheduled for retirement, the runbook will hibernate the instance.<br>B. Enable EC2 Auto Recovery on all of the instances. Create an AWS Cong rule to limit the recovery to occur during a maintenance<br>window only.<br>C. Reboot all EC2 instances during an approved maintenance window that is outside of standard business hours. Set up Amazon<br>CloudWatch alarms to send a notication in case any instance is failing EC2 instance status checks.<br>D. Set up an AWS Health Amazon EventBridge rule to run AWS Systems Manager Automation runbooks that stop and start the EC2 instance<br>when a retirement scheduled event occurs.</td>
                    <td>一家电商公司使用大量基于Amazon Elastic Block Store (Amazon EBS)的Amazon EC2实例。为了减少所有实例的手动工作，DevOps工程师需要在EC2实例退役事件被安排时自动化重启操作。<br>如何实现这一目标？</td>
                    <td>A. 创建一个定时的Amazon EventBridge规则来运行AWS Systems Manager自动化运行手册，每周检查一次是否有EC2实例被安排退役。如果实例被安排退役，运行手册将休眠该实例。这个选项的问题在于：首先，每周检查频率太低，可能错过紧急的退役通知；其次，休眠实例并不能解决退役问题，退役的实例仍然会被AWS强制停止。<br><br>B. 在所有实例上启用EC2自动恢复。创建AWS Config规则来限制恢复只在维护窗口期间发生。这个选项的问题是EC2自动恢复主要用于硬件故障恢复，而不是处理计划的实例退役事件。退役事件需要主动迁移到新的硬件，而不是简单的恢复。<br><br>C. 在标准工作时间之外的批准维护窗口期间重启所有EC2实例。设置Amazon CloudWatch告警，在任何实例未通过EC2实例状态检查时发送通知。这个选项通过定期重启实例来主动避免退役问题，因为重启会将实例迁移到新的底层硬件。CloudWatch告警提供了额外的监控保障。<br><br>D. 设置AWS Health Amazon EventBridge规则来运行AWS Systems Manager自动化运行手册，当退役计划事件发生时停止并启动EC2实例。这个选项看起来合理，但问题在于AWS Health事件通知可能不够及时，而且依赖于事件触发的被动响应可能存在延迟风险。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>159</td>
                    <td>A company manages AWS accounts for application teams in AWS Control Tower. Individual application teams are responsible for securing their<br>respective AWS accounts.<br>A DevOps engineer needs to enable Amazon GuardDuty for all AWS accounts in which the application teams have not already enabled<br>GuardDuty. The DevOps engineer is using AWS CloudFormation StackSets from the AWS Control Tower management account.<br>How should the DevOps engineer congure the CloudFormation template to prevent failure during the StackSets deployment?</td>
                    <td>A. Create a CloudFormation custom resource that invokes an AWS Lambda function. Congure the Lambda function to conditionally<br>enable GuardDuty if GuardDuty is not already enabled in the accounts.<br>B. Use the Conditions section of the CloudFormation template to enable GuardDuty in accounts where GuardDuty is not already enabled.<br>C. Use the CloudFormation Fn::GetAtt intrinsic function to check whether GuardDuty is already enabled. If GuardDuty is not already<br>enabled, use the Resources section of the CloudFormation template to enable GuardDuty.<br>D. Manually discover the list of AWS account IDs where GuardDuty is not enabled. Use the CloudFormation Fn::ImportValue intrinsic<br>function to import the list of account IDs into the CloudFormation template to skip deployment for the listed AWS accounts.</td>
                    <td>一家公司在AWS Control Tower中为应用团队管理AWS账户。各个应用团队负责保护各自的AWS账户安全。<br>一名DevOps工程师需要为所有应用团队尚未启用Amazon GuardDuty的AWS账户启用GuardDuty。该DevOps工程师正在使用来自AWS Control Tower管理账户的AWS CloudFormation StackSets。<br>DevOps工程师应该如何配置CloudFormation模板以防止在StackSets部署过程中出现失败？</td>
                    <td>A. 创建一个CloudFormation自定义资源来调用AWS Lambda函数。配置Lambda函数在账户中尚未启用GuardDuty时有条件地启用GuardDuty。这个方案虽然技术上可行，但增加了复杂性，需要额外的Lambda函数和自定义资源，不是最优解决方案。<br><br>B. 使用CloudFormation模板的Conditions部分在尚未启用GuardDuty的账户中启用GuardDuty。这是正确的方法，CloudFormation的Conditions功能可以根据特定条件决定是否创建资源，能够有效防止在已启用GuardDuty的账户中重复创建资源而导致部署失败。<br><br>C. 使用CloudFormation Fn::GetAtt内置函数检查GuardDuty是否已启用。如果尚未启用，则使用模板的Resources部分启用GuardDuty。Fn::GetAtt函数用于获取资源属性，但不能用于检查资源是否存在，这个方法在技术上不可行。<br><br>D. 手动发现未启用GuardDuty的AWS账户ID列表。使用CloudFormation Fn::ImportValue内置函数将账户ID列表导入模板以跳过列出的AWS账户的部署。这种方法需要手动维护，且Fn::ImportValue的使用方式不正确，不是自动化的解决方案。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>160</td>
                    <td>A company has an AWS Control Tower landing zone. The company&#x27;s DevOps team creates a workload OU. A development OU and a production<br>OU are nested under the workload OU. The company grants users full access to the company&#x27;s AWS accounts to deploy applications.<br>The DevOps team needs to allow only a specic management IAM role to manage the IAM roles and policies of any AWS accounts in only the<br>production OU.</td>
                    <td>A. Create an SCP that denies full access with a condition to exclude the management IAM role for the organization root.<br>B. Ensure that the FullAWSAccess SCP is applied at the organization root.<br>C. Create an SCP that allows IAM related actions. Attach the SCP to the development OU.<br>D. Create an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the workload<br>OU.<br>E. Create an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the production<br>OU.</td>
                    <td>一家公司有一个AWS Control Tower着陆区。公司的DevOps团队创建了一个工作负载OU。开发OU和生产OU嵌套在工作负载OU下。公司授予用户对公司AWS账户的完全访问权限来部署应用程序。DevOps团队需要仅允许特定的管理IAM角色来管理仅生产OU中任何AWS账户的IAM角色和策略。</td>
                    <td>A. 在组织根级别创建一个拒绝完全访问的SCP，条件是排除管理IAM角色。这个选项过于宽泛，会影响整个组织的所有账户，包括开发环境，不符合只限制生产OU的要求。<br><br>B. 确保FullAWSAccess SCP应用于组织根级别。这个选项实际上是允许完全访问，与题目要求限制IAM权限的目标相反，不能解决问题。<br><br>C. 创建一个允许IAM相关操作的SCP，将SCP附加到开发OU。这个选项是针对开发OU的，而题目要求是限制生产OU的IAM权限，方向完全错误。<br><br>D. 创建一个拒绝IAM相关操作的SCP，条件是排除管理IAM角色，将SCP附加到工作负载OU。这个选项会影响工作负载OU下的所有子OU（包括开发和生产），范围过大，会不必要地限制开发环境。<br><br>E. 创建一个拒绝IAM相关操作的SCP，条件是排除管理IAM角色，将SCP附加到生产OU。这个选项精确地针对生产OU，只限制生产环境的IAM权限，同时通过条件排除允许特定管理角色进行操作。</td>
                    <td>E</td>
                </tr>
                <tr>
                    <td>161</td>
                    <td>A company hired a penetration tester to simulate an internal security breach. The tester performed port scans on the company&#x27;s Amazon EC2<br>instances. The company&#x27;s security measures did not detect the port scans.<br>The company needs a solution that automatically provides notication when port scans are performed on EC2 instances. The company<br>creates and subscribes to an Amazon Simple Notication Service (Amazon SNS) topic.</td>
                    <td>A. Ensure that Amazon GuardDuty is enabled. Create an Amazon CloudWatch alarm for detected EC2 and port scan ndings. Connect the<br>alarm to the SNS topic.<br>B. Ensure that Amazon Inspector is enabled. Create an Amazon EventBridge event for detected network reachability ndings that indicate<br>port scans. Connect the event to the SNS topic.<br>C. Ensure that Amazon Inspector is enabled. Create an Amazon EventBridge event for detected CVEs that cause open port vulnerabilities.<br>Connect the event to the SNS topic.<br>D. Ensure that AWS CloudTrail is enabled. Create an AWS Lambda function to analyze the CloudTrail logs for unusual amounts of trac<br>from an IP address range. Connect the Lambda function to the SNS topic.</td>
                    <td>一家公司雇佣了一名渗透测试人员来模拟内部安全漏洞。测试人员对公司的Amazon EC2实例执行了端口扫描。公司的安全措施没有检测到端口扫描。<br>公司需要一个解决方案，当对EC2实例执行端口扫描时自动提供通知。公司创建并订阅了一个Amazon简单通知服务(Amazon SNS)主题。</td>
                    <td>选项A：确保启用Amazon GuardDuty。为检测到的EC2和端口扫描发现创建Amazon CloudWatch警报。将警报连接到SNS主题。<br>这是正确的方法。GuardDuty是AWS的威胁检测服务，专门设计用于检测恶意活动和异常行为，包括端口扫描。它可以自动检测针对EC2实例的端口扫描活动，并通过CloudWatch Events发送警报，然后可以触发SNS通知。<br><br>选项B：确保启用Amazon Inspector。为检测到的网络可达性发现（表明端口扫描）创建Amazon EventBridge事件。将事件连接到SNS主题。<br>这是不正确的。Amazon Inspector主要用于应用程序安全评估和漏洞管理，它不是实时威胁检测服务，无法检测正在进行的端口扫描活动。<br><br>选项C：确保启用Amazon Inspector。为检测到的导致开放端口漏洞的CVE创建Amazon EventBridge事件。将事件连接到SNS主题。<br>这也是不正确的。Inspector检测CVE（通用漏洞和暴露）是静态安全评估，不能检测实时的端口扫描攻击行为。<br><br>选项D：确保启用AWS CloudTrail。创建AWS Lambda函数来分析CloudTrail日志，查找来自IP地址范围的异常流量。将Lambda函数连接到SNS主题。<br>这种方法不合适。CloudTrail记录API调用，而不是网络流量或端口扫描活动，因此无法有效检测端口扫描。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>162</td>
                    <td>A company runs applications in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster uses an Application Load<br>Balancer to route trac to the applications that run in the cluster.<br>A new application that was migrated to the EKS cluster is performing poorly. All the other applications in the EKS cluster maintain appropriate<br>operation. The new application scales out horizontally to the precongured maximum number of pods immediately upon deployment, before<br>any user trac routes to the web application.</td>
                    <td>A. Implement the Horizontal Pod Autoscaler in the EKS cluster.<br>B. Implement the Vertical Pod Autoscaler in the EKS cluster.<br>C. Implement the Cluster Autoscaler.<br>D. Implement the AWS Load Balancer Controller in the EKS cluster.</td>
                    <td>一家公司在Amazon Elastic Kubernetes Service (Amazon EKS)集群中运行应用程序。EKS集群使用应用负载均衡器将流量路由到集群中运行的应用程序。<br>一个迁移到EKS集群的新应用程序性能表现不佳。EKS集群中的所有其他应用程序都保持正常运行。这个新应用程序在部署后立即水平扩展到预配置的最大pod数量，甚至在任何用户流量路由到web应用程序之前就发生了这种情况。</td>
                    <td>A. 在EKS集群中实施水平Pod自动扩缩器(HPA)：HPA基于CPU使用率、内存使用率或自定义指标来水平扩展pod数量。但题目描述的问题是应用程序在没有流量的情况下就立即扩展到最大pod数量，这表明问题不在于水平扩展的触发条件，而可能是资源配置不当导致的。<br><br>B. 在EKS集群中实施垂直Pod自动扩缩器(VPA)：VPA会自动调整pod的CPU和内存请求和限制。如果新应用程序的资源配置不合理（比如资源请求过低导致调度器认为需要更多pod，或资源限制过低导致性能问题），VPA可以帮助优化单个pod的资源配置，从而解决性能问题和不必要的扩展。<br><br>C. 实施集群自动扩缩器：集群自动扩缩器用于根据pod调度需求自动添加或删除节点。题目没有提到节点资源不足的问题，而是应用程序本身的性能和扩展行为问题。<br><br>D. 在EKS集群中实施AWS负载均衡器控制器：这个控制器用于管理AWS负载均衡器与EKS的集成，主要解决流量路由问题。题目中提到其他应用程序运行正常，说明负载均衡器配置没有问题。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>163</td>
                    <td>A company has an AWS Control Tower landing zone that manages its organization in AWS Organizations. The company created an OU<br>structure that is based on the company&#x27;s requirements. The company&#x27;s DevOps team has established the core accounts for the solution and<br>an account for all centralized AWS CloudFormation and AWS Service Catalog solutions.<br>The company wants to offer a series of customizations that an account can request through AWS Control Tower.</td>
                    <td>A. Enable trusted access for CloudFormation with Organizations by using service-managed permissions.<br>B. Create an IAM role that is named AWSControlTowerBlueprintAccess. Congure the role with a trust policy that allows the<br>AWSControlTowerAdmin role in the management account to assume the role. Attach the AWSServiceCatalogAdminFullAccess IAM policy to<br>the AWSControlTowerBlueprintAccess role.<br>C. Create a Service Catalog product for each CloudFormation template.<br>D. Create a CloudFormation stack set for each CloudFormation template. Enable automatic deployment for each stack set. Create a<br>CloudFormation stack instance that targets specic OUs.<br>E. Deploy the Customizations for AWS Control Tower (CfCT) CloudFormation stack.<br>F. Create a CloudFormation template that contains the resources for each customization.</td>
                    <td>一家公司拥有AWS Control Tower着陆区，用于管理其在AWS Organizations中的组织。该公司根据自身需求创建了OU（组织单元）结构。公司的DevOps团队已经为解决方案建立了核心账户，以及一个用于所有集中式AWS CloudFormation和AWS Service Catalog解决方案的账户。<br>公司希望提供一系列定制化服务，账户可以通过AWS Control Tower请求这些服务。</td>
                    <td>A. 通过使用服务管理权限为CloudFormation启用与Organizations的可信访问 - 这是一个基础配置步骤，但不足以实现完整的定制化解决方案。虽然可信访问是必要的，但仅此一项无法提供账户请求定制化服务的完整功能。<br><br>B. 创建名为AWSControlTowerBlueprintAccess的IAM角色，配置信任策略允许管理账户中的AWSControlTowerAdmin角色承担该角色，并附加AWSServiceCatalogAdminFullAccess策略 - 这个选项描述的是手动配置IAM角色，但这不是实现Control Tower定制化的标准方法。<br><br>C. 为每个CloudFormation模板创建Service Catalog产品 - 这是正确的方法。Service Catalog允许组织创建和管理批准的产品组合，用户可以通过自助服务门户请求这些产品，这正符合题目要求的&quot;账户可以请求定制化服务&quot;。<br><br>D. 为每个CloudFormation模板创建CloudFormation堆栈集，启用自动部署，创建针对特定OU的堆栈实例 - 虽然堆栈集可以跨账户部署资源，但这种方法是自动推送而非按需请求，不符合题目要求的请求机制。<br><br>E. 部署AWS Control Tower定制化(CfCT) CloudFormation堆栈 - 这是AWS官方推荐的Control Tower定制化解决方案。CfCT提供了一个框架来管理和部署Control Tower环境中的定制化配置。<br><br>F. 创建包含每个定制化资源的CloudFormation模板 - 这只是创建模板，但没有提供请求和管理机制，不是完整的解决方案。</td>
                    <td>CE</td>
                </tr>
                <tr>
                    <td>164</td>
                    <td>A company runs a workload on Amazon EC2 instances. The company needs a control that requires the use of Instance Metadata Service<br>Version 2 (IMDSv2) on all EC2 instances in the AWS account. If an EC2 instance does not prevent the use of Instance Metadata Service<br>Version 1 (IMDSv1), the EC2 instance must be terminated.</td>
                    <td>A. Set up AWS Cong in the account. Use a managed rule to check EC2 instances. Congure the rule to remediate the ndings by using<br>AWS Systems Manager Automation to terminate the instance.<br>B. Create a permissions boundary that prevents the ec2:RunInstance action if the ec2:MetadataHttpTokens condition key is not set to a<br>value of required. Attach the permissions boundary to the IAM role that was used to launch the instance.<br>C. Set up Amazon Inspector in the account. Congure Amazon Inspector to activate deep inspection for EC2 instances. Create an Amazon<br>EventBridge rule for an Inspector2 nding. Set an AWS Lambda function as the target to terminate the instance.<br>D. Create an Amazon EventBridge rule for the EC2 instance launch successful event. Send the event to an AWS Lambda function to<br>inspect the EC2 metadata and to terminate the instance.</td>
                    <td>一家公司在Amazon EC2实例上运行工作负载。公司需要一个控制措施，要求AWS账户中的所有EC2实例都必须使用实例元数据服务版本2（IMDSv2）。如果EC2实例没有阻止使用实例元数据服务版本1（IMDSv1），则必须终止该EC2实例。</td>
                    <td>选项A：在账户中设置AWS Config。使用托管规则检查EC2实例。配置规则通过使用AWS Systems Manager Automation来修复发现的问题并终止实例。这个方案可以工作，AWS Config有专门的托管规则来检查IMDSv2配置，并且可以通过自动化修复来终止不合规的实例。但是Config规则的检查可能有延迟，不是实时的。<br><br>选项B：创建权限边界，如果ec2:MetadataHttpTokens条件键未设置为required值，则阻止ec2:RunInstance操作。将权限边界附加到用于启动实例的IAM角色。这个方案是预防性的，可以防止启动不符合要求的实例，但无法处理已经存在的不合规实例，也不能终止已启动但配置错误的实例。<br><br>选项C：在账户中设置Amazon Inspector。配置Amazon Inspector为EC2实例激活深度检查。为Inspector2发现创建Amazon EventBridge规则。设置AWS Lambda函数作为目标来终止实例。Inspector主要用于安全漏洞扫描，不是专门用来检查IMDS配置的工具，这不是其主要用途。<br><br>选项D：为EC2实例启动成功事件创建Amazon EventBridge规则。将事件发送到AWS Lambda函数来检查EC2元数据并终止实例。这个方案可以实时响应实例启动事件，立即检查IMDS配置，如果不符合要求就立即终止实例，是最直接和实时的解决方案。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>165</td>
                    <td>A company builds an application that uses an Application Load Balancer in front of Amazon EC2 instances that are in an Auto Scaling group.<br>The application is stateless. The Auto Scaling group uses a custom AMI that is fully prebuilt. The EC2 instances do not have a custom<br>bootstrapping process.<br>The AMI that the Auto Scaling group uses was recently deleted. The Auto Scaling group&#x27;s scaling activities show failures because the AMI ID<br>does not exist.</td>
                    <td>A. Create a new launch template that uses the new AMI.<br>B. Update the Auto Scaling group to use the new launch template.<br>C. Reduce the Auto Scaling group&#x27;s desired capacity to 0.<br>D. Increase the Auto Scaling group&#x27;s desired capacity by 1.<br>E. Create a new AMI from a running EC2 instance in the Auto Scaling group.</td>
                    <td>一家公司构建了一个应用程序，该应用程序在Auto Scaling组中的Amazon EC2实例前面使用Application Load Balancer。该应用程序是无状态的。Auto Scaling组使用完全预构建的自定义AMI。EC2实例没有自定义引导过程。Auto Scaling组使用的AMI最近被删除了。Auto Scaling组的扩展活动显示失败，因为AMI ID不存在。</td>
                    <td>A. 创建使用新AMI的新启动模板 - 这是正确的第一步。由于原AMI被删除，需要创建新的启动模板来指定新的可用AMI，这样Auto Scaling组才能正常启动新实例。<br><br>B. 更新Auto Scaling组以使用新启动模板 - 这是必要的第二步，但单独执行此操作而不先创建新启动模板是不可能的。<br><br>C. 将Auto Scaling组的期望容量减少到0 - 这只是临时措施，不能解决根本问题。即使容量为0，当需要扩展时仍会遇到相同的AMI不存在问题。<br><br>D. 将Auto Scaling组的期望容量增加1 - 这会加剧问题，因为会尝试使用不存在的AMI启动更多实例，导致更多失败。<br><br>E. 从Auto Scaling组中正在运行的EC2实例创建新AMI - 虽然这可以创建新AMI，但题目要求的是立即解决扩展失败问题的步骤。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>166</td>
                    <td>A company deploys a web application on Amazon EC2 instances that are behind an Application Load Balancer (ALB). The company stores the<br>application code in an AWS CodeCommit repository. When code is merged to the main branch, an AWS Lambda function invokes an AWS<br>CodeBuild project. The CodeBuild project packages the code, stores the packaged code in AWS CodeArtifact, and invokes AWS Systems<br>Manager Run Command to deploy the packaged code to the EC2 instances.<br>Previous deployments have resulted in defects, EC2 instances that are not running the latest version of the packaged code, and<br>inconsistencies between instances.</td>
                    <td>A. Create a pipeline in AWS CodePipeline that uses the CodeCommit repository as a source provider. Congure pipeline stages that run<br>the CodeBuild project in parallel to build and test the application. In the pipeline, pass the CodeBuild project output artifact to an AWS<br>CodeDeploy action.<br>B. Create a pipeline in AWS CodePipeline that uses the CodeCommit repository as a source provider. Create separate pipeline stages that<br>run a CodeBuild project to build and then test the application. In the pipeline, pass the CodeBuild project output artifact to an AWS<br>CodeDeploy action.<br>C. Create an AWS CodeDeploy application and a deployment group to deploy the packaged code to the EC2 instances. Congure the ALB<br>for the deployment group.<br>D. Create individual Lambda functions that use AWS CodeDeploy instead of Systems Manager to run build, test, and deploy actions.<br>E. Create an Amazon S3 bucket. Modify the CodeBuild project to store the packages in the S3 bucket instead of in CodeArtifact. Use<br>deploy actions in CodeDeploy to deploy the artifact to the EC2 instances.</td>
                    <td>一家公司在应用负载均衡器(ALB)后面的Amazon EC2实例上部署Web应用程序。该公司将应用程序代码存储在AWS CodeCommit存储库中。当代码合并到主分支时，AWS Lambda函数会调用AWS CodeBuild项目。CodeBuild项目打包代码，将打包的代码存储在AWS CodeArtifact中，并调用AWS Systems Manager Run Command将打包的代码部署到EC2实例。<br>之前的部署导致了缺陷、EC2实例没有运行最新版本的打包代码，以及实例之间的不一致性。</td>
                    <td>A. 在AWS CodePipeline中创建管道，使用CodeCommit存储库作为源提供者。配置管道阶段并行运行CodeBuild项目来构建和测试应用程序。在管道中，将CodeBuild项目输出工件传递给AWS CodeDeploy操作。这个选项提供了完整的CI/CD管道解决方案，使用CodePipeline统一管理整个流程，CodeDeploy可以确保一致性部署，解决了当前的问题。<br><br>B. 在AWS CodePipeline中创建管道，使用CodeCommit存储库作为源提供者。创建单独的管道阶段运行CodeBuild项目来构建然后测试应用程序。在管道中，将CodeBuild项目输出工件传递给AWS CodeDeploy操作。这个选项也提供了完整的解决方案，但是构建和测试是串行的，不如选项A高效。<br><br>C. 创建AWS CodeDeploy应用程序和部署组来将打包的代码部署到EC2实例。为部署组配置ALB。这个选项只解决了部署一致性问题，但没有解决整个CI/CD流程的问题。<br><br>D. 创建单独的Lambda函数，使用AWS CodeDeploy而不是Systems Manager来运行构建、测试和部署操作。这个选项用CodeDeploy替换了有问题的Systems Manager，可以提高部署一致性，但仍然使用分散的Lambda函数方式。<br><br>E. 创建Amazon S3存储桶。修改CodeBuild项目将包存储在S3存储桶而不是CodeArtifact中。使用CodeDeploy中的部署操作将工件部署到EC2实例。这个选项只是改变了存储位置，没有解决核心的部署一致性问题。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>167</td>
                    <td>A company uses an organization in AWS Organizations to manage its AWS accounts. The company&#x27;s automation account contains a CI/CD<br>pipeline that creates and congures new AWS accounts.<br>The company has a group of internal service teams that provide services to accounts in the organization. The service teams operate out of a<br>set of services accounts. The service teams want to receive an AWS CloudTrail event in their services accounts when the CreateAccount API<br>call creates a new account.<br>How should the company share this CloudTrail event with the service accounts?</td>
                    <td>A. Create an Amazon EventBridge rule in the automation account to send account creation events to the default event bus in the services<br>accounts. Update the default event bus in the services accounts to allow events from the automation account.<br>B. Create a custom Amazon EventBridge event bus in the services accounts. Update the custom event bus to allow events from the<br>automation account. Create an EventBridge rule in the services account that directly listens to CloudTrail events from the automation<br>account.<br>C. Create a custom Amazon EventBridge event bus in the automation account and the services accounts. Create an EventBridge rule and<br>policy that connects the custom event buses that are in the automation account and the services accounts.<br>D. Create a custom Amazon EventBridge event bus in the automation account. Create an EventBridge rule and policy that connects the<br>custom event bus to the default event buses in the services accounts.</td>
                    <td>一家公司使用AWS Organizations中的组织来管理其AWS账户。该公司的自动化账户包含一个CI/CD管道，用于创建和配置新的AWS账户。<br>该公司有一组内部服务团队，为组织中的账户提供服务。服务团队在一组服务账户中运营。当CreateAccount API调用创建新账户时，服务团队希望在其服务账户中接收AWS CloudTrail事件。<br>公司应该如何与服务账户共享此CloudTrail事件？</td>
                    <td>选项A：在自动化账户中创建Amazon EventBridge规则，将账户创建事件发送到服务账户的默认事件总线。更新服务账户中的默认事件总线以允许来自自动化账户的事件。这种方法存在问题，因为直接向其他账户的默认事件总线发送事件需要复杂的跨账户权限配置，且默认事件总线通常不建议用于跨账户事件共享。<br><br>选项B：在服务账户中创建自定义Amazon EventBridge事件总线。更新自定义事件总线以允许来自自动化账户的事件。在服务账户中创建EventBridge规则，直接监听来自自动化账户的CloudTrail事件。这种方法的问题是EventBridge规则无法直接跨账户监听CloudTrail事件，需要通过事件总线进行中转。<br><br>选项C：在自动化账户和服务账户中都创建自定义Amazon EventBridge事件总线。创建EventBridge规则和策略，连接自动化账户和服务账户中的自定义事件总线。这种方法过于复杂，在服务账户中创建自定义事件总线是不必要的，增加了管理复杂性。<br><br>选项D：在自动化账户中创建自定义Amazon EventBridge事件总线。创建EventBridge规则和策略，将自定义事件总线连接到服务账户中的默认事件总线。这是最佳实践，通过在源账户创建自定义事件总线，然后配置跨账户规则将事件转发到目标账户的默认事件总线。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>168</td>
                    <td>A DevOps engineer is building a solution that uses Amazon Simple Queue Service (Amazon SQS) standard queues. The solution also includes<br>an AWS Lambda function and an Amazon DynamoDB table. The Lambda function pulls content from an SQS queue event source and writes the<br>content to the DynamoDB table.<br>The solution must maximize the scalability of Lambda and must prevent successfully processed SQS messages from being processed multiple<br>times.</td>
                    <td>A. Decrease the batch window to 1 second when conguring the Lambda function&#x27;s event source mapping.<br>B. Decrease the batch size to 1 when conguring the Lambda function&#x27;s event source mapping.<br>C. Include the ReportBatchItemFailures value in the FunctionResponseTypes list in the Lambda function&#x27;s event source mapping.<br>D. Set the queue visibility timeout on the Lambda function&#x27;s event source mapping to account for invocation throttling of the Lambda<br>function.</td>
                    <td>一位DevOps工程师正在构建一个使用Amazon Simple Queue Service (Amazon SQS)标准队列的解决方案。该解决方案还包括一个AWS Lambda函数和一个Amazon DynamoDB表。Lambda函数从SQS队列事件源拉取内容并将内容写入DynamoDB表。<br>该解决方案必须最大化Lambda的可扩展性，并且必须防止已成功处理的SQS消息被多次处理。</td>
                    <td>A. 在配置Lambda函数的事件源映射时将批处理窗口减少到1秒：减少批处理窗口主要影响消息收集的时间，但不能直接解决重复处理问题。虽然可能提高响应速度，但对防止重复处理和最大化可扩展性的帮助有限。<br><br>B. 在配置Lambda函数的事件源映射时将批处理大小减少到1：将批处理大小设置为1意味着每次Lambda调用只处理一条消息。这样可以最大化并发性，因为每条消息都可以独立处理，提高可扩展性。同时，如果单个消息处理失败，不会影响批次中的其他消息，减少重复处理的风险。<br><br>C. 在Lambda函数的事件源映射的FunctionResponseTypes列表中包含ReportBatchItemFailures值：这个配置允许部分批处理失败报告，可以防止整个批次重新处理，但主要解决的是批处理中部分消息失败的问题，对最大化可扩展性的直接帮助不如选项B。<br><br>D. 在Lambda函数的事件源映射上设置队列可见性超时以考虑Lambda函数的调用限制：可见性超时设置可以防止消息在处理期间被其他消费者看到，但这主要是防止并发处理同一消息，对最大化可扩展性的帮助有限。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>169</td>
                    <td>A company has a new AWS account that teams will use to deploy various applications. The teams will create many Amazon S3 buckets for<br>application-specic purposes and to store AWS CloudTrail logs. The company has enabled Amazon Macie for the account.<br>A DevOps engineer needs to optimize the Macie costs for the account without compromising the account&#x27;s functionality.</td>
                    <td>A. Exclude S3 buckets that contain CloudTrail logs from automated discovery.<br>B. Exclude S3 buckets that have public read access from automated discovery.<br>C. Congure scheduled daily discovery jobs for all S3 buckets in the account.<br>D. Congure discovery jobs to include S3 objects based on the last modied criterion.<br>E. Congure discovery jobs to include S3 objects that are tagged as production only.</td>
                    <td>一家公司有一个新的AWS账户，团队将使用该账户部署各种应用程序。团队将为特定应用程序目的创建许多Amazon S3存储桶，并存储AWS CloudTrail日志。公司已为该账户启用了Amazon Macie。<br>DevOps工程师需要在不影响账户功能的情况下优化该账户的Macie成本。</td>
                    <td>A. 将包含CloudTrail日志的S3存储桶从自动发现中排除 - 这是一个合理的成本优化策略。CloudTrail日志通常包含结构化的日志数据，不太可能包含敏感的个人身份信息(PII)或其他需要Macie扫描的敏感数据。排除这些存储桶可以显著降低扫描成本而不影响安全性。<br><br>B. 将具有公共读取访问权限的S3存储桶从自动发现中排除 - 这个选项是错误的。具有公共访问权限的存储桶实际上更需要被Macie扫描，因为它们可能意外暴露敏感数据，这正是需要重点监控的区域。<br><br>C. 为账户中的所有S3存储桶配置计划的每日发现作业 - 这是正确的优化策略。通过使用计划作业而不是持续监控，可以控制扫描频率和成本，同时仍然保持定期的安全检查。<br><br>D. 配置发现作业以基于最后修改标准包含S3对象 - 这是有效的成本优化方法。只扫描最近修改的对象可以减少不必要的重复扫描，因为未更改的对象的敏感数据状态不太可能发生变化。<br><br>E. 配置发现作业仅包含标记为生产环境的S3对象 - 这个策略过于限制性，可能会遗漏开发或测试环境中的敏感数据，不是最佳的优化方案。</td>
                    <td>ACD</td>
                </tr>
                <tr>
                    <td>170</td>
                    <td>A company uses an organization in AWS Organizations to manage its AWS accounts. The company recently acquired another company that<br>has standalone AWS accounts. The acquiring company&#x27;s DevOps team needs to consolidate the administration of the AWS accounts for both<br>companies and retain full administrative control of the accounts. The DevOps team also needs to collect and group ndings across all the<br>accounts to implement and maintain a security posture.</td>
                    <td>A. Invite the acquired company&#x27;s AWS accounts to join the organization. Create an SCP that has full administrative privileges. Attach the<br>SCP to the management account.<br>B. Invite the acquired company&#x27;s AWS accounts to join the organization. Create the OrganizationAccountAccessRole IAM role in the invited<br>accounts. Grant permission to the management account to assume the role.<br>C. Use AWS Security Hub to collect and group ndings across all accounts. Use Security Hub to automatically detect new accounts as the<br>accounts are added to the organization.<br>D. Use AWS Firewall Manager to collect and group ndings across all accounts. Enable all features for the organization. Designate an<br>account in the organization as the delegated administrator account for Firewall Manager.<br>E. Use Amazon Inspector to collect and group ndings across all accounts. Designate an account in the organization as the delegated<br>administrator account for Amazon Inspector.</td>
                    <td>一家公司使用AWS Organizations中的组织来管理其AWS账户。该公司最近收购了另一家拥有独立AWS账户的公司。收购方公司的DevOps团队需要整合两家公司AWS账户的管理，并保留对账户的完全管理控制权。DevOps团队还需要收集和分组所有账户的发现结果，以实施和维护安全态势。</td>
                    <td>A. 邀请被收购公司的AWS账户加入组织。创建具有完全管理权限的SCP。将SCP附加到管理账户。<br>这个选项有问题。SCP（服务控制策略）是用来限制权限的，不是用来授予权限的。将限制性策略附加到管理账户会限制管理账户的权限，这与题目要求的&quot;保留完全管理控制权&quot;相矛盾。<br><br>B. 邀请被收购公司的AWS账户加入组织。在被邀请的账户中创建OrganizationAccountAccessRole IAM角色。授予管理账户假设该角色的权限。<br>这个选项部分正确，但不完整。虽然创建OrganizationAccountAccessRole可以实现跨账户管理，但这个选项没有解决安全发现结果收集和分组的需求。<br><br>C. 使用AWS Security Hub收集和分组所有账户的发现结果。使用Security Hub在账户添加到组织时自动检测新账户。<br>这个选项正确。AWS Security Hub是专门用于收集、聚合和分析来自多个AWS安全服务的安全发现结果的服务。它可以跨组织中的所有账户工作，并能自动检测新加入的账户。<br><br>D. 使用AWS Firewall Manager收集和分组所有账户的发现结果。为组织启用所有功能。指定组织中的一个账户作为Firewall Manager的委托管理员账户。<br>这个选项不正确。AWS Firewall Manager主要用于管理防火墙规则和安全策略，而不是收集和分组安全发现结果。<br><br>E. 使用Amazon Inspector收集和分组所有账户的发现结果。指定组织中的一个账户作为Amazon Inspector的委托管理员账户。<br>这个选项正确。Amazon Inspector可以跨多个账户进行漏洞评估和安全发现，通过委托管理员账户可以集中管理所有账户的Inspector发现结果。</td>
                    <td>CE</td>
                </tr>
                <tr>
                    <td>171</td>
                    <td>A company has an application and a CI/CD pipeline. The CI/CD pipeline consists of an AWS CodePipeline pipeline and an AWS CodeBuild<br>project. The CodeBuild project runs tests against the application as part of the build process and outputs a test report. The company must<br>keep the test reports for 90 days.</td>
                    <td>A. Add a new stage in the CodePipeline pipeline after the stage that contains the CodeBuild project. Create an Amazon S3 bucket to store<br>the reports. Congure an S3 deploy action type in the new CodePipeline stage with the appropriate path and format for the reports.<br>B. Add a report group in the CodeBuild project buildspec le with the appropriate path and format for the reports. Create an Amazon S3<br>bucket to store the reports. Congure an Amazon EventBridge rule that invokes an AWS Lambda function to copy the reports to the S3<br>bucket when a build is completed. Create an S3 Lifecycle rule to expire the objects after 90 days.<br>C. Add a new stage in the CodePipeline pipeline. Congure a test action type with the appropriate path and format for the reports.<br>Congure the report expiration time to be 90 days in the CodeBuild project buildspec le.<br>D. Add a report group in the CodeBuild project buildspec le with the appropriate path and format for the reports. Create an Amazon S3<br>bucket to store the reports. Congure the report group as an artifact in the CodeBuild project buildspec le. Congure the S3 bucket as<br>the artifact destination. Set the object expiration to 90 days.</td>
                    <td>一家公司有一个应用程序和一个CI/CD流水线。CI/CD流水线由AWS CodePipeline流水线和AWS CodeBuild项目组成。CodeBuild项目在构建过程中对应用程序运行测试，并输出测试报告。公司必须将测试报告保留90天。</td>
                    <td>选项A：在包含CodeBuild项目的阶段之后，在CodePipeline流水线中添加一个新阶段。创建Amazon S3存储桶来存储报告。在新的CodePipeline阶段中配置S3部署操作类型，使用适当的路径和格式来处理报告。<br>这个方案存在问题，因为它没有正确配置CodeBuild来生成报告组，也没有设置90天的过期策略。仅仅添加S3部署阶段无法自动处理测试报告的生命周期管理。<br><br>选项B：在CodeBuild项目buildspec文件中添加报告组，使用适当的路径和格式来处理报告。创建Amazon S3存储桶来存储报告。配置Amazon EventBridge规则，当构建完成时调用AWS Lambda函数将报告复制到S3存储桶。创建S3生命周期规则，在90天后使对象过期。<br>这是一个完整且可行的解决方案。通过报告组配置测试报告，使用EventBridge和Lambda自动化报告传输，并通过S3生命周期规则实现90天自动过期。<br><br>选项C：在CodePipeline流水线中添加新阶段。配置测试操作类型，使用适当的路径和格式来处理报告。在CodeBuild项目buildspec文件中配置报告过期时间为90天。<br>这个方案简洁有效，直接在CodePipeline中配置测试操作类型，并在buildspec中设置报告过期时间，能够满足90天保留要求。<br><br>选项D：在CodeBuild项目buildspec文件中添加报告组，使用适当的路径和格式来处理报告。创建Amazon S3存储桶来存储报告。在CodeBuild项目buildspec文件中将报告组配置为构件。将S3存储桶配置为构件目标。设置对象过期时间为90天。<br>这个方案存在技术实现上的问题，报告组不能直接配置为构件，这不是AWS CodeBuild支持的标准做法。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>172</td>
                    <td>A company uses an Amazon API Gateway regional REST API to host its application API. The REST API has a custom domain. The REST API&#x27;s<br>default endpoint is deactivated.<br>The company&#x27;s internal teams consume the API. The company wants to use mutual TLS between the API and the internal teams as an<br>additional layer of authentication.</td>
                    <td>A. <br>B. Provision a client certicate that is signed by a public certicate authority (CA). Import the certicate into AWS Certicate Manager<br>(ACM).<br>C. Upload the provisioned client certicate to an Amazon S3 bucket. Congure the API Gateway mutual TLS to use the client certicate<br>that is stored in the S3 bucket as the trust store.<br>D. Upload the provisioned client certicate private key to an Amazon S3 bucket. Congure the API Gateway mutual TLS to use the private<br>key that is stored in the S3 bucket as the trust store.<br>E. Upload the root private certicate authority (CA) certicate to an Amazon S3 bucket. Congure the API Gateway mutual TLS to use the<br>private CA certicate that is stored in the S3 bucket as the trust store.</td>
                    <td>一家公司使用Amazon API Gateway区域REST API来托管其应用程序API。该REST API有一个自定义域名。REST API的默认端点已被停用。<br>公司的内部团队使用该API。公司希望在API和内部团队之间使用双向TLS作为额外的身份验证层。</td>
                    <td>选项A：题目中缺失了A选项的内容，无法进行分析。<br><br>选项B：提供由公共证书颁发机构(CA)签名的客户端证书，并将证书导入到AWS证书管理器(ACM)中。这是配置双向TLS的正确步骤之一。客户端需要有效的证书来进行双向认证，而ACM是AWS推荐的证书管理服务，可以安全地存储和管理SSL/TLS证书。<br><br>选项C：将提供的客户端证书上传到Amazon S3存储桶，并配置API Gateway双向TLS使用存储在S3存储桶中的客户端证书作为信任存储。这个做法是错误的，因为客户端证书不应该作为信任存储使用，信任存储应该包含的是CA证书。<br><br>选项D：将提供的客户端证书私钥上传到Amazon S3存储桶，并配置API Gateway双向TLS使用存储在S3存储桶中的私钥作为信任存储。这是完全错误的做法，私钥不应该用作信任存储，而且私钥应该保密，不应该上传到S3。<br><br>选项E：将根私有证书颁发机构(CA)证书上传到Amazon S3存储桶，并配置API Gateway双向TLS使用存储在S3存储桶中的私有CA证书作为信任存储。这个做法在概念上是正确的，因为信任存储确实应该包含CA证书来验证客户端证书，但表述中的&quot;私有CA证书&quot;可能指的是CA的公钥证书部分。</td>
                    <td>BE</td>
                </tr>
                <tr>
                    <td>173</td>
                    <td>A company uses AWS Directory Service for Microsoft Active Directory as its identity provider (IdP). The company requires all infrastructure to<br>be dened and deployed by AWS CloudFormation.<br>A DevOps engineer needs to create a eet of Windows-based Amazon EC2 instances to host an application. The DevOps engineer has created<br>a CloudFormation template that contains an EC2 launch template, IAM role, EC2 security group, and EC2 Auto Scaling group. The DevOps<br>engineer must implement a solution that joins all EC2 instances to the domain of the AWS Managed Microsoft AD directory.</td>
                    <td>A. In the CloudFormation template, create an AWS::SSM::Document resource that joins the EC2 instance to the AWS Managed Microsoft AD<br>domain by using the parameters for the existing directory. Update the launch template to include the SSMAssociation property to use the<br>new SSM document. Attach the AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess AWS managed policies to the<br>IAM role that the EC2 instances use.<br>B. In the CloudFormation template, update the launch template to include specic tags that propagate on launch. Create an<br>AWS::SSM::Association resource to associate the AWS-JoinDirectoryServiceDomain Automation runbook with the EC2 instances that have<br>the specied tags. Dene the required parameters to join the AWS Managed Microsoft AD directory. Attach the<br>AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess AWS managed policies to the IAM role that the EC2 instances<br>use.<br>C. Store the existing AWS Managed Microsoft AD domain connection details in AWS Secrets Manager. In the CloudFormation template,<br>create an AWS::SSM::Association resource to associate the AWS-CreateManagedWindowsInstanceWithApproval Automation runbook with<br>the EC2 Auto Scaling group. Pass the ARNs for the parameters from Secrets Manager to join the domain. Attach the<br>AmazonSSMDirectoryServiceAccess and SecretsManagerReadWrite AWS managed policies to the IAM role that the EC2 instances use.<br>D. Store the existing AWS Managed Microsoft AD domain administrator credentials in AWS Secrets Manager. In the CloudFormation<br>template, update the EC2 launch template to include user data. Congure the user data to pull the administrator credentials from Secrets<br>Manager and to join the AWS Managed Microsoft AD domain. Attach the AmazonSSMManagedInstanceCore and<br>SecretsManagerReadWrite AWS managed policies to the IAM role that the EC2 instances use.</td>
                    <td>一家公司使用AWS Directory Service for Microsoft Active Directory作为其身份提供商(IdP)。该公司要求所有基础设施都通过AWS CloudFormation定义和部署。<br>一名DevOps工程师需要创建一组基于Windows的Amazon EC2实例来托管应用程序。该DevOps工程师已经创建了一个CloudFormation模板，其中包含EC2启动模板、IAM角色、EC2安全组和EC2 Auto Scaling组。该DevOps工程师必须实现一个解决方案，将所有EC2实例加入到AWS托管Microsoft AD目录的域中。</td>
                    <td>选项A：创建自定义SSM文档来加入域，并在启动模板中使用SSMAssociation属性。这种方法需要创建自定义文档，增加了复杂性，而且SSMAssociation属性在启动模板中的使用可能不是最佳实践。虽然权限配置正确，但整体方案不够优雅。<br><br>选项B：使用标签和AWS-JoinDirectoryServiceDomain自动化运行手册的组合。这是一个可行的方案，使用AWS预定义的自动化运行手册，通过标签来识别需要加入域的实例。权限配置也是正确的，包含了必要的SSM和目录服务访问权限。这种方法相对简洁且使用了AWS的最佳实践。<br><br>选项C：将域连接详细信息存储在Secrets Manager中，使用AWS-CreateManagedWindowsInstanceWithApproval自动化运行手册。但是这个运行手册主要用于创建实例并需要审批，不是专门用于域加入的，而且与Auto Scaling组的集成可能存在问题。权限配置中使用SecretsManagerReadWrite过于宽泛。<br><br>选项D：使用用户数据脚本从Secrets Manager获取管理员凭据并加入域。这种方法将敏感操作放在用户数据中，存在安全风险，而且用户数据中的脚本可能不够可靠。虽然使用了Secrets Manager，但整体方案不如使用SSM自动化运行手册安全和可靠。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>174</td>
                    <td>A company uses AWS Organizations to manage its AWS accounts. The company has a root OU that has a child OU. The root OU has an SCP<br>that allows all actions on all resources. The child OU has an SCP that allows all actions for Amazon DynamoDB and AWS Lambda, and denies<br>all other actions.<br>The company has an AWS account that is named vendor-data in the child OU. A DevOps engineer has an IAM user that is attached to the<br>Administrator Access IAM policy in the vendor-data account. The DevOps engineer attempts to launch an Amazon EC2 instance in the vendor-<br>data account but receives an access denied error.</td>
                    <td>A. Attach the AmazonEC2FullAccess IAM policy to the IAM user.<br>B. Create a new SCP that allows all actions for Amazon EC2. Attach the SCP to the vendor-data account.<br>C. Update the SCP in the child OU to allow all actions for Amazon EC2.<br>D. Create a new SCP that allows all actions for Amazon EC2. Attach the SCP to the root OU.</td>
                    <td>一家公司使用AWS Organizations来管理其AWS账户。该公司有一个根OU，根OU下有一个子OU。根OU有一个SCP（服务控制策略），允许对所有资源执行所有操作。子OU有一个SCP，只允许对Amazon DynamoDB和AWS Lambda执行所有操作，拒绝所有其他操作。<br><br>该公司在子OU中有一个名为vendor-data的AWS账户。一名DevOps工程师拥有一个IAM用户，该用户在vendor-data账户中附加了Administrator Access IAM策略。DevOps工程师尝试在vendor-data账户中启动Amazon EC2实例，但收到访问拒绝错误。</td>
                    <td>A. 将AmazonEC2FullAccess IAM策略附加到IAM用户 - 这个选项不正确。问题的根本原因不在于IAM策略权限不足，而在于SCP的限制。即使用户已经有Administrator Access策略，SCP仍然会限制EC2操作。添加更多IAM策略无法覆盖SCP的限制。<br><br>B. 创建一个新的SCP允许Amazon EC2的所有操作，将SCP附加到vendor-data账户 - 这个选项不正确。SCP不能直接附加到单个账户，只能附加到OU。而且即使可以，子OU的SCP仍然会继续限制EC2操作。<br><br>C. 更新子OU中的SCP以允许Amazon EC2的所有操作 - 这个选项是正确的。当前子OU的SCP只允许DynamoDB和Lambda操作，拒绝其他所有操作包括EC2。更新SCP以包含EC2权限将解决访问拒绝问题。<br><br>D. 创建一个新的SCP允许Amazon EC2的所有操作，将SCP附加到根OU - 这个选项不正确。根OU已经允许所有操作，问题在于子OU的限制性SCP。在根OU添加新的SCP不会覆盖子OU的限制。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>175</td>
                    <td>A company&#x27;s security policies require the use of security hardened AMIs in production environments. A DevOps engineer has used EC2 Image<br>Builder to create a pipeline that builds the AMIs on a recurring schedule.<br>The DevOps engineer needs to update the launch templates of the company&#x27;s Auto Scaling groups. The Auto Scaling groups must use the<br>newest AMIs during the launch of Amazon EC2 instances.</td>
                    <td>A. Congure an Amazon EventBridge rule to receive new AMI events from Image Builder. Target an AWS Systems Manager Run Command<br>document that updates the launch templates of the Auto Scaling groups with the newest AMI I<br>D. Congure the Image Builder distribution settings to update the launch templates with the newest AMI ICongure the Auto Scaling<br>groups to use the newest version of the launch template.<br>B. Congure an Amazon EventBridge rule to receive new AMI events from Image Builder. Target an AWS Lambda function that updates the<br>launch templates of the Auto Scaling groups with the newest AMI I<br>C. Congure the launch template to use a value from AWS Systems Manager Parameter Store for the AMI I</td>
                    <td>一家公司的安全策略要求在生产环境中使用经过安全加固的AMI。DevOps工程师已经使用EC2 Image Builder创建了一个管道，该管道按照定期计划构建AMI。<br>DevOps工程师需要更新公司Auto Scaling组的启动模板。Auto Scaling组在启动Amazon EC2实例时必须使用最新的AMI。</td>
                    <td>A. 配置Amazon EventBridge规则来接收来自Image Builder的新AMI事件。目标是AWS Systems Manager Run Command文档，该文档使用最新的AMI ID更新Auto Scaling组的启动模板。这种方法需要额外的自动化脚本来处理启动模板更新，虽然可行但相对复杂，需要维护额外的Run Command文档和相关权限配置。<br><br>B. 配置Image Builder分发设置以使用最新AMI更新启动模板，配置Auto Scaling组使用启动模板的最新版本。这个选项描述不够清晰，Image Builder本身并不直接提供更新启动模板的功能，这种方法在实际操作中存在技术可行性问题。<br><br>C. 配置Amazon EventBridge规则来接收来自Image Builder的新AMI事件。目标是AWS Lambda函数，该函数使用最新的AMI ID更新Auto Scaling组的启动模板。这是一个常见且有效的自动化解决方案，Lambda函数可以灵活处理启动模板更新逻辑。<br><br>D. 配置启动模板使用AWS Systems Manager Parameter Store中的值作为AMI ID。这种方法通过参数存储来管理AMI ID，当新AMI构建完成时更新参数值，启动模板会自动引用最新值，是最简洁和可维护的解决方案。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>176</td>
                    <td>A company has congured an Amazon S3 event source on an AWS Lambda function. The company needs the Lambda function to run when a<br>new object is created or an existing object is modied in a particular S3 bucket. The Lambda function will use the S3 bucket name and the S3<br>object key of the incoming event to read the contents of the created or modied S3 object. The Lambda function will parse the contents and<br>save the parsed contents to an Amazon DynamoDB table.<br>The Lambda function&#x27;s execution role has permissions to read from the S3 bucket and to write to the DynamoDB table. During testing, a<br>DevOps engineer discovers that the Lambda function does not run when objects are added to the S3 bucket or when existing objects are<br>modied.</td>
                    <td>A. Increase the memory of the Lambda function to give the function the ability to process large les from the S3 bucket.<br>B. Create a resource policy on the Lambda function to grant Amazon S3 the permission to invoke the Lambda function for the S3 bucket.<br>C. Congure an Amazon Simple Queue Service (Amazon SQS) queue as an OnFailure destination for the Lambda function.<br>D. Provision space in the /tmp folder of the Lambda function to give the function the ability to process large les from the S3 bucket.</td>
                    <td>一家公司在AWS Lambda函数上配置了Amazon S3事件源。公司需要Lambda函数在特定S3存储桶中创建新对象或修改现有对象时运行。Lambda函数将使用传入事件的S3存储桶名称和S3对象键来读取已创建或修改的S3对象的内容。Lambda函数将解析内容并将解析后的内容保存到Amazon DynamoDB表中。<br><br>Lambda函数的执行角色具有从S3存储桶读取和写入DynamoDB表的权限。在测试期间，DevOps工程师发现当对象添加到S3存储桶或修改现有对象时，Lambda函数不会运行。</td>
                    <td>A. 增加Lambda函数的内存以使函数能够处理来自S3存储桶的大文件 - 这个选项解决的是性能问题，但题目描述的问题是Lambda函数根本没有被触发，而不是执行时的性能问题。内存增加不会解决函数不被调用的问题。<br><br>B. 在Lambda函数上创建资源策略，授予Amazon S3调用Lambda函数的权限 - 这是正确的解决方案。当S3事件触发Lambda函数时，S3服务需要有权限调用Lambda函数。如果没有适当的资源策略或权限配置，S3无法触发Lambda函数执行，这正是题目中描述的问题。<br><br>C. 配置Amazon SQS队列作为Lambda函数的OnFailure目标 - 这是用于处理Lambda函数执行失败后的错误处理，但不会解决函数不被触发的根本问题。<br><br>D. 在Lambda函数的/tmp文件夹中预留空间以使函数能够处理来自S3存储桶的大文件 - 这同样是处理执行时的存储问题，但不会解决函数不被调用的问题。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>177</td>
                    <td>A company has deployed a critical application in two AWS Regions. The application uses an Application Load Balancer (ALB) in both Regions.<br>The company has Amazon Route 53 alias DNS records for both ALBs.<br>The company uses Amazon Route 53 Application Recovery Controller to ensure that the application can fail over between the two Regions.<br>The Route 53 ARC conguration includes a routing control for both Regions. The company uses Route 53 ARC to perform quarterly disaster<br>recovery (DR) tests.<br>During the most recent DR test, a DevOps engineer accidentally turned off both routing controls. The company needs to ensure that at least<br>one routing control is turned on at all times.</td>
                    <td>A. In Route 53 ARC, create a new assertion safety rule. Apply the assertion safety rule to the two routing controls. Congure the rule with<br>the ATLEAST type with a threshold of 1.<br>B. In Route 53 ARC, create a new gating safety rule. Apply the assertion safety rule to the two routing controls. Congure the rule with the<br>OR type with a threshold of 1.<br>C. In Route 53 ARC, create a new resource set. Congure the resource set with an AWS::Route53::HealthCheck resource type. Specify the<br>ARNs of the two routing controls as the target resource. Create a new readiness check for the resource set.<br>D. In Route 53 ARC, create a new resource set. Congure the resource set with an AWS::Route53RecoveryReadiness::DNSTargetResource<br>resource type. Add the domain names of the two Route 53 alias DNS records as the target resource. Create a new readiness check for the<br>resource set.</td>
                    <td>一家公司在两个AWS区域部署了一个关键应用程序。该应用程序在两个区域都使用应用负载均衡器(ALB)。公司为两个ALB都配置了Amazon Route 53别名DNS记录。公司使用Amazon Route 53应用恢复控制器来确保应用程序可以在两个区域之间进行故障转移。Route 53 ARC配置包括两个区域的路由控制。公司使用Route 53 ARC执行季度灾难恢复(DR)测试。在最近的DR测试中，DevOps工程师意外地关闭了两个路由控制。公司需要确保至少有一个路由控制始终处于开启状态。</td>
                    <td>A. 在Route 53 ARC中创建新的断言安全规则，将断言安全规则应用于两个路由控制，配置ATLEAST类型规则，阈值为1。这个选项是正确的方法，断言安全规则专门用于防止意外关闭所有路由控制，ATLEAST类型确保至少有指定数量的路由控制保持开启状态。<br><br>B. 在Route 53 ARC中创建新的门控安全规则，将断言安全规则应用于两个路由控制，配置OR类型规则，阈值为1。这个选项混淆了概念，门控安全规则用于控制对路由控制的访问，而不是确保最小数量的活跃控制。<br><br>C. 在Route 53 ARC中创建新的资源集，配置AWS::Route53::HealthCheck资源类型，指定两个路由控制的ARN作为目标资源，为资源集创建新的就绪检查。这个选项错误地使用了就绪检查功能，就绪检查用于验证资源是否准备好处理流量，不是用于防止意外关闭路由控制。<br><br>D. 在Route 53 ARC中创建新的资源集，配置AWS::Route53RecoveryReadiness::DNSTargetResource资源类型，添加两个Route 53别名DNS记录的域名作为目标资源，为资源集创建新的就绪检查。这个选项同样错误地使用了就绪检查功能，不能解决防止意外关闭路由控制的问题。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>178</td>
                    <td>A healthcare services company is concerned about the growing costs of software licensing for an application for monitoring patient wellness.<br>The company wants to create an audit process to ensure that the application is running exclusively on Amazon EC2 Dedicated Hosts. A<br>DevOps engineer must create a workow to audit the application to ensure compliance.</td>
                    <td>A. Use AWS Systems Manager Conguration Compliance. Use calls to the put-compliance-items API action to scan and build a database of<br>noncompliant EC2 instances based on their host placement conguration. Use an Amazon DynamoDB table to store these instance IDs for<br>fast access. Generate a report through Systems Manager by calling the list-compliance-summaries API action.<br>B. Use custom Java code running on an EC2 instance. Set up EC2 Auto Scaling for the instance depending on the number of instances to<br>be checked. Send the list of noncompliant EC2 instance IDs to an Amazon SQS queue. Set up another worker instance to process instance<br>IDs from the SQS queue and write them to Amazon DynamoDUse an AWS Lambda function to terminate noncompliant instance IDs<br>obtained from the queue, and send them to an Amazon SNS email topic for distribution.<br>C. Use AWS Cong. Identify all EC2 instances to be audited by enabling Cong Recording on all Amazon EC2 resources for the region.<br>Create a custom AWS Cong rule that triggers an AWS Lambda function by using the &quot;cong-rule-change -triggered&quot; blueprint. Modify the<br>Lambda evaluateCompliance() function to verify host placement to return a NON_COMPLIANT result if the instance is not running on an<br>EC2 Dedicated Host. Use the AWS Cong report to address noncompliant instances.<br>D. Use AWS CloudTrail. Identify all EC2 instances to be audited by analyzing all calls to the EC2 RunCommand API action. Invoke an AWS<br>Lambda function that analyzes the host placement of the instance. Store the EC2 instance ID of noncompliant resources in an Amazon<br>RDS for MySQL DB instance. Generate a report by querying the RDS instance and exporting the query results to a CSV text le.</td>
                    <td>一家医疗服务公司担心用于监控患者健康状况的应用程序的软件许可成本不断增长。该公司希望创建一个审计流程，以确保应用程序仅在Amazon EC2专用主机上运行。DevOps工程师必须创建一个工作流来审计应用程序以确保合规性。</td>
                    <td>选项A：使用AWS Systems Manager配置合规性。通过调用put-compliance-items API操作来扫描并构建基于主机放置配置的不合规EC2实例数据库。使用Amazon DynamoDB表存储这些实例ID以便快速访问。通过调用list-compliance-summaries API操作通过Systems Manager生成报告。这个方案技术上可行，但需要手动调用API来构建合规性数据库，相对复杂且不够自动化。<br><br>选项B：使用运行在EC2实例上的自定义Java代码。根据需要检查的实例数量为该实例设置EC2 Auto Scaling。将不合规的EC2实例ID列表发送到Amazon SQS队列。设置另一个工作实例来处理SQS队列中的实例ID并将其写入Amazon DynamoDB。使用AWS Lambda函数终止从队列获得的不合规实例ID，并将其发送到Amazon SNS邮件主题进行分发。这个方案过于复杂，需要维护自定义代码和多个组件，且自动终止实例可能存在风险。<br><br>选项C：使用AWS Config。通过在该区域的所有Amazon EC2资源上启用Config记录来识别所有要审计的EC2实例。创建一个自定义AWS Config规则，使用&quot;config-rule-change-triggered&quot;蓝图触发AWS Lambda函数。修改Lambda的evaluateCompliance()函数来验证主机放置，如果实例未在EC2专用主机上运行则返回NON_COMPLIANT结果。使用AWS Config报告来处理不合规实例。这是最合适的方案，AWS Config专门用于资源合规性监控，提供了完整的审计和报告功能。<br><br>选项D：使用AWS CloudTrail。通过分析所有对EC2 RunCommand API操作的调用来识别所有要审计的EC2实例。调用AWS Lambda函数分析实例的主机放置。将不合规资源的EC2实例ID存储在Amazon RDS for MySQL数据库实例中。通过查询RDS实例并将查询结果导出到CSV文本文件来生成报告。这个方案存在概念错误，CloudTrail主要用于API调用日志记录，不是最佳的合规性监控工具。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>179</td>
                    <td>A DevOps engineer is planning to deploy a Ruby-based application to production. The application needs to interact with an Amazon RDS for<br>MySQL database and should have automatic scaling and high availability. The stored data in the database is critical and should persist<br>regardless of the state of the application stack.<br>The DevOps engineer needs to set up an automated deployment strategy for the application with automatic rollbacks. The solution also must<br>alert the application team when a deployment fails.</td>
                    <td>A. Deploy the application on AWS Elastic Beanstalk. Deploy an Amazon RDS for MySQL DB instance as part of the Elastic Beanstalk<br>conguration.<br>B. Deploy the application on AWS Elastic Beanstalk. Deploy a separate Amazon RDS for MySQL DB instance outside of Elastic Beanstalk.<br>C. Congure a notication email address that alerts the application team in the AWS Elastic Beanstalk conguration.<br>D. Congure an Amazon EventBridge rule to monitor AWS Health events. Use an Amazon Simple Notication Service (Amazon SNS) topic<br>as a target to alert the application team.<br>E. Use the immutable deployment method to deploy new application versions.</td>
                    <td>一名DevOps工程师计划将一个基于Ruby的应用程序部署到生产环境。该应用程序需要与Amazon RDS for MySQL数据库交互，并且应该具有自动扩展和高可用性。数据库中存储的数据是关键的，无论应用程序堆栈的状态如何都应该持久保存。<br>DevOps工程师需要为应用程序设置自动化部署策略，并具有自动回滚功能。该解决方案还必须在部署失败时向应用程序团队发出警报。</td>
                    <td>A. 在AWS Elastic Beanstalk上部署应用程序。将Amazon RDS for MySQL数据库实例作为Elastic Beanstalk配置的一部分进行部署。<br>这个选项不正确。虽然Elastic Beanstalk确实提供自动扩展和高可用性，但将RDS作为Elastic Beanstalk配置的一部分部署会违反题目要求。当Elastic Beanstalk环境被删除或重新创建时，数据库也会被删除，这与&quot;数据应该持久保存，无论应用程序堆栈状态如何&quot;的要求相冲突。<br><br>B. 在AWS Elastic Beanstalk上部署应用程序。在Elastic Beanstalk外部部署单独的Amazon RDS for MySQL数据库实例。<br>这个选项是正确的。Elastic Beanstalk提供了自动扩展、高可用性和自动部署功能，包括自动回滚能力。将RDS数据库部署在Elastic Beanstalk外部确保了数据的持久性，即使应用程序堆栈发生变化，数据库也不会受到影响。<br><br>C. 配置通知电子邮件地址，在AWS Elastic Beanstalk配置中向应用程序团队发出警报。<br>这个选项是正确的。Elastic Beanstalk内置了通知功能，可以配置电子邮件地址来接收部署状态通知，包括部署失败的警报。这直接满足了题目中关于部署失败时发送警报的要求。<br><br>D. 配置Amazon EventBridge规则来监控AWS Health事件。使用Amazon Simple Notification Service (Amazon SNS)主题作为目标来向应用程序团队发出警报。<br>这个选项不是最佳选择。虽然EventBridge和SNS可以用于通知，但AWS Health事件主要关注AWS服务的整体健康状况，而不是特定应用程序的部署失败。这种方法过于复杂，且不能直接监控Elastic Beanstalk的部署状态。<br><br>E. 使用不可变部署方法来部署新的应用程序版本。<br>这个选项是正确的。不可变部署是Elastic Beanstalk提供的一种部署方法，它创建全新的实例来部署新版本，如果部署失败会自动回滚到之前的版本。这种方法提供了最安全的部署和回滚机制，完全符合题目要求。</td>
                    <td>BCE</td>
                </tr>
                <tr>
                    <td>180</td>
                    <td>A company is using AWS CodePipeline to deploy an application. According to a new guideline, a member of the company&#x27;s security team must<br>sign off on any application changes before the changes are deployed into production. The approval must be recorded and retained.</td>
                    <td>A. Congure CodePipeline to write actions to Amazon CloudWatch Logs.<br>B. Congure CodePipeline to write actions to an Amazon S3 bucket at the end of each pipeline stage.<br>C. Create an AWS CloudTrail trail to deliver logs to Amazon S3.<br>D. Create a CodePipeline custom action to invoke an AWS Lambda function for approval. Create a policy that gives the security team<br>access to manage CodePipeline custom actions.<br>E. Create a CodePipeline manual approval action before the deployment step. Create a policy that grants the security team access to<br>approve manual approval stages.</td>
                    <td>一家公司正在使用AWS CodePipeline来部署应用程序。根据新的指导方针，公司安全团队的成员必须在任何应用程序更改部署到生产环境之前对这些更改进行签署批准。该批准必须被记录和保留。</td>
                    <td>A. 配置CodePipeline将操作写入Amazon CloudWatch Logs - 这个选项可以记录和保留管道操作的日志信息，包括批准操作的详细记录。CloudWatch Logs提供了持久化存储和查询功能，能够满足审计和合规要求。这是一个有效的日志记录解决方案。<br><br>B. 配置CodePipeline在每个管道阶段结束时将操作写入Amazon S3存储桶 - 虽然S3可以存储日志，但这个选项没有解决如何实现安全团队批准的核心问题，只是提供了另一种日志存储方式，不能单独解决批准流程的需求。<br><br>C. 创建AWS CloudTrail跟踪以将日志传递到Amazon S3 - CloudTrail主要记录API调用，虽然可以记录CodePipeline的操作，但它不能解决实现批准流程的问题，只是提供了审计跟踪功能。<br><br>D. 创建CodePipeline自定义操作来调用AWS Lambda函数进行批准，创建策略给安全团队访问管理CodePipeline自定义操作的权限 - 这个选项直接解决了批准流程的核心需求，通过Lambda函数可以实现自定义的批准逻辑，并且可以通过IAM策略精确控制安全团队的权限。<br><br>E. 在部署步骤之前创建CodePipeline手动批准操作，创建策略授予安全团队批准手动批准阶段的访问权限 - 这个选项提供了内置的手动批准功能，是CodePipeline的标准功能，可以直接满足批准需求，但参考答案中没有包含此选项。</td>
                    <td>AE</td>
                </tr>
                <tr>
                    <td>181</td>
                    <td>A company requires its internal business teams to launch resources through pre-approved AWS CloudFormation templates only. The security<br>team requires automated monitoring when resources drift from their expected state.</td>
                    <td>A. Allow users to deploy CloudFormation stacks using a CloudFormation service role only. Use CloudFormation drift detection to detect<br>when resources have drifted from their expected state.<br>B. Allow users to deploy CloudFormation stacks using a CloudFormation service role only. Use AWS Cong rules to detect when resources<br>have drifted from their expected state.<br>C. Allow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a launch constraint. Use AWS Cong<br>rules to detect when resources have drifted from their expected state.<br>D. Allow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a template constraint. Use Amazon<br>EventBridge notications to detect when resources have drifted from their expected state.</td>
                    <td>一家公司要求其内部业务团队仅通过预先批准的AWS CloudFormation模板来启动资源。安全团队要求在资源偏离其预期状态时进行自动监控。</td>
                    <td>选项A：允许用户仅使用CloudFormation服务角色部署CloudFormation堆栈。使用CloudFormation漂移检测来检测资源何时偏离其预期状态。<br>这个选项提供了基本的权限控制，通过服务角色限制用户权限，并使用CloudFormation原生的漂移检测功能。但是，仅使用服务角色可能无法完全确保只使用预批准的模板，因为用户仍然可以使用任何模板。CloudFormation漂移检测是检测配置偏移的有效方法。<br><br>选项B：允许用户仅使用CloudFormation服务角色部署CloudFormation堆栈。使用AWS Config规则来检测资源何时偏离其预期状态。<br>与选项A类似，这个选项在模板控制方面存在同样的问题。AWS Config规则可以监控资源合规性，但主要用于检测资源配置是否符合预定义的规则，而不是专门检测CloudFormation堆栈的漂移。<br><br>选项C：允许用户仅通过AWS Service Catalog部署CloudFormation堆栈。强制使用启动约束。使用AWS Config规则来检测资源何时偏离其预期状态。<br>AWS Service Catalog是管理预批准模板的理想解决方案，可以确保用户只能使用预先批准的产品。启动约束可以控制谁可以启动产品以及使用什么角色。AWS Config规则可以提供合规性监控，但对于CloudFormation漂移检测来说不是最直接的方法。<br><br>选项D：允许用户仅通过AWS Service Catalog部署CloudFormation堆栈。强制使用模板约束。使用Amazon EventBridge通知来检测资源何时偏离其预期状态。<br>Service Catalog很好地解决了模板控制问题，但模板约束主要用于限制模板参数，而不是控制访问。EventBridge主要用于事件路由，不是专门用于检测配置漂移的工具。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>182</td>
                    <td>A company has multiple development groups working in a single shared AWS account. The senior manager of the groups wants to be alerted<br>via a third-party API call when the creation of resources approaches the service limits for the account.</td>
                    <td>A. Create an Amazon EventBridge rule that runs periodically and targets an AWS Lambda function. Within the Lambda function, evaluate<br>the current state of the AWS environment and compare deployed resource values to resource limits on the account. Notify the senior<br>manager if the account is approaching a service limit.<br>B. Deploy an AWS Lambda function that refreshes AWS Trusted Advisor checks, and congure an Amazon EventBridge rule to run the<br>Lambda function periodically. Create another EventBridge rule with an event pattern matching Trusted Advisor events and a target<br>Lambda function. In the target Lambda function, notify the senior manager.<br>C. Deploy an AWS Lambda function that refreshes AWS Health Dashboard checks, and congure an Amazon EventBridge rule to run the<br>Lambda function periodically. Create another EventBridge rule with an event pattern matching Health Dashboard events and a target<br>Lambda function. In the target Lambda function, notify the senior manager.<br>D. Add an AWS Cong custom rule that runs periodically, checks the AWS service limit status, and streams notications to an Amazon<br>Simple Notication Service (Amazon SNS) topic. Deploy an AWS Lambda function that noties the senior manager, and subscribe the<br>Lambda function to the SNS topic.</td>
                    <td>一家公司有多个开发团队在单个共享的AWS账户中工作。团队的高级经理希望在资源创建接近账户服务限制时，通过第三方API调用收到警报。</td>
                    <td>A. 创建一个定期运行的Amazon EventBridge规则，目标为AWS Lambda函数。在Lambda函数中，评估AWS环境的当前状态，并将已部署的资源值与账户的资源限制进行比较。如果账户接近服务限制，则通知高级经理。这个方案需要手动实现所有的服务限制检查逻辑，工作量大且容易出错，不是最优解决方案。<br><br>B. 部署一个刷新AWS Trusted Advisor检查的AWS Lambda函数，并配置Amazon EventBridge规则定期运行该Lambda函数。创建另一个EventBridge规则，使用事件模式匹配Trusted Advisor事件和目标Lambda函数。在目标Lambda函数中通知高级经理。Trusted Advisor主要提供成本优化、性能、安全性建议，虽然包含一些服务限制信息，但不是专门用于监控服务限制的工具。<br><br>C. 部署一个刷新AWS Health Dashboard检查的AWS Lambda函数，并配置Amazon EventBridge规则定期运行该Lambda函数。创建另一个EventBridge规则，使用事件模式匹配Health Dashboard事件和目标Lambda函数。在目标Lambda函数中通知高级经理。AWS Health Dashboard专门用于监控AWS服务的运行状况和服务限制，是监控服务限制的最佳工具。<br><br>D. 添加一个定期运行的AWS Config自定义规则，检查AWS服务限制状态，并将通知流式传输到Amazon SNS主题。部署一个通知高级经理的AWS Lambda函数，并将Lambda函数订阅到SNS主题。AWS Config主要用于配置合规性检查，不是监控服务限制的理想工具。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>183</td>
                    <td>A DevOps engineer is setting up a container-based architecture. The engineer has decided to use AWS CloudFormation to automatically<br>provision an Amazon ECS cluster and an Amazon EC2 Auto Scaling group to launch the EC2 container instances. After successfully creating<br>the CloudFormation stack, the engineer noticed that, even though the ECS cluster and the EC2 instances were created successfully and the<br>stack nished the creation, the EC2 instances were associating with a different cluster.<br>How should the DevOps engineer update the CloudFormation template to resolve this issue?</td>
                    <td>A. Reference the EC2 instances in the AWS::ECS::Cluster resource and reference the ECS cluster in the AWS::ECS::Service resource.<br>B. Reference the ECS cluster in the AWS::AutoScaling::LaunchConguration resource of the UserData property.<br>C. Reference the ECS cluster in the AWS::EC2::Instance resource of the UserData property.<br>D. Reference the ECS cluster in the AWS::CloudFormation::CustomResource resource to trigger an AWS Lambda function that registers the<br>EC2 instances with the appropriate ECS cluster.</td>
                    <td>一位DevOps工程师正在设置基于容器的架构。该工程师决定使用AWS CloudFormation自动配置Amazon ECS集群和Amazon EC2 Auto Scaling组来启动EC2容器实例。在成功创建CloudFormation堆栈后，工程师注意到，尽管ECS集群和EC2实例都成功创建并且堆栈完成了创建，但EC2实例关联到了不同的集群。<br>DevOps工程师应该如何更新CloudFormation模板来解决这个问题？</td>
                    <td>A. 在AWS::ECS::Cluster资源中引用EC2实例，并在AWS::ECS::Service资源中引用ECS集群。这个选项是错误的，因为AWS::ECS::Cluster资源不需要直接引用EC2实例，而且这种配置方式不能解决EC2实例加入错误集群的问题。ECS集群和服务的关系配置也不是解决实例关联问题的正确方法。<br><br>B. 在AWS::AutoScaling::LaunchConfiguration资源的UserData属性中引用ECS集群。这是正确的选项。在Auto Scaling组中，EC2实例是通过启动配置创建的，需要在UserData中指定正确的ECS集群名称，通过运行ECS代理配置脚本来确保实例加入正确的集群。<br><br>C. 在AWS::EC2::Instance资源的UserData属性中引用ECS集群。虽然在UserData中配置ECS集群是正确的思路，但题目中使用的是Auto Scaling组，不是直接的EC2实例资源，所以这个选项不适用于当前场景。<br><br>D. 在AWS::CloudFormation::CustomResource资源中引用ECS集群，触发AWS Lambda函数将EC2实例注册到适当的ECS集群。这种方法过于复杂，不是标准的解决方案，而且会增加不必要的复杂性和维护成本。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>184</td>
                    <td>A DevOps engineer is implementing governance controls for a company that requires its infrastructure to be housed within the United States.<br>The engineer must restrict which AWS Regions can be used, and ensure an alert is sent as soon as possible if any activity outside the<br>governance policy takes place. The controls should be automatically enabled on any new Region outside the United States (US).</td>
                    <td>A. Create an AWS Organizations SCP that denies access to all non-global services in non-US Regions. Attach the policy to the root of the<br>organization.<br>B. Congure AWS CloudTrail to send logs to Amazon CloudWatch Logs and enable it for all Regions. Use a CloudWatch Logs metric lter to<br>send an alert on any service activity in non-US Regions.<br>C. Use an AWS Lambda function that checks for AWS service activity and deploy it to all Regions. Write an Amazon EventBridge rule that<br>runs the Lambda function every hour, sending an alert if activity is found in a non-US Region.<br>D. Use an AWS Lambda function to query Amazon Inspector to look for service activity in non-US Regions and send alerts if any activity is<br>found.<br>E. Write an SCP using the aws:RequestedRegion condition key limiting access to US Regions. Apply the policy to all users, groups, and<br>roles.</td>
                    <td>一名DevOps工程师正在为一家要求其基础设施必须位于美国境内的公司实施治理控制。工程师必须限制可以使用哪些AWS区域，并确保一旦发生任何违反治理政策的活动就尽快发送警报。这些控制措施应该在美国以外的任何新区域上自动启用。</td>
                    <td>A. 创建AWS Organizations SCP拒绝在非美国区域访问所有非全球服务，并将策略附加到组织根部。<br>这个选项可以有效限制对非美国区域的访问，但只是预防性措施，无法提供实时监控和警报功能。SCP可以阻止违规操作，但不会在尝试违规时发送警报。<br><br>B. 配置AWS CloudTrail将日志发送到Amazon CloudWatch Logs并为所有区域启用。使用CloudWatch Logs指标过滤器在非美国区域的任何服务活动上发送警报。<br>这个选项提供了很好的监控和警报功能。CloudTrail可以记录所有API调用，CloudWatch Logs指标过滤器可以检测非美国区域的活动并发送警报，满足实时监控需求。<br><br>C. 使用AWS Lambda函数检查AWS服务活动并将其部署到所有区域。编写Amazon EventBridge规则每小时运行Lambda函数，如果在非美国区域发现活动则发送警报。<br>这个选项提供了监控功能，但每小时检查一次的频率可能不够及时。不过它确实提供了自动化的监控和警报机制，可以检测违规活动。<br><br>D. 使用AWS Lambda函数查询Amazon Inspector以查找非美国区域的服务活动，如果发现任何活动则发送警报。<br>Amazon Inspector主要用于安全评估和漏洞检测，不是用于监控区域活动的合适工具。这个选项在技术上不太合适。<br><br>E. 使用aws:RequestedRegion条件键编写SCP限制对美国区域的访问，将策略应用于所有用户、组和角色。<br>这是一个很好的预防性措施，可以有效限制对非美国区域的访问，但同样缺乏实时监控和警报功能。</td>
                    <td>AB</td>
                </tr>
                <tr>
                    <td>185</td>
                    <td>A company sells products through an ecommerce web application. The company wants a dashboard that shows a pie chart of product<br>transaction details. The company wants to integrate the dashboard with the company&#x27;s existing Amazon CloudWatch dashboards.</td>
                    <td>A. Update the ecommerce application to emit a JSON object to a CloudWatch log group for each processed transaction. Use CloudWatch<br>Logs Insights to query the log group and to visualize the results in a pie chart format. Attach the results to the desired CloudWatch<br>dashboard.<br>B. Create a Lambda subscription lter for the log le. Attach the<br>results to the desired CloudWatch dashboard.<br>C. Update the ecommerce application to use AWS X-Ray for instrumentation. Create a new X-Ray subsegment. Add an annotation for each<br>processed transaction. Use X-Ray traces to query the data and to visualize the results in a pie chart format. Attach the results to the<br>desired CloudWatch dashboard.<br>D. Update the ecommerce application to emit a JSON object to a CloudWatch log group for each processed transaction. Create an AWS<br>Lambda function to aggregate and write the results to Amazon DynamoD</td>
                    <td>一家公司通过电子商务网络应用程序销售产品。该公司希望有一个仪表板，显示产品交易详情的饼图。公司希望将此仪表板与公司现有的Amazon CloudWatch仪表板集成。<br><br>选项：<br>A. 更新电子商务应用程序，为每个处理的交易向CloudWatch日志组发出JSON对象。使用CloudWatch Logs Insights查询日志组并以饼图格式可视化结果。将结果附加到所需的CloudWatch仪表板。<br>B. 为日志文件创建Lambda订阅过滤器。将结果附加到所需的CloudWatch仪表板。<br>C. 更新电子商务应用程序以使用AWS X-Ray进行检测。创建新的X-Ray子段。为每个处理的交易添加注释。使用X-Ray跟踪查询数据并以饼图格式可视化结果。将结果附加到所需的CloudWatch仪表板。<br>D. 更新电子商务应用程序，为每个处理的交易向CloudWatch日志组发出JSON对象。创建AWS Lambda函数来聚合结果并写入Amazon DynamoDB。</td>
                    <td>选项A：这个方案技术上可行，通过应用程序发送JSON到CloudWatch日志组，然后使用Logs Insights查询和可视化。但CloudWatch Logs Insights主要用于日志分析，不是专门为创建饼图等复杂可视化设计的，且直接集成到CloudWatch仪表板的能力有限。<br><br>选项B：选项描述不完整，只提到创建Lambda订阅过滤器，但没有说明具体如何处理数据、生成饼图或集成到仪表板。Lambda订阅过滤器可以实时处理日志数据，但需要配合其他服务来实现完整的解决方案。<br><br>选项C：使用X-Ray进行应用程序检测是可行的，X-Ray可以收集交易数据并提供分析功能。但X-Ray主要用于应用程序性能监控和分布式跟踪，不是专门为业务数据可视化设计的，且与CloudWatch仪表板的集成不如CloudWatch原生指标直接。<br><br>选项D：方案不完整，只提到将数据写入DynamoDB，但没有说明如何从DynamoDB生成饼图或集成到CloudWatch仪表板，缺少关键的可视化步骤。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>186</td>
                    <td>A company is launching an application. The application must use only approved AWS services. The account that runs the application was<br>created less than 1 year ago and is assigned to an AWS Organizations OU.<br>The company needs to create a new Organizations account structure. The account structure must have an appropriate SCP that supports the<br>use of only services that are currently active in the AWS account. The company will use AWS Identity and Access Management (IAM) Access<br>Analyzer in the solution.</td>
                    <td>A. Create an SCP that allows the services that IAM Access Analyzer identies. Create an OU for the account. Move the account into the<br>new OU. Attach the new SCP to the new OU. Detach the default FullAWSAccess SCP from the new OU.<br>B. Create an SCP that denies the services that IAM Access Analyzer identies. Create an OU for the account. Move the account into the<br>new OU. Attach the new SCP to the new OU.<br>C. Create an SCP that allows the services that IAM Access Analyzer identies. Attach the new SCP to the organization&#x27;s root.<br>D. Create an SCP that allows the services that IAM Access Analyzer identies. Create an OU for the account. Move the account into the<br>new OU. Attach the new SCP to the management account. Detach the default FullAWSAccess SCP from the new OU.</td>
                    <td>一家公司正在启动一个应用程序。该应用程序必须仅使用已批准的AWS服务。运行该应用程序的账户创建时间不到1年，并被分配到一个AWS Organizations组织单元(OU)中。<br>公司需要创建一个新的Organizations账户结构。该账户结构必须具有适当的服务控制策略(SCP)，支持仅使用当前在AWS账户中活跃的服务。公司将在解决方案中使用AWS身份和访问管理(IAM) Access Analyzer。</td>
                    <td>选项A：创建一个允许IAM Access Analyzer识别的服务的SCP，为账户创建OU，将账户移入新OU，将新SCP附加到新OU，并从新OU分离默认的FullAWSAccess SCP。这个方案在技术上是可行的，通过白名单方式限制服务使用，并正确地移除了默认的完全访问权限，但没有将SCP附加到管理账户上进行集中管理。<br><br>选项B：创建一个拒绝IAM Access Analyzer识别的服务的SCP。这个方案是错误的，因为题目要求只允许使用当前活跃的服务，而不是拒绝这些服务。这种黑名单方式与需求相反。<br><br>选项C：创建一个允许IAM Access Analyzer识别的服务的SCP，并将其附加到组织的根部。这种方案会影响整个组织的所有账户，范围过于广泛，不符合只针对特定账户的要求。<br><br>选项D：创建一个允许IAM Access Analyzer识别的服务的SCP，为账户创建OU，将账户移入新OU，将新SCP附加到管理账户，并从新OU分离默认的FullAWSAccess SCP。这个方案结合了白名单策略、适当的组织结构和集中的策略管理。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>187</td>
                    <td>A company has multiple development teams in different business units that work in a shared single AWS account. All Amazon EC2 resources<br>that are created in the account must include tags that specify who created the resources. The tagging must occur within the rst hour of<br>resource creation.<br>A DevOps engineer needs to add tags to the created resources that include the user ID that created the resource and the cost center I</td>
                    <td>D. Create an Amazon EventBridge rule that uses Amazon EC2 as the event source. Congure the rule to match events delivered by<br>CloudTrail. Congure the rule to target the Lambda function.<br>A. Create an S3 event notication on the S3 bucket to invoke the Lambda function for s3:ObjectTagging:Put events. Enable bucket<br>versioning on the S3 bucket.<br>B. Enable server access logging on the S3 bucket. Create an S3 event notication on the S3 bucket for s3:ObjectTagging:* events.<br>C. Create a recurring hourly Amazon EventBridge scheduled rule that invokes the Lambda function. Modify the Lambda function to read<br>the logs from the S3 bucket.</td>
                    <td>一家公司在单个共享的AWS账户中有多个不同业务单元的开发团队。在该账户中创建的所有Amazon EC2资源都必须包含标签，指定是谁创建了这些资源。标签添加必须在资源创建后的第一个小时内完成。<br>DevOps工程师需要为创建的资源添加标签，包括创建资源的用户ID和成本中心ID。</td>
                    <td>选项A：在S3存储桶上创建S3事件通知，为s3:ObjectTagging:Put事件调用Lambda函数，并在S3存储桶上启用版本控制。这个选项不适用，因为题目要求的是为EC2资源添加标签，而不是S3对象标签事件。<br><br>选项B：在S3存储桶上启用服务器访问日志记录，为s3:ObjectTagging:*事件创建S3事件通知。同样，这个选项关注的是S3对象标签事件，与题目要求的EC2资源标签需求不匹配。<br><br>选项C：创建一个每小时重复的Amazon EventBridge计划规则来调用Lambda函数，修改Lambda函数以从S3存储桶读取日志。这个选项通过定时任务的方式，可以定期检查和处理EC2资源的标签需求，符合&quot;第一个小时内&quot;的要求。<br><br>选项D：创建一个使用Amazon EC2作为事件源的Amazon EventBridge规则，配置规则匹配CloudTrail传递的事件，配置规则以Lambda函数为目标。这个选项可以实时响应EC2资源创建事件，但题目描述似乎不完整。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>188</td>
                    <td>A company runs an application for multiple environments in a single AWS account. An AWS CodePipeline pipeline uses a development<br>Amazon Elastic Container Service (Amazon ECS) cluster to test an image for the application from an Amazon Elastic Container Registry<br>(Amazon ECR) repository. The pipeline promotes the image to a production ECS cluster.<br>The company needs to move the production cluster into a separate AWS account in the same AWS Region. The production cluster must be<br>able to download the images over a private connection.</td>
                    <td>A. Use Amazon ECR VPC endpoints and an Amazon S3 gateway endpoint. In the separate AWS account, create an ECR repository. Set the<br>repository policy to allow the production ECS tasks to pull images from the main AWS account. Congure the production ECS task<br>execution role to have permission to download the image from the ECR repository.<br>B. Set a repository policy on the production ECR repository in the main AWS account. Congure the repository policy to allow the<br>production ECS tasks in the separate AWS account to pull images from the main account. Congure the production ECS task execution<br>role to have permission to download the image from the ECR repository.<br>C. Congure ECR private image replication in the main AWS account. Activate cross-account replication. Dene the destination account ID<br>of the separate AWS account.<br>D. Use Amazon ECR VPC endpoints and an Amazon S3 gateway endpoint. Set a repository policy on the production ECR repository in the<br>main AWS account. Congure the repository policy to allow the production ECS tasks in the separate AWS account to pull images from the<br>main account. Congure the production ECS task execution role to have permission to download the image from the ECR repository.</td>
                    <td>一家公司在单个AWS账户中为多个环境运行应用程序。AWS CodePipeline管道使用开发环境的Amazon Elastic Container Service (Amazon ECS)集群来测试来自Amazon Elastic Container Registry (Amazon ECR)存储库的应用程序镜像。该管道将镜像推广到生产ECS集群。<br>公司需要将生产集群移动到同一AWS区域内的单独AWS账户中。生产集群必须能够通过私有连接下载镜像。</td>
                    <td>选项A：在单独的AWS账户中创建ECR存储库，并设置存储库策略允许生产ECS任务从主AWS账户拉取镜像。这种方法存在问题，因为它要求在目标账户中创建新的ECR存储库，但镜像实际存储在主账户中，这会导致配置复杂性和潜在的同步问题。<br><br>选项B：在主AWS账户的生产ECR存储库上设置存储库策略，配置该策略以允许单独账户中的生产ECS任务从主账户拉取镜像，并配置生产ECS任务执行角色具有从ECR存储库下载镜像的权限。这是一个直接且有效的跨账户访问解决方案，通过资源策略和IAM角色权限的组合来实现安全的跨账户镜像访问。<br><br>选项C：在主AWS账户中配置ECR私有镜像复制，激活跨账户复制，定义单独账户的目标账户ID。虽然这种方法可以工作，但它会增加存储成本和复制延迟，对于简单的跨账户访问需求来说过于复杂。<br><br>选项D：使用Amazon ECR VPC端点和Amazon S3网关端点，并在主AWS账户的生产ECR存储库上设置存储库策略。虽然VPC端点提供了私有连接，但题目没有明确要求必须使用VPC端点，而且这增加了不必要的复杂性。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>189</td>
                    <td>A company needs to ensure that ow logs remain congured for all existing and new VPCs in its AWS account. The company uses an AWS<br>CloudFormation stack to manage its VPCs. The company needs a solution that will work for any VPCs that any IAM user creates.</td>
                    <td>A. Add the AWS::EC2::FlowLog resource to the CloudFormation stack that creates the VPCs.<br>B. Create an organization in AWS Organizations. Add the company&#x27;s AWS account to the organization. Create an SCP to prevent users from<br>modifying VPC ow logs.<br>C. Turn on AWS Cong. Create an AWS Cong rule to check whether VPC ow logs are turned on. Congure automatic remediation to turn<br>on VPC ow logs.<br>D. Create an IAM policy to deny the use of API calls for VPC ow logs. Attach the IAM policy to all IAM users.</td>
                    <td>一家公司需要确保其AWS账户中所有现有和新建的VPC都配置了流日志。该公司使用AWS CloudFormation堆栈来管理其VPC。公司需要一个解决方案，能够适用于任何IAM用户创建的任何VPC。</td>
                    <td>A. 将AWS::EC2::FlowLog资源添加到创建VPC的CloudFormation堆栈中。<br>这个选项只能确保通过特定CloudFormation堆栈创建的VPC配置流日志，但无法覆盖IAM用户通过其他方式（如控制台、CLI或其他模板）创建的VPC。不能满足&quot;任何IAM用户创建的任何VPC&quot;的要求，覆盖范围有限。<br><br>B. 在AWS Organizations中创建组织，将公司的AWS账户添加到组织中，创建SCP来防止用户修改VPC流日志。<br>SCP（服务控制策略）只能限制用户的权限，防止修改流日志，但不能自动为新创建的VPC配置流日志。这个选项是被动防护而非主动配置，无法解决自动配置流日志的核心需求。<br><br>C. 启用AWS Config，创建AWS Config规则来检查VPC流日志是否开启，配置自动修复来开启VPC流日志。<br>这是最全面的解决方案。AWS Config可以持续监控所有VPC的配置状态，无论这些VPC是如何创建的。Config规则可以检测到任何未配置流日志的VPC，自动修复功能可以自动为这些VPC启用流日志，完全满足题目要求。<br><br>D. 创建IAM策略拒绝VPC流日志的API调用，将IAM策略附加到所有IAM用户。<br>这个选项会阻止用户操作流日志，但不会自动配置流日志。实际上这与需求相反，会阻止流日志的配置而不是确保流日志被配置。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>190</td>
                    <td>A company&#x27;s application teams use AWS CodeCommit repositories for their applications. The application teams have repositories in multiple<br>AWS accounts. All accounts are in an organization in AWS Organizations.<br>Each application team uses AWS IAM Identity Center (AWS Single Sign-On) congured with an external IdP to assume a developer IAM role.<br>The developer role allows the application teams to use Git to work with the code in the repositories.<br>A security audit reveals that the application teams can modify the main branch in any repository. A DevOps engineer must implement a<br>solution that allows the application teams to modify the main branch of only the repositories that they manage.</td>
                    <td>A. Update the SAML assertion to pass the user&#x27;s team name. Update the IAM role&#x27;s trust policy to add an access-team session tag that has<br>the team name.<br>B. Create an approval rule template for each team in the Organizations management account. Associate the template with all the<br>repositories. Add the developer role ARN as an approver.<br>C. Create an approval rule template for each account. Associate the template with all repositories. Add the &quot;aws:ResourceTag/access-<br>team&quot;: &quot;$ ;{aws:PrincipalTag/access-team}&quot; condition to the approval rule template.<br>D. For each CodeCommit repository, add an access-team tag that has the value set to the name of the associated team.<br>E. Attach an SCP to the accounts. Include the following statement:</td>
                    <td>一家公司的应用团队使用AWS CodeCommit存储库来管理他们的应用程序。应用团队在多个AWS账户中都有存储库。所有账户都在AWS Organizations的一个组织中。<br>每个应用团队使用配置了外部IdP的AWS IAM Identity Center（AWS Single Sign-On）来承担开发者IAM角色。<br>开发者角色允许应用团队使用Git来处理存储库中的代码。<br>安全审计发现应用团队可以修改任何存储库的主分支。DevOps工程师必须实施一个解决方案，允许应用团队只能修改他们管理的存储库的主分支。</td>
                    <td>A. 更新SAML断言以传递用户的团队名称。更新IAM角色的信任策略以添加包含团队名称的access-team会话标签。这个选项是必要的第一步，通过SAML断言传递团队信息并在IAM角色中设置会话标签，为后续的权限控制提供身份标识基础。<br><br>B. 在Organizations管理账户中为每个团队创建审批规则模板。将模板与所有存储库关联。添加开发者角色ARN作为审批者。这个选项通过创建审批规则模板来控制主分支的修改权限，但仅仅添加开发者角色ARN作为审批者并不能区分不同团队，无法实现细粒度的权限控制。<br><br>C. 为每个账户创建审批规则模板。将模板与所有存储库关联。在审批规则模板中添加&quot;aws:ResourceTag/access-team&quot;: &quot;${aws:PrincipalTag/access-team}&quot;条件。这个选项通过条件语句匹配资源标签和主体标签来实现权限控制，确保只有标签匹配的团队才能修改对应的存储库。<br><br>D. 为每个CodeCommit存储库添加access-team标签，值设置为关联团队的名称。这个选项为存储库添加团队标识标签，是实现基于标签的权限控制的必要组件，与选项C配合使用。<br><br>E. 将SCP附加到账户。包含以下语句：（语句内容被截断）。由于语句内容不完整，无法完全评估此选项的有效性。</td>
                    <td>ACD</td>
                </tr>
                <tr>
                    <td>191</td>
                    <td>A company uses AWS WAF to protect its cloud infrastructure. A DevOps engineer needs to give an operations team the ability to analyze log<br>messages from AWS WAF. The operations team needs to be able to create alarms for specic patterns in the log output.</td>
                    <td>A. Create an Amazon CloudWatch Logs log group. Congure the appropriate AWS WAF web ACL to send log messages to the log group.<br>Instruct the operations team to create CloudWatch metric lters.<br>B. Create an Amazon OpenSearch Service cluster and appropriate indexes. Congure an Amazon Kinesis Data Firehose delivery stream to<br>stream log data to the indexes. Use OpenSearch Dashboards to create lters and widgets.<br>C. Create an Amazon S3 bucket for the log output. Congure AWS WAF to send log outputs to the S3 bucket. Instruct the operations team<br>to create AWS Lambda functions that detect each desired log message pattern. Congure the Lambda functions to publish to an Amazon<br>Simple Notication Service (Amazon SNS) topic.<br>D. Create an Amazon S3 bucket for the log output. Congure AWS WAF to send log outputs to the S3 bucket. Use Amazon Athena to create<br>an external table denition that ts the log message pattern. Instruct the operations team to write SQL queries and to create Amazon<br>CloudWatch metric lters for the Athena queries.</td>
                    <td>一家公司使用AWS WAF来保护其云基础设施。DevOps工程师需要为运维团队提供分析AWS WAF日志消息的能力。运维团队需要能够为日志输出中的特定模式创建告警。</td>
                    <td>选项A：创建Amazon CloudWatch Logs日志组，配置相应的AWS WAF web ACL将日志消息发送到日志组，指导运维团队创建CloudWatch指标过滤器。这个方案可以实现基本的日志收集和告警功能，CloudWatch Logs支持WAF日志，指标过滤器可以检测特定模式并创建告警。但是对于复杂的日志分析和可视化功能相对有限。<br><br>选项B：创建Amazon OpenSearch Service集群和相应的索引，配置Amazon Kinesis Data Firehose传输流将日志数据流式传输到索引中，使用OpenSearch Dashboards创建过滤器和小部件。这个方案提供了强大的日志分析、搜索、可视化和告警功能。OpenSearch专门设计用于日志分析，Kinesis Data Firehose可以可靠地传输WAF日志，OpenSearch Dashboards提供丰富的可视化和告警功能。<br><br>选项C：创建Amazon S3存储桶用于日志输出，配置AWS WAF将日志发送到S3存储桶，指导运维团队创建Lambda函数检测每个所需的日志消息模式，配置Lambda函数发布到SNS主题。这个方案过于复杂，需要为每个模式编写Lambda函数，维护成本高，实时性也不够好。<br><br>选项D：创建Amazon S3存储桶用于日志输出，配置AWS WAF将日志发送到S3存储桶，使用Amazon Athena创建符合日志消息模式的外部表定义，指导运维团队编写SQL查询并为Athena查询创建CloudWatch指标过滤器。这个方案主要适用于批量分析，不适合实时告警，而且Athena查询的CloudWatch指标过滤器配置复杂。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>192</td>
                    <td>A software team is using AWS CodePipeline to automate its Java application release pipeline. The pipeline consists of a source stage, then a<br>build stage, and then a deploy stage. Each stage contains a single action that has a runOrder value of 1.<br>The team wants to integrate unit tests into the existing release pipeline. The team needs a solution that deploys only the code changes that<br>pass all unit tests.</td>
                    <td>A. Modify the build stage. Add a test action that has a runOrder value of 1. Use AWS CodeDeploy as the action provider to run unit tests.<br>B. Modify the build stage. Add a test action that has a runOrder value of 2. Use AWS CodeBuild as the action provider to run unit tests.<br>C. Modify the deploy stage. Add a test action that has a runOrder value of 1. Use AWS CodeDeploy as the action provider to run unit tests.<br>D. Modify the deploy stage. Add a test action that has a runOrder value of 2. Use AWS CodeBuild as the action provider to run unit tests.</td>
                    <td>一个软件团队正在使用AWS CodePipeline来自动化其Java应用程序发布管道。该管道包含一个源代码阶段，然后是构建阶段，最后是部署阶段。每个阶段都包含一个runOrder值为1的单一操作。<br>团队希望将单元测试集成到现有的发布管道中。团队需要一个解决方案，只部署通过所有单元测试的代码更改。</td>
                    <td>A. 修改构建阶段，添加runOrder值为1的测试操作，使用AWS CodeDeploy作为操作提供者运行单元测试。这个选项有两个问题：首先，CodeDeploy主要用于部署应用程序，不是用来运行单元测试的合适工具；其次，runOrder值为1意味着测试会与现有的构建操作并行运行，这可能导致在构建完成前就开始测试。<br><br>B. 修改构建阶段，添加runOrder值为2的测试操作，使用AWS CodeBuild作为操作提供者运行单元测试。这个选项使用了正确的服务（CodeBuild适合运行测试），runOrder值为2确保测试在构建完成后运行，这是合理的顺序。<br><br>C. 修改部署阶段，添加runOrder值为1的测试操作，使用AWS CodeDeploy作为操作提供者运行单元测试。这个选项的问题是使用CodeDeploy来运行单元测试，这不是CodeDeploy的主要用途。CodeDeploy主要用于应用程序部署，而不是测试执行。<br><br>D. 修改部署阶段，添加runOrder值为2的测试操作，使用AWS CodeBuild作为操作提供者运行单元测试。虽然使用了正确的服务（CodeBuild），但将测试放在部署阶段的runOrder 2意味着测试会在部署完成后运行，这违背了&quot;只部署通过测试的代码&quot;的要求。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>193</td>
                    <td>A company uses an organization in AWS Organizations to manage several AWS accounts that the company&#x27;s developers use. The company<br>requires all data to be encrypted in transit.<br>Multiple Amazon S3 buckets that were created in developer accounts allow unencrypted connections. A DevOps engineer must enforce<br>encryption of data in transit for all existing S3 buckets that are created in accounts in the organization.</td>
                    <td>A. Use AWS CloudFormation StackSets to deploy an AWS Network Firewall rewall to each account. Route all outbound requests from the<br>AWS environment through the rewall. Deploy a policy to block access to all outbound requests on port 80.<br>B. Use AWS CloudFormation StackSets to deploy an AWS Network Firewall rewall to each account. Route all inbound requests to the AWS<br>environment through the rewall. Deploy a policy to block access to all inbound requests on port 80.<br>C. Turn on AWS Cong for the organization. Deploy a conformance pack that uses the s3-bucket-ssl-requests-only managed rule and an<br>AWS Systems Manager Automation runbook. Use a runbook that adds a bucket policy statement to deny access to an S3 bucket when the<br>value of the aws:SecureTransport condition key is false.<br>D. Turn on AWS Cong for the organization. Deploy a conformance pack that uses the s3-bucket-ssl-requests-only managed rule and an<br>AWS Systems Manager Automation runbook. Use a runbook that adds a bucket policy statement to deny access to an S3 bucket when the<br>value of the s3:x-amz-server-side-encryption-aws-kms-key-id condition key is null.</td>
                    <td>一家公司使用AWS Organizations中的组织来管理开发人员使用的多个AWS账户。公司要求所有数据在传输过程中都必须加密。在开发人员账户中创建的多个Amazon S3存储桶允许未加密连接。DevOps工程师必须为组织中账户创建的所有现有S3存储桶强制执行传输中数据加密。</td>
                    <td>选项A：使用AWS CloudFormation StackSets在每个账户中部署AWS Network Firewall防火墙，通过防火墙路由所有来自AWS环境的出站请求，部署策略阻止端口80上的所有出站请求。这个方案虽然能阻止HTTP流量，但过于复杂且成本高昂，而且主要针对出站流量，不能直接解决S3存储桶的HTTPS强制问题。<br><br>选项B：使用AWS CloudFormation StackSets在每个账户中部署AWS Network Firewall防火墙，通过防火墙路由所有到AWS环境的入站请求，部署策略阻止端口80上的所有入站请求。这个方案针对入站流量，但S3访问通常是出站请求，方向不对，且同样过于复杂。<br><br>选项C：为组织启用AWS Config，部署使用s3-bucket-ssl-requests-only托管规则和AWS Systems Manager自动化运行手册的合规包，使用运行手册添加存储桶策略语句，当aws:SecureTransport条件键值为false时拒绝访问S3存储桶。这个方案正确使用了aws:SecureTransport条件来强制HTTPS连接。<br><br>选项D：为组织启用AWS Config，部署使用s3-bucket-ssl-requests-only托管规则和AWS Systems Manager自动化运行手册的合规包，使用运行手册添加存储桶策略语句，当s3:x-amz-server-side-encryption-aws-kms-key-id条件键值为null时拒绝访问S3存储桶。这个条件键是关于服务器端加密的，不是传输中加密。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>194</td>
                    <td>A company is reviewing its IAM policies. One policy written by the DevOps engineer has been agged as too permissive. The policy is used by<br>an AWS Lambda function that issues a stop command to Amazon EC2 instances tagged with Environment: NonProduction over the weekend.<br>The current policy is:</td>
                    <td>A. Add the following conditional expression:<br>B. Change &quot;Resource&quot;: &quot;*&quot;to &quot;Resource&quot;: &quot;arn:aws:ec2:*:*:instance/*&quot;<br>C. Add the following conditional expression:<br>D. Add the following conditional expression:<br>E. Change &quot;Action&quot;: &quot;ec2:*&quot;to &quot;Action&quot;: &quot;ec2:StopInstances&quot;<br>F. Add the following conditional expression:</td>
                    <td>一家公司正在审查其IAM策略。DevOps工程师编写的一个策略被标记为权限过于宽泛。该策略被一个AWS Lambda函数使用，该函数在周末向标记为Environment: NonProduction的Amazon EC2实例发出停止命令。当前策略是：</td>
                    <td>由于题目中没有提供完整的选项内容和当前策略的具体内容，我只能基于选项的部分信息进行分析：<br><br>选项A：添加条件表达式 - 没有显示具体的条件表达式内容，无法判断其有效性。<br><br>选项B：将&quot;Resource&quot;: &quot;*&quot;改为&quot;Resource&quot;: &quot;arn:aws:ec2:*:*:instance/*&quot; - 这个修改将资源范围从所有资源缩小到仅EC2实例，这是一个很好的安全改进，可以减少权限范围。<br><br>选项C：添加条件表达式 - 虽然没有显示具体内容，但作为参考答案，很可能是添加了基于标签的条件，限制只能操作Environment: NonProduction标签的实例。<br><br>选项D：添加条件表达式 - 内容未知，无法具体分析。<br><br>选项E：将&quot;Action&quot;: &quot;ec2:*&quot;改为&quot;Action&quot;: &quot;ec2:StopInstances&quot; - 这个修改将操作权限从所有EC2操作缩小到仅停止实例操作，符合最小权限原则。<br><br>选项F：添加条件表达式 - 内容未知，无法具体分析。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>195</td>
                    <td>A company is developing an application that will generate log events. The log events consist of ve distinct metrics every one tenth of a<br>second and produce a large amount of data.<br>The company needs to congure the application to write the logs to Amazon Timestream. The company will congure a daily query against<br>the Timestream table.</td>
                    <td>A. Use batch writes to write multiple log events in a single write operation.<br>B. Write each log event as a single write operation.<br>C. Treat each log as a single-measure record.<br>D. Treat each log as a multi-measure record.<br>E. Congure the memory store retention period to be longer than the magnetic store retention period.<br>F. Congure the memory store retention period to be shorter than the magnetic store retention period.</td>
                    <td>一家公司正在开发一个会生成日志事件的应用程序。这些日志事件包含五个不同的指标，每十分之一秒产生一次，会产生大量数据。<br>公司需要配置应用程序将日志写入Amazon Timestream。公司将配置对Timestream表的每日查询。</td>
                    <td>A. 使用批量写入在单个写入操作中写入多个日志事件。<br>这个选项是正确的。由于应用程序每0.1秒产生一次日志事件，会产生大量数据，使用批量写入可以显著提高写入效率，减少API调用次数，降低成本并提高性能。Amazon Timestream支持批量写入操作，可以在一次调用中写入多条记录。<br><br>B. 将每个日志事件作为单个写入操作写入。<br>这个选项是错误的。考虑到高频率的数据生成（每0.1秒一次），单独写入每个事件会导致大量的API调用，增加延迟、成本和系统负载，不是最佳实践。<br><br>C. 将每个日志作为单一度量记录处理。<br>这个选项是正确的。虽然每个日志事件包含五个不同的指标，但将其作为单一度量记录处理在某些场景下是合适的，特别是当这些指标需要分别查询和分析时。<br><br>D. 将每个日志作为多度量记录处理。<br>这个选项在技术上可行，但不是最佳选择。多度量记录适合于相关指标需要一起查询的场景，但会增加查询复杂性。<br><br>E. 配置内存存储保留期长于磁性存储保留期。<br>这个选项是错误的。在Amazon Timestream中，内存存储用于最近的热数据，磁性存储用于较旧的冷数据。内存存储的保留期应该短于磁性存储保留期，这样数据会从内存存储自动迁移到更便宜的磁性存储。<br><br>F. 配置内存存储保留期短于磁性存储保留期。<br>这个选项是正确的。这是Amazon Timestream的标准配置模式，新数据首先存储在内存存储中以获得快速查询性能，然后自动迁移到成本更低的磁性存储中进行长期保留。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>196</td>
                    <td>A DevOps engineer has created an AWS CloudFormation template that deploys an application on Amazon EC2 instances. The EC2 instances<br>run Amazon Linux. The application is deployed to the EC2 instances by using shell scripts that contain user data. The EC2 instances have an<br>IAM instance prole that has an IAM role with the AmazonSSMManagedinstanceCore managed policy attached.<br>The DevOps engineer has modied the user data in the CloudFormation template to install a new version of the application. The engineer has<br>also applied the stack update. However, the application was not updated on the running EC2 instances. The engineer needs to ensure that the<br>changes to the application are installed on the running EC2 instances.</td>
                    <td>A. Congure the user data content to use the Multipurpose Internet Mail Extensions (MIME) multipart format. Set the scripts-user<br>parameter to always in the text/cloud-cong section.<br>B. Refactor the user data commands to use the cfn-init helper script. Update the user data to install and congure the cfn-hup and cfn-init<br>helper scripts to monitor and apply the metadata changes.<br>C. Congure an EC2 launch template for the EC2 instances. Create a new EC2 Auto Scaling group. Associate the Auto Scaling group with<br>the EC2 launch template. Use the AutoScalingScheduledAction update policy for the Auto Scaling group.<br>D. Refactor the user data commands to use an AWS Systems Manager document (SSM document). Add an AWS CLI command in the user<br>data to use Systems Manager Run Command to apply the SSM document to the EC2 instances.<br>E. Refactor the user data command to use an AWS Systems Manager document (SSM document). Use Systems Manager State Manager to<br>create an association between the SSM document and the EC2 instances.</td>
                    <td>一名DevOps工程师创建了一个AWS CloudFormation模板，该模板在Amazon EC2实例上部署应用程序。EC2实例运行Amazon Linux。应用程序通过包含用户数据的shell脚本部署到EC2实例。EC2实例具有IAM实例配置文件，该配置文件附加了具有AmazonSSMManagedInstanceCore托管策略的IAM角色。<br><br>DevOps工程师已修改CloudFormation模板中的用户数据以安装新版本的应用程序。工程师还应用了堆栈更新。但是，应用程序未在运行的EC2实例上更新。工程师需要确保应用程序的更改安装在运行的EC2实例上。</td>
                    <td>A. 配置用户数据内容使用多用途互联网邮件扩展(MIME)多部分格式，在text/cloud-config部分将scripts-user参数设置为always。这个选项虽然可以让脚本重新运行，但仅适用于cloud-init，且不是最佳的持续管理方案，无法很好地处理后续的配置变更。<br><br>B. 重构用户数据命令使用cfn-init辅助脚本，更新用户数据以安装和配置cfn-hup和cfn-init辅助脚本来监控和应用元数据变更。这是CloudFormation的标准做法，cfn-hup可以监控元数据变更并自动应用更新，是处理此类问题的经典解决方案。<br><br>C. 为EC2实例配置EC2启动模板，创建新的EC2 Auto Scaling组，将Auto Scaling组与EC2启动模板关联，为Auto Scaling组使用AutoScalingScheduledAction更新策略。这个方案过于复杂，且AutoScalingScheduledAction是用于计划扩展的，不适用于应用程序更新场景。<br><br>D. 重构用户数据命令使用AWS Systems Manager文档(SSM文档)，在用户数据中添加AWS CLI命令使用Systems Manager Run Command将SSM文档应用到EC2实例。这是一个有效的现代化方案，利用SSM的Run Command可以在运行时执行命令，且实例已有必要的IAM权限。<br><br>E. 重构用户数据命令使用AWS Systems Manager文档(SSM文档)，使用Systems Manager State Manager在SSM文档和EC2实例之间创建关联。State Manager可以确保实例保持所需的配置状态，是持续配置管理的好方案，且实例已具备SSM权限。</td>
                    <td>BE</td>
                </tr>
                <tr>
                    <td>197</td>
                    <td>A company is refactoring applications to use AWS. The company identies an internal web application that needs to make Amazon S3 API<br>calls in a specic AWS account.<br>The company wants to use its existing identity provider (IdP) auth.company.com for authentication. The IdP supports only OpenID Connect<br>(OIDC). A DevOps engineer needs to secure the web application&#x27;s access to the AWS account.</td>
                    <td>A. Congure AWS IAM Identity Center (AWS Single Sign-On). Congure an IdP. Upload the IdP metadata from the existing IdP.<br>B. Create an IAM IdP by using the provider URL, audience, and signature from the existing IP.<br>C. Create an IAM role that has a policy that allows the necessary S3 actions. Congure the role&#x27;s trust policy to allow the OIDC IP to<br>assume the role if the sts.amazon.com:aud context key is appid_from_idp.<br>D. Create an IAM role that has a policy that allows the necessary S3 actions. Congure the role&#x27;s trust policy to allow the OIDC IP to<br>assume the role if the auth.company.com:aud context key is appid_from_idp.<br>E. Congure the web application to use the AssumeRoleWithWebIdentity API operation to retrieve temporary credentials. Use the<br>temporary credentials to make the S3 API calls.</td>
                    <td>一家公司正在重构应用程序以使用AWS。该公司识别出一个内部Web应用程序需要在特定AWS账户中进行Amazon S3 API调用。<br>该公司希望使用其现有的身份提供商(IdP) auth.company.com进行身份验证。该IdP仅支持OpenID Connect (OIDC)。DevOps工程师需要保护Web应用程序对AWS账户的访问。</td>
                    <td>A. 配置AWS IAM Identity Center (AWS Single Sign-On)。配置IdP。从现有IdP上传IdP元数据。<br>这个选项不正确，因为题目明确提到IdP只支持OIDC，而IAM Identity Center主要用于SAML集成，不是解决OIDC集成的最佳方案。<br><br>B. 使用现有IdP的提供商URL、受众和签名创建IAM IdP。<br>这个选项是正确的。要使用OIDC进行AWS集成，需要在IAM中创建OIDC身份提供商，配置提供商URL、受众和签名等参数。<br><br>C. 创建具有允许必要S3操作策略的IAM角色。配置角色的信任策略以允许OIDC IdP在sts.amazon.com:aud上下文键为appid_from_idp时承担角色。<br>这个选项不正确，因为使用了错误的上下文键。对于OIDC，应该使用IdP特定的上下文键，而不是sts.amazon.com。<br><br>D. 创建具有允许必要S3操作策略的IAM角色。配置角色的信任策略以允许OIDC IdP在auth.company.com:aud上下文键为appid_from_idp时承担角色。<br>这个选项是正确的。创建IAM角色并配置信任策略，使用正确的IdP域名作为上下文键前缀，这是OIDC集成的标准做法。<br><br>E. 配置Web应用程序使用AssumeRoleWithWebIdentity API操作检索临时凭证。使用临时凭证进行S3 API调用。<br>这个选项描述了应用程序如何使用凭证，但不是配置AWS端安全访问的必要步骤，更多是实现细节。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>198</td>
                    <td>A company uses Amazon RDS for all databases in its AWS accounts. The company uses AWS Control Tower to build a landing zone that has an<br>audit and logging account. All databases must be encrypted at rest for compliance reasons. The company&#x27;s security engineer needs to receive<br>notication about any noncompliant databases that are in the company’s accounts.</td>
                    <td>A. Use AWS Control Tower to activate the optional detective control (guardrail) to determine whether the RDS storage is encrypted. Create<br>an Amazon Simple Notication Service (Amazon SNS) topic in the company&#x27;s audit account. Create an Amazon EventBridge rule to lter<br>noncompliant events from the AWS Control Tower control (guardrail) to notify the SNS topic. Subscribe the security engineer&#x27;s email<br>address to the SNS topic.<br>B. Use AWS CloudFormation StackSets to deploy AWS Lambda functions to every account. Write the Lambda function code to determine<br>whether the RDS storage is encrypted in the account the function is deployed to. Send the ndings as an Amazon CloudWatch metric to<br>the management account. Create an Amazon Simple Notication Service (Amazon SNS) topic. Create a CloudWatch alarm that noties the<br>SNS topic when metric thresholds are met. Subscribe the security engineer&#x27;s email address to the SNS topic.<br>C. Create a custom AWS Cong rule in every account to determine whether the RDS storage is encrypted. Create an Amazon Simple<br>Notication Service (Amazon SNS) topic in the audit account. Create an Amazon EventBidge rule to lter noncompliant events from the<br>AWS Control Tower control (guardrail) to notify the SNS topic. Subscribe the security engineer&#x27;s email address to the SNS topic.<br>D. Launch an Amazon C2 instance. Run an hourly cron job by using the AWS CLI to determine whether the RDS storage is encrypted in each<br>AWS account. Store the results in an RDS database. Notify the security engineer by sending email messages from the EC2 instance when<br>noncompliance is detected</td>
                    <td>一家公司在其AWS账户中的所有数据库都使用Amazon RDS。该公司使用AWS Control Tower构建了一个包含审计和日志记录账户的着陆区。出于合规原因，所有数据库都必须进行静态加密。公司的安全工程师需要接收关于公司账户中任何不合规数据库的通知。</td>
                    <td>选项A：使用AWS Control Tower激活可选的检测控制（护栏）来确定RDS存储是否加密。在公司的审计账户中创建Amazon SNS主题。创建Amazon EventBridge规则来过滤来自AWS Control Tower控制（护栏）的不合规事件以通知SNS主题。将安全工程师的电子邮件地址订阅到SNS主题。这个方案充分利用了AWS Control Tower的原生功能，通过激活预定义的护栏来检测RDS加密状态，配合EventBridge和SNS实现自动化通知，是最符合AWS最佳实践的解决方案。<br><br>选项B：使用AWS CloudFormation StackSets将AWS Lambda函数部署到每个账户。编写Lambda函数代码来确定部署该函数的账户中RDS存储是否加密。将发现结果作为CloudWatch指标发送到管理账户。创建SNS主题和CloudWatch告警来通知。这个方案过于复杂，需要自定义开发Lambda函数，而且使用CloudWatch指标来传递合规性信息不是最佳实践。<br><br>选项C：在每个账户中创建自定义AWS Config规则来确定RDS存储是否加密。在审计账户中创建SNS主题，使用EventBridge规则过滤不合规事件。虽然AWS Config可以检测合规性，但题目中提到使用Control Tower，应该优先使用Control Tower的原生护栏功能而不是自定义Config规则。<br><br>选项D：启动EC2实例，运行每小时的cron作业使用AWS CLI检查每个账户中的RDS存储加密状态。将结果存储在RDS数据库中，检测到不合规时从EC2实例发送邮件通知。这是最不推荐的方案，需要手动管理基础设施，缺乏自动化，且存在单点故障风险。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>199</td>
                    <td>A company is migrating from its on-premises data center to AWS. The company currently uses a custom on-premises Cl/CD pipeline solution<br>to build and package software.<br>The company wants its software packages and dependent public repositories to be available in AWS CodeArtifact to facilitate the creation of<br>application-specic pipelines.</td>
                    <td>A. Update the C1ICD pipeline to create a VM image that contains newly packaged software. Use AWS Import/Export to make the VM image<br>available as an Amazon EC2 AMI. Launch the AMI with an attached IAM instance prole that allows CodeArtifact actions. Use AWS CLI<br>commands to publish the packages to a CodeArtifact repository.<br>B. Create an AWS Identity and Access Management Roles Anywhere trust anchor. Create an IAM role that allows CodeArtifact actions and<br>that has a trust relationship on the trust anchor. Update the on-premises CI/CD pipeline to assume the new IAM role and to publish the<br>packages to CodeArtifact.<br>C. Create a new Amazon S3 bucket. Generate a presigned URL that allows the PutObject request. Update the on-premises CI/CD pipeline<br>to use the presigned URL to publish the packages from the on-premises location to the S3 bucket. Create an AWS Lambda function that<br>runs when packages are created in the bucket through a put command. Congure the Lambda function to publish the packages to<br>CodeArtifact.<br>D. For each public repository, create a CodeArutact repository that is congured with an external connection. Congure the dependent<br>repositories as upstream public repositories.<br>E. Create a Codeartitact repository that is congured with a set of external connections to the public repositories. Congure the external<br>connections to be downstream of the repository.</td>
                    <td>一家公司正在从其本地数据中心迁移到AWS。该公司目前使用自定义的本地CI/CD管道解决方案来构建和打包软件。<br>该公司希望其软件包和依赖的公共仓库能够在AWS CodeArtifact中可用，以便于创建特定应用程序的管道。</td>
                    <td>选项A：更新CI/CD管道创建包含新打包软件的VM镜像，使用AWS Import/Export使VM镜像作为Amazon EC2 AMI可用，启动带有允许CodeArtifact操作的IAM实例配置文件的AMI，使用AWS CLI命令将包发布到CodeArtifact仓库。这个方案过于复杂，需要创建VM镜像和EC2实例，增加了不必要的基础设施成本和复杂性，不是最优解决方案。<br><br>选项B：创建AWS Identity and Access Management Roles Anywhere信任锚点，创建允许CodeArtifact操作并与信任锚点有信任关系的IAM角色，更新本地CI/CD管道以承担新的IAM角色并将包发布到CodeArtifact。这是一个很好的解决方案，IAM Roles Anywhere允许本地系统安全地承担AWS角色，无需长期凭证，直接解决了本地CI/CD管道访问AWS服务的问题。<br><br>选项C：创建新的Amazon S3存储桶，生成允许PutObject请求的预签名URL，更新本地CI/CD管道使用预签名URL将包从本地发布到S3存储桶，创建AWS Lambda函数在通过put命令在存储桶中创建包时运行，配置Lambda函数将包发布到CodeArtifact。这个方案增加了不必要的中间步骤，通过S3和Lambda的间接方式比直接发布到CodeArtifact更复杂。<br><br>选项D：为每个公共仓库创建配置了外部连接的CodeArtifact仓库，将依赖仓库配置为上游公共仓库。这正确地解决了公共依赖仓库的问题，通过外部连接可以从公共仓库（如npm、Maven Central等）获取依赖包。<br><br>选项E：创建配置了一组到公共仓库的外部连接的CodeArtifact仓库，将外部连接配置为仓库的下游。这个配置是错误的，外部连接应该是上游而不是下游，下游的概念在这里不适用。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>200</td>
                    <td>A DevOps team uses AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy to deploy an application. The application is a REST API that<br>uses AWS Lambda functions and Amazon API Gateway. Recent deployments have introduced errors that have affected many customers.<br>The DevOps team needs a solution that reverts to the most recent stable version of the application when an error is detected. The solution<br>must affect the fewest customers possible.</td>
                    <td>A. Set the deployment conguration in CodeDeploy to LambdaAllAtOnce. Congure automatic rollbacks on the deployment group. Create<br>an Amazon CloudWatch alarm that detects HTTP Bad Gateway errors on API Gateway. Congure the deployment group to roll back when<br>the number of alarms meets the alarm threshold.<br>B. Set the deployment conguration in CodeDeploy to LambdaCanary10Percent10Minutes. Congure automatic rollbacks on the<br>deployment group. Create an Amazon CloudWatch alarm that detects HTTP Bad Gateway errors on API Gateway. Congure the<br>deployment group to roll back when the number of alarms meets the alarm threshold.<br>C. Set the deployment conguration in CodeDeploy to LambdaAllAtOnce. Congure manual rollbacks on the deployment group. Create an<br>Amazon Simple Notication Service (Amazon SNS) topic to send notications every time a deployment fails. Congure the SNS topic to<br>invoke a new Lambda function that stops the current deployment and starts the most recent successful deployment.<br>D. Set the deployment conguration in CodeDeploy to LambdaCanary10Percent10Minutes. Congure manual rollbacks on the deployment<br>group. Create a metric lter on an Amazon CloudWatch log group for API Gateway to monitor HTTP Bad Gateway errors. Congure the<br>metric lter to invoke a new Lambda function that stops the current deployment and starts the most recent successful deployment.</td>
                    <td>一个DevOps团队使用AWS CodePipeline、AWS CodeBuild和AWS CodeDeploy来部署应用程序。该应用程序是一个使用AWS Lambda函数和Amazon API Gateway的REST API。最近的部署引入了影响许多客户的错误。DevOps团队需要一个解决方案，当检测到错误时能够回滚到应用程序的最新稳定版本。该解决方案必须影响尽可能少的客户。</td>
                    <td>选项A：使用LambdaAllAtOnce部署配置，这意味着所有流量会立即切换到新版本，如果新版本有问题，会影响所有客户。虽然配置了自动回滚和CloudWatch告警监控API Gateway的HTTP Bad Gateway错误，但由于是全量部署，风险较高，不符合&quot;影响最少客户&quot;的要求。<br><br>选项B：使用LambdaCanary10Percent10Minutes部署配置，这是金丝雀部署策略，只有10%的流量会在10分钟内路由到新版本，大大降低了影响范围。配置了自动回滚和CloudWatch告警监控API Gateway错误，当告警阈值达到时自动回滚，既能快速响应问题又能最小化客户影响。<br><br>选项C：使用LambdaAllAtOnce全量部署，会影响所有客户。虽然配置了SNS通知和Lambda函数来处理回滚，但是手动回滚响应速度较慢，且全量部署的风险依然存在，不符合最小化客户影响的要求。<br><br>选项D：使用了金丝雀部署策略，能够最小化影响，但配置的是手动回滚而不是自动回滚。虽然通过CloudWatch日志组的指标过滤器监控错误并触发Lambda函数处理，但手动回滚的响应时间比自动回滚慢，可能导致问题持续更长时间。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>201</td>
                    <td>A company recently deployed its web application on AWS. The company is preparing for a large-scale sales event and must ensure that the<br>web application can scale to meet the demand.<br>The application&#x27;s frontend infrastructure includes an Amazon CloudFront distribution that has an Amazon S3 bucket as an origin. The backend<br>infrastructure includes an Amazon API Gateway API, several AWS Lambda functions, and an Amazon Aurora DB cluster.<br>The company&#x27;s DevOps engineer conducts a load test and identies that the Lambda functions can full the peak number of requests.<br>However, the DevOps engineer notices request latency during the initial burst of requests. Most of the requests to the Lambda functions<br>produce queries to the database. A large portion of the invocation time is used to establish database connections.</td>
                    <td>A. Congure a higher reserved concurrency for the Lambda functions.<br>B. Congure a higher provisioned concurrency for the Lambda functions.<br>C. Convert the DB cluster to an Aurora global database. Add additional Aurora Replicas in AWS Regions based on the locations of the<br>company&#x27;s customers.<br>D. Refactor the Lambda functions. Move the code blocks that initialize database connections into the function handlers.</td>
                    <td>一家公司最近在AWS上部署了其Web应用程序。该公司正在为大规模销售活动做准备，必须确保Web应用程序能够扩展以满足需求。<br>应用程序的前端基础设施包括一个以Amazon S3存储桶为源的Amazon CloudFront分发。后端基础设施包括一个Amazon API Gateway API、几个AWS Lambda函数和一个Amazon Aurora数据库集群。<br>公司的DevOps工程师进行了负载测试，发现Lambda函数可以满足峰值请求数量。但是，DevOps工程师注意到在初始请求突发期间存在请求延迟。大部分对Lambda函数的请求都会产生对数据库的查询。大部分调用时间用于建立数据库连接。</td>
                    <td>A. 为Lambda函数配置更高的预留并发：预留并发可以确保Lambda函数有足够的并发执行容量，但这不能解决数据库连接建立时间长的问题。预留并发主要解决的是函数被限流的问题，而题目中明确说明函数可以满足峰值请求数量，所以这个选项对解决延迟问题帮助有限。<br><br>B. 为Lambda函数配置更高的预置并发：预置并发可以预先初始化Lambda函数实例，包括建立数据库连接，这样可以显著减少冷启动时间和数据库连接建立时间。这是解决初始请求突发延迟的有效方案，因为预置的函数实例已经准备好处理请求。<br><br>C. 将数据库集群转换为Aurora全球数据库，并根据客户位置在AWS区域中添加额外的Aurora副本：这可以通过就近访问减少数据库查询延迟，提高全球用户的访问性能。虽然不直接解决连接建立时间问题，但可以减少整体数据库访问延迟。<br><br>D. 重构Lambda函数，将初始化数据库连接的代码块移动到函数处理程序中：这实际上是错误的做法。数据库连接初始化应该放在处理程序外部（在全局范围内），这样可以在Lambda容器重用时复用连接，而不是每次调用都重新建立连接。</td>
                    <td>BC</td>
                </tr>
                <tr>
                    <td>202</td>
                    <td>A company runs a web application that extends across multiple Availability Zones. The company uses an Application Load Balancer (ALB) for<br>routing, AWS Fargate for the application, and Amazon Aurora for the application data. The company uses AWS CloudFormation templates to<br>deploy the application. The company stores all Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository in the same<br>AWS account and AWS Region.<br>A DevOps engineer needs to establish a disaster recovery (DR) process in another Region. The solution must meet an RPO of 8 hours and an<br>RTO of 2 hours. The company sometimes needs more than 2 hours to build the Docker images from the Dockerle.</td>
                    <td>A. Copy the CloudFormation templates and the Dockerle to an Amazon S3 bucket in the DR Region. Use AWS Backup to congure<br>automated Aurora cross-Region hourly snapshots. In case of DR, build the most recent Docker image and upload the Docker image to an<br>ECR repository in the DR Region. Use the CloudFormation template that has the most recent Aurora snapshot and the Docker image from<br>the ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new AL<br>B. <br>C. Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Use Amazon EventBridge to schedule an AWS Lambda<br>function to take an hourly snapshot of the Aurora database and of the most recent Docker image in the ECR repository. Copy the snapshot<br>and the Docker image to the DR Region. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the<br>Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region.<br>D. Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Deploy a second application CloudFormation stack in the<br>DR Region. Recongure Aurora to be a global database. Update both CloudFormation stacks when a new application release in the current<br>Region is needed. In case of DR, update the application DNS records to point to the new AL</td>
                    <td>一家公司运行一个跨多个可用区的Web应用程序。该公司使用应用负载均衡器(ALB)进行路由，使用AWS Fargate运行应用程序，使用Amazon Aurora存储应用程序数据。公司使用AWS CloudFormation模板部署应用程序。公司将所有Docker镜像存储在同一AWS账户和区域的Amazon弹性容器注册表(Amazon ECR)存储库中。<br><br>DevOps工程师需要在另一个区域建立灾难恢复(DR)流程。解决方案必须满足8小时的RPO和2小时的RTO。公司有时需要超过2小时才能从Dockerfile构建Docker镜像。</td>
                    <td>选项A：将CloudFormation模板和Dockerfile复制到DR区域的S3存储桶。使用AWS Backup配置Aurora跨区域每小时快照。在DR情况下，构建最新的Docker镜像并上传到DR区域的ECR存储库。这个方案的问题是在DR时才开始构建Docker镜像，而题目明确提到构建镜像需要超过2小时，这将违反2小时的RTO要求。<br><br>选项B：将CloudFormation模板复制到DR区域的S3存储桶。使用EventBridge调度Lambda函数每小时对Aurora数据库和ECR中最新Docker镜像进行快照。将快照和Docker镜像复制到DR区域。这个方案预先将Docker镜像复制到DR区域，避免了在DR时构建镜像的时间延迟，能够满足RTO要求。<br><br>选项C：将CloudFormation模板复制到DR区域的S3存储桶。在DR区域部署第二个应用程序CloudFormation堆栈。重新配置Aurora为全球数据库。这个方案需要维护两套完整的基础设施，成本较高，且选项描述不完整。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>203</td>
                    <td>A company&#x27;s application runs on Amazon EC2 instances. The application writes to a log le that records the username, date, time, and source<br>IP address of the login. The log is published to a log group in Amazon CloudWatch Logs.<br>The company is performing a root cause analysis for an event that occurred on the previous day. The company needs to know the number of<br>logins for a specic user from the past 7 days.</td>
                    <td>A. Create a CloudWatch Logs metric lter on the log group. Use a lter pattern that matches the username. Publish a CloudWatch metric<br>that sums the number of logins over the past 7 days.<br>B. Create a CloudWatch Logs subscription on the log group. Use a lter pattern that matches the username. Publish a CloudWatch metric<br>that sums the number of logins over the past 7 days.<br>C. Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the<br>past 7 days. Run the query against the log group.<br>D. Create a CloudWatch dashboard. Add a number widget that has a lter pattern that counts the number of logins for the username over<br>the past 7 days directly from the log group.</td>
                    <td>一家公司的应用程序运行在Amazon EC2实例上。该应用程序写入一个日志文件，记录登录的用户名、日期、时间和源IP地址。该日志发布到Amazon CloudWatch Logs中的一个日志组。<br>公司正在对前一天发生的事件进行根本原因分析。公司需要知道过去7天内特定用户的登录次数。</td>
                    <td>A. 在日志组上创建CloudWatch Logs指标过滤器。使用匹配用户名的过滤器模式。发布一个CloudWatch指标，汇总过去7天的登录次数。这个选项技术上可行，但需要预先设置指标过滤器，且主要用于实时监控而非历史数据分析。对于一次性的根本原因分析来说，这种方法过于复杂且不够灵活。<br><br>B. 在日志组上创建CloudWatch Logs订阅。使用匹配用户名的过滤器模式。发布一个CloudWatch指标，汇总过去7天的登录次数。订阅主要用于将日志数据流式传输到其他服务（如Kinesis、Lambda等），而不是直接创建指标。这个选项在技术实现上存在概念错误。<br><br>C. 创建一个CloudWatch Logs Insights查询，使用聚合函数来统计过去7天内该用户名的登录次数。对日志组运行该查询。这是最直接和高效的方法，专门为日志分析和查询设计，可以灵活地查询历史数据，非常适合根本原因分析的场景。<br><br>D. 创建一个CloudWatch仪表板。添加一个数字小部件，该小部件具有过滤器模式，直接从日志组统计过去7天内该用户名的登录次数。仪表板主要用于可视化展示，而且无法直接从日志组进行复杂的历史数据查询和聚合。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>204</td>
                    <td>A company has an AWS CodeDeploy application. The application has a deployment group that uses a single tag group to identify instances for<br>the deployment of Application. The single tag group conguration identies instances that have Environment=Production and<br>Name=ApplicationA tags for the deployment of Application</td>
                    <td>A. Change the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only<br>the Name=ApplicationA tag.<br>B. Change the current single tag group to include the Department=Marketing, Environment=production, and Name=ApplicationA tags.<br>C. Add another single tag group that includes only the Department=Marketing tag. Keep the Environment=Production and<br>Name=ApplicationA tags with the current single tag group.<br>D. Change the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only<br>the Department=Marketing tag.</td>
                    <td>一家公司有一个AWS CodeDeploy应用程序。该应用程序有一个部署组，使用单个标签组来识别用于应用程序部署的实例。单个标签组配置识别具有Environment=Production和Name=ApplicationA标签的实例用于应用程序部署。</td>
                    <td>A. 将当前的单个标签组更改为仅包含Environment=Production标签。添加另一个仅包含Name=ApplicationA标签的单个标签组。<br>这个选项会改变部署逻辑。原来是AND关系（同时具有两个标签的实例），现在变成OR关系（具有任一标签的实例都会被部署），这会导致部署到不应该部署的实例上。<br><br>B. 将当前的单个标签组更改为包含Department=Marketing、Environment=production和Name=ApplicationA标签。<br>这个选项要求实例必须同时具有三个标签才能被部署，这比原来的要求更严格，可能会遗漏一些应该部署的实例。<br><br>C. 添加另一个仅包含Department=Marketing标签的单个标签组。保持当前单个标签组的Environment=Production和Name=ApplicationA标签。<br>这个选项会创建OR关系，任何具有Department=Marketing标签的实例都会被部署，即使它们不是生产环境或不是ApplicationA，这不符合要求。<br><br>D. 将当前的单个标签组更改为仅包含Environment=Production标签。添加另一个仅包含Department=Marketing标签的单个标签组。<br>这个选项创建了OR关系，具有Environment=Production或Department=Marketing标签的实例都会被部署，这样可以扩展部署范围到Marketing部门的实例。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>205</td>
                    <td>A company is launching an application that stores raw data in an Amazon S3 bucket. Three applications need to access the data to generate<br>reports. The data must be redacted differently for each application before the applications can access the data.</td>
                    <td>A. Create an S3 bucket for each application. Congure S3 Same-Region Replication (SRR) from the raw data&#x27;s S3 bucket to each<br>application&#x27;s S3 bucket. Congure each application to consume data from its own S3 bucket.<br>B. Create an Amazon Kinesis data stream. Create an AWS Lambda function that is invoked by object creation events in the raw data’s S3<br>bucket. Program the Lambda function to redact data for each application. Publish the data on the Kinesis data stream. Congure each<br>application to consume data from the Kinesis data stream.<br>C. For each application, create an S3 access point that uses the raw data&#x27;s S3 bucket as the destination. Create an AWS Lambda function<br>that is invoked by object creation events in the raw data&#x27;s S3 bucket. Program the Lambda function to redact data for each application.<br>Store the data in each application&#x27;s S3 access point. Congure each application to consume data from its own S3 access point.<br>D. Create an S3 access point that uses the raw data’s S3 bucket as the destination. For each application, create an S3 Object Lambda<br>access point that uses the S3 access point. Congure the AWS Lambda function for each S3 Object Lambda access point to redact data<br>when objects are retrieved. Congure each application to consume data from its own S3 Object Lambda access point</td>
                    <td>一家公司正在启动一个应用程序，该应用程序将原始数据存储在Amazon S3存储桶中。三个应用程序需要访问这些数据来生成报告。在应用程序能够访问数据之前，必须为每个应用程序以不同的方式对数据进行脱敏处理。</td>
                    <td>选项A：为每个应用程序创建一个S3存储桶。配置从原始数据S3存储桶到每个应用程序S3存储桶的S3同区域复制(SRR)。配置每个应用程序从其自己的S3存储桶消费数据。<br>这个方案的问题是S3同区域复制只是简单地复制数据，无法在复制过程中进行数据脱敏处理。每个应用程序仍然会收到相同的原始数据，不符合题目要求的&quot;为每个应用程序以不同方式脱敏数据&quot;的需求。<br><br>选项B：创建一个Amazon Kinesis数据流。创建一个AWS Lambda函数，由原始数据S3存储桶中的对象创建事件触发。编程Lambda函数为每个应用程序脱敏数据。将数据发布到Kinesis数据流上。配置每个应用程序从Kinesis数据流消费数据。<br>这个方案可以实现数据脱敏，但存在架构复杂性问题。所有应用程序都从同一个Kinesis流消费数据，难以确保每个应用程序只接收到为其特定脱敏的数据。<br><br>选项C：为每个应用程序创建一个S3访问点，使用原始数据的S3存储桶作为目标。创建一个AWS Lambda函数，由原始数据S3存储桶中的对象创建事件触发。编程Lambda函数为每个应用程序脱敏数据。将数据存储在每个应用程序的S3访问点中。配置每个应用程序从其自己的S3访问点消费数据。<br>这个方案在技术实现上存在问题。S3访问点不能直接存储数据，它只是提供对底层S3存储桶的访问控制。<br><br>选项D：创建一个S3访问点，使用原始数据的S3存储桶作为目标。为每个应用程序创建一个S3 Object Lambda访问点，使用该S3访问点。为每个S3 Object Lambda访问点配置AWS Lambda函数，在检索对象时脱敏数据。配置每个应用程序从其自己的S3 Object Lambda访问点消费数据。<br>这是最优雅的解决方案，使用S3 Object Lambda可以在数据检索时实时进行脱敏处理，每个应用程序通过不同的Object Lambda访问点获得定制化的脱敏数据。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>206</td>
                    <td>A company uses AWS Control Tower and AWS CloudFormation to manage its AWS accounts and to create AWS resources. The company<br>requires all Amazon S3 buckets to be encrypted with AWS Key Management Service (AWS KMS) when the S3 buckets are created in a<br>CloudFormation stack.</td>
                    <td>A. Use AWS Organizations. Attach an SCP that denies the s3:PutObject permission if the request does not include an x-amz-server-side-<br>encryption header that requests server-side encryption with AWS KMS keys (SSE-KMS).<br>B. Use AWS Control Tower with a multi-account environment. Congure and enable proactive AWS Control Tower controls on all OUs with<br>CloudFormation hooks.<br>C. Use AWS Control Tower with a multi-account environment. Congure and enable detective AWS Control Tower controls on all OUs with<br>CloudFormation hooks.<br>D. Use AWS Organizations. Create an AWS Cong organizational rule to check whether a KMS encryption key is enabled for all S3 buckets.<br>Deploy the rule. Create and apply an SCP to prevent users from stopping and deleting AWS Cong across all AWS accounts,</td>
                    <td>一家公司使用AWS Control Tower和AWS CloudFormation来管理其AWS账户并创建AWS资源。该公司要求在CloudFormation堆栈中创建Amazon S3存储桶时，所有S3存储桶都必须使用AWS密钥管理服务(AWS KMS)进行加密。</td>
                    <td>选项A：使用AWS Organizations，附加一个SCP策略，如果请求不包含要求使用AWS KMS密钥进行服务器端加密(SSE-KMS)的x-amz-server-side-encryption标头，则拒绝s3:PutObject权限。这个方案主要针对对象上传时的加密控制，但题目要求的是在CloudFormation创建S3存储桶时就确保加密配置，而不是在上传对象时。此外，这种方法可能会影响正常的业务操作，因为它限制的是对象上传操作。<br><br>选项B：使用AWS Control Tower的多账户环境，在所有组织单位(OU)上配置并启用主动式AWS Control Tower控制，配合CloudFormation钩子。主动式控制可以在资源创建之前进行检查和阻止，但Control Tower的主要控制机制通常不直接与CloudFormation钩子集成来强制S3加密要求。<br><br>选项C：使用AWS Control Tower的多账户环境，在所有组织单位上配置并启用检测式AWS Control Tower控制，配合CloudFormation钩子。检测式控制只能在资源创建后进行检测和报告，无法在创建时强制执行加密要求，不符合题目的预防性需求。<br><br>选项D：使用AWS Organizations，创建AWS Config组织规则来检查所有S3存储桶是否启用了KMS加密密钥，部署该规则，并创建和应用SCP来防止用户在所有AWS账户中停止和删除AWS Config。这个方案通过Config规则持续监控S3存储桶的加密状态，同时用SCP保护Config服务不被禁用，确保合规性检查的持续性。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>207</td>
                    <td>A DevOps engineer has developed an AWS Lambda function. The Lambda function starts an AWS CloudFormation drift detection operation on<br>all supported resources for a specic CloudFormation stack. The Lambda function then exits its invocation.<br>The DevOps engineer has created an Amazon EventBridge scheduled rule that invokes the Lambda function every hour. An Amazon Simple<br>Notication Service (Amazon SNS) topic already exists in the AWS account. The DevOps engineer has subscribed to the SNS topic to receive<br>notications.<br>The DevOps engineer needs to receive a notication as soon as possible when drift is detected in this specic stack conguration.</td>
                    <td>A. Congure the existing EventBridge rule to also target the SNS topic. Congure an SNS subscription lter policy to match the<br>CloudFormation stack. Attach the subscription lter policy to the SNS topic.<br>B. Create a second Lambda function to query the CloudFormation API for the drift detection results for the stack. Congure the second<br>Lambda function to publish a message to the SNS topic if drift is detected. Adjust the existing EventBridge rule to also target the second<br>Lambda function.<br>C. Congure Amazon GuardDuty in the account with drift detection for all CloudFormation stacks. Create a second EventBridge rule that<br>reacts to the GuardDuty drift detection event nding for the specic CloudFormation stack. Congure the SNS topic as a target of the<br>second EventBridge rule.<br>D. Congure AWS Cong in the account. Use the cloudformation-stack-drift-detection-check managed rule. Create a second EventBridge<br>rule that reacts to a compliance change event for the CloudFormation stack. Congure the SNS topic as a target of the second<br>EventBridge rule.</td>
                    <td>一名DevOps工程师开发了一个AWS Lambda函数。该Lambda函数对特定CloudFormation堆栈的所有支持资源启动AWS CloudFormation漂移检测操作。Lambda函数然后退出其调用。<br>DevOps工程师创建了一个Amazon EventBridge计划规则，每小时调用一次Lambda函数。AWS账户中已经存在一个Amazon Simple Notification Service (Amazon SNS)主题。DevOps工程师已订阅SNS主题以接收通知。<br>DevOps工程师需要在检测到此特定堆栈配置中的漂移时尽快收到通知。</td>
                    <td>A. 配置现有的EventBridge规则同时以SNS主题为目标。配置SNS订阅过滤策略以匹配CloudFormation堆栈。将订阅过滤策略附加到SNS主题。这个选项不正确，因为EventBridge规则只是触发漂移检测，而不是检测结果。SNS过滤策略无法直接获取漂移检测的结果，这种方法无法实现当检测到漂移时发送通知的需求。<br><br>B. 创建第二个Lambda函数来查询CloudFormation API以获取堆栈的漂移检测结果。配置第二个Lambda函数在检测到漂移时向SNS主题发布消息。调整现有的EventBridge规则同时以第二个Lambda函数为目标。这个选项正确，因为它创建了一个专门的Lambda函数来检查漂移检测结果，并在发现漂移时发送通知，这是一个完整的解决方案。<br><br>C. 在账户中配置Amazon GuardDuty，为所有CloudFormation堆栈启用漂移检测。创建第二个EventBridge规则来响应特定CloudFormation堆栈的GuardDuty漂移检测事件发现。将SNS主题配置为第二个EventBridge规则的目标。这个选项不正确，因为GuardDuty主要用于安全威胁检测，而不是CloudFormation堆栈漂移检测。<br><br>D. 在账户中配置AWS Config。使用cloudformation-stack-drift-detection-check托管规则。创建第二个EventBridge规则来响应CloudFormation堆栈的合规性变更事件。将SNS主题配置为第二个EventBridge规则的目标。这个选项虽然技术上可行，但增加了额外的AWS Config服务成本和复杂性，不是最优解决方案。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>208</td>
                    <td>A company has deployed a complex container-based workload on AWS. The workload uses Amazon Managed Service for Prometheus for<br>monitoring. The workload runs in an Amazon<br>Elastic Kubernetes Service (Amazon EKS) cluster in an AWS account.<br>The company’s DevOps team wants to receive workload alerts by using the company’s Amazon Simple Notication Service (Amazon SNS)<br>topic. The SNS topic is in the same AWS account as the EKS cluster.</td>
                    <td>A. Use the Amazon Managed Service for Prometheus remote write URL to send alerts to the SNS topic<br>B. Create an alerting rule that checks the availability of each of the workload’s containers.<br>C. Create an alert manager conguration for the SNS topic.<br>D. Modify the access policy of the SNS topic. Grant the aps.amazonaws.com service principal the sns:Publish permission and the<br>sns:GetTopicAttributes permission for the SNS topic.<br>E. Modify the IAM role that Amazon Managed Service for Prometheus uses. Grant the role the sns:Publish permission and the<br>sns:GetTopicAttributes permission for the SNS topic.</td>
                    <td>一家公司在AWS上部署了复杂的基于容器的工作负载。该工作负载使用Amazon Managed Service for Prometheus进行监控。工作负载运行在AWS账户中的Amazon Elastic Kubernetes Service (Amazon EKS)集群中。公司的DevOps团队希望通过公司的Amazon Simple Notification Service (Amazon SNS)主题接收工作负载告警。SNS主题与EKS集群在同一个AWS账户中。</td>
                    <td>A. 使用Amazon Managed Service for Prometheus远程写入URL向SNS主题发送告警 - 这个选项不正确。远程写入URL是用于将指标数据发送到Prometheus，而不是用于发送告警到SNS。告警需要通过Alert Manager来处理，而不是通过远程写入功能。<br><br>B. 创建一个告警规则来检查工作负载中每个容器的可用性 - 这个选项正确。在Prometheus中，需要创建告警规则来定义什么条件下触发告警。告警规则是监控系统的核心组件，用于检测异常情况并触发相应的通知。<br><br>C. 为SNS主题创建告警管理器配置 - 这个选项正确。Alert Manager是Prometheus生态系统中负责处理告警的组件，需要配置它来将告警发送到SNS主题。这是连接Prometheus告警和SNS通知的关键步骤。<br><br>D. 修改SNS主题的访问策略，授予aps.amazonaws.com服务主体对SNS主题的sns:Publish权限和sns:GetTopicAttributes权限 - 这个选项正确。aps.amazonaws.com是Amazon Managed Service for Prometheus的服务主体，需要相应的权限才能向SNS主题发布消息。<br><br>E. 修改Amazon Managed Service for Prometheus使用的IAM角色，授予该角色对SNS主题的sns:Publish权限和sns:GetTopicAttributes权限 - 这个选项不是最佳实践。通常应该通过资源策略而不是IAM角色来授予服务权限，特别是对于托管服务。</td>
                    <td>BCD</td>
                </tr>
                <tr>
                    <td>209</td>
                    <td>A company&#x27;s organization in AWS Organizations has a single OU. The company runs Amazon EC2 instances in the OU accounts. The company<br>needs to limit the use of each EC2 instance’s credentials to the specic EC2 instance that the credential is assigned to. A DevOps engineer<br>must congure security for the EC2 instances.</td>
                    <td>A. Create an SCP that species the VPC CIDR block. Congure the SCP to check whether the value of the aws:VpcSourcelp condition key is<br>in the specied block. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivatelPv4 and aws:SourceVpc<br>condition keys are the same. Deny access if either condition is false. Apply the SCP to the OU.<br>B. Create an SCP that checks whether the values of the aws:EC2InstanceSourceVPC and aws:SourceVpc condition keys are the same.<br>Deny access if the values are not the same. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivateIPv4<br>and aws:VpcSourceIp condition keys are the same. Deny access if the values are not the same. Apply the SCP to the OU.<br>C. Create an SCP that includes a list of acceptable VPC values and checks whether the value of the aws:SourceVpc condition key is in the<br>list. In the same SCP check, dene a list of acceptable IP address values and check whether the value of the aws:VpcSourceIp condition<br>key is in the list. Deny access if either condition is false. Apply the SCP to each account in the organization.<br>D. Create an SCP that checks whether the values of the aws:EC2InstanceSourceVPC and aws:VpcSourceIp condition keys are the same.<br>Deny access if the values are not the same. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivateIPv4<br>and aws:SourceVpc condition keys are the same. Deny access if the values are not the same. Apply the SCP to each account in the<br>organization.</td>
                    <td>一家公司在AWS Organizations中的组织有一个单独的OU。该公司在OU账户中运行Amazon EC2实例。公司需要限制每个EC2实例的凭证使用，使其仅限于分配该凭证的特定EC2实例。DevOps工程师必须为EC2实例配置安全性。</td>
                    <td>选项A：创建一个SCP来指定VPC CIDR块，检查aws:VpcSourceIp条件键的值是否在指定块中，同时检查aws:EC2InstanceSourcePrivateIPv4和aws:SourceVpc条件键的值是否相同。这个方案存在问题，因为它混合了不同类型的条件键比较，aws:EC2InstanceSourcePrivateIPv4是IP地址而aws:SourceVpc是VPC ID，两者不应该直接比较相等性。<br><br>选项B：创建一个SCP检查aws:EC2InstanceSourceVPC和aws:SourceVpc条件键的值是否相同，如果不同则拒绝访问。同时检查aws:EC2InstanceSourcePrivateIPv4和aws:VpcSourceIp条件键的值是否相同，如果不同则拒绝访问。这个方案逻辑正确，通过比较实例源VPC与请求源VPC，以及实例私有IP与VPC源IP来确保凭证只能从正确的实例使用。<br><br>选项C：创建一个SCP包含可接受的VPC值列表，检查aws:SourceVpc条件键是否在列表中，同时定义可接受的IP地址值列表并检查aws:VpcSourceIp条件键是否在列表中。这种方法需要维护静态列表，不够灵活且难以管理。<br><br>选项D：检查aws:EC2InstanceSourceVPC和aws:VpcSourceIp条件键是否相同，这是错误的比较，因为一个是VPC ID另一个是IP地址。同时检查aws:EC2InstanceSourcePrivateIPv4和aws:SourceVpc是否相同也是错误的，因为比较的是IP地址和VPC ID。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>210</td>
                    <td>A company has a eet of Amazon EC2 instances that run Linux in a single AWS account. The company is using an AWS Systems Manager<br>Automation task across the EC2 instances.<br>During the most recent patch cycle, several EC2 instances went into an error state because of insucient available disk space. A DevOps<br>engineer needs to ensure that the EC2 instances have sucient available disk space during the patching process in the future.</td>
                    <td>A. Ensure that the Amazon CloudWatch agent is installed on all EC2 instances.<br>B. Create a cron job that is installed on each EC2 instance to periodically delete temporary les.<br>C. Create an Amazon CloudWatch log group for the EC2 instances. Congure a cron job that is installed on each EC2 instance to write the<br>available disk space to a CloudWatch log stream for the relevant EC2 instance.<br>D. Create an Amazon CloudWatch alarm to monitor available disk space on all EC2 instances. Add the alarm as a safety control to the<br>Systems Manager Automation task.<br>E. Create an AWS Lambda function to periodically check for sucient available disk space on all EC2 instances by evaluating each EC2<br>instance&#x27;s respective Amazon CloudWatch log stream.</td>
                    <td>一家公司在单个AWS账户中拥有一组运行Linux的Amazon EC2实例。该公司正在跨EC2实例使用AWS Systems Manager自动化任务。在最近的补丁周期中，由于可用磁盘空间不足，几个EC2实例进入了错误状态。DevOps工程师需要确保EC2实例在未来的补丁过程中有足够的可用磁盘空间。</td>
                    <td>A. 确保在所有EC2实例上安装Amazon CloudWatch代理。<br>这个选项是正确的。CloudWatch代理可以收集系统级指标，包括磁盘使用率。安装代理后，可以监控磁盘空间使用情况，为后续的监控和告警提供数据基础。这是实现磁盘空间监控的必要前提条件。<br><br>B. 创建一个安装在每个EC2实例上的cron作业，定期删除临时文件。<br>这个选项虽然有助于释放磁盘空间，但它是一个被动的解决方案，不能保证在补丁过程中有足够的磁盘空间。而且定期删除文件可能会影响正在运行的应用程序，存在风险。<br><br>C. 为EC2实例创建Amazon CloudWatch日志组。配置安装在每个EC2实例上的cron作业，将可用磁盘空间写入相关EC2实例的CloudWatch日志流。<br>这个选项过于复杂且不是最佳实践。使用日志流来监控磁盘空间不如直接使用CloudWatch指标有效，而且需要额外的配置和维护工作。<br><br>D. 创建Amazon CloudWatch告警来监控所有EC2实例的可用磁盘空间。将告警作为安全控制添加到Systems Manager自动化任务中。<br>这个选项是正确的。CloudWatch告警可以主动监控磁盘空间，当磁盘空间不足时触发告警。将其作为Systems Manager自动化任务的安全控制，可以在补丁过程开始前检查磁盘空间，避免因空间不足导致的错误。<br><br>E. 创建AWS Lambda函数，通过评估每个EC2实例各自的Amazon CloudWatch日志流来定期检查所有EC2实例上是否有足够的可用磁盘空间。<br>这个选项不必要地复杂化了解决方案。使用Lambda函数读取日志流来检查磁盘空间不如直接使用CloudWatch指标和告警有效。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>211</td>
                    <td>A DevOps engineer is building an application that uses an AWS Lambda function to query an Amazon Aurora MySQL DB cluster. The Lambda<br>function performs only read queries. Amazon EventBridge events invoke the Lambda function.<br>As more events invoke the Lambda function each second, the database&#x27;s latency increases and the database&#x27;s throughput decreases. The<br>DevOps engineer needs to improve the performance of the application.</td>
                    <td>A. Use Amazon RDS Proxy to create a proxy. Connect the proxy to the Aurora cluster reader endpoint. Set a maximum connections<br>percentage on the proxy.<br>B. Implement database connection pooling inside the Lambda code. Set a maximum number of connections on the database connection<br>pool.<br>C. Implement the database connection opening outside the Lambda event handler code.<br>D. Implement the database connection opening and closing inside the Lambda event handler code.<br>E. Connect to the proxy endpoint from the Lambda function.</td>
                    <td>一名DevOps工程师正在构建一个应用程序，该应用程序使用AWS Lambda函数来查询Amazon Aurora MySQL数据库集群。Lambda函数只执行读取查询。Amazon EventBridge事件调用Lambda函数。<br>随着每秒调用Lambda函数的事件增多，数据库的延迟增加，数据库的吞吐量下降。DevOps工程师需要改善应用程序的性能。</td>
                    <td>A. 使用Amazon RDS Proxy创建代理。将代理连接到Aurora集群读取器端点。在代理上设置最大连接百分比。<br>这个选项是正确的。RDS Proxy可以有效管理数据库连接池，减少连接开销，并且可以设置连接限制来防止数据库过载。连接到读取器端点可以利用Aurora的读取副本来分散读取负载。<br><br>B. 在Lambda代码内部实现数据库连接池。在数据库连接池上设置最大连接数。<br>这个选项不太理想。Lambda函数是无状态的，每次调用都可能在不同的容器中执行，传统的连接池在Lambda环境中效果有限。而且Lambda的并发特性使得连接池管理变得复杂。<br><br>C. 在Lambda事件处理程序代码外部实现数据库连接开启。<br>这个选项是正确的。将数据库连接放在处理程序外部（如全局变量或初始化代码中）可以实现连接重用，减少每次函数调用时的连接开销，这是Lambda最佳实践之一。<br><br>D. 在Lambda事件处理程序代码内部实现数据库连接的开启和关闭。<br>这个选项是错误的。在每次函数调用时都开启和关闭连接会造成大量的连接开销，这正是导致性能问题的原因之一。<br><br>E. 从Lambda函数连接到代理端点。<br>这个选项是正确的。如果使用了RDS Proxy（选项A），那么Lambda函数应该连接到代理端点而不是直接连接数据库，这样才能获得连接池和连接管理的好处。</td>
                    <td>ACE</td>
                </tr>
                <tr>
                    <td>212</td>
                    <td>A company has an AWS CloudFormation stack that is deployed in a single AWS account. The company has congured the stack to send event<br>notications to an Amazon Simple Notication Service (Amazon SNS) topic.<br>A DevOps engineer must implement an automated solution that applies a tag to the specic CloudFormation stack instance only after a<br>successful stack update occurs. The DevOps engineer has created an AWS Lambda function that applies and updates this tag for the specic<br>stack instance.</td>
                    <td>A. Run the AWS-UpdateCloudFormationStack AWS Systems ManagerAutomation runbook when Systems Manager detects an<br>UPDATE_COMPLETE event for the instance status of the CloudFormation stack. Congure the runbook to invoke the Lambda function.<br>B. Create a custom AWS Cong rule that produces a compliance change event if the CloudFormation stack has an UPDATE_COMPLETE<br>instance status. Congure AWS Cong to directly invoke the Lambda function to automatically remediate the change event.<br>C. Create an Amazon EventBridge rule that matches the UPDATE_COMPLETE event pattern for the instance status of the CloudFormation<br>stack. Congure the rule to invoke the Lambda function.<br>D. Adjust the conguration of the CloudFormation stack to send notications for only an UPDATE_COMPLETE instance status event to the<br>SNS topic. Subscribe the Lambda function to the SNS topic.</td>
                    <td>一家公司有一个部署在单个AWS账户中的AWS CloudFormation堆栈。该公司已配置堆栈将事件通知发送到Amazon Simple Notification Service (Amazon SNS)主题。<br>DevOps工程师必须实现一个自动化解决方案，该解决方案仅在成功的堆栈更新发生后才对特定的CloudFormation堆栈实例应用标签。DevOps工程师已创建了一个AWS Lambda函数，该函数为特定堆栈实例应用和更新此标签。</td>
                    <td>A. 当Systems Manager检测到CloudFormation堆栈实例状态的UPDATE_COMPLETE事件时，运行AWS-UpdateCloudFormationStack AWS Systems Manager自动化运行手册。配置运行手册调用Lambda函数。这个选项过于复杂，Systems Manager不是监控CloudFormation事件的最佳选择，而且需要额外的配置和管理开销。<br><br>B. 创建一个自定义AWS Config规则，如果CloudFormation堆栈具有UPDATE_COMPLETE实例状态，则产生合规性更改事件。配置AWS Config直接调用Lambda函数以自动修复更改事件。AWS Config主要用于资源合规性监控，不是处理CloudFormation状态事件的理想工具，而且会增加不必要的复杂性。<br><br>C. 创建一个Amazon EventBridge规则，该规则匹配CloudFormation堆栈实例状态的UPDATE_COMPLETE事件模式。配置规则调用Lambda函数。这是最直接和高效的解决方案，EventBridge专门设计用于处理AWS服务事件，可以直接监听CloudFormation的状态变化事件并触发Lambda函数。<br><br>D. 调整CloudFormation堆栈的配置，仅为UPDATE_COMPLETE实例状态事件向SNS主题发送通知。将Lambda函数订阅到SNS主题。虽然这个方案可行，但需要修改现有的CloudFormation配置，而且通过SNS的间接调用不如EventBridge直接高效。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>213</td>
                    <td>A company deploys an application to two AWS Regions. The application creates and stores objects in an Amazon S3 bucket that is in the same<br>Region as the application. Both deployments of the application need to have access to all the objects and their metadata from both Regions.<br>The company has congured two-way replication between the S3 buckets and has enabled S3 Replication metrics on each S3 bucket.<br>A DevOps engineer needs to implement a solution that retries the replication process if an object fails to replicate.</td>
                    <td>A. Create an Amazon EventBridge rule that listens to S3 event notications for failed replication events. Create an AWS Lambda function<br>that downloads the failed replication object and then runs a PutObject command for the object to the destination bucket. Congure the<br>EventBridge rule to invoke the Lambda function to handle the object that failed to replicate.<br>B. Create an Amazon Simple Queue Service (Amazon SQS) queue. Congure S3 event notications to send failed replication notications<br>to the SQS queue. Create an AWS Lambda function that downloads the failed replication object and then runs a PutObject command for<br>the object to the destination bucket. Congure the Lambda function to poll the queue for notications to process.<br>C. Create an Amazon EventBridge rule that listens to S3 event notications for failed replications. Create an AWS Lambda function that<br>downloads the failed replication object and then runs a PutObject command for the object to the destination bucket.<br>D. Create an AWS Lambda function that will use S3 batch operations to retry the replication on the existing object for a failed replication.<br>Congure S3 event notications to send failed replication notications to the Lambda function.</td>
                    <td>一家公司将应用程序部署到两个AWS区域。该应用程序在与应用程序相同区域的Amazon S3存储桶中创建和存储对象。两个应用程序部署都需要访问来自两个区域的所有对象及其元数据。公司已在S3存储桶之间配置了双向复制，并在每个S3存储桶上启用了S3复制指标。DevOps工程师需要实现一个解决方案，当对象复制失败时重试复制过程。</td>
                    <td>选项A：使用EventBridge规则监听S3复制失败事件通知，创建Lambda函数下载失败对象并执行PutObject命令到目标存储桶。这个方案在技术上可行，但EventBridge在处理大量事件时可能存在延迟和可靠性问题，且没有内置的重试机制来处理Lambda函数执行失败的情况。<br><br>选项B：使用SQS队列接收S3复制失败通知，创建Lambda函数轮询队列处理失败对象。SQS提供了可靠的消息传递、死信队列支持和自动重试机制，Lambda可以配置为从SQS队列触发，提供更好的错误处理和重试能力。这是一个更稳健的解决方案。<br><br>选项C：与选项A类似，但描述不完整，没有明确说明如何配置EventBridge规则来调用Lambda函数，缺少关键的配置细节。<br><br>选项D：使用S3批处理操作重试复制，但S3批处理操作主要用于大规模操作，不适合实时处理单个失败的复制事件，且配置复杂度较高。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>214</td>
                    <td>A company needs to implement failover for its application. The application includes an Amazon CloudFront distribution and a public<br>Application Load Balancer (ALB) in an AWS Region. The company has congured the ALB as the default origin for the distribution.<br>After some recent application outages, the company wants a zero-second RTO. The company deploys the application to a secondary Region in<br>a warm standby conguration. A DevOps engineer needs to automate the failover of the application to the secondary Region so that HTTP<br>GET requests meet the desired RTO.</td>
                    <td>A. Create a second CloudFront distribution that has the secondary ALB as the default origin. Create Amazon Route 53 alias records that<br>have a failover policy and Evaluate Target Health set to Yes for both CloudFront distributions. Update the application to use the new<br>record set.<br>B. Create a new origin on the distribution for the secondary ALCreate a new origin group. Set the original ALB as the primary origin.<br>Congure the origin group to fail over for HTTP 5xx status codes. Update the default behavior to use the origin group.<br>C. Create Amazon Route 53 alias records that have a failover policy and Evaluate Target Health set to Yes for both ALBs. Set the TTL of<br>both records to 0. Update the distribution&#x27;s origin to use the new record set.<br>D. Create a CloudFront function that detects HTTP 5xx status codes. Congure the function to return a 307 Temporary Redirect error<br>response to the secondary ALB if the function detects 5xx status codes. Update the distribution&#x27;s default behavior to send origin<br>responses to the function.</td>
                    <td>一家公司需要为其应用程序实现故障转移。该应用程序包括一个Amazon CloudFront分发和一个位于AWS区域中的公共应用程序负载均衡器(ALB)。公司已将ALB配置为分发的默认源站。在最近的一些应用程序中断后，公司希望实现零秒RTO(恢复时间目标)。公司在辅助区域以温备用配置部署了应用程序。DevOps工程师需要自动化应用程序到辅助区域的故障转移，以便HTTP GET请求满足所需的RTO。</td>
                    <td>A. 创建第二个CloudFront分发，将辅助ALB作为默认源站。创建Amazon Route 53别名记录，为两个CloudFront分发设置故障转移策略并将&quot;评估目标健康状况&quot;设置为是。更新应用程序以使用新记录集。这种方法需要DNS解析时间，无法实现零秒RTO，且需要管理两个CloudFront分发增加复杂性。<br><br>B. 在分发上为辅助ALB创建新源站。创建新的源站组。将原始ALB设置为主源站。配置源站组在HTTP 5xx状态码时进行故障转移。更新默认行为以使用源站组。这种方法利用CloudFront的原生故障转移功能，可以实现近乎零秒的RTO，因为故障转移发生在CloudFront边缘位置，无需DNS传播。<br><br>C. 为两个ALB创建Amazon Route 53别名记录，设置故障转移策略并将&quot;评估目标健康状况&quot;设置为是。将两个记录的TTL设置为0。更新分发的源站以使用新记录集。即使TTL设置为0，DNS解析仍需要时间，无法实现真正的零秒RTO。<br><br>D. 创建CloudFront函数检测HTTP 5xx状态码。配置函数在检测到5xx状态码时返回307临时重定向错误响应到辅助ALB。更新分发的默认行为将源站响应发送到函数。这种方法会增加延迟，且重定向响应不符合零秒RTO要求。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>215</td>
                    <td>A cloud team uses AWS Organizations and AWS IAM Identity Center (AWS Single Sign-On) to manage a company&#x27;s AWS accounts. The company<br>recently established a research team. The research team requires the ability to fully manage the resources in its account. The research team<br>must not be able to create IAM users.<br>The cloud team creates a Research Administrator permission set in IAM Identity Center for the research team. The permission set has the<br>AdministratorAccess AWS managed policy attached. The cloud team must ensure that no one on the research team can create IAM users.</td>
                    <td>A. Create an IAM policy that denies the iam:CreateUser action. Attach the IAM policy to the Research Administrator permission set.<br>B. Create an IAM policy that allows all actions except the iam:CreateUser action. Use the IAM policy to set the permissions boundary for<br>the Research Administrator permission set.<br>C. Create an SCP that denies the iam:CreateUser action. Attach the SCP to the research team&#x27;s AWS account.<br>D. Create an AWS Lambda function that deletes IAM users. Create an Amazon EventBridge rule that detects the IAM CreateUser event.<br>Congure the rule to invoke the Lambda function.</td>
                    <td>一个云团队使用AWS Organizations和AWS IAM Identity Center（AWS Single Sign-On）来管理公司的AWS账户。公司最近成立了一个研究团队。研究团队需要能够完全管理其账户中的资源。研究团队不得创建IAM用户。<br>云团队在IAM Identity Center中为研究团队创建了一个Research Administrator权限集。该权限集附加了AdministratorAccess AWS托管策略。云团队必须确保研究团队中的任何人都不能创建IAM用户。</td>
                    <td>A. 创建一个拒绝iam:CreateUser操作的IAM策略，并将该IAM策略附加到Research Administrator权限集。这个方法在技术上可行，但由于AdministratorAccess策略具有很高的权限优先级，可能会与拒绝策略产生冲突。而且在权限集层面添加拒绝策略可能不是最佳实践，因为权限集主要用于授予权限而非限制权限。<br><br>B. 创建一个允许除iam:CreateUser之外所有操作的IAM策略，使用该IAM策略为Research Administrator权限集设置权限边界。权限边界确实可以限制权限，但这种方法过于复杂，需要重新定义几乎所有的AWS服务权限，维护成本高且容易出错。<br><br>C. 创建一个拒绝iam:CreateUser操作的SCP（服务控制策略），并将SCP附加到研究团队的AWS账户。SCP在组织级别工作，可以覆盖账户内的所有权限，包括AdministratorAccess。这是最直接有效的方法，因为SCP具有最高优先级，即使用户有管理员权限也无法绕过SCP的限制。<br><br>D. 创建一个删除IAM用户的AWS Lambda函数，创建一个检测IAM CreateUser事件的Amazon EventBridge规则，配置规则调用Lambda函数。这种方法是被动的，允许用户先创建IAM用户然后再删除，不符合&quot;必须确保无人能创建IAM用户&quot;的要求，而且可能存在时间窗口问题。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>216</td>
                    <td>A company releases a new application in a new AWS account. The application includes an AWS Lambda function that processes messages<br>from an Amazon Simple Queue Service (Amazon SQS) standard queue. The Lambda function stores the results in an Amazon S3 bucket for<br>further downstream processing. The Lambda function needs to process the messages within a specic period of time after the messages are<br>published. The Lambda function has a batch size of 10 messages and takes a few seconds to process a batch of messages.<br>As load increases on the application&#x27;s rst day of service, messages in the queue accumulate at a greater rate than the Lambda function can<br>process the messages. Some messages miss the required processing timelines. The logs show that many messages in the queue have data<br>that is not valid. The company needs to meet the timeline requirements for messages that have valid data.</td>
                    <td>A. Increase the Lambda function&#x27;s batch size. Change the SQS standard queue to an SQS FIFO queue. Request a Lambda concurrency<br>increase in the AWS Region.<br>B. Reduce the Lambda function&#x27;s batch size. Increase the SQS message throughput quota. Request a Lambda concurrency increase in the<br>AWS Region.<br>C. Increase the Lambda function&#x27;s batch size. Congure S3 Transfer Acceleration on the S3 bucket. Congure an SQS dead-letter queue.<br>D. Keep the Lambda function&#x27;s batch size the same. Congure the Lambda function to report failed batch items. Congure an SQS dead-<br>letter queue.</td>
                    <td>一家公司在新的AWS账户中发布了一个新应用程序。该应用程序包含一个AWS Lambda函数，用于处理来自Amazon Simple Queue Service (Amazon SQS)标准队列的消息。Lambda函数将结果存储在Amazon S3存储桶中，供进一步的下游处理。Lambda函数需要在消息发布后的特定时间段内处理消息。Lambda函数的批处理大小为10条消息，处理一批消息需要几秒钟时间。<br><br>随着应用程序服务第一天负载的增加，队列中的消息积累速度超过了Lambda函数处理消息的速度。一些消息错过了所需的处理时间线。日志显示队列中的许多消息包含无效数据。公司需要满足具有有效数据的消息的时间线要求。</td>
                    <td>选项A：增加Lambda函数的批处理大小，将SQS标准队列更改为SQS FIFO队列，在AWS区域中请求Lambda并发增加。虽然增加批处理大小和并发可能有助于提高处理速度，但将标准队列改为FIFO队列会降低吞吐量，且没有解决无效消息的问题。无效消息仍会占用处理时间。<br><br>选项B：减少Lambda函数的批处理大小，增加SQS消息吞吐量配额，在AWS区域中请求Lambda并发增加。减少批处理大小实际上可能降低处理效率，虽然增加并发有帮助，但仍未解决核心问题：无效消息继续占用处理资源和时间。<br><br>选项C：增加Lambda函数的批处理大小，在S3存储桶上配置S3传输加速，配置SQS死信队列。虽然S3传输加速可能稍微提高性能，死信队列有助于处理失败消息，但增加批处理大小可能不是最佳解决方案，且没有直接解决无效消息的处理问题。<br><br>选项D：保持Lambda函数的批处理大小不变，配置Lambda函数报告失败的批处理项，配置SQS死信队列。这个方案通过配置Lambda函数报告失败项和死信队列，可以有效地将无效消息从主处理流程中移除，让Lambda函数专注于处理有效消息，从而提高整体处理效率并满足时间线要求。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>217</td>
                    <td>A company has an application that runs on AWS Lambda and sends logs to Amazon CloudWatch Logs. An Amazon Kinesis data stream is<br>subscribed to the log groups in CloudWatch Logs. A single consumer Lambda function processes the logs from the data stream and stores the<br>logs in an Amazon S3 bucket.<br>The company’s DevOps team has noticed high latency during the processing and ingestion of some logs.</td>
                    <td>A. Create a data stream consumer with enhanced fan-out. Set the Lambda function that processes the logs as the consumer.<br>B. Increase the ParallelizationFactor setting in the Lambda event source mapping.<br>C. Congure reserved concurrency for the Lambda function that processes the logs.<br>D. Increase the batch size in the Kinesis data stream.<br>E. Turn off the ReportBatchItemFailures setting in the Lambda event source mapping.<br>F. Increase the number of shards in the Kinesis data stream.</td>
                    <td>一家公司有一个运行在AWS Lambda上的应用程序，该应用程序将日志发送到Amazon CloudWatch Logs。一个Amazon Kinesis数据流订阅了CloudWatch Logs中的日志组。单个消费者Lambda函数处理来自数据流的日志，并将日志存储在Amazon S3存储桶中。<br>公司的DevOps团队注意到在处理和摄取某些日志时出现了高延迟。</td>
                    <td>A. 创建具有增强扇出功能的数据流消费者，将处理日志的Lambda函数设置为消费者。增强扇出(Enhanced Fan-out)为每个消费者提供专用的2MB/秒吞吐量，减少了消费者之间的竞争，显著降低延迟，这是解决高延迟问题的有效方案。<br><br>B. 增加Lambda事件源映射中的ParallelizationFactor设置。这个参数允许Lambda并行处理来自同一分片的多个批次，提高处理吞吐量，减少延迟，特别适合处理大量数据的场景。<br><br>C. 为处理日志的Lambda函数配置预留并发。预留并发主要用于确保函数有足够的执行容量，但不能直接解决Kinesis数据流处理的延迟问题，因为延迟主要来自数据流的吞吐量限制。<br><br>D. 增加Kinesis数据流中的批次大小。虽然可能提高吞吐量，但也会增加每个批次的处理时间，可能不会显著改善延迟问题，甚至可能使延迟更严重。<br><br>E. 关闭Lambda事件源映射中的ReportBatchItemFailures设置。这个设置用于错误处理，关闭它不会改善处理性能或延迟，反而可能影响错误处理能力。<br><br>F. 增加Kinesis数据流中的分片数量。虽然可以提高整体吞吐量，但由于只有单个Lambda消费者，增加分片数量的效果有限，不如其他优化方案直接有效。</td>
                    <td>AB</td>
                </tr>
                <tr>
                    <td>218</td>
                    <td>A company operates sensitive workloads across the AWS accounts that are in the company&#x27;s organization in AWS Organizations. The company<br>uses an IP address range to delegate IP addresses for Amazon VPC CIDR blocks and all non-cloud hardware.<br>The company needs a solution that prevents principals that are outside the company’s IP address range from performing AWS actions in the<br>organization&#x27;s accounts.</td>
                    <td>A. Congure AWS Firewall Manager for the organization. Create an AWS Network Firewall policy that allows only source trac from the<br>company&#x27;s IP address range. Set the policy scope to all accounts in the organization.<br>B. In Organizations, create an SCP that denies source IP addresses that are outside of the company’s IP address range. Attach the SCP to<br>the organization&#x27;s root.<br>C. Congure Amazon GuardDuty for the organization. Create a GuardDuty trusted IP address list for the company&#x27;s IP range. Activate the<br>trusted IP list for the organization.<br>D. In Organizations, create an SCP that allows source IP addresses that are inside of the company’s IP address range. Attach the SCP to<br>the organization&#x27;s root.</td>
                    <td>一家公司在AWS Organizations组织中的AWS账户上运行敏感工作负载。该公司使用IP地址范围来为Amazon VPC CIDR块和所有非云硬件分配IP地址。<br>公司需要一个解决方案，防止公司IP地址范围之外的主体在组织账户中执行AWS操作。</td>
                    <td>A. 为组织配置AWS Firewall Manager。创建一个AWS Network Firewall策略，仅允许来自公司IP地址范围的源流量。将策略范围设置为组织中的所有账户。<br>这个选项不正确。AWS Firewall Manager和Network Firewall主要用于网络流量过滤，而不是控制AWS API访问。它们无法阻止来自外部IP的AWS管理操作，只能控制网络层面的流量。<br><br>B. 在Organizations中，创建一个SCP（服务控制策略），拒绝来自公司IP地址范围之外的源IP地址。将SCP附加到组织的根。<br>这个选项正确。SCP可以基于源IP地址条件来控制AWS API访问权限。通过创建拒绝外部IP访问的SCP并附加到组织根，可以有效防止来自公司IP范围外的任何AWS操作，这正是题目要求的解决方案。<br><br>C. 为组织配置Amazon GuardDuty。为公司的IP范围创建GuardDuty可信IP地址列表。为组织激活可信IP列表。<br>这个选项不正确。GuardDuty是安全监控和威胁检测服务，可信IP列表只是用来减少误报，并不能阻止或控制AWS API访问。它无法实现题目要求的访问控制功能。<br><br>D. 在Organizations中，创建一个SCP，允许来自公司IP地址范围内的源IP地址。将SCP附加到组织的根。<br>这个选项不完整。仅仅允许内部IP是不够的，因为SCP默认是允许的，需要明确拒绝外部IP才能实现完整的访问控制。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>219</td>
                    <td>A company deploys an application in two AWS Regions. The application currently uses an Amazon S3 bucket in the primary Region to store<br>data.<br>A DevOps engineer needs to ensure that the application is highly available in both Regions. The DevOps engineer has created a new S3<br>bucket in the secondary Region. All existing and new objects must be in both S3 buckets. The application must fail over between the Regions<br>with no data loss.</td>
                    <td>A. Create a new IAM role that allows the Amazon S3 and S3 Batch Operations service principals to assume the role that has the necessary<br>permissions for S3 replication.<br>B. Create a new IAM role that allows the AWS Batch service principal to assume the role that has the necessary permissions for S3<br>replication.<br>C. Create an S3 Cross-Region Replication (CRR) rule on the source S3 bucket. Congure the rule to use the IAM role for Amazon S3 to<br>replicate to the target S3 bucket.<br>D. Create a two-way replication rule on the source S3 bucket. Congure the rule to use the IAM role for Amazon S3 to replicate to the<br>target S3 bucket.<br>E. Create an AWS Batch job that has an AWS Fargate orchestration type. Congure the job to use the IAM role for AWS Batch. Specify a<br>Bash command to use the AWS CLI to synchronize the contents of the source S3 bucket and the target S3 bucket<br>F. Create an operation in S3 Batch Operations to replicate the contents of the source S3 bucket to the target S3 bucket. Congure the<br>operation to use the IAM role for Amazon S3.</td>
                    <td>一家公司在两个AWS区域部署应用程序。该应用程序目前使用主区域中的Amazon S3存储桶来存储数据。<br>DevOps工程师需要确保应用程序在两个区域都具有高可用性。DevOps工程师已在辅助区域创建了一个新的S3存储桶。所有现有和新的对象都必须存在于两个S3存储桶中。应用程序必须能够在区域之间进行故障转移且不丢失数据。</td>
                    <td>A. 创建一个新的IAM角色，允许Amazon S3和S3批处理操作服务主体承担该角色，该角色具有S3复制的必要权限。这是正确的，因为需要为S3复制和批处理操作创建适当的IAM权限。<br><br>B. 创建一个新的IAM角色，允许AWS Batch服务主体承担该角色，该角色具有S3复制的必要权限。这是不正确的，因为AWS Batch不是S3复制的正确服务，应该使用S3服务本身。<br><br>C. 在源S3存储桶上创建S3跨区域复制(CRR)规则。配置规则使用Amazon S3的IAM角色复制到目标S3存储桶。这是正确的，CRR是实现跨区域数据复制的标准AWS服务。<br><br>D. 在源S3存储桶上创建双向复制规则。配置规则使用Amazon S3的IAM角色复制到目标S3存储桶。这是不正确的，S3不支持原生的双向复制，需要分别配置两个方向的复制。<br><br>E. 创建具有AWS Fargate编排类型的AWS Batch作业。配置作业使用AWS Batch的IAM角色。指定Bash命令使用AWS CLI同步源S3存储桶和目标S3存储桶的内容。这不是最佳实践，使用批处理作业进行同步不如原生S3复制可靠。<br><br>F. 在S3批处理操作中创建操作以将源S3存储桶的内容复制到目标S3存储桶。配置操作使用Amazon S3的IAM角色。这可以处理现有对象，但不是持续复制的解决方案。</td>
                    <td>AC</td>
                </tr>
                <tr>
                    <td>220</td>
                    <td>A company uses an organization in AWS Organizations to manage multiple AWS accounts. The company needs an automated process across<br>all AWS accounts to isolate any compromised Amazon EC2 instances when the instances receive a specic tag.</td>
                    <td>A. Use AWS CloudFormation StackSets to deploy the CloudFormation stacks in all AWS accounts.<br>B. Create an SCP that has a Deny statement for the ec2:* action with a condition of &quot;aws:RequestTag/isolation&quot;: false.<br>C. Attach the SCP to the root of the organization.<br>D. Create an AWS CloudFormation template that creates an EC2 instance role that has no IAM policies attached. Congure the template to<br>have a security group that has an explicit Deny rule on all trac. Use the CloudFormation template to create an AWS Lambda function<br>that attaches the IAM role to instances. Congure the Lambda function to add a network ACL. Set up an Amazon EventBridge rule to<br>invoke the Lambda function when a specic tag is applied to a compromised EC2 instance.<br>E. Create an AWS CloudFormation template that creates an EC2 instance role that has no IAM policies attached. Congure the template to<br>have a security group that has no inbound rules or outbound rules. Use the CloudFormation template to create an AWS Lambda function<br>that attaches the IAM role to instances. Congure the Lambda function to replace any existing security groups with the new security<br>group. Set up an Amazon EventBridge rule to invoke the Lambda function when a specic tag is applied to a compromised EC2 instance.</td>
                    <td>一家公司使用AWS Organizations中的组织来管理多个AWS账户。该公司需要一个跨所有AWS账户的自动化流程，当Amazon EC2实例收到特定标签时，能够隔离任何被入侵的EC2实例。</td>
                    <td>A. 使用AWS CloudFormation StackSets在所有AWS账户中部署CloudFormation堆栈。<br>这个选项是正确的。CloudFormation StackSets允许在AWS Organizations的多个账户中统一部署和管理CloudFormation模板，这正是实现跨账户自动化流程所需要的基础设施部署方式。通过StackSets可以确保隔离机制在所有账户中都得到一致的部署。<br><br>B. 创建一个SCP（服务控制策略），对ec2:*操作使用Deny语句，条件为&quot;aws:RequestTag/isolation&quot;: false。<br>这个选项不正确。这个SCP会阻止所有不带isolation标签或isolation标签为false的EC2操作，这会严重影响正常的EC2操作，而不是仅仅隔离被标记的实例。<br><br>C. 将SCP附加到组织的根部。<br>这个选项单独来看是SCP部署的标准做法，但结合选项B的错误逻辑，这个选项也不适用于当前需求。<br><br>D. 创建一个CloudFormation模板，创建一个没有附加IAM策略的EC2实例角色，配置一个对所有流量有明确拒绝规则的安全组，使用模板创建Lambda函数来附加IAM角色，配置Lambda函数添加网络ACL，设置EventBridge规则在特定标签应用时调用Lambda函数。<br>这个选项不正确。安全组不能有&quot;明确拒绝规则&quot;，安全组只支持允许规则，拒绝是默认行为。<br><br>E. 创建一个CloudFormation模板，创建一个没有附加IAM策略的EC2实例角色，配置一个没有入站或出站规则的安全组，使用模板创建Lambda函数来附加IAM角色，配置Lambda函数用新安全组替换现有安全组，设置EventBridge规则在特定标签应用时调用Lambda函数。<br>这个选项是正确的。没有规则的安全组会阻止所有网络流量，Lambda函数可以响应标签事件并自动替换安全组来隔离实例，这是一个有效的隔离机制。</td>
                    <td>AE</td>
                </tr>
                <tr>
                    <td>221</td>
                    <td>A company manages multiple AWS accounts by using AWS Organizations with OUs for the different business divisions. The company is<br>updating their corporate network to use new IP address ranges. The company has 10 Amazon S3 buckets in different AWS accounts. The S3<br>buckets store reports for the different divisions. The S3 bucket congurations allow only private corporate network IP addresses to access the<br>S3 buckets.<br>A DevOps engineer needs to change the range of IP addresses that have permission to access the contents of the S3 buckets. The DevOps<br>engineer also needs to revoke the permissions of two OUs in the company.</td>
                    <td>A. Create a new SCP that has two statements, one that allows access to the new range of IP addresses for all the S3 buckets and one that<br>denies access to the old range of IP addresses for all the S3 buckets. Set a permissions boundary for the OrganizationAccountAccessRole<br>role in the two OUs to deny access to the S3 buckets.<br>B. Create a new SCP that has a statement that allows only the new range of IP addresses to access the S3 buckets. Create another SCP<br>that denies access to the S3 buckets. Attach the second SCP to the two OUs.<br>C. On all the S3 buckets, congure resource-based policies that allow only the new range of IP addresses to access the S3 buckets. Create<br>a new SCP that denies access to the S3 buckets. Attach the SCP to the two OUs.<br>D. On all the S3 buckets, congure resource-based policies that allow only the new range of IP addresses to access the S3 buckets. Set a<br>permissions boundary for the OrganizationAccountAccessRole role in the two OUs to deny access to the S3 buckets.</td>
                    <td>一家公司使用AWS Organizations管理多个AWS账户，为不同的业务部门设置了组织单元(OU)。该公司正在更新其企业网络以使用新的IP地址范围。该公司在不同AWS账户中有10个Amazon S3存储桶，这些S3存储桶存储不同部门的报告。S3存储桶配置仅允许私有企业网络IP地址访问S3存储桶。<br><br>DevOps工程师需要更改有权限访问S3存储桶内容的IP地址范围。DevOps工程师还需要撤销公司中两个OU的权限。</td>
                    <td>选项A：创建一个新的SCP，包含两个语句，一个允许新IP地址范围访问所有S3存储桶，另一个拒绝旧IP地址范围访问所有S3存储桶。为两个OU中的OrganizationAccountAccessRole角色设置权限边界以拒绝访问S3存储桶。这个方案有问题，因为SCP不能直接控制基于IP地址的访问，IP地址限制应该在资源策略中配置。权限边界也不是最佳的撤销OU权限的方法。<br><br>选项B：创建一个新的SCP，包含仅允许新IP地址范围访问S3存储桶的语句。创建另一个SCP拒绝访问S3存储桶，并将第二个SCP附加到两个OU。这个方案同样有问题，因为SCP主要用于权限限制，而IP地址访问控制应该在S3存储桶策略中实现，不是通过SCP来控制IP访问。<br><br>选项C：在所有S3存储桶上配置基于资源的策略，仅允许新IP地址范围访问S3存储桶。创建一个新的SCP拒绝访问S3存储桶，并将SCP附加到两个OU。这个方案正确地将IP地址控制放在S3存储桶的资源策略中，同时使用SCP来拒绝特定OU的访问权限，这是合理的架构设计。<br><br>选项D：在所有S3存储桶上配置基于资源的策略，仅允许新IP地址范围访问S3存储桶。为两个OU中的OrganizationAccountAccessRole角色设置权限边界以拒绝访问S3存储桶。虽然正确使用了资源策略控制IP访问，但权限边界不如SCP更适合在组织级别控制访问权限。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>222</td>
                    <td>A company has started using AWS across several teams. Each team has multiple accounts and unique security proles. The company<br>manages the accounts in an organization in AWS Organizations. Each account has its own conguration and security controls.<br>The company&#x27;s DevOps team wants to use preventive and detective controls to govern all accounts. The DevOps team needs to ensure the<br>security of accounts now and in the future as the company creates new accounts in the organization.</td>
                    <td>A. Use Organizations to create OUs that have appropriate SCPs attached for each team. Place team accounts in the appropriate OUs to<br>apply security controls. Create any new team accounts in the appropriate OUs.<br>B. Create an AWS Control Tower landing zone. Congure OUs and appropriate controls in AWS Control Tower for the existing teams.<br>Congure trusted access for AWS Control Tower. Enroll the existing accounts in the appropriate OUs that match the appropriate security<br>policies for each team. Use AWS Control Tower to provision any new accounts.<br>C. Create AWS CloudFormation stack sets in the organization&#x27;s management account. Congure a stack set that deploys AWS Cong with<br>conguration rules and remediation actions for all controls to each account in the organization. Update the stack sets to deploy to new<br>accounts as the accounts are created.<br>D. Congure AWS Cong to manage the AWS Cong rules across all AWS accounts in the organization. Deploy conformance packs that<br>provide AWS Cong rules and remediation actions across the organization.</td>
                    <td>一家公司已经开始在多个团队中使用AWS。每个团队都有多个账户和独特的安全配置文件。公司在AWS Organizations中的组织内管理这些账户。每个账户都有自己的配置和安全控制。<br>公司的DevOps团队希望使用预防性和检测性控制来治理所有账户。DevOps团队需要确保账户现在和未来的安全性，因为公司会在组织中创建新账户。</td>
                    <td>A. 使用Organizations创建具有适当SCP（服务控制策略）的OU（组织单元），为每个团队附加相应的SCP。将团队账户放置在适当的OU中以应用安全控制。在适当的OU中创建任何新的团队账户。这个选项只提供了预防性控制（SCP），但缺乏检测性控制，无法满足题目要求的完整治理需求。<br><br>B. 创建AWS Control Tower着陆区。在AWS Control Tower中为现有团队配置OU和适当的控制。为AWS Control Tower配置可信访问。将现有账户注册到与每个团队适当安全策略匹配的相应OU中。使用AWS Control Tower来配置任何新账户。这个选项提供了完整的治理解决方案，包括预防性和检测性控制，并且能够自动化新账户的创建和治理。<br><br>C. 在组织的管理账户中创建AWS CloudFormation堆栈集。配置一个堆栈集，将AWS Config与配置规则和修复操作部署到组织中的每个账户。更新堆栈集以在创建新账户时部署到新账户。这个选项主要关注检测性控制，但缺乏预防性控制，且管理复杂度较高。<br><br>D. 配置AWS Config来管理组织中所有AWS账户的AWS Config规则。部署提供AWS Config规则和修复操作的合规包到整个组织。这个选项主要提供检测性控制，但缺乏预防性控制，无法提供完整的治理解决方案。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>223</td>
                    <td>A company uses an AWS CodeCommit repository to store its source code and corresponding unit tests. The company has congured an AWS<br>CodePipeline pipeline that includes an AWS CodeBuild project that runs when code is merged to the main branch of the repository.<br>The company wants the CodeBuild project to run the unit tests. If the unit tests pass, the CodeBuild project must tag the most recent commit.<br>How should the company congure the CodeBuild project to meet these requirements?</td>
                    <td>A. Congure the CodeBuild project to use native Git to done the CodeCommit repository. Congure the project to run the unit tests.<br>Congure the project to use native Git to create a tag and to push the Git tag to the repository if the code passes the unit tests.<br>B. Congure the CodeBuild projed to use native Git to done the CodeCommit repository. Congure the project to run the unit tests.<br>Congure the project to use AWS CLI commands to create a new repository tag in the repository if the code passes the unit tests.<br>C. Congure the CodeBuild project to use AWS CLI commands to copy the code from the CodeCommit repository. Congure the project to<br>run the unit tests. Congure the project to use AWS CLI commands to create a new Git tag in the repository if the code passes the unit<br>tests.<br>D. Congure the CodeBuild project to use AWS CLI commands to copy the code from the CodeCommit repository. Congure the project to<br>run the unit tests. Congure the project to use AWS CLI commands to create a new repository tag in the repository if the code passes the<br>unit tests.</td>
                    <td>一家公司使用AWS CodeCommit存储库来存储其源代码和相应的单元测试。该公司已配置了一个AWS CodePipeline管道，其中包含一个AWS CodeBuild项目，当代码合并到存储库的主分支时运行。<br>公司希望CodeBuild项目运行单元测试。如果单元测试通过，CodeBuild项目必须标记最新的提交。<br>公司应该如何配置CodeBuild项目以满足这些要求？</td>
                    <td>选项A：配置CodeBuild项目使用原生Git克隆CodeCommit存储库，运行单元测试，并使用原生Git创建标签并推送到存储库。这个方案在技术上可行，但在CodeBuild环境中使用原生Git推送操作可能会遇到权限和认证问题，特别是推送标签回CodeCommit时需要额外的配置。<br><br>选项B：与选项A类似，使用原生Git克隆存储库和运行测试，但使用AWS CLI命令创建存储库标签。这里存在概念混淆，AWS CLI创建的是存储库标签而不是Git标签，这两者是不同的概念。<br><br>选项C：使用AWS CLI命令从CodeCommit复制代码，运行单元测试，然后使用AWS CLI命令在存储库中创建新的Git标签。这个方案使用AWS CLI进行所有CodeCommit操作，确保了适当的权限管理和认证，同时明确指出创建Git标签。<br><br>选项D：使用AWS CLI命令复制代码和运行测试，但创建的是存储库标签而不是Git标签。存储库标签和Git标签是不同的概念，题目要求的是标记提交，应该使用Git标签。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>224</td>
                    <td>A DevOps engineer manages a company&#x27;s Amazon Elastic Container Service (Amazon ECS) cluster. The cluster runs on several Amazon EC2<br>instances that are in an Auto Scaling group. The DevOps engineer must implement a solution that logs and reviews all stopped tasks for<br>errors.</td>
                    <td>A. Create an Amazon EventBridge rule to capture task state changes. Send the event to Amazon CloudWatch Logs. Use CloudWatch Logs<br>Insights to investigate stopped tasks.<br>B. Congure tasks to write log data in the embedded metric format. Store the logs in Amazon CloudWatch Logs. Monitor the<br>ContainerInstanceCount metric for changes.<br>C. Congure the EC2 instances to store logs in Amazon CloudWatch Logs. Create a CloudWatch Contributor Insights rule that uses the EC2<br>instance log data. Use the Contributor Insights rule to investigate stopped tasks.<br>D. Congure an EC2 Auto Scaling lifecycle hook for the EC2_INSTANCE_TERMINATING scale-in event. Write the SystemEventLog le to<br>Amazon S3. Use Amazon Athena to query the log le for errors.</td>
                    <td>一名DevOps工程师管理公司的Amazon弹性容器服务(Amazon ECS)集群。该集群运行在位于Auto Scaling组中的多个Amazon EC2实例上。DevOps工程师必须实施一个解决方案来记录和审查所有已停止的任务以查找错误。</td>
                    <td>A. 创建Amazon EventBridge规则来捕获任务状态变化。将事件发送到Amazon CloudWatch Logs。使用CloudWatch Logs Insights来调查已停止的任务。<br>这个选项是正确的方法。EventBridge可以监控ECS任务状态变化事件，当任务停止时会触发规则。将这些事件发送到CloudWatch Logs进行存储，然后使用CloudWatch Logs Insights进行查询和分析，可以有效地识别和调查停止的任务及其错误原因。这是一个完整且针对性的解决方案。<br><br>B. 配置任务以嵌入式指标格式写入日志数据。将日志存储在Amazon CloudWatch Logs中。监控ContainerInstanceCount指标的变化。<br>这个选项不够准确。虽然配置任务写入CloudWatch Logs是好的做法，但监控ContainerInstanceCount指标只能告诉你容器实例数量的变化，而不能具体识别哪些任务停止了以及停止的原因。这个方法无法满足审查停止任务错误的需求。<br><br>C. 配置EC2实例将日志存储在Amazon CloudWatch Logs中。创建CloudWatch Contributor Insights规则使用EC2实例日志数据。使用Contributor Insights规则来调查已停止的任务。<br>这个选项关注的是EC2实例级别的日志，而不是ECS任务级别的事件。Contributor Insights主要用于分析高基数数据，但对于追踪特定的任务停止事件来说不是最佳选择。这种方法可能会遗漏重要的任务级别信息。<br><br>D. 为EC2_INSTANCE_TERMINATING缩容事件配置EC2 Auto Scaling生命周期钩子。将SystemEventLog文件写入Amazon S3。使用Amazon Athena查询日志文件以查找错误。<br>这个选项关注的是EC2实例终止事件，而不是ECS任务停止事件。实例终止和任务停止是两个不同的概念，任务可能因为多种原因停止，而不一定是因为实例终止。这种方法无法捕获所有的任务停止情况。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>225</td>
                    <td>A company wants to deploy a workload on several hundred Amazon EC2 instances. The company will provision the EC2 instances in an Auto<br>Scaling group by using a launch template.<br>The workload will pull les from an Amazon S3 bucket, process the data, and put the results into a different S3 bucket. The EC2 instances<br>must have least-privilege permissions and must use temporary security credentials.</td>
                    <td>A. Create an IAM role that has the appropriate permissions for S3 buckets Add the IAM role to an instance prole.<br>B. Update the launch template to include the IAM instance prole.<br>C. Create an IAM user that has the appropriate permissions for Amazon S3 Generate a secret key and token.<br>D. Create a trust anchor and prole Attach the IAM role to the prole.<br>E. Update the launch template Modify the user data to use the new secret key and token.</td>
                    <td>一家公司想要在数百个Amazon EC2实例上部署工作负载。公司将使用启动模板在Auto Scaling组中配置EC2实例。<br>工作负载将从Amazon S3存储桶中拉取文件，处理数据，并将结果放入不同的S3存储桶中。EC2实例必须具有最小权限，并且必须使用临时安全凭证。</td>
                    <td>A. 创建一个具有S3存储桶适当权限的IAM角色，将IAM角色添加到实例配置文件中。这个选项是正确的，因为IAM角色可以提供临时安全凭证，符合最小权限原则，并且实例配置文件是将IAM角色附加到EC2实例的标准方式。<br><br>B. 更新启动模板以包含IAM实例配置文件。这个选项也是正确的，因为要让Auto Scaling组中的EC2实例使用IAM角色，必须在启动模板中指定实例配置文件，这样新启动的实例才能自动获得相应的权限。<br><br>C. 创建一个具有Amazon S3适当权限的IAM用户，生成密钥和令牌。这个选项不正确，因为IAM用户提供的是长期凭证而不是临时安全凭证，不符合题目要求。<br><br>D. 创建信任锚点和配置文件，将IAM角色附加到配置文件。这个选项描述不准确，信任锚点通常用于其他身份验证场景，不是EC2实例获取权限的标准方式。<br><br>E. 更新启动模板，修改用户数据以使用新的密钥和令牌。这个选项不正确，因为在用户数据中硬编码凭证是不安全的做法，且不符合使用临时凭证的要求。</td>
                    <td>AB</td>
                </tr>
                <tr>
                    <td>226</td>
                    <td>A company is using AWS CodeDeploy to automate software deployment. The deployment must meet these requirements:<br>• A number of instances must be available to serve trac during the deployment. Trac must be balanced across those instances, and the<br>instances must automatically heal in the event of failure. • A new eet of instances must be launched for deploying a new revision<br>automatically, with no manual provisioning.<br>• Trac must be rerouted to the new environment to half of the new instances at a time. The deployment should succeed if trac is rerouted<br>to at least half of the instances: otherwise, it should fail.<br>• Before routing trac to the new eet of instances, the temporary les generated during the deployment process must be deleted.<br>• At the end of a successful deployment, the original instances in the deployment group must be deleted immediately to reduce costs.<br>How can a DevOps engineer meet these requirements?</td>
                    <td>A. Use an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group with the deployment group. Use the<br>Automatically copy Auto Scaling group option, and use CodeDeployDefault.OneAtAtime as the deployment conguration. Instruct AWS<br>CodeDeploy to terminate the original instances in the deployment group, and use the AllowTrac hook within appspec.yml to delete the<br>temporary les.<br>B. Use an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group and Application Load Balancer<br>target group with the deployment group. Use the Automatically copy Auto Scaling group option, create a custom deployment conguration<br>with minimum healthy hosts dened as 50%, and assign the conguration to the deployment group. Instruct AWS CodeDeploy to terminate<br>the original instances in the deployment group, and use the BeforeBlockTrac hook within appspec.yml to delete the temporary les.<br>C. Use an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group and the Application Load Balancer<br>target group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.HalfAtAtime<br>as the deployment conguration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the<br>BeforeAllowTrac hook within appspec.yml to delete the temporary les.<br>D. Use an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group and Application Load Balancer target<br>group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.AllatOnce as a<br>deployment conguration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BlockTrac<br>hook within appspec.yml to delete the temporary les.</td>
                    <td>一家公司正在使用AWS CodeDeploy来自动化软件部署。部署必须满足以下要求：<br>• 在部署期间必须有一定数量的实例可用来服务流量。流量必须在这些实例之间进行负载均衡，并且实例必须在发生故障时自动修复。<br>• 必须自动启动新的实例集群来部署新版本，无需手动配置。<br>• 流量必须重新路由到新环境，每次路由到一半的新实例。如果流量至少路由到一半的实例，部署应该成功；否则应该失败。<br>• 在将流量路由到新实例集群之前，必须删除部署过程中生成的临时文件。<br>• 在成功部署结束时，部署组中的原始实例必须立即删除以降低成本。<br>DevOps工程师如何满足这些要求？</td>
                    <td>选项A：使用应用负载均衡器和就地部署。这种方案不符合要求，因为就地部署不会创建新的实例集群，而是在现有实例上进行更新。CodeDeployDefault.OneAtAtime配置每次只更新一个实例，不符合&quot;一半实例&quot;的要求。AllowTraffic钩子的时机也不正确。<br><br>选项B：使用应用负载均衡器和蓝绿部署。这个方案符合所有要求：蓝绿部署会创建新的实例集群，自动复制Auto Scaling组选项满足自动启动新实例的需求，自定义部署配置设置最小健康主机为50%符合&quot;至少一半实例&quot;的要求，BeforeBlockTraffic钩子在阻止流量前执行，适合删除临时文件，最后可以终止原始实例。<br><br>选项C：使用蓝绿部署是正确的，但CodeDeployDefault.HalfAtAtime是就地部署的配置，不适用于蓝绿部署。BeforeAllowTraffic钩子在允许流量前执行，但题目要求在路由流量到新实例前删除临时文件，时机不够准确。<br><br>选项D：使用就地部署不符合创建新实例集群的要求。CodeDeployDefault.AllAtOnce会同时更新所有实例，不符合渐进式部署的要求。BlockTraffic钩子用于阻止流量，不是删除临时文件的合适时机。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>227</td>
                    <td>A company needs to adopt a multi-account strategy to deploy its applications and the associated CI/CD infrastructure. The company has<br>created an organization in AWS Organizations that has all features enabled. The company has congured AWS Control Tower and has set up a<br>landing zone.<br>The company needs to use AWS Control Tower controls (guardrails) in all AWS accounts in the organization. The company must create the<br>accounts for a multi-environment application and must ensure that all accounts are congured to an initial baseline.</td>
                    <td>A. Create an AWS Control Tower Account Factory Customization (AFC) blueprint that uses the baseline conguration. Use AWS Control<br>Tower Account Factory to provision a dedicated AWS account for each environment and a CI/CD account by using the blueprint.<br>B. Use AWS Control Tower Account Factory to provision a dedicated AWS account for each environment and a CI/CD account. Use AWS<br>CloudFormation StackSets to apply the baseline conguration to the new accounts.<br>C. Use Organizations to provision a multi-environment AWS account and a CI/CD account. In the Organizations management account,<br>create an AWS Lambda function that assumes the Organizations access role to apply the baseline conguration to the new accounts.<br>D. Use Organizations to provision a dedicated AWS account for each environment, an audit account, and a CI/CD account. Use AWS<br>CloudFormation StackSets to apply the baseline conguration to the new accounts.</td>
                    <td>一家公司需要采用多账户策略来部署其应用程序和相关的CI/CD基础设施。该公司已在AWS Organizations中创建了一个启用所有功能的组织。公司已配置了AWS Control Tower并设置了着陆区。<br><br>公司需要在组织中的所有AWS账户中使用AWS Control Tower控制（护栏）。公司必须为多环境应用程序创建账户，并确保所有账户都配置为初始基线。</td>
                    <td>选项A：创建AWS Control Tower Account Factory Customization (AFC)蓝图使用基线配置，然后使用AWS Control Tower Account Factory通过蓝图为每个环境和CI/CD账户提供专用AWS账户。这个方案理论上可行，但AFC是一个相对较新的功能，可能在实际实施中存在复杂性，而且题目中没有明确提到需要自定义蓝图。<br><br>选项B：使用AWS Control Tower Account Factory为每个环境和CI/CD账户提供专用AWS账户，然后使用AWS CloudFormation StackSets将基线配置应用到新账户。这是一个标准且成熟的方案，Account Factory确保账户在Control Tower管理下创建，StackSets可以有效地跨多个账户部署标准化配置。<br><br>选项C：使用Organizations提供多环境AWS账户和CI/CD账户，在Organizations管理账户中创建Lambda函数来应用基线配置。这个方案绕过了Control Tower Account Factory，不能充分利用Control Tower的护栏功能，而且使用Lambda函数管理配置不如StackSets标准化。<br><br>选项D：使用Organizations为每个环境、审计账户和CI/CD账户提供专用AWS账户，使用CloudFormation StackSets应用基线配置。虽然StackSets部分正确，但直接使用Organizations而不是Control Tower Account Factory会失去Control Tower护栏的自动应用。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>228</td>
                    <td>A DevOps team has created a Custom Lambda rule in AWS Cong. The rule monitors Amazon Elastic Container Repository (Amazon ECR)<br>policy statements for ecr:* actions. When a noncompliant repository is detected, Amazon EventBridge uses Amazon Simple Notication<br>Service (Amazon SNS) to route the notication to a security team.<br>When the custom AWS Cong rule is evaluated, the AWS Lambda function fails to run.</td>
                    <td>A. Modify the Lambda function&#x27;s resource policy to grant AWS Cong permission to invoke the function.<br>B. Modify the SNS topic policy to include conguration changes for EventBridge to publish to the SNS topic.<br>C. Modify the Lambda function&#x27;s execution role to include conguration changes for custom AWS Cong rules.<br>D. Modify all the ECR repository policies to grant AWS Cong access to the necessary ECR API actions.</td>
                    <td>一个DevOps团队在AWS Config中创建了一个自定义Lambda规则。该规则监控Amazon弹性容器仓库(Amazon ECR)策略语句中的ecr:*操作。当检测到不合规的仓库时，Amazon EventBridge使用Amazon简单通知服务(Amazon SNS)将通知路由到安全团队。当评估自定义AWS Config规则时，AWS Lambda函数运行失败。</td>
                    <td>A. 修改Lambda函数的资源策略以授予AWS Config调用该函数的权限 - 这个选项涉及的是Lambda函数的资源策略，用于控制谁可以调用Lambda函数。虽然AWS Config需要调用Lambda函数的权限，但通常这种权限问题会在函数调用阶段出现，而不是在函数执行过程中失败。<br><br>B. 修改SNS主题策略以包含EventBridge发布到SNS主题的配置更改 - 这个选项关注的是SNS主题的权限配置。但是题目描述的问题是Lambda函数运行失败，而不是通知发送失败，所以这个选项不是问题的根本原因。<br><br>C. 修改Lambda函数的执行角色以包含自定义AWS Config规则的配置更改 - 这个选项针对Lambda函数的执行角色权限。当Lambda函数作为AWS Config自定义规则运行时，它需要特定的权限来访问AWS Config服务、读取配置项、以及访问被监控的ECR仓库。如果执行角色缺少必要的权限，函数在运行时会失败。<br><br>D. 修改所有ECR仓库策略以授予AWS Config访问必要ECR API操作的权限 - 这个选项涉及ECR仓库级别的权限。虽然Config需要读取ECR配置，但通常通过Lambda执行角色的权限就足够了，不需要修改每个仓库的策略。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>229</td>
                    <td>A developer is creating a proof of concept for a new software as a service (SaaS) application. The application is in a shared development AWS<br>account that is part of an organization in AWS Organizations.<br>The developer needs to create service-linked IAM roles for the AWS services that are being considered for the proof of concept. The solution<br>needs to give the developer the ability to create and congure the service-linked roles only.</td>
                    <td>A. Create an IAM user for the developer in the organization&#x27;s management account. Congure a cross-account role in the development<br>account for the developer to use. Limit the scope of the cross-account role to common services.<br>B. Add the developer to an IAM group. Attach the PowerUserAccess managed policy to the IAM group. Enforce multi-factor authentication<br>(MFA) on the user account.<br>C. Add an SCP to the development account in Organizations. Congure the SCP with a Deny rule for iam:* to limit the developer&#x27;s access.<br>D. Create an IAM role that has the necessary IAM access to allow the developer to create policies and roles. Create and attach a<br>permissions boundary to the role. Grant the developer access to assume the role.</td>
                    <td>一名开发人员正在为新的软件即服务(SaaS)应用程序创建概念验证。该应用程序位于一个共享的开发AWS账户中，该账户是AWS Organizations中某个组织的一部分。开发人员需要为正在考虑用于概念验证的AWS服务创建服务链接的IAM角色。解决方案需要仅给予开发人员创建和配置服务链接角色的能力。</td>
                    <td>A. 在组织的管理账户中为开发人员创建IAM用户，在开发账户中配置跨账户角色供开发人员使用，将跨账户角色的范围限制为通用服务。这种方法过于复杂，需要跨账户访问，而且没有精确控制开发人员只能创建服务链接角色的权限，可能会给予过多权限。<br><br>B. 将开发人员添加到IAM组，为IAM组附加PowerUserAccess托管策略，在用户账户上强制执行多因素身份验证(MFA)。PowerUserAccess策略提供了几乎所有AWS服务的完全访问权限，这远超过了只创建服务链接角色的需求，违反了最小权限原则。<br><br>C. 向Organizations中的开发账户添加SCP，使用Deny规则配置SCP以限制开发人员对iam:*的访问。这种方法会完全阻止开发人员进行任何IAM操作，包括创建服务链接角色，这与需求相矛盾。<br><br>D. 创建一个具有必要IAM访问权限的IAM角色，允许开发人员创建策略和角色，创建并附加权限边界到该角色，授予开发人员假设该角色的访问权限。这种方法通过权限边界精确控制了开发人员的权限范围，确保只能执行特定的IAM操作。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>230</td>
                    <td>A company uses AWS Organizations to manage its AWS accounts. The company wants its monitoring system to receive an alert when a root<br>user logs in. The company also needs a dashboard to display any log activity that the root user generates.</td>
                    <td>A. Enable AWS Cong with a multi-account aggregator. Congure log forwarding to Amazon CloudWatch Logs.<br>B. Create an Amazon QuickSight dashboard that uses an Amazon CloudWatch Logs query.<br>C. Create an Amazon CloudWatch Logs metric lter to match root user login events. Congure a CloudWatch alarm and an Amazon Simple<br>Notication Service (Amazon SNS) topic to send alerts to the company&#x27;s monitoring system.<br>D. Create an Amazon CloudWatch Logs subscription lter to match root user login events. Congure the lter to forward events to an<br>Amazon Simple Notication Service (Amazon SNS) topic. Congure the SNS topic to send alerts to the company&#x27;s monitoring system.<br>E. Create an AWS CloudTrail organization trail. Congure the organization trail to send events to Amazon CloudWatch Logs.</td>
                    <td>一家公司使用AWS Organizations来管理其AWS账户。该公司希望其监控系统在root用户登录时收到警报。该公司还需要一个仪表板来显示root用户生成的任何日志活动。</td>
                    <td>A. 启用AWS Config与多账户聚合器。配置日志转发到Amazon CloudWatch Logs。<br>这个选项不正确。AWS Config主要用于配置合规性监控和资源配置跟踪，而不是用于监控用户登录活动。Config无法捕获root用户的登录事件，因为这些事件属于API调用日志，需要通过CloudTrail来记录。此外，Config的多账户聚合器主要用于合规性数据聚合，不适用于用户活动监控场景。<br><br>B. 创建一个使用Amazon CloudWatch Logs查询的Amazon QuickSight仪表板。<br>这个选项是正确的。QuickSight可以连接到CloudWatch Logs作为数据源，通过查询CloudWatch Logs中的数据来创建可视化仪表板。一旦CloudTrail日志被发送到CloudWatch Logs，QuickSight就可以查询这些日志数据并创建仪表板来显示root用户的活动，满足题目中关于仪表板显示日志活动的需求。<br><br>C. 创建Amazon CloudWatch Logs指标过滤器来匹配root用户登录事件。配置CloudWatch警报和Amazon Simple Notification Service (Amazon SNS)主题向公司的监控系统发送警报。<br>这个选项是正确的。CloudWatch Logs指标过滤器可以扫描日志数据并提取特定模式（如root用户登录），然后创建自定义指标。基于这些指标可以设置CloudWatch警报，当检测到root用户登录时触发警报，通过SNS发送通知到监控系统。这是实现实时警报的有效方法。<br><br>D. 创建Amazon CloudWatch Logs订阅过滤器来匹配root用户登录事件。配置过滤器将事件转发到Amazon Simple Notification Service (Amazon SNS)主题。配置SNS主题向公司的监控系统发送警报。<br>这个选项是正确的。CloudWatch Logs订阅过滤器可以实时过滤日志流，当匹配到root用户登录事件时，直接将这些事件转发到SNS主题，然后SNS可以将警报发送到监控系统。这提供了近实时的事件通知机制，非常适合安全监控场景。<br><br>E. 创建AWS CloudTrail组织跟踪。配置组织跟踪将事件发送到Amazon CloudWatch Logs。<br>这个选项是正确的。CloudTrail组织跟踪是整个解决方案的基础，它可以记录所有组织账户中的API调用，包括root用户的登录活动。将CloudTrail事件发送到CloudWatch Logs是后续进行日志分析、创建警报和仪表板的前提条件。没有这一步，其他选项都无法工作。</td>
                    <td>BCDE</td>
                </tr>
                <tr>
                    <td>231</td>
                    <td>A company uses AWS Organizations to manage its AWS accounts. A DevOps engineer must ensure that all users who access the AWS<br>Management Console are authenticated through the company’s corporate identity provider (IdP).</td>
                    <td>A. Use Amazon GuardDuty with a delegated administrator account Use GuardDuty to enforce denial of IAM user logins.<br>B. Use AWS IAM Identity Center to congure identity federation with SAML 2.0.<br>C. Create a permissions boundary in AWS IAM Identity Center to deny password logins for IAM users.<br>D. Create IAM groups in the Organizations management account to apply consistent permissions for all IAM users.<br>E. Create an SCP in Organizations to deny password creation for IAM users.</td>
                    <td>一家公司使用AWS Organizations来管理其AWS账户。DevOps工程师必须确保所有访问AWS管理控制台的用户都通过公司的企业身份提供商(IdP)进行身份验证。</td>
                    <td>A. 使用Amazon GuardDuty与委托管理员账户，使用GuardDuty强制拒绝IAM用户登录。<br>这个选项不正确。GuardDuty是一个威胁检测服务，主要用于监控恶意活动和异常行为，而不是用于身份验证或访问控制。GuardDuty无法强制执行身份验证策略或阻止IAM用户登录，它的功能范围不包括身份管理。<br><br>B. 使用AWS IAM Identity Center配置与SAML 2.0的身份联合。<br>这个选项正确。IAM Identity Center（原AWS SSO）专门设计用于集中管理身份和访问权限，支持与企业IdP的SAML 2.0联合身份验证。通过配置身份联合，可以确保用户必须通过企业IdP进行身份验证才能访问AWS资源，这正是题目要求的解决方案。<br><br>C. 在AWS IAM Identity Center中创建权限边界以拒绝IAM用户的密码登录。<br>这个选项部分正确。权限边界可以限制IAM用户的权限，包括阻止密码登录。通过设置适当的权限边界，可以强制用户只能通过联合身份验证访问，而不能使用本地IAM用户密码登录。<br><br>D. 在Organizations管理账户中创建IAM组，为所有IAM用户应用一致的权限。<br>这个选项不正确。虽然IAM组可以管理权限，但它们无法强制用户通过企业IdP进行身份验证。IAM组只是权限管理工具，不能解决身份验证源的问题。<br><br>E. 在Organizations中创建SCP以拒绝IAM用户的密码创建。<br>这个选项部分有效但不完整。SCP可以阻止创建新的IAM用户密码，但无法处理现有的IAM用户，也不能直接强制使用企业IdP进行身份验证。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>232</td>
                    <td>A company has deployed a new platform that runs on Amazon Elastic Kubernetes Service (Amazon EKS). The new platform hosts web<br>applications that users frequently update. The application developers build the Docker images for the applications and deploy the Docker<br>images manually to the platform.<br>The platform usage has increased to more than 500 users every day. Frequent updates, building the updated Docker images for the<br>applications, and deploying the Docker images on the platform manually have all become dicult to manage.<br>The company needs to receive an Amazon Simple Notication Service (Amazon SNS) notication if Docker image scanning returns any HIGH<br>or CRITICAL ndings for operating system or programming language package vulnerabilities.</td>
                    <td>A. Create an AWS CodeCommit repository to store the Dockerle and Kubernetes deployment les. Create a pipeline in AWS CodePipeline.<br>Use an Amazon S3 event to invoke the pipeline when a newer version of the Dockerle is committed. Add a step to the pipeline to initiate<br>the AWS CodeBuild project.<br>B. Create an AWS CodeCommit repository to store the Dockerle and Kubernetes deployment les. Create a pipeline in AWS CodePipeline.<br>Use an Amazon EventBridge event to invoke the pipeline when a newer version of the Dockerle is committed. Add a step to the pipeline<br>to initiate the AWS CodeBuild project.<br>C. Create an AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry<br>(Amazon ECR) repository. Turn on basic scanning for the ECR repository. Create an Amazon EventBridge rule that monitors Amazon<br>GuardDuty events. Congure the EventBridge rule to send an event to an SNS topic when the nding-severity-counts parameter is more<br>than 0 at a CRITICAL or HIGH level.<br>D. Create an AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry<br>(Amazon ECR) repository. Turn on enhanced scanning for the ECR repository. Create an Amazon EventBridge rule that monitors ECR image<br>scan events. Congure the EventBridge rule to send an event to an SNS topic when the nding-severity-counts parameter is more than 0<br>at a CRITICAL or HIGH level.<br>E. Create an AWS CodeBuild project that scans the Dockerle. Congure the project to build the Docker images and store the Docker<br>images in an Amazon Elastic Container Registry (Amazon ECR) repository if the scan is successful. Congure an SNS topic to provide<br>notication if the scan returns any vulnerabilities.</td>
                    <td>一家公司在Amazon Elastic Kubernetes Service (Amazon EKS)上部署了一个新平台。该新平台托管着用户经常更新的Web应用程序。应用程序开发人员构建应用程序的Docker镜像，并手动将Docker镜像部署到平台上。<br><br>平台使用量已增加到每天超过500个用户。频繁的更新、为应用程序构建更新的Docker镜像以及在平台上手动部署Docker镜像都变得难以管理。<br><br>公司需要在Docker镜像扫描返回操作系统或编程语言包漏洞的任何HIGH或CRITICAL发现时，接收Amazon Simple Notification Service (Amazon SNS)通知。</td>
                    <td>选项A：创建AWS CodeCommit存储库来存储Dockerfile和Kubernetes部署文件，创建CodePipeline管道，使用Amazon S3事件在提交新版本Dockerfile时调用管道。这个方案的问题是使用S3事件来触发CodeCommit的变更是不正确的架构设计，CodeCommit的变更应该直接触发管道，而且没有涉及镜像扫描和漏洞通知的具体实现。<br><br>选项B：与选项A类似，但使用EventBridge事件来调用管道。虽然EventBridge可以监控CodeCommit事件，但这个方案仍然没有具体说明如何实现Docker镜像的漏洞扫描和SNS通知功能，缺少关键的安全扫描组件。<br><br>选项C：创建CodeBuild项目构建Docker镜像并存储到ECR，启用基础扫描，创建EventBridge规则监控GuardDuty事件。问题在于GuardDuty主要用于威胁检测而不是容器镜像漏洞扫描，而且基础扫描功能相对有限，不如增强扫描全面。<br><br>选项D：创建CodeBuild项目构建Docker镜像并存储到ECR，启用增强扫描，创建EventBridge规则监控ECR镜像扫描事件。这是最完整的方案，ECR增强扫描可以检测操作系统和编程语言包漏洞，EventBridge可以正确监控扫描结果并触发SNS通知。<br><br>选项E：创建CodeBuild项目扫描Dockerfile，成功后构建镜像存储到ECR。这个方案描述过于简化，没有明确说明如何实现持续的漏洞扫描和通知机制，而且扫描Dockerfile本身不如扫描完整的镜像全面。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>233</td>
                    <td>A company groups its AWS accounts in OUs in an organization in AWS Organizations. The company has deployed a set of Amazon API Gateway<br>APIs in one of the Organizations accounts. The APIs are bound to the account&#x27;s VPC and have no existing authentication mechanism. Only<br>principals in a specic OU can have permissions to invoke the APIs.<br>The company applies the following policy to the API Gateway interface VPC endpoint:<br>The company also updates the API Gateway resource policies to deny invocations that do not come through the interface VPC endpoint. After<br>the updates, the following error message appears during attempts to use the interface VPC endpoint URL to invoke an API: &quot;User: anonymous<br>is not authorized.&quot;</td>
                    <td>A. Enable IAM authentication on all API methods by setting AWS JAM as the authorization method.<br>B. Create a token-based AWS Lambda authorizer that passes the caller&#x27;s identity in a bearer token.<br>C. Create a request parameter-based AWS Lambda authorizer that passes the caller&#x27;s identity in a combination of headers, query string<br>parameters, stage variables, and $cortext variables.<br>D. Use Amazon Cognito user pools as the authorizer to control access to the API.<br>E. Verify the identity of the requester by using Signature Version 4 to sign client requests by using AWS credentials.</td>
                    <td>一家公司在AWS Organizations中将其AWS账户分组到组织单位(OU)中。该公司在其中一个Organizations账户中部署了一组Amazon API Gateway API。这些API绑定到账户的VPC，并且没有现有的身份验证机制。只有特定OU中的主体才能拥有调用这些API的权限。<br>公司将以下策略应用到API Gateway接口VPC端点：<br>公司还更新了API Gateway资源策略，以拒绝不通过接口VPC端点的调用。更新后，在尝试使用接口VPC端点URL调用API时出现以下错误消息：&quot;User: anonymous is not authorized.&quot;</td>
                    <td>A. 通过将AWS IAM设置为授权方法在所有API方法上启用IAM身份验证 - 这是一个可行的解决方案，因为错误显示用户是匿名的，需要身份验证机制。IAM身份验证可以验证调用者身份并授权访问。<br><br>B. 创建基于令牌的AWS Lambda授权器，在承载令牌中传递调用者的身份 - 这也是有效的解决方案。Lambda授权器可以验证令牌并确定调用者身份，解决匿名用户问题。<br><br>C. 创建基于请求参数的AWS Lambda授权器，通过标头、查询字符串参数、阶段变量和$context变量的组合传递调用者身份 - 虽然技术上可行，但比选项B复杂，且不是最佳实践。<br><br>D. 使用Amazon Cognito用户池作为授权器来控制对API的访问 - 这主要用于用户身份验证，但题目强调的是组织内部的主体访问，不太适合这种企业内部场景。<br><br>E. 通过使用AWS凭证的签名版本4签署客户端请求来验证请求者身份 - 这是AWS服务间通信的标准方法，可以有效验证调用者身份并解决匿名访问问题。</td>
                    <td>BE</td>
                </tr>
                <tr>
                    <td>234</td>
                    <td>A company wants to decrease the time it takes to develop new features. The company uses AWS CodeBuild and AWS CodeDeploy to build and<br>deploy its applications. The company uses AWS CodePipeline to deploy each microservice with its own CI/CD pipeline.<br>The company needs more visibility into the average time between the release of new features and the average time to recover after a failed<br>deployment.</td>
                    <td>A. Program an AWS Lambda function that creates Amazon CloudWatch custom metrics with information about successful runs and failed<br>runs for each pipeline. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. Use the metrics to build a<br>CloudWatch dashboard.<br>B. <br>C. Program an AWS Lambda function that writes information about successful runs and failed runs to Amazon DynamoD<br>D. Program an AWS Lambda function that writes information about successful runs and failed runs to Amazon DynamoD</td>
                    <td>一家公司希望减少开发新功能所需的时间。该公司使用AWS CodeBuild和AWS CodeDeploy来构建和部署应用程序。公司使用AWS CodePipeline为每个微服务部署各自的CI/CD管道。<br>公司需要更多的可见性来了解新功能发布的平均时间以及部署失败后的平均恢复时间。</td>
                    <td>选项A：编写一个AWS Lambda函数，创建Amazon CloudWatch自定义指标，包含每个管道成功运行和失败运行的信息。创建Amazon EventBridge规则每5分钟调用一次Lambda函数。使用这些指标构建CloudWatch仪表板。这个方案能够提供实时监控和可视化，CloudWatch指标和仪表板是监控CI/CD管道性能的标准做法，可以有效跟踪部署时间和恢复时间。<br><br>选项B：题目中显示为空白，无法进行分析。<br><br>选项C：编写一个AWS Lambda函数，将成功运行和失败运行的信息写入Amazon DynamoDB。虽然DynamoDB可以存储数据，但这个方案缺少可视化组件，不能直接提供管道性能的可见性，需要额外的工具来分析和展示数据。<br><br>选项D：与选项C相同，编写Lambda函数将信息写入DynamoDB，同样存在缺少可视化和分析工具的问题。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>235</td>
                    <td>A company has developed a static website hosted on an Amazon S3 bucket. The website is deployed using AWS CloudFormation. The<br>CloudFormation template denes an S3 bucket and a custom resource that copies content into the bucket from a source location.<br>The company has decided that it needs to move the website to a new location, so the existing CloudFormation stack must be deleted and re-<br>created. However, CloudFormation reports that the stack could not be deleted cleanly.</td>
                    <td>A. Deletion has failed because the S3 bucket has an active website conguration. Modify the CloudFormation template to remove the<br>WebsiteConguration property from the S3 bucket resource.<br>B. Deletion has failed because the S3 bucket is not empty. Modify the custom resource&#x27;s AWS Lambda function code to recursively empty<br>the bucket when RequestType is Delete.<br>C. Deletion has failed because the custom resource does not dene a deletion policy. Add a DeletionPolicy property to the custom<br>resource denition with a value of RemoveOnDeletion.<br>D. Deletion has failed because the S3 bucket is not empty. Modify the S3 bucket resource in the CloudFormation template to add a<br>DeletionPolicy property with a value of Empty.</td>
                    <td>一家公司开发了一个托管在Amazon S3存储桶上的静态网站。该网站使用AWS CloudFormation进行部署。CloudFormation模板定义了一个S3存储桶和一个自定义资源，该自定义资源将内容从源位置复制到存储桶中。<br>公司决定需要将网站迁移到新位置，因此必须删除并重新创建现有的CloudFormation堆栈。但是，CloudFormation报告无法干净地删除堆栈。</td>
                    <td>A. 删除失败是因为S3存储桶有活跃的网站配置。修改CloudFormation模板以从S3存储桶资源中移除WebsiteConfiguration属性。这个选项是错误的，因为S3存储桶的网站配置不会阻止CloudFormation删除堆栈。网站配置只是存储桶的一个属性设置，不会影响删除操作。<br><br>B. 删除失败是因为S3存储桶不为空。修改自定义资源的AWS Lambda函数代码，在RequestType为Delete时递归清空存储桶。这个选项是正确的，因为CloudFormation无法删除非空的S3存储桶。当自定义资源在创建时向存储桶中复制了内容，删除时必须先清空存储桶才能成功删除整个堆栈。<br><br>C. 删除失败是因为自定义资源没有定义删除策略。向自定义资源定义添加DeletionPolicy属性，值为RemoveOnDeletion。这个选项是错误的，因为RemoveOnDeletion不是有效的DeletionPolicy值，而且自定义资源的删除行为应该在Lambda函数中处理。<br><br>D. 删除失败是因为S3存储桶不为空。修改CloudFormation模板中的S3存储桶资源，添加DeletionPolicy属性，值为Empty。这个选项是错误的，因为Empty不是有效的DeletionPolicy值，有效值包括Delete、Retain和Snapshot。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>236</td>
                    <td>A company uses Amazon EC2 as its primary compute platform. A DevOps team wants to audit the company&#x27;s EC2 instances to check whether<br>any prohibited applications have been installed on the EC2 instances.</td>
                    <td>A. Congure AWS Systems Manager on each instance. Use AWS Systems Manager Inventory. Use Systems Manager resource data sync to<br>synchronize and store ndings in an Amazon S3 bucket. Create an AWS Lambda function that runs when new objects are added to the S3<br>bucket. Congure the Lambda function to identify prohibited applications.<br>B. Congure AWS Systems Manager on each instance. Use Systems Manager Inventory Create AWS Cong rules that monitor changes<br>from Systems Manager Inventory to identify prohibited applications.<br>C. Congure AWS Systems Manager on each instance. Use Systems Manager Inventory. Filter a trail in AWS CloudTrail for Systems<br>Manager Inventory events to identify prohibited applications.<br>D. Designate Amazon CloudWatch Logs as the log destination for all application instances. Run an automated script across all instances<br>to create an inventory of installed applications. Congure the script to forward the results to CloudWatch Logs. Create a CloudWatch<br>alarm that uses lter patterns to search log data to identify prohibited applications.</td>
                    <td>一家公司使用Amazon EC2作为其主要计算平台。DevOps团队希望审计公司的EC2实例，以检查是否在EC2实例上安装了任何禁用的应用程序。</td>
                    <td>选项A：在每个实例上配置AWS Systems Manager。使用AWS Systems Manager Inventory。使用Systems Manager资源数据同步将发现结果同步并存储在Amazon S3存储桶中。创建一个AWS Lambda函数，当新对象添加到S3存储桶时运行。配置Lambda函数来识别禁用的应用程序。这个方案技术上可行，但过于复杂，需要额外的Lambda函数和S3存储桶，增加了架构复杂性和成本。虽然能够实现目标，但不是最优解决方案。<br><br>选项B：在每个实例上配置AWS Systems Manager。使用Systems Manager Inventory。创建AWS Config规则来监控Systems Manager Inventory的变化，以识别禁用的应用程序。这是一个优雅的解决方案，直接利用AWS Config的合规性监控功能，可以持续监控库存变化并自动检测违规应用程序，无需额外的存储或计算资源。<br><br>选项C：在每个实例上配置AWS Systems Manager。使用Systems Manager Inventory。在AWS CloudTrail中过滤Systems Manager Inventory事件的跟踪来识别禁用的应用程序。CloudTrail主要用于记录API调用和管理事件，不是用于分析应用程序库存内容的合适工具，这种方法无法有效识别具体的禁用应用程序。<br><br>选项D：将Amazon CloudWatch Logs指定为所有应用程序实例的日志目标。在所有实例上运行自动化脚本来创建已安装应用程序的库存。配置脚本将结果转发到CloudWatch Logs。创建使用过滤模式搜索日志数据的CloudWatch警报来识别禁用的应用程序。这种方法需要自定义脚本开发和维护，增加了运维复杂性，而且不如使用AWS原生服务高效。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>237</td>
                    <td>A company has an event-driven JavaScript application. The application uses decoupled AWS managed services that publish, consume, and<br>route events. During application testing, events are not delivered to the target that is specied by an Amazon EventBridge rule.<br>A DevOps team must provide application testers with additional functionality to view, troubleshoot, and prevent the loss of events without<br>redeployment of the application.</td>
                    <td>A. Launch AWS Device Farm with a standard test environment and project to run a specic build of the application.<br>B. Create an Amazon S3 bucket. Enable AWS CloudTrail. Create a CloudTrail trail that species the S3 bucket as the storage location.<br>C. Congure the EventBridge rule to use an Amazon Simple Queue Service (Amazon SQS) standard queue as a dead-letter queue.<br>D. Congure the EventBridge rule to use an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a dead-letter queue.<br>E. Create a log group in Amazon CloudWatch Logs Specify the log group as an additional target of the EventBridge rule.<br>F. Update the application code base to use the AWS X-Ray SDK tracing feature to instrument the code with support for the X-Amzn-Trace-Id<br>header.</td>
                    <td>一家公司有一个事件驱动的JavaScript应用程序。该应用程序使用解耦的AWS托管服务来发布、消费和路由事件。在应用程序测试期间，事件没有被传递到Amazon EventBridge规则指定的目标。DevOps团队必须为应用程序测试人员提供额外的功能来查看、故障排除和防止事件丢失，而无需重新部署应用程序。</td>
                    <td>A. 启动AWS Device Farm并使用标准测试环境和项目来运行应用程序的特定构建版本 - 这个选项不正确。AWS Device Farm主要用于移动应用程序的测试，不适用于解决EventBridge事件传递问题，也无法帮助查看和防止事件丢失。<br><br>B. 创建Amazon S3存储桶，启用AWS CloudTrail，创建指定S3存储桶作为存储位置的CloudTrail跟踪 - 这个选项部分有用但不是最佳解决方案。CloudTrail可以记录API调用，但对于实时事件监控和防止事件丢失来说不够直接有效。<br><br>C. 配置EventBridge规则使用Amazon SQS标准队列作为死信队列 - 这个选项正确。死信队列可以捕获未能成功传递到目标的事件，防止事件丢失，并允许测试人员查看和分析失败的事件。<br><br>D. 配置EventBridge规则使用Amazon SQS FIFO队列作为死信队列 - 这个选项也正确。FIFO队列提供了与标准队列相同的死信队列功能，同时还保证了事件的顺序和去重，这对于某些应用场景可能更重要。<br><br>E. 在Amazon CloudWatch Logs中创建日志组，将日志组指定为EventBridge规则的附加目标 - 这个选项正确。通过将CloudWatch Logs作为附加目标，可以记录所有事件，提供实时监控和故障排除能力，无需重新部署应用程序。<br><br>F. 更新应用程序代码库以使用AWS X-Ray SDK跟踪功能，通过支持X-Amzn-Trace-Id头来检测代码 - 这个选项需要重新部署应用程序，不符合题目要求的&quot;无需重新部署&quot;条件。</td>
                    <td>CE</td>
                </tr>
                <tr>
                    <td>238</td>
                    <td>A company is migrating its container-based workloads to an AWS Organizations multi-account environment. The environment consists of<br>application workload accounts that the company uses to deploy and run the containerized workloads. The company has also provisioned a<br>shared services account for shared workloads in the organization.<br>The company must follow strict compliance regulations. All container images must receive security scanning before they are deployed to any<br>environment. Images can be consumed by downstream deployment mechanisms after the images pass a scan with no critical vulnerabilities.<br>Pre-scan and post-scan images must be isolated from one another so that a deployment can never use pre-scan images.<br>A DevOps engineer needs to create a strategy to centralize this process.</td>
                    <td>A. Create Amazon Elastic Container Registry (Amazon ECR) repositories in the shared services account: one repository for each pre-scan<br>image and one repository for each post-scan image. Congure Amazon ECR image scanning to run on new image pushes to the pre-scan<br>repositories. Use resource-based policies to grant the organization write access to the pre-scan repositories and read access to the post-<br>scan repositories.<br>B. Create pre-scan Amazon Elastic Container Registry (Amazon ECR) repositories in each account that publishes container images. Create<br>repositories for post-scan images in the shared services account. Congure Amazon ECR image scanning to run on new image pushes to<br>the pre-scan repositories. Use resource-based policies to grant the organization read access to the post-scan repositories.<br>C. Congure image replication for each image from the image&#x27;s pre-scan repository to the image&#x27;s post-scan repository.<br>D. Create a pipeline in AWS CodePipeline for each pre-scan repository. Create a source stage that runs when new images are pushed to<br>the pre-scan repositories. Create a stage that uses AWS CodeBuild as the action provider. Write a buildspec.yaml denition that<br>determines the image scanning status and pushes images without critical vulnerabilities to the post-scan repositories.<br>E. Create an AWS Lambda function. Create an Amazon EventBridge rule that reacts to image scanning completed events and invokes the<br>Lambda function. Write function code that determines the image scanning status and pushes images without critical vulnerabilities to the<br>post-scan repositories.</td>
                    <td>一家公司正在将其基于容器的工作负载迁移到AWS Organizations多账户环境中。该环境包含应用程序工作负载账户，公司使用这些账户来部署和运行容器化工作负载。公司还为组织中的共享工作负载配置了一个共享服务账户。<br><br>公司必须遵循严格的合规法规。所有容器镜像在部署到任何环境之前都必须接受安全扫描。镜像在通过扫描且没有关键漏洞后，才能被下游部署机制使用。扫描前和扫描后的镜像必须相互隔离，以确保部署永远不会使用扫描前的镜像。<br><br>DevOps工程师需要创建一个策略来集中化这个过程。</td>
                    <td>选项A：在共享服务账户中创建Amazon ECR存储库，为每个扫描前镜像和扫描后镜像分别创建存储库。配置Amazon ECR镜像扫描在新镜像推送到扫描前存储库时运行。使用基于资源的策略授予组织对扫描前存储库的写入访问权限和对扫描后存储库的读取访问权限。这个选项提供了良好的集中化管理和隔离机制，符合合规要求。<br><br>选项B：在每个发布容器镜像的账户中创建扫描前ECR存储库，在共享服务账户中创建扫描后镜像存储库。这种方法分散了扫描前存储库的管理，不够集中化，增加了管理复杂性。<br><br>选项C：配置从扫描前存储库到扫描后存储库的镜像复制。这个选项没有提供扫描验证机制，会将所有镜像（包括有漏洞的）都复制过去，不符合安全要求。<br><br>选项D：为每个扫描前存储库在AWS CodePipeline中创建管道。创建源阶段，当新镜像推送到扫描前存储库时运行。创建使用AWS CodeBuild作为操作提供者的阶段，编写buildspec.yaml定义来确定镜像扫描状态，并将没有关键漏洞的镜像推送到扫描后存储库。这提供了自动化的扫描验证和镜像转移机制。<br><br>选项E：创建AWS Lambda函数和Amazon EventBridge规则，响应镜像扫描完成事件并调用Lambda函数。编写函数代码确定镜像扫描状态，将没有关键漏洞的镜像推送到扫描后存储库。这也是一个有效的自动化解决方案，但相比CodePipeline可能在复杂工作流管理方面稍逊一筹。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>239</td>
                    <td>A company uses an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to deploy its web applications on containers. The web<br>applications contain condential data that cannot be decrypted without specic credentials.<br>A DevOps engineer has stored the credentials in AWS Secrets Manager. The secrets are encrypted by an AWS Key Management Service (AWS<br>KMS) customer managed key. A Kubernetes service account for a third-party tool makes the secrets available to the applications. The service<br>account assumes an IAM role that the company created to access the secrets.<br>The service account receives an Access Denied (403 Forbidden) error while trying to retrieve the secrets from Secrets Manager.</td>
                    <td>A. The IAM role that is attached to the EKS cluster does not have access to retrieve the secrets from Secrets Manager.<br>B. The key policy for the customer managed key does not allow the Kubernetes service account IAM role to use the key.<br>C. The key policy for the customer managed key does not allow the EKS cluster IAM role to use the key.<br>D. The IAM role that is assumed by the Kubernetes service account does not have permission to access the EKS cluster.</td>
                    <td>一家公司使用Amazon Elastic Kubernetes Service (Amazon EKS)集群在容器上部署其Web应用程序。这些Web应用程序包含机密数据，如果没有特定的凭证就无法解密。<br>一名DevOps工程师已将凭证存储在AWS Secrets Manager中。这些密钥由AWS Key Management Service (AWS KMS)客户管理密钥加密。第三方工具的Kubernetes服务账户使这些密钥对应用程序可用。该服务账户承担公司创建的IAM角色来访问这些密钥。<br>该服务账户在尝试从Secrets Manager检索密钥时收到访问拒绝(403 Forbidden)错误。</td>
                    <td>A. 附加到EKS集群的IAM角色没有从Secrets Manager检索密钥的访问权限。<br>这个选项不正确，因为题目明确说明是Kubernetes服务账户承担IAM角色来访问密钥，而不是EKS集群的IAM角色直接访问Secrets Manager。访问Secrets Manager的是服务账户承担的IAM角色，而不是集群本身的角色。<br><br>B. 客户管理密钥的密钥策略不允许Kubernetes服务账户IAM角色使用该密钥。<br>这个选项是正确的。由于密钥存储在Secrets Manager中并由KMS客户管理密钥加密，要成功检索和解密这些密钥，服务账户承担的IAM角色不仅需要访问Secrets Manager的权限，还需要在KMS密钥策略中被明确授权使用该加密密钥。如果密钥策略中没有允许该IAM角色使用KMS密钥，就会导致403错误。<br><br>C. 客户管理密钥的密钥策略不允许EKS集群IAM角色使用该密钥。<br>这个选项不正确，因为访问密钥的不是EKS集群的IAM角色，而是Kubernetes服务账户承担的特定IAM角色。集群角色与访问Secrets Manager的操作无直接关系。<br><br>D. 由Kubernetes服务账户承担的IAM角色没有访问EKS集群的权限。<br>这个选项不正确，因为服务账户已经在集群内运行，问题不是访问集群的权限，而是访问外部AWS服务(Secrets Manager)的权限问题。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>240</td>
                    <td>A company is migrating its product development teams from an on-premises data center to a hybrid environment. The new environment will<br>add four AWS Regions and will give the developers the ability to use the Region that is geographically closest to them.<br>All the development teams use a shared set of Linux applications. The on-premises data center stores the applications on a NetApp ONTAP<br>storage device. The storage volume is mounted read-only on the development on-premises VMs. The company updates the applications on<br>the shared volume once a week.<br>A DevOps engineer needs to replicate the data to all the new Regions. The DevOps engineer must ensure that the data is always up to date<br>with deduplication. The data also must not be dependent on the availability of the on-premises storage device.</td>
                    <td>A. Create an Amazon S3 File Gateway in the on-premises data center. Create S3 buckets in each Region. Set up a cron job to copy the data<br>from the storage device to the S3 File Gateway. Set up S3 Cross-Region Replication (CRR) to the S3 buckets in each Region.<br>B. Create an Amazon FSx File Gateway in one Region. Create le servers in Amazon FSx for Windows File Server in each Region. Set up a<br>cron job to copy the data from the storage device to the FSx File Gateway.<br>C. Create Multi-AZ Amazon FSx for NetApp ONTAP instances and volumes in each Region. Congure a scheduled SnapMirror relationship<br>between the on-premises storage device and the FSx for ONTAP instances.<br>D. Create an Amazon Elastic File System (Amazon EFS) le system in each Region. Deploy an AWS DataSync agent in the on-premises data<br>center. Congure a schedule for DataSync to copy the data to Amazon EFS daily.</td>
                    <td>一家公司正在将其产品开发团队从本地数据中心迁移到混合环境。新环境将添加四个AWS区域，并让开发人员能够使用地理位置最接近他们的区域。<br>所有开发团队都使用一套共享的Linux应用程序。本地数据中心将应用程序存储在NetApp ONTAP存储设备上。存储卷以只读方式挂载在本地开发虚拟机上。公司每周更新一次共享卷上的应用程序。<br>DevOps工程师需要将数据复制到所有新区域。DevOps工程师必须确保数据始终保持最新状态并具有重复数据删除功能。数据也不能依赖于本地存储设备的可用性。</td>
                    <td>A. 在本地数据中心创建Amazon S3文件网关，在每个区域创建S3存储桶，设置定时任务将数据从存储设备复制到S3文件网关，然后设置S3跨区域复制。这个方案可以实现数据复制和去重，但S3文件网关主要用于文件共享场景，对于Linux应用程序的挂载使用可能不够理想。<br><br>B. 在一个区域创建Amazon FSx文件网关，在每个区域创建FSx for Windows文件服务器。这个选项有明显问题：题目明确提到使用Linux应用程序，而FSx for Windows File Server是为Windows环境设计的，不适合Linux环境使用。<br><br>C. 在每个区域创建多可用区Amazon FSx for NetApp ONTAP实例和卷，配置本地存储设备与FSx ONTAP实例之间的定时SnapMirror关系。这个方案技术上可行，因为保持了NetApp ONTAP的兼容性，但成本较高，且SnapMirror主要用于灾难恢复而非日常数据同步。<br><br>D. 在每个区域创建Amazon EFS文件系统，在本地数据中心部署AWS DataSync代理，配置DataSync每日将数据复制到Amazon EFS的计划。EFS原生支持Linux环境，DataSync提供高效的数据传输和同步功能，支持重复数据删除，且一旦数据复制到EFS后就不依赖本地存储设备。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>241</td>
                    <td>A company has an application that stores data that includes personally identiable information (PII) in an Amazon S3 bucket. All data is<br>encrypted with AWS Key Management Service (AWS KMS) customer managed keys. All AWS resources are deployed from an AWS<br>CloudFormation template.<br>A DevOps engineer needs to set up a development environment for the application in a different AWS account. The data in the development<br>environment&#x27;s S3 bucket needs to be updated once a week from the production environment&#x27;s S3 bucket.<br>The company must not move PII from the production environment without anonymizing the PII rst. The data in each environment must be<br>encrypted with different KMS customer managed keys.</td>
                    <td>A. Activate Amazon Macie on the S3 bucket in the production account. Create an AWS Step Functions state machine to initiate a discovery<br>job and redact all PII before copying les to the S3 bucket in the development account. Give the state machine tasks decrypt permissions<br>on the KMS key in the production account. Give the state machine tasks encrypt permissions on the KMS key in the development account.<br>B. Set up S3 replication between the production S3 bucket and the development S3 bucket. Activate Amazon Macie on the development<br>S3 bucket. Create an AWS Step Functions state machine to initiate a discovery job and redact all PII as the les are copied to the<br>development S3 bucket. Give the state machine tasks encrypt and decrypt permissions on the KMS key in the development account.<br>C. Set up an S3 Batch Operations job to copy les from the production S3 bucket to the development S3 bucket. In the development<br>account, congure an AWS Lambda function to redact ail PII. Congure S3 Object Lambda to use the Lambda function for S3 GET requests.<br>Give the Lambda function&#x27;s IAM role encrypt and decrypt permissions on the KMS key in the development account.<br>D. Create a development environment from the CloudFormation template in the development account. Schedule an Amazon EventBridge<br>rule to start the AWS Step Functions state machine once a week.<br>E. Create a development environment from the CloudFormation template in the development account. Schedule a cron job on an Amazon<br>EC2 instance to run once a week to start the S3 Batch Operations job.</td>
                    <td>一家公司有一个应用程序，将包括个人身份信息(PII)的数据存储在Amazon S3存储桶中。所有数据都使用AWS密钥管理服务(AWS KMS)客户管理的密钥进行加密。所有AWS资源都从AWS CloudFormation模板部署。<br><br>DevOps工程师需要在不同的AWS账户中为应用程序设置开发环境。开发环境S3存储桶中的数据需要每周从生产环境的S3存储桶更新一次。<br><br>公司不得在未先匿名化PII的情况下将PII从生产环境中移出。每个环境中的数据必须使用不同的KMS客户管理密钥进行加密。</td>
                    <td>选项A：在生产账户的S3存储桶上激活Amazon Macie，创建AWS Step Functions状态机来启动发现作业并在复制文件到开发账户S3存储桶之前编辑所有PII。这个方案技术上可行，能够在数据传输前进行PII匿名化处理，并正确配置了跨账户的KMS权限。<br><br>选项B：在生产和开发S3存储桶之间设置S3复制，在开发S3存储桶上激活Amazon Macie。这个方案存在问题，因为S3复制会直接复制包含PII的原始数据到开发环境，违反了&quot;不得在未先匿名化PII的情况下移出生产环境&quot;的要求。<br><br>选项C：设置S3批处理操作作业复制文件，在开发账户中配置Lambda函数来编辑PII，使用S3 Object Lambda处理GET请求。这个方案同样存在问题，因为原始包含PII的数据会先被复制到开发环境，然后才进行匿名化处理。<br><br>选项D：从CloudFormation模板在开发账户中创建开发环境，安排Amazon EventBridge规则每周启动AWS Step Functions状态机。这提供了基础设施部署和调度机制。<br><br>选项E：从CloudFormation模板创建开发环境，在EC2实例上安排cron作业每周运行启动S3批处理操作作业。这个方案使用了不够现代化的调度方式。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>242</td>
                    <td>A company uses an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host its machine learning (ML) application. As the ML model<br>and the container image size grow, the time that new pods take to start up has increased to several minutes.<br>A DevOps engineer needs to reduce the startup time to seconds. The solution must also reduce the startup time to seconds when the pod<br>runs on nodes that were recently added to the cluster.<br>The DevOps engineer creates an Amazon EventBridge rule that invokes an automation in AWS Systems Manager. The automation prefetches<br>the container images from an Amazon Elastic Container Registry (Amazon ECR) repository when new images are pushed to the repository. The<br>DevOps engineer also congures tags to be applied to the cluster and the node groups.</td>
                    <td>A. Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster&#x27;s control<br>plane nodes. Create a Systems Manager State Manager association that uses the control plane nodes&#x27; tags to prefetch corresponding<br>container images.<br>B. Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster&#x27;s nodes.<br>Create a Systems Manager State Manager association that uses the nodes&#x27; machine size to prefetch corresponding container images.<br>C. Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster&#x27;s nodes.<br>Create a Systems Manager State Manager association that uses the nodes&#x27; tags to prefetch corresponding container images.<br>D. Create an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster&#x27;s control<br>plane nodes. Create a Systems Manager State Manager association that uses the nodes&#x27; tags to prefetch corresponding container<br>images.</td>
                    <td>一家公司使用Amazon Elastic Kubernetes Service (Amazon EKS)集群来托管其机器学习(ML)应用程序。随着ML模型和容器镜像大小的增长，新pod启动所需的时间已增加到几分钟。<br>DevOps工程师需要将启动时间减少到几秒钟。该解决方案还必须在pod运行在最近添加到集群的节点上时，也能将启动时间减少到几秒钟。<br>DevOps工程师创建了一个Amazon EventBridge规则，该规则调用AWS Systems Manager中的自动化。当新镜像推送到仓库时，该自动化从Amazon Elastic Container Registry (Amazon ECR)仓库预取容器镜像。DevOps工程师还配置了要应用于集群和节点组的标签。</td>
                    <td>选项A：创建IAM角色允许EventBridge使用Systems Manager在EKS集群的控制平面节点上运行命令，并使用控制平面节点的标签预取镜像。这个选项错误，因为控制平面节点是AWS管理的，用户无法直接访问或在其上运行命令，且容器镜像应该预取到工作节点而不是控制平面节点。<br><br>选项B：创建IAM角色允许EventBridge使用Systems Manager在EKS集群节点上运行命令，并使用节点的机器大小来预取镜像。虽然目标节点正确（工作节点），但使用机器大小作为预取条件不合理，应该使用标签来更精确地控制哪些镜像需要预取到哪些节点。<br><br>选项C：创建IAM角色允许EventBridge使用Systems Manager在EKS集群节点上运行命令，并使用节点标签来预取镜像。这个选项正确，因为它针对正确的目标（工作节点），并使用标签进行精确的镜像预取控制，符合题目中提到的标签配置要求。<br><br>选项D：创建IAM角色允许EventBridge使用Systems Manager在控制平面节点上运行命令，但使用节点标签预取镜像。这个选项同样错误，因为控制平面节点不可直接访问，且存在逻辑矛盾。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>243</td>
                    <td>A company&#x27;s application has an API that retrieves workload metrics. The company needs to audit, analyze, and visualize these metrics from<br>the application to detect issues at scale.</td>
                    <td>A. Congure an Amazon EventBridge schedule to invoke an AWS Lambda function that calls the API to retrieve workload metrics. Store the<br>workload metric data in an Amazon S3 bucket.<br>B. Congure an Amazon EventBridge schedule to invoke an AWS Lambda function that calls the API to retrieve workload metrics. Store the<br>workload metric data in an Amazon DynamoDB table that has a DynamoDB stream enabled.<br>C. Create an AWS Glue crawler to catalog the workload metric data in the Amazon S3 bucket. Create views in Amazon Athena for the<br>cataloged data.<br>D. Connect an AWS Glue crawler to the Amazon DynamoDB stream to catalog the workload metric data. Create views in Amazon Athena for<br>the cataloged data.<br>E. Create Amazon QuickSight datasets from the Amazon Athena views. Create a QuickSight analysis to visualize the workload metric data<br>as a dashboard.<br>F. Create an Amazon CloudWatch dashboard that has custom widgets that invoke AWS Lambda functions. Congure the Lambda functions<br>to query the workload metrics data from the Amazon Athena views.</td>
                    <td>一家公司的应用程序有一个API用于检索工作负载指标。该公司需要审计、分析和可视化这些来自应用程序的指标，以大规模检测问题。</td>
                    <td>A. 配置Amazon EventBridge计划调度来调用AWS Lambda函数，该函数调用API检索工作负载指标。将工作负载指标数据存储在Amazon S3存储桶中。这个选项提供了一个可靠的数据收集机制，使用EventBridge进行定时调度，Lambda函数执行API调用，S3作为数据湖存储大量指标数据，非常适合后续的分析和处理。<br><br>B. 配置Amazon EventBridge计划调度来调用AWS Lambda函数，该函数调用API检索工作负载指标。将工作负载指标数据存储在启用了DynamoDB流的Amazon DynamoDB表中。虽然这也是一个数据收集方案，但DynamoDB更适合事务性数据而非大量的指标数据存储和分析。<br><br>C. 创建AWS Glue爬虫来编目Amazon S3存储桶中的工作负载指标数据。在Amazon Athena中为编目数据创建视图。这个选项基于选项A的S3存储，使用Glue爬虫自动发现和编目数据结构，然后通过Athena提供SQL查询能力，非常适合大规模数据分析。<br><br>D. 将AWS Glue爬虫连接到Amazon DynamoDB流来编目工作负载指标数据。在Amazon Athena中为编目数据创建视图。这个方案基于选项B，但Glue爬虫与DynamoDB流的集成不如与S3的集成成熟和高效。<br><br>E. 从Amazon Athena视图创建Amazon QuickSight数据集。创建QuickSight分析以将工作负载指标数据可视化为仪表板。这提供了强大的可视化能力，但需要依赖前面的数据存储和查询基础设施。<br><br>F. 创建具有自定义小部件的Amazon CloudWatch仪表板，这些小部件调用AWS Lambda函数。配置Lambda函数从Amazon Athena视图查询工作负载指标数据。这个方案过于复杂，CloudWatch仪表板通过Lambda查询Athena不是最佳实践。</td>
                    <td>ACE</td>
                </tr>
                <tr>
                    <td>244</td>
                    <td>A DevOps engineer is building the infrastructure for an application. The application needs to run on an Amazon Elastic Kubernetes Service<br>(Amazon EKS) cluster that includes Amazon EC2 instances. The EC2 instances need to use an Amazon Elastic File System (Amazon EFS) le<br>system as a storage backend. The Amazon EFS Container Storage Interface (CSI) driver is installed on the EKS cluster.<br>When the DevOps engineer starts the application, the EC2 instances do not mount the EFS le system.</td>
                    <td>A. Switch the EKS nodes from Amazon EC2 to AWS Fargate.<br>B. Add an inbound rule to the EFS le system’s security group to allow NFS trac from the EKS cluster.<br>C. Create an IAM role that allows the Amazon EFS CSI driver to interact with the le system<br>D. Set up AWS DataSync to congure le transfer between the EFS le system and the EKS nodes.<br>E. Create a mount target for the EFS le system in the subnet of the EKS nodes.</td>
                    <td>一名DevOps工程师正在为应用程序构建基础设施。该应用程序需要在包含Amazon EC2实例的Amazon Elastic Kubernetes Service (Amazon EKS)集群上运行。EC2实例需要使用Amazon Elastic File System (Amazon EFS)文件系统作为存储后端。Amazon EFS容器存储接口(CSI)驱动程序已安装在EKS集群上。当DevOps工程师启动应用程序时，EC2实例无法挂载EFS文件系统。</td>
                    <td>A. 将EKS节点从Amazon EC2切换到AWS Fargate。<br>这个选项不正确。切换到Fargate并不能解决EFS挂载问题，而且题目明确要求使用EC2实例。Fargate虽然支持EFS，但这不是解决当前挂载问题的方案，而是改变了整个架构需求。<br><br>B. 在EFS文件系统的安全组中添加入站规则，允许来自EKS集群的NFS流量。<br>这个选项正确。EFS使用NFS协议(端口2049)进行通信。如果安全组没有正确配置允许NFS流量，EC2实例就无法连接到EFS文件系统。这是EFS挂载失败的常见原因之一。<br><br>C. 创建一个IAM角色，允许Amazon EFS CSI驱动程序与文件系统交互。<br>这个选项正确。EFS CSI驱动程序需要适当的IAM权限才能创建、挂载和管理EFS文件系统。没有正确的IAM角色和权限，CSI驱动程序无法执行必要的操作来挂载文件系统。<br><br>D. 设置AWS DataSync来配置EFS文件系统和EKS节点之间的文件传输。<br>这个选项不正确。DataSync是用于数据迁移和同步的服务，不是用于解决文件系统挂载问题的。它不能解决EFS无法挂载到EC2实例的根本问题。<br><br>E. 在EKS节点的子网中为EFS文件系统创建挂载目标。<br>这个选项可能相关但不是主要问题。虽然挂载目标是必需的，但通常在创建EFS时会自动创建，且题目没有提到网络连接问题。</td>
                    <td>BC</td>
                </tr>
                <tr>
                    <td>245</td>
                    <td>A company deploys an application on on-premises devices in the company’s on-premises data center. The company uses an AWS Direct<br>Connect connection between the data center and the company&#x27;s AWS account. During initial setup of the on-premises devices and during<br>application updates, the application needs to retrieve conguration les from an Amazon Elastic File System (Amazon EFS) le system.<br>All trac from the on-premises devices to Amazon EFS must remain private and encrypted. The on-premises devices must follow the principle<br>of least privilege for AWS access. The company&#x27;s DevOps team needs the ability to revoke access from a single device without affecting the<br>access of the other devices.</td>
                    <td>A. Create an IAM role that trust IAM Roles Anywhere. Attach the<br>AmazonElasticFileSystemClientReadWriteAccess to the role. Create an IAM Roles Anywhere prole for the IAM role. Congure the AWS CLI<br>on the on-premises devices to use the aws_signing_helper command to obtain credentials.<br>B. Generate certicates for each on-premises device in AWS Private Certicate Authority. Create a trust anchor in IAM Roles Anywhere that<br>references an AWS Private C<br>C. Create an IAM user that has an access key and a secret key for all devices. Attach the AmazonElasticFileSystemClientReadWriteAccess<br>policy to the IAM user. Congure the AWS CLI on the on-premises devices to use the IAM user&#x27;s access key and secret key.<br>D. Use the amazon-efs-utils package to mount the EFS le system.<br>E. Use the native Linux NFS client to mount the EFS le system.</td>
                    <td>一家公司在其本地数据中心的本地设备上部署应用程序。该公司在数据中心和公司的AWS账户之间使用AWS Direct Connect连接。在本地设备的初始设置期间和应用程序更新期间，应用程序需要从Amazon弹性文件系统(Amazon EFS)文件系统中检索配置文件。从本地设备到Amazon EFS的所有流量必须保持私有和加密。本地设备必须遵循AWS访问的最小权限原则。公司的DevOps团队需要能够撤销单个设备的访问权限而不影响其他设备的访问。</td>
                    <td>A. 创建一个信任IAM Roles Anywhere的IAM角色。将AmazonElasticFileSystemClientReadWriteAccess附加到该角色。为IAM角色创建IAM Roles Anywhere配置文件。在本地设备上配置AWS CLI使用aws_signing_helper命令获取凭证。这个选项符合最小权限原则，允许为每个设备单独管理访问权限，并且可以撤销单个设备的访问而不影响其他设备。<br><br>B. 在AWS私有证书颁发机构中为每个本地设备生成证书。在IAM Roles Anywhere中创建引用AWS私有证书颁发机构的信任锚点。这个选项被截断了，但它与选项A类似，提供了基于证书的身份验证机制，符合安全要求。<br><br>C. 创建一个具有访问密钥和秘密密钥的IAM用户供所有设备使用。将AmazonElasticFileSystemClientReadWriteAccess策略附加到IAM用户。在本地设备上配置AWS CLI使用IAM用户的访问密钥和秘密密钥。这个选项不符合要求，因为所有设备共享同一个IAM用户，无法单独撤销某个设备的访问权限。<br><br>D. 使用amazon-efs-utils包挂载EFS文件系统。这个选项提供了安全的EFS挂载方式，支持加密传输，符合流量加密的要求。<br><br>E. 使用原生Linux NFS客户端挂载EFS文件系统。这个选项不提供加密传输，不符合流量必须加密的要求。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>246</td>
                    <td>A DevOps engineer is setting up an Amazon Elastic Container Service (Amazon ECS) blue/green deployment for an application by using AWS<br>CodeDeploy and AWS CloudFormation. During the deployment window, the application must be highly available and CodeDeploy must shift<br>10% of trac to a new version of the application every minute until all trac is shifted.</td>
                    <td>A. Add an AppSpec le with the CodeDeployDefault.ECSLinearl OPercentEveryl Minutes deployment conguration.<br>B. Add the AWS::CodeDeployBlueGreen transform and the AWS::CodeDeploy::BlueGreen hook parameter with the<br>CodeDeployDefault.ECSLinear10PercentEvery1Minutes deployment conguration.<br>C. Add an AppSpec le with the ECSCanary10Percent5Minutes deployment conguration.<br>D. Add the AWS::CodeDeployBlueGreen transform and the AWS::CodeDepioy::BlueGreen hook parameter with the<br>ECSCanary10Percent5Minutes deployment conguration.</td>
                    <td>一名DevOps工程师正在使用AWS CodeDeploy和AWS CloudFormation为应用程序设置Amazon弹性容器服务(Amazon ECS)蓝/绿部署。在部署窗口期间，应用程序必须保持高可用性，并且CodeDeploy必须每分钟将10%的流量转移到应用程序的新版本，直到所有流量都转移完成。</td>
                    <td>A. 添加一个AppSpec文件，使用CodeDeployDefault.ECSLinear10PercentEvery1Minutes部署配置。这个选项提到了正确的部署配置名称，但是对于CloudFormation中的ECS蓝/绿部署，仅仅使用AppSpec文件是不够的，还需要在CloudFormation模板中使用特定的转换和钩子参数。<br><br>B. 添加AWS::CodeDeployBlueGreen转换和AWS::CodeDeploy::BlueGreen钩子参数，使用CodeDeployDefault.ECSLinear10PercentEvery1Minutes部署配置。这个选项正确地指出了在CloudFormation中实现ECS蓝/绿部署需要的组件：转换、钩子参数和正确的线性部署配置，该配置每分钟转移10%的流量。<br><br>C. 添加一个AppSpec文件，使用ECSCanary10Percent5Minutes部署配置。这个选项使用了Canary部署配置，但Canary部署是先转移10%流量然后等待5分钟再转移剩余90%，不符合题目要求的每分钟转移10%的渐进式部署。<br><br>D. 添加AWS::CodeDeployBlueGreen转换和AWS::CodeDeploy::BlueGreen钩子参数，使用ECSCanary10Percent5Minutes部署配置。虽然CloudFormation组件正确，但部署配置是Canary类型，不符合题目要求的线性渐进部署模式。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>247</td>
                    <td>A company uses an organization in AWS Organizations to manage its AWS accounts. The company&#x27;s DevOps team has developed an AWS<br>Lambda function that calls the Organizations API to create new AWS accounts.<br>The Lambda function runs in the organization&#x27;s management account. The DevOps team needs to move the Lambda function from the<br>management account to a dedicated AWS account. The DevOps team must ensure that the Lambda function has the ability to create new AWS<br>accounts only in Organizations before the team deploys the Lambda function to the new account.</td>
                    <td>A. In the management account, create a new IAM role that has the necessary permission to create new accounts in Organizations. Allow<br>the role to be assumed by the Lambda execution role in the new AWS account. Update the Lambda function code to assume the role when<br>the Lambda function creates new AWS accounts. Update the Lambda execution role to ensure that it has permission to assume the new<br>role.<br>B. In the management account, turn on delegated administration for Organizations. Create a new delegation policy that grants the new<br>AWS account permission to create new AWS accounts in Organizations. Ensure that the Lambda execution role has the<br>organizations:CreateAccount permission.<br>C. In the management account, create a new IAM role that has the necessary permission to create new accounts in Organizations. Allow<br>the role to be assumed by the Lambda service principal. Update the Lambda function code to assume the role when the Lambda function<br>creates new AWS accounts. Update the Lambda execution role to ensure that it has permission to assume the new role.<br>D. In the management account, enable AWS Control Tower. Turn on delegated administration for AWS Control Tower. Create a resource<br>policy that allows the new AWS account to create new AWS accounts in AWS Control Tower. Update the Lambda function code to use the<br>AWS Control Tower API in the new AWS account. Ensure that the Lambda execution role has the controltower:CreateManagedAccount<br>permission.</td>
                    <td>一家公司使用AWS Organizations中的组织来管理其AWS账户。该公司的DevOps团队开发了一个AWS Lambda函数，该函数调用Organizations API来创建新的AWS账户。Lambda函数在组织的管理账户中运行。DevOps团队需要将Lambda函数从管理账户迁移到一个专用的AWS账户。DevOps团队必须确保Lambda函数在部署到新账户之前，仍然具有在Organizations中创建新AWS账户的能力。</td>
                    <td>选项A：在管理账户中创建一个具有在Organizations中创建新账户必要权限的新IAM角色。允许该角色被新AWS账户中的Lambda执行角色承担。更新Lambda函数代码，在创建新AWS账户时承担该角色。更新Lambda执行角色以确保它有权限承担新角色。这个方案是正确的，因为它通过跨账户角色承担的方式，让新账户中的Lambda函数能够获得管理账户的Organizations权限，这是标准的跨账户权限委派方式。<br><br>选项B：在管理账户中为Organizations启用委派管理。创建一个新的委派策略，授予新AWS账户在Organizations中创建新AWS账户的权限。确保Lambda执行角色具有organizations:CreateAccount权限。这个方案不正确，因为Organizations的委派管理功能主要用于将特定服务的管理权限委派给成员账户，但创建账户的权限通常不能通过这种方式委派。<br><br>选项C：在管理账户中创建一个具有在Organizations中创建新账户必要权限的新IAM角色。允许该角色被Lambda服务主体承担。更新Lambda函数代码，在创建新AWS账户时承担该角色。更新Lambda执行角色以确保它有权限承担新角色。这个方案不正确，因为它允许Lambda服务主体直接承担角色，这样会绕过账户边界的安全控制，不是最佳实践。<br><br>选项D：在管理账户中启用AWS Control Tower。为AWS Control Tower启用委派管理。创建一个资源策略，允许新AWS账户在AWS Control Tower中创建新AWS账户。更新Lambda函数代码以在新账户中使用AWS Control Tower API。确保Lambda执行角色具有controltower:CreateManagedAccount权限。这个方案过于复杂，引入了不必要的Control Tower服务，而且题目要求使用Organizations API而不是Control Tower API。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>248</td>
                    <td>A company has deployed an application in a single AWS Region. The application backend uses Amazon DynamoDB tables and Amazon S3<br>buckets.<br>The company wants to deploy the application in a secondary Region. The company must ensure that the data in the DynamoDB tables and the<br>S3 buckets persists across both Regions. The data must also immediately propagate across Regions.</td>
                    <td>A. Implement two-way S3 bucket replication between the primary Region&#x27;s S3 buckets and the secondary Region’s S3 buckets. Convert the<br>DynamoDB tables into global tables. Set the secondary Region as the additional Region.<br>B. Implement S3 Batch Operations copy jobs between the primary Region and the secondary Region for all S3 buckets. Convert the<br>DynamoDB tables into global tables. Set the secondary Region as the additional Region.<br>C. Implement two-way S3 bucket replication between the primary Region&#x27;s S3 buckets and the secondary Region&#x27;s S3 buckets. Enable<br>DynamoDB streams on the DynamoDB tables in both Regions. In each Region, create an AWS Lambda function that subscribes to the<br>DynamoDB streams. Congure the Lambda function to copy new records to the DynamoDB tables in the other Region.<br>D. Implement S3 Batch Operations copy jobs between the primary Region and the secondary Region for all S3 buckets. Enable DynamoDB<br>streams on the DynamoDB tables in both Regions. In each Region, create an AWS Lambda function that subscribes to the DynamoDB<br>streams. Congure the Lambda function to copy new records to the DynamoDB tables in the other Region.</td>
                    <td>一家公司在单个AWS区域部署了一个应用程序。应用程序后端使用Amazon DynamoDB表和Amazon S3存储桶。<br>该公司希望在辅助区域部署应用程序。公司必须确保DynamoDB表和S3存储桶中的数据在两个区域之间持久化。数据还必须立即在区域之间传播。</td>
                    <td>选项A：实施主区域S3存储桶和辅助区域S3存储桶之间的双向S3存储桶复制。将DynamoDB表转换为全局表。将辅助区域设置为附加区域。这个方案使用了AWS原生的S3跨区域复制功能，可以实现近实时的数据同步。DynamoDB全局表是AWS托管的多区域复制解决方案，能够自动处理跨区域的数据同步和冲突解决，满足立即传播的要求。<br><br>选项B：在主区域和辅助区域的所有S3存储桶之间实施S3批处理操作复制作业。将DynamoDB表转换为全局表。S3批处理操作是用于大规模数据处理的服务，但它不是实时的，而是批量处理，无法满足数据必须立即传播的要求。虽然DynamoDB全局表部分正确，但S3部分不符合实时性要求。<br><br>选项C：实施双向S3存储桶复制，启用DynamoDB流，创建Lambda函数订阅流并复制记录到其他区域。虽然技术上可行，但这种自定义解决方案比使用DynamoDB全局表更复杂，需要处理冲突解决、错误处理等问题，而且可能存在数据一致性风险。<br><br>选项D：使用S3批处理操作和自定义Lambda函数方案。结合了选项B和C的缺点，既有S3批处理的非实时性问题，又有自定义DynamoDB复制的复杂性问题。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>249</td>
                    <td>A company has congured Amazon RDS storage autoscaling for its RDS DB instances. A DevOps team needs to visualize the autoscaling<br>events on an Amazon CloudWatch dashboard.</td>
                    <td>A. Create an Amazon EventBridge rule that reacts to RDS storage autoscaling events from RDS events. Create an AWS Lambda function<br>that publishes a CloudWatch custom metric. Congure the EventBridge rule to invoke the Lambda function. Visualize the custom metric<br>by using the CloudWatch dashboard.<br>B. Create a trail by using AWS CloudTrail with management events congured. Congure the trail to send the management events to<br>Amazon CloudWatch Logs. Create a metric lter in CloudWatch Logs to match the RDS storage autoscaling events. Visualize the metric<br>lter by using the CloudWatch dashboard.<br>C. Create an Amazon EventBridge rule that reacts to RDS storage autoscaling events from the RDS events. Create a CloudWatch alarm.<br>Congure the EventBridge rule to change the status of the CloudWatch alarm. Visualize the alarm status by using the CloudWatch<br>dashboard.<br>D. Create a trail by using AWS CloudTrail with data events congured. Congure the trail to send the data events to Amazon CloudWatch<br>Logs. Create a metric lter in CloudWatch Logs to match the RDS storage autoscaling events. Visualize the metric lter by using the<br>CloudWatch dashboard.</td>
                    <td>一家公司已经为其RDS数据库实例配置了Amazon RDS存储自动扩展功能。DevOps团队需要在Amazon CloudWatch仪表板上可视化自动扩展事件。</td>
                    <td>A. 创建一个Amazon EventBridge规则来响应来自RDS事件的RDS存储自动扩展事件。创建一个AWS Lambda函数来发布CloudWatch自定义指标。配置EventBridge规则来调用Lambda函数。使用CloudWatch仪表板可视化自定义指标。这个方案是正确的，因为RDS存储自动扩展事件会作为RDS事件发布到EventBridge，通过Lambda函数可以将这些事件转换为CloudWatch自定义指标，然后在仪表板上进行可视化展示。<br><br>B. 使用AWS CloudTrail创建一个配置了管理事件的跟踪。配置跟踪将管理事件发送到Amazon CloudWatch Logs。在CloudWatch Logs中创建指标过滤器来匹配RDS存储自动扩展事件。使用CloudWatch仪表板可视化指标过滤器。这个方案不正确，因为RDS存储自动扩展事件不是通过CloudTrail管理事件记录的，而是通过RDS事件系统发布的。<br><br>C. 创建一个Amazon EventBridge规则来响应来自RDS事件的RDS存储自动扩展事件。创建一个CloudWatch告警。配置EventBridge规则来改变CloudWatch告警的状态。使用CloudWatch仪表板可视化告警状态。虽然技术上可行，但这种方法不是最佳实践，因为它滥用了告警系统来进行可视化，而不是用于实际的监控告警目的。<br><br>D. 使用AWS CloudTrail创建一个配置了数据事件的跟踪。配置跟踪将数据事件发送到Amazon CloudWatch Logs。在CloudWatch Logs中创建指标过滤器来匹配RDS存储自动扩展事件。使用CloudWatch仪表板可视化指标过滤器。这个方案不正确，因为RDS存储自动扩展事件不是数据事件，也不会通过CloudTrail记录。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>250</td>
                    <td>A company uses containers for its applications. The company learns that some container images are missing required security congurations.<br>A DevOps engineer needs to implement a solution to create a standard base image. The solution must publish the base image weekly to the<br>us-west-2 Region, us-east-2 Region, and eu-central-1 Region.</td>
                    <td>A. Create an EC2 Image Builder pipeline that uses a container recipe to build the image. Congure the pipeline to distribute the image to<br>an Amazon Elastic Container Registry (Amazon ECR) repository in us-west-2. Congure ECR replication from us-west-2 to us-east-2 and<br>from us-east-2 to eu-central-1. Congure the pipeline to run weekly.<br>B. Create an AWS CodePipeline pipeline that uses an AWS CodeBuild project to build the image. Use AWS CodeDeploy to publish the<br>image to an Amazon Elastic Container Registry (Amazon ECR) repository in us-west-2. Congure ECR replication from us-west-2 to us-east-<br>2 and from us-east-2 to eu-central-1. Congure the pipeline to run weekly.<br>C. Create an EC2 Image Builder pipeline that uses a container recipe to build the image. Congure the pipeline to distribute the image to<br>Amazon Elastic Container Registry (Amazon ECR) repositories in all three Regions. Congure the pipeline to run weekly.<br>D. Create an AWS CodePipeline pipeline that uses an AWS CodeBuild project to build the image. Use AWS CodeDeploy to publish the<br>image to Amazon Elastic Container Registry (Amazon ECR) repositories in all three Regions. Congure the pipeline to run weekly.</td>
                    <td>一家公司为其应用程序使用容器。该公司发现一些容器镜像缺少必需的安全配置。DevOps工程师需要实施一个解决方案来创建标准基础镜像。该解决方案必须每周将基础镜像发布到us-west-2区域、us-east-2区域和eu-central-1区域。</td>
                    <td>选项A：使用EC2 Image Builder管道和容器配方构建镜像，将镜像分发到us-west-2的ECR存储库，然后配置ECR复制从us-west-2到us-east-2，再从us-east-2到eu-central-1。这种方案使用了链式复制，增加了复杂性和潜在的故障点，而且复制延迟可能影响镜像的及时可用性。<br><br>选项B：使用CodePipeline和CodeBuild构建镜像，用CodeDeploy发布到us-west-2的ECR，然后配置链式ECR复制。这个方案存在两个问题：首先CodeDeploy不是用于发布容器镜像到ECR的正确服务，其次同样使用了不必要的链式复制架构。<br><br>选项C：使用EC2 Image Builder管道和容器配方构建镜像，直接将镜像分发到所有三个区域的ECR存储库。EC2 Image Builder专门设计用于创建和管理容器镜像，支持直接多区域分发，避免了复制的复杂性和延迟。<br><br>选项D：使用CodePipeline和CodeBuild构建镜像，用CodeDeploy发布到所有三个区域的ECR存储库。虽然覆盖了所有区域，但CodeDeploy不是发布容器镜像到ECR的合适工具，CodeDeploy主要用于应用程序部署而非镜像发布。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>251</td>
                    <td>A DevOps engineer needs to implement a solution to install antivirus software on all the Amazon EC2 instances in an AWS account. The EC2<br>instances run the most recent version of Amazon Linux.<br>The solution must detect all instances and must use an AWS Systems Manager document to install the software if the software is not present.</td>
                    <td>A. Create an association in Systems Manager State Manager. Target all the managed nodes. Include the software in the association.<br>Congure the association to use the Systems Manager document.<br>B. Set up AWS Cong to record all the resources in the account. Create an AWS Cong custom rule to determine if the software is installed<br>on all the EC2 instances. Congure an automatic remediation action that uses the Systems Manager document for noncompliant EC2<br>instances.<br>C. Activate Amazon EC2 scanning on Amazon Inspector to determine if the software is installed on all the EC2 instances. Associate the<br>ndings with the Systems Manager document.<br>D. Create an Amazon EventBridge rule that uses AWS CloudTrail to detect the Runinstances API call. Congure inventory collection in<br>Systems Manager Inventory to determine if the software is installed on the EC2 instances. Associate the Systems Manager inventory with<br>the Systems Manager document.</td>
                    <td>一名DevOps工程师需要实施一个解决方案，在AWS账户中的所有Amazon EC2实例上安装防病毒软件。这些EC2实例运行最新版本的Amazon Linux。<br>该解决方案必须检测所有实例，并且必须使用AWS Systems Manager文档来安装软件（如果软件不存在）。</td>
                    <td>选项A：在Systems Manager State Manager中创建关联。目标为所有托管节点。在关联中包含软件。配置关联使用Systems Manager文档。<br>这个选项是正确的。State Manager是专门用于维护EC2实例配置状态的服务，可以自动检测所有托管节点，并确保指定的软件始终安装在实例上。当软件缺失时，它会自动使用指定的Systems Manager文档进行安装。这完全符合题目要求的自动检测和安装功能。<br><br>选项B：设置AWS Config记录账户中的所有资源。创建AWS Config自定义规则来确定软件是否安装在所有EC2实例上。为不合规的EC2实例配置使用Systems Manager文档的自动修复操作。<br>虽然这个方案在技术上可行，但AWS Config主要用于合规性检查和资源配置监控，不是专门为软件安装和维护设计的。相比State Manager，这种方法更复杂且不是最佳实践。<br><br>选项C：在Amazon Inspector上激活Amazon EC2扫描以确定软件是否安装在所有EC2实例上。将发现结果与Systems Manager文档关联。<br>Amazon Inspector主要用于安全漏洞评估和应用程序安全分析，不是用于软件安装管理的工具。它无法直接与Systems Manager文档集成来执行软件安装任务。<br><br>选项D：创建Amazon EventBridge规则，使用AWS CloudTrail检测RunInstances API调用。在Systems Manager Inventory中配置库存收集以确定软件是否安装在EC2实例上。将Systems Manager库存与Systems Manager文档关联。<br>这个方案过于复杂，需要多个服务协调工作。虽然可以检测新实例和软件状态，但无法提供像State Manager那样的持续状态管理和自动修复功能。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>252</td>
                    <td>A company needs to increase the security of the container images that run in its production environment. The company wants to integrate<br>operating system scanning and programming language package vulnerability scanning for the containers in its CI/CD pipeline. The CI/CD<br>pipeline is an AWS CodePipeline pipeline that includes an AWS CodeBuild build project, AWS CodeDeploy actions, and an Amazon Elastic<br>Container Registry (Amazon ECR) repository.<br>A DevOps engineer needs to add an image scan to the CI/CD pipeline. The CI/CD pipeline must deploy only images without CRITICAL and<br>HIGH ndings into production.</td>
                    <td>A. Use Amazon ECR basic scanning.<br>B. Use Amazon ECR enhanced scanning.<br>C. Congure Amazon ECR to submit a Rejected status to the CI/CD pipeline when the image scan returns CRITICAL or HIGH ndings.<br>D. Congure an Amazon EventBridge rule to invoke an AWS Lambda function when the image scan is completed. Congure the Lambda<br>function to consume the Amazon Inspector scan status and to submit an Approved or Rejected status to the CI/CD pipeline.<br>E. Congure an Amazon EventBridge rule to invoke an AWS Lambda function when the image scan is completed. Congure the Lambda<br>function to consume the Clair scan status and to submit an Approved or Rejected status to the CI/CD pipeline.</td>
                    <td>一家公司需要提高在其生产环境中运行的容器镜像的安全性。该公司希望在其CI/CD流水线中集成操作系统扫描和编程语言包漏洞扫描功能。该CI/CD流水线是一个AWS CodePipeline流水线，包括AWS CodeBuild构建项目、AWS CodeDeploy操作和Amazon弹性容器注册表(Amazon ECR)存储库。<br>DevOps工程师需要向CI/CD流水线添加镜像扫描功能。CI/CD流水线必须仅将没有CRITICAL和HIGH级别发现的镜像部署到生产环境中。</td>
                    <td>A. 使用Amazon ECR基础扫描 - 这个选项不够完整。ECR基础扫描只提供基本的漏洞扫描功能，主要针对已知的CVE漏洞，但缺乏对操作系统和编程语言包的全面扫描能力，且无法自动阻止包含严重漏洞的镜像部署。<br><br>B. 使用Amazon ECR增强扫描 - 这是正确选项。ECR增强扫描集成了Amazon Inspector，能够提供更全面的漏洞扫描，包括操作系统包和编程语言依赖项的扫描，完全满足题目要求的操作系统扫描和编程语言包漏洞扫描需求。<br><br>C. 配置Amazon ECR在镜像扫描返回CRITICAL或HIGH级别发现时向CI/CD流水线提交拒绝状态 - 这个选项在技术上不准确。ECR本身不会直接向CodePipeline提交状态，需要通过其他机制来实现这种集成。<br><br>D. 配置Amazon EventBridge规则在镜像扫描完成时调用AWS Lambda函数。配置Lambda函数消费Amazon Inspector扫描状态并向CI/CD流水线提交批准或拒绝状态 - 这是正确选项。这提供了完整的自动化解决方案，通过EventBridge监听扫描完成事件，Lambda函数处理扫描结果并控制流水线流程。<br><br>E. 配置Amazon EventBridge规则在镜像扫描完成时调用AWS Lambda函数。配置Lambda函数消费Clair扫描状态并向CI/CD流水线提交批准或拒绝状态 - 这个选项不正确。Clair是第三方扫描工具，而ECR增强扫描使用的是Amazon Inspector，不是Clair。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>253</td>
                    <td>A company&#x27;s DevOps team manages a set of AWS accounts that are in an organization in AWS Organizations.<br>The company needs a solution that ensures that all Amazon EC2 instances use approved AM Is that the DevOps team manages. The solution<br>also must remediate the usage of AMIs that are not approved. The individual account administrators must not be able to remove the<br>restriction to use approved AMIs.</td>
                    <td>A. Use AWS CloudFormation StackSets to deploy an Amazon EventBridge rule to each account. Congure the rule to react to AWS<br>CloudTrail events for Amazon EC2 and to send a notication to an Amazon Simple Notication Service (Amazon SNS) topic. Subscribe the<br>DevOps team to the SNS topic.<br>B. Use AWS CloudFormation StackSets to deploy the approved-amis-by-id AWS Cong managed rule to each account. Congure the rule<br>with the list of approved AMIs. Congure the rule to run the AWS-StopEC2Instance AWS Systems Manager Automation runbook for the<br>noncompliant EC2 instances.<br>C. Create an AWS Lambda function that processes AWS CloudTrail events for Amazon EC2. Congure the Lambda function to send a<br>notication to an Amazon Simple Notication Service (Amazon SNS) topic. Subscribe the DevOps team to the SNS topic. Deploy the<br>Lambda function in each account in the organization. Create an Amazon EventBridge rule in each account. Congure the EventBridge<br>rules to react to AWS CloudTrail events for Amazon EC2 and to invoke the Lambda function.<br>D. Enable AWS Cong across the organization. Create a conformance pack that uses the approved-amis-by-id AWS Cong managed rule<br>with the list of approved AMIs. Deploy the conformance pack across the organization. Congure the rule to run the AWS-StopEC2lnstance<br>AWS Systems Manager Automation runbook for the noncompliant EC2 instances.</td>
                    <td>一家公司的DevOps团队管理着AWS Organizations中组织内的一组AWS账户。公司需要一个解决方案来确保所有Amazon EC2实例都使用DevOps团队管理的已批准AMI。该解决方案还必须修复使用未批准AMI的情况。各个账户管理员不得能够移除使用已批准AMI的限制。</td>
                    <td>选项A：使用AWS CloudFormation StackSets在每个账户部署Amazon EventBridge规则，配置规则响应Amazon EC2的AWS CloudTrail事件并发送通知到SNS主题。这个方案只提供通知功能，没有自动修复机制，不能满足题目要求的修复未批准AMI使用的需求。<br><br>选项B：使用AWS CloudFormation StackSets在每个账户部署approved-amis-by-id AWS Config托管规则，配置已批准AMI列表，并为不合规的EC2实例运行AWS-StopEC2Instance自动化运行手册。这个方案提供了检测和修复功能，但在组织级别的管理和防止账户管理员移除限制方面不如选项D强大。<br><br>选项C：创建Lambda函数处理Amazon EC2的AWS CloudTrail事件，配置函数发送通知到SNS主题，在组织内每个账户部署Lambda函数和EventBridge规则。这个方案需要在每个账户单独部署，管理复杂度高，且同样只提供通知功能，缺乏自动修复能力。<br><br>选项D：在整个组织启用AWS Config，创建使用approved-amis-by-id AWS Config托管规则的合规包，在组织范围内部署合规包，配置规则为不合规的EC2实例运行AWS-StopEC2Instance自动化运行手册。这个方案提供组织级别的统一管理，具备检测、修复和防止移除限制的完整功能。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>254</td>
                    <td>A company gives its employees limited rights to AWS. DevOps engineers have the ability to assume an administrator role. For tracking<br>purposes, the security team wants to receive a near-real-time notication when the administrator role is assumed.<br>How should this be accomplished?</td>
                    <td>A. Congure AWS Cong to publish logs to an Amazon S3 bucket. Use Amazon Athena to query the logs and send a notication to the<br>security team when the administrator role is assumed.<br>B. Congure Amazon GuardDuty to monitor when the administrator role is assumed and send a notication to the security team.<br>C. Create an Amazon EventBridge event rule using an AWS Management Console sign-in events event pattern that publishes a message to<br>an Amazon SNS topic if the administrator role is assumed.<br>D. Create an Amazon EventBridge events rule using an AWS API call that uses an AWS CloudTrail event pattern to invoke an AWS Lambda<br>function that publishes a message to an Amazon SNS topic if the administrator role is assumed.</td>
                    <td>一家公司给员工有限的AWS权限。DevOps工程师有能力承担管理员角色。出于跟踪目的，安全团队希望在管理员角色被承担时收到近实时通知。<br>应该如何实现这一点？</td>
                    <td>选项A：配置AWS Config将日志发布到Amazon S3存储桶，使用Amazon Athena查询日志并在管理员角色被承担时向安全团队发送通知。这个方案存在明显问题：首先AWS Config主要用于监控资源配置变更，不是专门用于监控角色承担事件的最佳工具；其次，使用Athena查询S3中的日志是批处理方式，无法提供近实时的通知能力，存在明显的延迟。<br><br>选项B：配置Amazon GuardDuty监控管理员角色被承担的情况并向安全团队发送通知。GuardDuty是一个威胁检测服务，主要用于识别恶意活动和异常行为，虽然它可能会检测到一些可疑的角色承担活动，但它不是专门为监控正常的角色承担事件而设计的，而且无法保证对所有角色承担事件都能及时检测和通知。<br><br>选项C：使用AWS管理控制台登录事件模式创建Amazon EventBridge事件规则，如果管理员角色被承担则发布消息到Amazon SNS主题。这个选项的问题在于它只监控管理控制台的登录事件，但角色承担可能通过多种方式发生（API调用、CLI等），不仅仅是通过管理控制台，因此监控范围过于局限。<br><br>选项D：使用AWS API调用创建Amazon EventBridge事件规则，该规则使用AWS CloudTrail事件模式调用AWS Lambda函数，如果管理员角色被承担则发布消息到Amazon SNS主题。这是最完整的解决方案：CloudTrail记录所有API调用包括角色承担事件，EventBridge可以实时监控这些事件，Lambda可以处理事件逻辑，SNS提供通知功能，整个流程可以实现近实时监控。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>255</td>
                    <td>A company needs a strategy for failover and disaster recovery of its data and application. The application uses a MySQL database and<br>Amazon EC2 instances. The company requires a maximum RPO of 2 hours and a maximum RTO of 10 minutes for its data and application at<br>all times.</td>
                    <td>A. Create an Amazon Aurora Single-AZ cluster in multiple AWS Regions as the data store. Use Aurora&#x27;s automatic recovery capabilities in<br>the event of a disaster.<br>B. Create an Amazon Aurora global database in two AWS Regions as the data store. In the event of a failure, promote the secondary<br>Region to the primary for the application. Update the application to use the Aurora cluster endpoint in the secondary Region.<br>C. Create an Amazon Aurora cluster in multiple AWS Regions as the data store. Use a Network Load Balancer to balance the database<br>trac in different Regions.<br>D. Set up the application in two AWS Regions. Use Amazon Route 53 failover routing that points to Application Load Balancers in both<br>Regions. Use health checks and Auto Scaling groups in each Region.<br>E. Set up the application in two AWS Regions. Congure AWS Global Accelerator to point to Application Load Balancers (ALBs) in both<br>Regions. Add both ALBs to a single endpoint group. Use health checks and Auto Scaling groups in each Region.</td>
                    <td>一家公司需要制定其数据和应用程序的故障转移和灾难恢复策略。该应用程序使用MySQL数据库和Amazon EC2实例。公司要求其数据和应用程序在任何时候的最大RPO为2小时，最大RTO为10分钟。</td>
                    <td>A. 在多个AWS区域创建Amazon Aurora单可用区集群作为数据存储。在发生灾难时使用Aurora的自动恢复功能。<br>这个选项不正确，因为单可用区集群无法提供跨区域的灾难恢复能力，而且自动恢复功能无法满足10分钟的RTO要求。单AZ部署在区域级别故障时无法提供保护。<br><br>B. 在两个AWS区域创建Amazon Aurora全球数据库作为数据存储。在发生故障时，将辅助区域提升为应用程序的主区域。更新应用程序以使用辅助区域中的Aurora集群端点。<br>这个选项正确。Aurora全球数据库可以提供跨区域复制，RPO通常在1秒内，远低于2小时要求。故障转移时间可以在1分钟内完成，满足10分钟RTO要求。<br><br>C. 在多个AWS区域创建Amazon Aurora集群作为数据存储。使用网络负载均衡器来平衡不同区域的数据库流量。<br>这个选项不正确。数据库不应该使用负载均衡器来分配流量，这会导致数据一致性问题。Aurora集群应该有明确的主从关系。<br><br>D. 在两个AWS区域设置应用程序。使用Amazon Route 53故障转移路由，指向两个区域的应用程序负载均衡器。在每个区域使用健康检查和Auto Scaling组。<br>这个选项正确。Route 53故障转移路由可以快速检测故障并重定向流量，配合Auto Scaling可以确保应用程序的高可用性，满足RTO要求。<br><br>E. 在两个AWS区域设置应用程序。配置AWS Global Accelerator指向两个区域的应用程序负载均衡器。将两个ALB添加到单个端点组中。在每个区域使用健康检查和Auto Scaling组。<br>这个选项虽然可以提供高可用性，但Global Accelerator主要用于性能优化而非灾难恢复，且成本较高，不是最佳的灾难恢复解决方案。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>256</td>
                    <td>A developer is using the AWS Serverless Application Model (AWS SAM) to create a prototype for an AWS Lambda function. The AWS SAM<br>template contains an AWS::Serverless::Function resource that has the CodeUri property that points to an Amazon S3 location. The developer<br>wants to identify the correct commands for deployment before creating a CI/CD pipeline.<br>The developer creates an archive of the Lambda function code named package.zip. The developer uploads the .zip le archive to the S3<br>location specied in the CodeUri property. The developer runs the sam deploy command and deploys the Lambda function. The developer<br>updates the Lambda function code and uses the same steps to deploy the new version of the Lambda function. The sam deploy command<br>fails and returns an error of no changes to deploy.</td>
                    <td>A. Use the aws cloudformation update-stack command instead of the sam deploy command.<br>B. Use the aws cloudformation update-stack-instances command instead of the sam deploy command.<br>C. Update the CodeUri property to reference the local application code folder. Use the sam deploy command.<br>D. Update the CodeUri property to reference the local application code folder. Use the aws cloudformation create-change-set command<br>and the aws cloudformation execute-change-set command.<br>E. Update the CodeUri property to reference the local application code folder. Use the aws cloudformation package command and the aws<br>cloudformation deploy command.</td>
                    <td>一名开发者正在使用AWS无服务器应用程序模型(AWS SAM)为AWS Lambda函数创建原型。AWS SAM模板包含一个AWS::Serverless::Function资源，该资源的CodeUri属性指向Amazon S3位置。开发者希望在创建CI/CD管道之前确定正确的部署命令。<br><br>开发者创建了一个名为package.zip的Lambda函数代码归档文件。开发者将.zip文件归档上传到CodeUri属性中指定的S3位置。开发者运行sam deploy命令并部署Lambda函数。开发者更新Lambda函数代码并使用相同的步骤部署新版本的Lambda函数。sam deploy命令失败并返回&quot;没有要部署的更改&quot;错误。</td>
                    <td>A. 使用aws cloudformation update-stack命令替代sam deploy命令 - 这个选项不能解决根本问题。问题在于SAM无法检测到S3中代码的变化，因为CloudFormation模板本身没有改变，只是S3中的文件内容更新了。直接使用CloudFormation命令也不会解决这个检测问题。<br><br>B. 使用aws cloudformation update-stack-instances命令替代sam deploy命令 - 这个命令主要用于StackSets的实例更新，不适用于单个Lambda函数的部署场景，完全不相关。<br><br>C. 更新CodeUri属性以引用本地应用程序代码文件夹，使用sam deploy命令 - 这是正确的解决方案之一。当CodeUri指向本地文件夹时，SAM会自动检测本地代码变化，打包并上传到S3，然后部署更新。这样SAM可以管理整个部署流程并正确检测变化。<br><br>D. 更新CodeUri属性以引用本地应用程序代码文件夹，使用aws cloudformation create-change-set和execute-change-set命令 - 虽然更新CodeUri是正确的，但使用CloudFormation的change-set命令过于复杂，SAM已经内置了这些功能。<br><br>E. 更新CodeUri属性以引用本地应用程序代码文件夹，使用aws cloudformation package和deploy命令 - 这也是正确的解决方案。cloudformation package命令会打包本地代码并上传到S3，然后cloudformation deploy会部署更新的模板。这是SAM底层使用的机制。</td>
                    <td>CE</td>
                </tr>
                <tr>
                    <td>257</td>
                    <td>A company runs its container workloads in AWS App Runner. A DevOps engineer manages the company&#x27;s container repository in Amazon<br>Elastic Container Registry (Amazon ECR).<br>The DevOps engineer must implement a solution that continuously monitors the container repository. The solution must create a new<br>container image when the solution detects an operating system vulnerability or language package vulnerability.</td>
                    <td>A. Use EC2 Image Builder to create a container image pipeline. Use Amazon ECR as the target repository. Turn on enhanced scanning on<br>the ECR repository. Create an Amazon EventBridge rule to capture an Inspector? nding event. Use the event to invoke the image pipeline.<br>Re-upload the container to the repository.<br>B. Use EC2 Image Builder to create a container image pipeline. Use Amazon ECR as the target repository. Enable Amazon GuardDuty<br>Malware Protection on the container workload. Create an Amazon EventBridge rule to capture a GuardDuty nding event. Use the event to<br>invoke the image pipeline.<br>C. Create an AWS CodeBuild project to create a container image. Use Amazon ECR as the target repository. Turn on basic scanning on the<br>repository. Create an Amazon EventBridge rule to capture an ECR image action event. Use the event to invoke the CodeBuild project. Re-<br>upload the container to the repository.<br>D. Create an AWS CodeBuild project to create a container image. Use Amazon ECR as the target repository. Congure AWS Systems<br>Manager Compliance to scan all managed nodes. Create an Amazon EventBridge rule to capture a conguration compliance state change<br>event. Use the event to invoke the CodeBuild project.</td>
                    <td>一家公司在AWS App Runner中运行其容器工作负载。DevOps工程师在Amazon弹性容器注册表(Amazon ECR)中管理公司的容器仓库。<br>DevOps工程师必须实施一个持续监控容器仓库的解决方案。当解决方案检测到操作系统漏洞或语言包漏洞时，必须创建一个新的容器镜像。</td>
                    <td>A. 使用EC2 Image Builder创建容器镜像管道，使用Amazon ECR作为目标仓库，在ECR仓库上启用增强扫描，创建Amazon EventBridge规则捕获Inspector发现事件，使用该事件调用镜像管道，重新上传容器到仓库。这个选项正确地使用了ECR的增强扫描功能来检测漏洞，Inspector可以发现操作系统和语言包漏洞，EventBridge可以自动触发镜像重建流程。<br><br>B. 使用EC2 Image Builder创建容器镜像管道，使用Amazon ECR作为目标仓库，在容器工作负载上启用Amazon GuardDuty恶意软件保护，创建EventBridge规则捕获GuardDuty发现事件。GuardDuty主要用于检测恶意软件和安全威胁，而不是专门检测操作系统或语言包漏洞，不符合题目要求。<br><br>C. 创建AWS CodeBuild项目来创建容器镜像，使用Amazon ECR作为目标仓库，在仓库上启用基础扫描，创建EventBridge规则捕获ECR镜像操作事件。基础扫描功能有限，且ECR镜像操作事件不是漏洞检测事件，无法准确触发漏洞修复流程。<br><br>D. 创建AWS CodeBuild项目来创建容器镜像，使用Amazon ECR作为目标仓库，配置AWS Systems Manager合规性扫描所有托管节点，创建EventBridge规则捕获配置合规状态变更事件。Systems Manager主要用于EC2实例管理，不适用于容器镜像漏洞扫描。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>258</td>
                    <td>A company wants to use AWS Systems Manager documents to bootstrap physical laptops for developers. The bootstrap code is stored in<br>GitHub. A DevOps engineer has already created a Systems Manager activation, installed the Systems Manager agent with the registration<br>code, and installed an activation ID on all the laptops.</td>
                    <td>A. Congure the Systems Manager document to use the AWS-RunShellScript command to copy the les from GitHub to Amazon S3, then<br>use the aws-downloadContent plugin with a sourceType of S3.<br>B. Congure the Systems Manager document to use the aws-congurePackage plugin with an install action and point to the Git repository.<br>C. Congure the Systems Manager document to use the aws-downloadContent plugin with a sourceType of GitHub and sourceInfo with the<br>repository details.<br>D. Congure the Systems Manager document to use the aws:softwareInventory plugin and run the script from the Git repository.</td>
                    <td>一家公司希望使用AWS Systems Manager文档来为开发人员的物理笔记本电脑进行引导配置。引导代码存储在GitHub中。DevOps工程师已经创建了Systems Manager激活，在所有笔记本电脑上安装了带有注册代码的Systems Manager代理，并安装了激活ID。</td>
                    <td>A. 配置Systems Manager文档使用AWS-RunShellScript命令从GitHub复制文件到Amazon S3，然后使用aws-downloadContent插件，sourceType为S3。这种方法过于复杂，需要额外的S3存储步骤，而且AWS-RunShellScript不是直接从GitHub复制文件的最佳方式。这增加了不必要的复杂性和潜在的安全风险。<br><br>B. 配置Systems Manager文档使用aws-configurePackage插件，执行安装操作并指向Git仓库。aws-configurePackage插件主要用于管理预打包的软件包，通常与Systems Manager Distributor一起使用，不是直接从Git仓库下载和执行代码的正确方法。<br><br>C. 配置Systems Manager文档使用aws-downloadContent插件，sourceType为GitHub，sourceInfo包含仓库详细信息。这是正确的方法，aws-downloadContent插件专门设计用于从各种源（包括GitHub）下载内容，可以直接指定GitHub作为源类型，并通过sourceInfo提供仓库详细信息。<br><br>D. 配置Systems Manager文档使用aws:softwareInventory插件并从Git仓库运行脚本。aws:softwareInventory插件主要用于收集系统上已安装软件的清单信息，不是用于下载和执行脚本的工具。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>259</td>
                    <td>A company&#x27;s development team uses AWS CloudFormation to deploy its application resources. The team must use CloudFormation for all<br>changes to the environment. The team cannot use the AWS Management Console or the AWS CLI to make manual changes directly.<br>The team uses a developer IAM role to access the environment. The role is congured with the AdministratorAccess managed IAM policy. The<br>company has created a new CloudFormationDeployment IAM role that has the following policy attached:<br>The company wants to ensure that only CloudFormation can use the new role. The development team cannot make any manual changes to<br>the deployed resources.</td>
                    <td>A. Remove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers<br>to use the CloudFormationDeployment role as a CloudFormation service role when the developers deploy new stacks.<br>B. Update the trust policy of the CloudFormationDeployment role to allow the developer IAM role to assume the<br>CloudFormationDeployment role.<br>C. Congure the developer IAM role to be able to get and pass the CloudFormationDeployment role if iam:PassedToService equals .<br>Congure the CloudFormationDeployment role to allow all cloudformation actions for all resources.<br>D. Update the trust policy of the CloudFormationDeployment role to allow the cloudformation.amazonaws.com AWS principal to perform<br>the iam:AssumeRole action.<br>E. Remove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers<br>to assume the CloudFormationDeployment role when the developers deploy new stacks.<br>F. Add an IAM policy to the CloudFormationDeployment role to allow cloudformation:* on all resources. Add a policy that allows the<br>iam:PassRole action for the ARN of the CloudFormationDeployment role if iam:PassedToService equals cloudformation.amazonaws.com.</td>
                    <td>一家公司的开发团队使用AWS CloudFormation来部署应用程序资源。团队必须使用CloudFormation进行环境的所有更改。团队不能使用AWS管理控制台或AWS CLI进行手动更改。团队使用开发者IAM角色来访问环境。该角色配置了AdministratorAccess托管IAM策略。公司创建了一个新的CloudFormationDeployment IAM角色，并附加了相应的策略。公司希望确保只有CloudFormation可以使用新角色。开发团队不能对已部署的资源进行任何手动更改。</td>
                    <td>A. 移除AdministratorAccess策略，为开发者角色分配ReadOnlyAccess托管IAM策略，指导开发者在部署新堆栈时使用CloudFormationDeployment角色作为CloudFormation服务角色。这个选项正确地限制了开发者的权限，只给予只读访问权限，并让CloudFormation使用专门的服务角色进行部署操作，符合最小权限原则。<br><br>B. 更新CloudFormationDeployment角色的信任策略以允许开发者IAM角色承担CloudFormationDeployment角色。这个选项允许开发者直接承担部署角色，这样开发者仍然可以进行手动更改，不符合题目要求。<br><br>C. 配置开发者IAM角色能够获取和传递CloudFormationDeployment角色，如果iam:PassedToService等于某个值。配置CloudFormationDeployment角色允许所有资源的所有cloudformation操作。这个选项描述不完整，且可能仍允许开发者进行手动操作。<br><br>D. 更新CloudFormationDeployment角色的信任策略以允许cloudformation.amazonaws.com AWS主体执行iam:AssumeRole操作。这个选项正确地配置了信任策略，确保只有CloudFormation服务可以承担这个角色，防止人为手动承担角色。<br><br>E. 移除AdministratorAccess策略，为开发者角色分配ReadOnlyAccess托管IAM策略，指导开发者在部署新堆栈时承担CloudFormationDeployment角色。这个选项仍然允许开发者直接承担部署角色，不符合要求。<br><br>F. 为CloudFormationDeployment角色添加IAM策略以允许所有资源的cloudformation:*操作，添加策略允许CloudFormationDeployment角色ARN的iam:PassRole操作。这个选项配置了权限但没有正确设置信任关系。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>260</td>
                    <td>A company is developing a web application&#x27;s infrastructure using AWS CloudFormation. The database engineering team maintains the<br>database resources in a CloudFormation template, and the software development team maintains the web application resources in a separate<br>CloudFormation template. As the scope of the application grows, the software development team needs to use resources maintained by the<br>database engineering team. However, both teams have their own review and lifecycle management processes that they want to keep. Both<br>teams also require resource-level change-set reviews. The software development team would like to deploy changes to this template using<br>their CI/CD pipeline.</td>
                    <td>A. Create a stack export from the database CloudFormation template and import those references into the web application<br>CloudFormation template.<br>B. Create a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks.<br>C. Create a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks.<br>D. Create input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack.</td>
                    <td>一家公司正在使用AWS CloudFormation开发Web应用程序的基础设施。数据库工程团队在CloudFormation模板中维护数据库资源，软件开发团队在单独的CloudFormation模板中维护Web应用程序资源。随着应用程序范围的扩大，软件开发团队需要使用数据库工程团队维护的资源。但是，两个团队都有自己的审查和生命周期管理流程，他们希望保持这些流程。两个团队还需要资源级别的变更集审查。软件开发团队希望使用他们的CI/CD管道部署对此模板的更改。</td>
                    <td>A. 从数据库CloudFormation模板创建堆栈导出，并将这些引用导入到Web应用程序CloudFormation模板中。这个选项使用CloudFormation的Export/Import功能，允许一个堆栈导出资源引用，另一个堆栈导入这些引用。这样可以保持两个团队独立的生命周期管理，同时实现跨堆栈资源共享，完全满足题目要求。<br><br>B. 创建CloudFormation嵌套堆栈以使跨堆栈资源引用和参数在两个堆栈中可用。嵌套堆栈会创建父子关系，这会影响独立的生命周期管理，因为子堆栈的生命周期会受到父堆栈的控制，不符合两个团队保持独立流程的要求。<br><br>C. 创建CloudFormation堆栈集以使跨堆栈资源引用和参数在两个堆栈中可用。堆栈集主要用于跨多个账户和区域部署相同的资源，而不是用于不同团队之间的资源共享，不适合这个场景。<br><br>D. 在Web应用程序CloudFormation模板中创建输入参数，并从数据库堆栈传递资源名称和ID。这种方法需要手动管理参数传递，增加了复杂性和出错的可能性，而且不如Export/Import机制优雅和自动化。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>261</td>
                    <td>A company has an organization in AWS Organizations. A DevOps engineer needs to maintain multiple AWS accounts that belong to different<br>OUs in the organization. All resources, including IAM policies and Amazon S3 policies within an account, are deployed through AWS<br>CloudFormation. All templates and code are maintained in an AWS CodeCommit repository. Recently, some developers have not been able to<br>access an S3 bucket from some accounts in the organization.<br>The following policy is attached to the S3 bucket:</td>
                    <td>A. Modify the S3 bucket policy. Turn off the S3 Block Public Access setting on the S3 bucket. In the S3 policy, add the aws:SourceAccount<br>condition. Add the AWS account IDs of all developers who are experiencing the issue.<br>B. Verify that no IAM permissions boundaries are denying developers access to the S3 bucket. Make the necessary changes to IAM<br>permissions boundaries. Use an AWS Cong recorder in the individual developer accounts that are experiencing the issue to revert any<br>changes that are blocking access. Commit the x back into the CodeCommit repository. Invoke deployment through CloudFormation to<br>apply the changes.<br>C. Congure an SCP that stops anyone from modifying IAM resources in developer OUs. In the S3 policy, add the aws:SourceAccount<br>condition. Add the AWS account IDs of all developers who are experiencing the issue. Commit the x back into the CodeCommit repository.<br>Invoke deployment through CloudFormation to apply the changes.<br>D. Ensure that no SCP is blocking access for developers to the S3 bucket. Ensure that no IAM policy permissions boundaries are denying<br>access to developer IAM users. Make the necessary changes to the SCP and IAM policy permissions boundaries in the CodeCommit<br>repository. Invoke deployment through CloudFormation to apply the changes.</td>
                    <td>一家公司在AWS Organizations中有一个组织。一名DevOps工程师需要维护属于组织中不同OU的多个AWS账户。账户内的所有资源，包括IAM策略和Amazon S3策略，都通过AWS CloudFormation部署。所有模板和代码都维护在AWS CodeCommit存储库中。最近，一些开发人员无法从组织中的某些账户访问S3存储桶。以下策略附加到S3存储桶上：</td>
                    <td>选项A：修改S3存储桶策略，关闭S3存储桶的S3 Block Public Access设置，在S3策略中添加aws:SourceAccount条件，添加所有遇到问题的开发人员的AWS账户ID。这个方案有问题，因为关闭Block Public Access设置会带来安全风险，而且只是添加账户ID到S3策略可能无法解决根本的权限问题。此外，这种方法没有考虑到可能存在的SCP或IAM权限边界的限制。<br><br>选项B：验证没有IAM权限边界拒绝开发人员访问S3存储桶，对IAM权限边界进行必要更改，使用AWS Config记录器在遇到问题的个别开发人员账户中恢复任何阻止访问的更改，将修复提交回CodeCommit存储库，通过CloudFormation调用部署以应用更改。这个方案只关注了IAM权限边界，但没有考虑SCP的影响，而在Organizations环境中，SCP是一个重要的权限控制机制。<br><br>选项C：配置SCP阻止任何人修改开发人员OU中的IAM资源，在S3策略中添加aws:SourceAccount条件，添加所有遇到问题的开发人员的AWS账户ID，将修复提交回CodeCommit存储库，通过CloudFormation调用部署以应用更改。这个方案虽然涉及了SCP，但是配置SCP阻止修改IAM资源可能会过于限制，而且主要关注的是防止未来的修改而不是解决当前的访问问题。<br><br>选项D：确保没有SCP阻止开发人员访问S3存储桶，确保没有IAM策略权限边界拒绝开发人员IAM用户的访问，在CodeCommit存储库中对SCP和IAM策略权限边界进行必要更改，通过CloudFormation调用部署以应用更改。这个方案全面考虑了在AWS Organizations环境中可能影响权限的两个主要机制：SCP和IAM权限边界，并且遵循了通过代码管理和CloudFormation部署的最佳实践。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>262</td>
                    <td>A company has an organization in AWS Organizations for its multi-account environment. A DevOps engineer is developing an AWS<br>CodeArtifact based strategy for application package management across the organization. Each application team at the company has its own<br>account in the organization. Each application team also has limited access to a centralized shared services account.<br>Each application team needs full access to download, publish, and grant access to its own packages. Some common library packages that the<br>application teams use must also be shared with the entire organization.</td>
                    <td>A. Create a domain in each application team&#x27;s account. Grant each application team&#x27;s account full read access and write access to the<br>application team&#x27;s domain.<br>B. Create a domain in the shared services account. Grant the organization read access and CreateRepository access.<br>C. Create a repository in each application team’s account. Grant each application team’s account full read access and write access to its<br>own repository.<br>D. Create a repository in the shared services account. Grant the organization read access to the repository in the shared services account<br>Set the repository as the upstream repository in each application team&#x27;s repository.<br>E. For teams that require shared packages, create resource-based policies that allow read access to the repository from other application<br>teams&#x27; accounts.<br>F. Set the other application teams&#x27; repositories as upstream repositories.</td>
                    <td>一家公司在AWS Organizations中为其多账户环境建立了组织。一名DevOps工程师正在开发基于AWS CodeArtifact的策略，用于整个组织的应用程序包管理。公司的每个应用团队在组织中都有自己的账户。每个应用团队对集中式共享服务账户也有有限的访问权限。每个应用团队需要完全访问权限来下载、发布和授权访问自己的包。应用团队使用的一些通用库包也必须与整个组织共享。</td>
                    <td>A. 在每个应用团队的账户中创建域，并授予每个应用团队账户对其域的完全读写访问权限。这种方法会导致域分散，不利于共享通用库包，因为域是CodeArtifact中的顶级容器，应该集中管理以便于包的共享和管理。<br><br>B. 在共享服务账户中创建域，授予组织读取访问权限和CreateRepository访问权限。这是正确的方法，因为域应该集中在共享服务账户中，这样可以更好地管理整个组织的包，同时允许各团队创建自己的仓库。<br><br>C. 在每个应用团队账户中创建仓库，并授予每个团队对其仓库的完全读写访问权限。这种方法缺少域的概念，而且无法有效实现包的组织级共享。<br><br>D. 在共享服务账户中创建仓库，授予组织对该仓库的读取访问权限，并将该仓库设置为每个应用团队仓库的上游仓库。这是正确的，因为上游仓库机制可以让各团队访问共享的通用库包。<br><br>E. 对于需要共享包的团队，创建基于资源的策略，允许其他应用团队账户对仓库的读取访问。这是正确的，因为资源策略可以精确控制跨账户的包访问权限。<br><br>F. 将其他应用团队的仓库设置为上游仓库。这种方法不合适，因为会创建复杂的依赖关系，不利于管理，且不符合集中共享通用库的需求。</td>
                    <td>BDE</td>
                </tr>
                <tr>
                    <td>263</td>
                    <td>A company deploys an application to Amazon EC2 instances. The application runs Amazon Linux 2 and uses AWS CodeDeploy. The application<br>has the following le structure for its code repository:<br>The appspec.yml le has the following contents in the les section:</td>
                    <td>A. The cong.txt le will be deployed to only /var/www/html/cong/cong.txt.<br>B. The cong.txt le will be deployed to /usr/local/src/cong.txt and to /var/www/html/cong/cong.txt.<br>C. The cong.txt le will be deployed to only /usr/local/src/cong.txt.<br>D. The cong.txt le will be deployed to /usr/local/src/cong.txt and to /var/www/html/application/web/cong.txt.</td>
                    <td>一家公司将应用程序部署到Amazon EC2实例上。该应用程序运行Amazon Linux 2并使用AWS CodeDeploy。应用程序的代码仓库具有以下文件结构：<br>appspec.yml文件在files部分包含以下内容：<br><br>（注：题目中缺少了关键的文件结构和appspec.yml的具体内容，但根据选项可以推断出涉及config.txt文件的部署路径问题）</td>
                    <td>A. config.txt文件将仅部署到/var/www/html/config/config.txt - 这个选项表示文件只会部署到一个特定的web目录位置。根据CodeDeploy的工作机制，如果appspec.yml中只定义了一个目标路径，那么文件确实只会部署到该路径。<br><br>B. config.txt文件将部署到/usr/local/src/config.txt和/var/www/html/config/config.txt - 这个选项表示文件会同时部署到两个不同的位置，一个是源代码目录，一个是web目录。这需要在appspec.yml中定义多个目标路径。<br><br>C. config.txt文件将仅部署到/usr/local/src/config.txt - 这个选项表示文件只会部署到源代码目录的单一位置。如果appspec.yml的files部分只指定了这一个目标路径，那么文件确实只会部署到这里。<br><br>D. config.txt文件将部署到/usr/local/src/config.txt和/var/www/html/application/web/config.txt - 这个选项也表示双重部署，但第二个路径更深层，包含了application/web子目录。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>264</td>
                    <td>A company has set up AWS CodeArtifact repositories with public upstream repositories. The company&#x27;s development team consumes open<br>source dependencies from the repositories in the company&#x27;s internal network.<br>The company&#x27;s security team recently discovered a critical vulnerability in the most recent version of a package that the development team<br>consumes. The security team has produced a patched version to x the vulnerability. The company needs to prevent the vulnerable version<br>from being downloaded. The company also needs to allow the security team to publish the patched version.</td>
                    <td>A. Update the status of the affected CodeArtifact package version to unlisted.<br>B. Update the status of the affected CodeArtifact package version to deleted.<br>C. Update the status of the affected CodeArtifact package version to archived.<br>D. Update the CodeArtifact package origin control settings to allow direct publishing and to block upstream operations.<br>E. Update the CodeArtifact package origin control settings to block direct publishing and to allow upstream operations.</td>
                    <td>一家公司已经设置了AWS CodeArtifact仓库，并配置了公共上游仓库。公司的开发团队在公司内部网络中从这些仓库消费开源依赖包。<br>公司的安全团队最近发现开发团队使用的某个包的最新版本存在严重漏洞。安全团队已经制作了一个修补版本来修复这个漏洞。公司需要阻止下载存在漏洞的版本，同时也需要允许安全团队发布修补版本。</td>
                    <td>A. 将受影响的CodeArtifact包版本状态更新为unlisted（未列出）：将包版本设置为unlisted状态只是让它不在搜索结果中显示，但仍然可以通过直接指定版本号来下载，无法完全阻止下载存在漏洞的版本，因此不能满足安全要求。<br><br>B. 将受影响的CodeArtifact包版本状态更新为deleted（已删除）：将包版本设置为deleted状态可以完全阻止该版本被下载，这正是题目要求的防止漏洞版本被下载的需求。这个选项能够有效解决安全问题。<br><br>C. 将受影响的CodeArtifact包版本状态更新为archived（已归档）：archived状态通常用于长期保存但不活跃使用的版本，它并不能阻止包的下载，因此无法满足阻止漏洞版本下载的安全要求。<br><br>D. 更新CodeArtifact包源控制设置以允许直接发布并阻止上游操作：允许直接发布可以让安全团队发布修补版本，阻止上游操作可以防止从上游仓库获取漏洞版本，这能同时满足两个需求。<br><br>E. 更新CodeArtifact包源控制设置以阻止直接发布并允许上游操作：这个配置与需求相反，会阻止安全团队发布修补版本，同时允许从上游获取漏洞版本，完全不符合要求。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>265</td>
                    <td>A company is running a custom-built application that processes records. All the components run on Amazon EC2 instances that run in an Auto<br>Scaling group. Each record&#x27;s processing is a multistep sequential action that is compute-intensive. Each step is always completed in 5<br>minutes or less.<br>A limitation of the current system is that if any steps fail, the application has to reprocess the record from the beginning. The company wants<br>to update the architecture so that the application must reprocess only the failed steps.</td>
                    <td>A. Create a web application to write records to Amazon S3. Use S3 Event Notications to publish to an Amazon Simple Notication Service<br>(Amazon SNS) topic. Use an EC2 instance to poll Amazon SNS and start processing. Save intermediate results to Amazon S3 to pass on to<br>the next step.<br>B. Perform the processing steps by using logic in the application. Convert the application code to run in a container. Use AWS Fargate to<br>manage the container instances. Congure the container to invoke itself to pass the state from one step to the next.<br>C. Create a web application to pass records to an Amazon Kinesis data stream. Decouple the processing by using the Kinesis data stream<br>and AWS Lambda functions.<br>D. Create a web application to pass records to AWS Step Functions. Decouple the processing into Step Functions tasks and AWS Lambda<br>functions.</td>
                    <td>一家公司正在运行一个自定义构建的应用程序来处理记录。所有组件都运行在Auto Scaling组中的Amazon EC2实例上。每个记录的处理是一个多步骤的顺序操作，计算密集型。每个步骤总是在5分钟或更短时间内完成。<br>当前系统的一个限制是，如果任何步骤失败，应用程序必须从头开始重新处理记录。公司希望更新架构，使应用程序只需要重新处理失败的步骤。</td>
                    <td>选项A：创建Web应用程序将记录写入Amazon S3，使用S3事件通知发布到Amazon SNS主题，使用EC2实例轮询SNS并开始处理，将中间结果保存到S3传递给下一步。这个方案虽然可以保存中间结果，但使用SNS和EC2轮询的架构复杂且不够优雅，无法提供精确的步骤级错误处理和重试机制。<br><br>选项B：使用应用程序逻辑执行处理步骤，将应用程序代码转换为容器运行，使用AWS Fargate管理容器实例，配置容器调用自身以在步骤间传递状态。这种方案仍然是单体应用架构，无法真正解耦各个处理步骤，难以实现只重新处理失败步骤的需求。<br><br>选项C：创建Web应用程序将记录传递给Amazon Kinesis数据流，使用Kinesis数据流和AWS Lambda函数解耦处理。Kinesis主要用于流数据处理，虽然可以与Lambda结合，但不能提供复杂的工作流管理和步骤级的错误处理机制。<br><br>选项D：创建Web应用程序将记录传递给AWS Step Functions，将处理解耦为Step Functions任务和AWS Lambda函数。Step Functions专门设计用于管理多步骤工作流，提供内置的错误处理、重试机制和状态管理，完美匹配只重新处理失败步骤的需求。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>266</td>
                    <td>A company is migrating its on-premises Windows applications and Linux applications to AWS. The company will use automation to launch<br>Amazon EC2 instances to mirror the on-premises congurations. The migrated applications require access to shared storage that uses SMB<br>for Windows and NFS for Linux.<br>The company is also creating a pilot light disaster recovery (DR) environment in another AWS Region. The company will use automation to<br>launch and congure the EC2 instances in the DR Region. The company needs to replicate the storage to the DR Region.</td>
                    <td>A. Use Amazon S3 for the application storage. Create an S3 bucket in the primary Region and an S3 bucket in the DR Region. Congure S3<br>Cross-Region Replication (CRR) from the primary Region to the DR Region.<br>B. Use Amazon Elastic Block Store (Amazon EBS) for the application storage. Create a backup plan in AWS Backup that creates snapshots<br>of the EBS volumes that are in the primary Region and replicates the snapshots to the DR Region.<br>C. Use a Volume Gateway in AWS Storage Gateway for the application storage. Congure Cross-Region Replication (CRR) of the Volume<br>Gateway from the primary Region to the DR Region.<br>D. Use Amazon FSx for NetApp ONTAP for the application storage. Create an FSx for ONTAP instance in the DR Region. Congure NetApp<br>SnapMirror replication from the primary Region to the DR Region.</td>
                    <td>一家公司正在将其本地Windows应用程序和Linux应用程序迁移到AWS。该公司将使用自动化来启动Amazon EC2实例以镜像本地配置。迁移的应用程序需要访问共享存储，Windows使用SMB协议，Linux使用NFS协议。<br>该公司还在另一个AWS区域创建试点灯灾难恢复(DR)环境。公司将使用自动化在DR区域启动和配置EC2实例。公司需要将存储复制到DR区域。</td>
                    <td>选项A：使用Amazon S3作为应用程序存储，在主区域和DR区域分别创建S3存储桶，配置S3跨区域复制。这个选项的问题是S3不能直接提供SMB和NFS协议访问，需要额外的网关或挂载工具，不符合应用程序对共享存储的直接访问需求。<br><br>选项B：使用Amazon EBS作为应用程序存储，在AWS Backup中创建备份计划对主区域的EBS卷创建快照并复制到DR区域。EBS是块存储，不能直接提供SMB和NFS文件系统协议，无法满足应用程序对共享存储的需求。<br><br>选项C：使用AWS Storage Gateway的卷网关作为应用程序存储，配置卷网关的跨区域复制。虽然Storage Gateway可以提供一定的文件访问能力，但卷网关主要用于块存储，不是专门为SMB和NFS协议设计的最佳解决方案。<br><br>选项D：使用Amazon FSx for NetApp ONTAP作为应用程序存储，在DR区域创建FSx for ONTAP实例，配置NetApp SnapMirror从主区域到DR区域的复制。FSx for NetApp ONTAP原生支持SMB和NFS协议，完全满足Windows和Linux应用程序的共享存储需求，且SnapMirror提供了企业级的复制功能。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>267</td>
                    <td>A company&#x27;s application uses a eet of Amazon EC2 On-Demand Instances to analyze and process data. The EC2 instances are in an Auto<br>Scaling group. The Auto Scaling group is a target group for an Application Load Balancer (ALB). The application analyzes critical data that<br>cannot tolerate interruption. The application also analyzes noncritical data that can withstand interruption.<br>The critical data analysis requires quick scalability in response to real-time application demand. The noncritical data analysis involves<br>memory consumption. A DevOps engineer must implement a solution that reduces scale-out latency for the critical data. The solution also<br>must process the noncritical data.</td>
                    <td>A. For the critical data, modify the existing Auto Scaling group. Create a warm pool instance in the stopped state. Dene the warm pool<br>size. Create a new version of the launch template that has detailed monitoring enabled. Use Spot Instances.<br>B. Modify the application to use<br>two target groups for critical data and noncritical data.<br>C. For the critical data, modify the existing Auto Scaling group. Create a lifecycle hook to ensure that bootstrap scripts are completed<br>successfully. Ensure that the application on the instances is ready to accept trac before the instances are registered. Create a new<br>version of the launch template that has detailed monitoring enabled.<br>D. For the noncritical data, create a second Auto Scaling group that uses a launch template. Congure the launch template to install the<br>unied Amazon CloudWatch agent and to congure the CloudWatch agent with a custom memory utilization metric. Use Spot Instances.<br>Add the new Auto Scaling group as the target group for the AL<br>E. For the noncritical data, create a second Auto Scaling group. Choose the predened memory utilization metric type for the target<br>tracking scaling policy. Use Spot Instances. Add the new Auto Scaling group as the target group for the AL</td>
                    <td>一家公司的应用程序使用一组Amazon EC2按需实例来分析和处理数据。这些EC2实例位于Auto Scaling组中。Auto Scaling组是应用程序负载均衡器(ALB)的目标组。该应用程序分析不能容忍中断的关键数据，同时也分析可以承受中断的非关键数据。关键数据分析需要快速扩展以响应实时应用程序需求。非关键数据分析涉及内存消耗。DevOps工程师必须实施一个解决方案，减少关键数据的横向扩展延迟，同时该解决方案还必须处理非关键数据。</td>
                    <td>选项A：针对关键数据修改现有Auto Scaling组，创建处于停止状态的预热池实例，定义预热池大小，创建启用详细监控的启动模板新版本，使用Spot实例。这个选项的问题在于使用Spot实例处理关键数据是不合适的，因为Spot实例可能被中断，而题目明确说明关键数据不能容忍中断。<br><br>选项B：修改应用程序为关键数据和非关键数据使用两个目标组。这个选项过于简单，没有提供具体的技术实现细节，仅仅分离目标组并不能解决扩展延迟问题。<br><br>选项C：针对关键数据修改现有Auto Scaling组，创建生命周期钩子确保引导脚本成功完成，确保实例上的应用程序准备好接受流量后再注册实例，创建启用详细监控的启动模板新版本。这个方案能够减少扩展延迟，因为生命周期钩子确保实例完全准备就绪。<br><br>选项D：针对非关键数据创建第二个使用启动模板的Auto Scaling组，配置启动模板安装统一CloudWatch代理并配置自定义内存利用率指标，使用Spot实例，将新Auto Scaling组添加为ALB的目标组。这个方案适合处理非关键数据，使用Spot实例降低成本，自定义内存指标适合内存密集型工作负载。<br><br>选项E：针对非关键数据创建第二个Auto Scaling组，为目标跟踪扩展策略选择预定义的内存利用率指标类型，使用Spot实例，将新Auto Scaling组添加为ALB的目标组。这个选项的问题是AWS Auto Scaling没有预定义的内存利用率指标，需要自定义配置。</td>
                    <td>CD</td>
                </tr>
                <tr>
                    <td>268</td>
                    <td>A company recently migrated its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that uses Amazon EC2 instances.<br>The company congured the application to automatically scale based on CPU utilization.<br>The application produces memory errors when it experiences heavy loads. The application also does not scale out enough to handle the<br>increased load. The company needs to collect and analyze memory metrics for the application over time.</td>
                    <td>A. Attach the CloudWatchAgentServerPolicy managed IAM policy to the IAM instance prole that the cluster uses.<br>B. Attach the CloudWatchAgentServerPolicy managed IAM policy to a service account role for the cluster.<br>C. Collect performance metrics by deploying the unied Amazon CloudWatch agent to the existing EC2 instances in the cluster. Add the<br>agent to the AMI for any new EC2 instances that are added to the cluster.<br>D. Collect performance logs by deploying the AWS Distro for OpenTelemetry collector as a DaemonSet.<br>E. Analyze the pod_memory_utilization Amazon CloudWatch metric in the ContainerInsights namespace by using the Service dimension.</td>
                    <td>一家公司最近将其应用程序迁移到使用Amazon EC2实例的Amazon Elastic Kubernetes Service (Amazon EKS)集群。该公司配置应用程序根据CPU利用率自动扩展。当应用程序遇到重负载时会产生内存错误。应用程序也没有充分扩展以处理增加的负载。该公司需要收集和分析应用程序随时间变化的内存指标。</td>
                    <td>A. 将CloudWatchAgentServerPolicy托管IAM策略附加到集群使用的IAM实例配置文件 - 这个选项提供了必要的权限来运行CloudWatch代理，但仅仅附加策略并不能实际收集内存指标，还需要部署和配置代理本身。<br><br>B. 将CloudWatchAgentServerPolicy托管IAM策略附加到集群的服务账户角色 - 虽然这也是一种权限配置方式，但对于EC2实例上的CloudWatch代理，通常使用实例配置文件更为直接和常见。<br><br>C. 通过将统一的Amazon CloudWatch代理部署到集群中现有的EC2实例来收集性能指标。将代理添加到AMI中，用于添加到集群的任何新EC2实例 - 这个选项直接解决了收集内存指标的需求，CloudWatch代理可以收集详细的系统级指标包括内存使用情况。<br><br>D. 通过将AWS Distro for OpenTelemetry收集器作为DaemonSet部署来收集性能日志 - OpenTelemetry主要用于应用程序级别的追踪和指标收集，虽然可以收集一些指标，但对于系统级内存指标收集，CloudWatch代理更为合适。<br><br>E. 通过使用Service维度分析ContainerInsights命名空间中的pod_memory_utilization Amazon CloudWatch指标 - 这个选项提供了分析已收集内存指标的方法，Container Insights可以提供容器和Pod级别的内存利用率指标。</td>
                    <td>CE</td>
                </tr>
                <tr>
                    <td>269</td>
                    <td>A company&#x27;s video streaming platform usage has increased from 10,000 users each day to 50,000 users each day in multiple countries. The<br>company deploys the streaming platform on Amazon Elastic Kubernetes Service (Amazon EKS). The EKS workload scales up to thousands of<br>nodes during peak viewing time.<br>The company&#x27;s users report occurrences of unauthorized logins. Users also report sudden interruptions and logouts from the platform.<br>The company wants additional security measures for the entire platform. The company also needs a summarized view of the resource<br>behaviors and interactions across the company&#x27;s entire AWS environment. The summarized view must show login attempts, API calls, and<br>network trac. The solution must permit network trac analysis while minimizing the overhead of managing logs. The solution must also<br>quickly investigate any potential malicious behavior that is associated with the EKS workload.</td>
                    <td>A. Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable AWS CloudTrail logs. Store the EKS audit logs and CloudTrail log les in<br>an Amazon S3 bucket. Use Amazon Athena to create an external table. Use Amazon QuickSight to create a dashboard.<br>B. Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable Amazon Detective in the company&#x27;s AWS account. Enable EKS audit<br>logs from optional source packages in Detective.<br>C. Enable Amazon CloudWatch Container Insights. Enable AWS CloudTrail logs. Store the EKS audit logs and CloudTrail log les in an<br>Amazon S3 bucket. Use Amazon Athena to create an external table. Use Amazon QuickSight to create a dashboard.<br>D. Enable Amazon GuardDuty for EKS Audit Log Monitoring. Enable Amazon CloudWatch Container Insights and VPC Flow Logs. Enable<br>AWS CloudTrail logs.</td>
                    <td>一家公司的视频流媒体平台使用量从每天10,000用户增长到多个国家每天50,000用户。该公司在Amazon Elastic Kubernetes Service (Amazon EKS)上部署流媒体平台。EKS工作负载在高峰观看时间会扩展到数千个节点。<br><br>公司用户报告出现未经授权的登录。用户还报告平台突然中断和登出。公司希望为整个平台增加额外的安全措施。公司还需要一个汇总视图来显示整个AWS环境中资源行为和交互。汇总视图必须显示登录尝试、API调用和网络流量。解决方案必须允许网络流量分析，同时最小化管理日志的开销。解决方案还必须能够快速调查与EKS工作负载相关的任何潜在恶意行为。</td>
                    <td>选项A：启用Amazon GuardDuty的EKS审计日志监控，启用AWS CloudTrail日志，将EKS审计日志和CloudTrail日志文件存储在Amazon S3存储桶中，使用Amazon Athena创建外部表，使用Amazon QuickSight创建仪表板。这个方案提供了基本的安全监控和可视化，但缺少网络流量分析功能，无法满足网络流量分析的需求，且需要手动管理日志存储和查询。<br><br>选项B：启用Amazon GuardDuty的EKS审计日志监控，在公司AWS账户中启用Amazon Detective，从Detective的可选源包中启用EKS审计日志。这个方案提供了威胁检测和调查能力，Detective可以自动分析和关联安全数据，但同样缺少网络流量分析和CloudTrail日志，无法提供完整的环境监控视图。<br><br>选项C：启用Amazon CloudWatch Container Insights，启用AWS CloudTrail日志，将EKS审计日志和CloudTrail日志文件存储在Amazon S3存储桶中，使用Amazon Athena创建外部表，使用Amazon QuickSight创建仪表板。这个方案提供了容器监控和可视化，但缺少专门的安全威胁检测功能和网络流量分析。<br><br>选项D：启用Amazon GuardDuty的EKS审计日志监控，启用Amazon CloudWatch Container Insights和VPC Flow Logs，启用AWS CloudTrail日志。这个方案最全面，包含了威胁检测(GuardDuty)、容器监控(Container Insights)、网络流量分析(VPC Flow Logs)和API调用监控(CloudTrail)，能够满足所有需求。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>270</td>
                    <td>A company uses AWS Organizations to manage hundreds of AWS accounts. The company has a team that is responsible for AWS Identity and<br>Access Management (IAM).<br>The IAM team wants to implement AWS IAM Identity Center (AWS Single Sign-On). The IAM team must have only the minimum needed<br>permissions to manage IAM Identity Center. The IAM team must not be able to gain unneeded access to the Organizations management<br>account. The IAM team must be able to provision new IAM Identity Center permission sets and assignments for existing and new member<br>accounts.</td>
                    <td>A. Create a new AWS account for the IAM team. In the new account, enable IAM Identity Center. In the Organizations management<br>account, register the new account as a delegated administrator for IAM Identity Center.<br>B. Create a new AWS account for the IAM team. In the Organizations management account, enable IAM Identity Center. In the<br>Organizations management account, register the new account as a delegated administrator for IAM Identity Center.<br>C. In IAM Identity Center, create users and a group for the IAM team. Add the users to the group. Create a new permission set. Attach the<br>AWSSSODirectoryAdministrator managed IAM policy to the group.<br>D. In IAM Identity Center, create users and a group for the IAM team. Add the users to the group. Create a new permission set. Attach the<br>AWSSSOMemberAccountAdministrator managed IAM policy to the group.<br>E. Assign the permission set to the Organizations management account. Allow the IAM team group to use the permission set.<br>F. Assign the permission set to the new AWS account. Allow the IAM team group to use the permission set.</td>
                    <td>一家公司使用AWS Organizations来管理数百个AWS账户。该公司有一个负责AWS身份和访问管理(IAM)的团队。<br>IAM团队希望实施AWS IAM Identity Center(AWS Single Sign-On)。IAM团队必须只拥有管理IAM Identity Center所需的最小权限。IAM团队不得获得对Organizations管理账户的不必要访问权限。IAM团队必须能够为现有和新的成员账户配置新的IAM Identity Center权限集和分配。</td>
                    <td>A. 为IAM团队创建新的AWS账户，在新账户中启用IAM Identity Center，在Organizations管理账户中将新账户注册为IAM Identity Center的委托管理员。这个选项不正确，因为IAM Identity Center必须在Organizations管理账户中启用，而不是在委托管理员账户中启用。<br><br>B. 为IAM团队创建新的AWS账户，在Organizations管理账户中启用IAM Identity Center，然后将新账户注册为委托管理员。这个选项部分正确，但创建新账户并非必需，可以直接在现有环境中配置。<br><br>C. 在IAM Identity Center中为IAM团队创建用户和组，将用户添加到组中，创建新的权限集，并将AWSSSODirectoryAdministrator托管IAM策略附加到组。这是正确的，因为AWSSSODirectoryAdministrator策略提供了管理IAM Identity Center所需的适当权限。<br><br>D. 在IAM Identity Center中创建用户和组，附加AWSSSOMemberAccountAdministrator策略。这个选项不正确，因为该策略主要用于成员账户管理，不适合IAM Identity Center的管理需求。<br><br>E. 将权限集分配给Organizations管理账户，允许IAM团队组使用该权限集。这是正确的，因为需要在管理账户中进行IAM Identity Center的配置和管理。<br><br>F. 将权限集分配给新的AWS账户。如果没有创建新账户，这个选项就不适用。</td>
                    <td>CE</td>
                </tr>
                <tr>
                    <td>271</td>
                    <td>A company uses an organization in AWS Organizations that has all features enabled. The company uses AWS Backup in a primary account and<br>uses an AWS Key Management Service (AWS KMS) key to encrypt the backups.<br>The company needs to automate a cross-account backup of the resources that AWS Backup backs up in the primary account. The company<br>congures cross-account backup in the Organizations management account. The company creates a new AWS account in the organization and<br>congures an AWS Backup backup vault in the new account. The company creates a KMS key in the new account to encrypt the backups.<br>Finally, the company congures a new backup plan in the primary account. The destination for the new backup plan is the backup vault in the<br>new account.<br>When the AWS Backup job in the primary account is invoked, the job creates backups in the primary account. However, the backups are not<br>copied to the new account&#x27;s backup vault.</td>
                    <td>A. Edit the backup vault access policy in the new account to allow access to the primary account.<br>B. Edit the backup vault access policy in the primary account to allow access to the new account.<br>C. Edit the backup vault access policy in the primary account to allow access to the KMS key in the new account.<br>D. Edit the key policy of the KMS key in the primary account to share the key with the new account.<br>E. Edit the key policy of the KMS key in the new account to share the key with the primary account.</td>
                    <td>一家公司使用AWS Organizations中启用了所有功能的组织。该公司在主账户中使用AWS Backup，并使用AWS密钥管理服务(AWS KMS)密钥来加密备份。<br>该公司需要自动化跨账户备份AWS Backup在主账户中备份的资源。公司在Organizations管理账户中配置跨账户备份。公司在组织中创建了一个新的AWS账户，并在新账户中配置了AWS Backup备份保管库。公司在新账户中创建了一个KMS密钥来加密备份。<br>最后，公司在主账户中配置了一个新的备份计划。新备份计划的目标是新账户中的备份保管库。<br>当主账户中的AWS Backup作业被调用时，作业在主账户中创建备份。但是，备份没有被复制到新账户的备份保管库中。</td>
                    <td>A. 编辑新账户中的备份保管库访问策略以允许主账户访问 - 这是正确的。跨账户备份需要目标账户的备份保管库允许源账户访问，这样主账户才能将备份复制到新账户的保管库中。<br><br>B. 编辑主账户中的备份保管库访问策略以允许新账户访问 - 这是不正确的。主账户的保管库不需要允许新账户访问，因为数据流向是从主账户到新账户。<br><br>C. 编辑主账户中的备份保管库访问策略以允许访问新账户中的KMS密钥 - 这是不正确的。备份保管库访问策略不是用来管理KMS密钥访问权限的，这应该通过KMS密钥策略来处理。<br><br>D. 编辑主账户中KMS密钥的密钥策略以与新账户共享密钥 - 这是不正确的。主账户的KMS密钥用于加密主账户中的备份，不需要与新账户共享。<br><br>E. 编辑新账户中KMS密钥的密钥策略以与主账户共享密钥 - 这是正确的。当备份被复制到新账户时，需要使用新账户的KMS密钥进行加密，因此主账户需要有权限使用新账户的KMS密钥。</td>
                    <td>AE</td>
                </tr>
                <tr>
                    <td>272</td>
                    <td>A company runs an application that uses an Amazon S3 bucket to store images. A DevOps engineer needs to implement a multi-Region<br>strategy for the objects that are stored in the S3 bucket. The company needs to be able to fail over to an S3 bucket in another AWS Region.<br>When an image is added to either S3 bucket, the image must be replicated to the other S3 bucket within 15 minutes.<br>The DevOps engineer enables two-way replication between the S3 buckets.</td>
                    <td>A. Enable S3 Replication Time Control (S3 RTC) on each replication rule.<br>B. Create an S3 Multi-Region Access Point in an active-passive conguration.<br>C. Call the SubmitMultiRegionAccessPointRoutes operation in the AWS API when the company needs to fail over to the S3 bucket in the<br>other Region.<br>D. Enable S3 Transfer Acceleration on both S3 buckets.<br>E. Congure a routing control in Amazon Route 53 Recovery Controller. Add the S3 buckets in an active-passive conguration.<br>F. Call the UpdateRoutingControlStates operation in the AWS API when the company needs to fail over to the S3 bucket in the other<br>Region.</td>
                    <td>一家公司运行一个使用Amazon S3存储桶来存储图像的应用程序。DevOps工程师需要为存储在S3存储桶中的对象实施多区域策略。公司需要能够故障转移到另一个AWS区域的S3存储桶。当图像添加到任一S3存储桶时，图像必须在15分钟内复制到另一个S3存储桶。DevOps工程师在S3存储桶之间启用了双向复制。</td>
                    <td>A. 在每个复制规则上启用S3复制时间控制(S3 RTC) - 这是正确的。S3 RTC可以确保对象在指定时间内完成复制，满足15分钟内复制的要求。它提供复制时间的SLA保证，并提供复制指标和通知。<br><br>B. 创建主动-被动配置的S3多区域访问点 - 这不是最佳选择。多区域访问点主要用于简化跨区域访问，但不能直接解决15分钟复制时间要求。<br><br>C. 在需要故障转移时调用SubmitMultiRegionAccessPointRoutes操作 - 这与多区域访问点相关，但不是解决复制时间要求的最佳方案。<br><br>D. 在两个S3存储桶上启用S3传输加速 - 这是正确的。S3传输加速使用CloudFront的全球分布式边缘位置来加速上传，可以显著提高跨区域数据传输速度，有助于满足15分钟的复制要求。<br><br>E. 在Amazon Route 53恢复控制器中配置路由控制，以主动-被动配置添加S3存储桶 - Route 53恢复控制器主要用于应用程序级别的故障转移控制，不直接影响S3复制速度。<br><br>F. 在需要故障转移时调用UpdateRoutingControlStates操作 - 这与Route 53恢复控制器相关，但不解决复制时间问题。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>273</td>
                    <td>A company uses the AWS Cloud Development Kit (AWS CDK) to dene its application. The company uses a pipeline that consists of AWS<br>CodePipeline and AWS CodeBuild to deploy the CDK application.<br>The company wants to introduce unit tests to the pipeline to test various infrastructure components. The company wants to ensure that a<br>deployment proceeds if no unit tests result in a failure.</td>
                    <td>A. Update the CodeBuild build phase commands to run the tests then to deploy the application. Set the OnFailure phase property to<br>ABORT.<br>B. Update the CodeBuild build phase commands to run the tests then to deploy the application. Add the --rollback true ag to the cdk<br>deploy command.<br>C. Update the CodeBuild build phase commands to run the tests then to deploy the application. Add the --require-approval any-change ag<br>to the cdk deploy command.<br>D. Create a test that uses the AWS CDK assertions module. Use the template.hasResourceProperties assertion to test that resources have<br>the expected properties.<br>E. Create a test that uses the cdk diff command. Congure the test to fail if any resources have changed.</td>
                    <td>一家公司使用AWS云开发工具包(AWS CDK)来定义其应用程序。该公司使用由AWS CodePipeline和AWS CodeBuild组成的管道来部署CDK应用程序。<br>该公司希望在管道中引入单元测试来测试各种基础设施组件。公司希望确保如果没有单元测试失败，部署就会继续进行。</td>
                    <td>选项A：更新CodeBuild构建阶段命令以运行测试然后部署应用程序。将OnFailure阶段属性设置为ABORT。<br>这个选项是正确的。通过在构建阶段先运行测试再部署，可以确保测试通过后才进行部署。设置OnFailure为ABORT意味着如果测试失败，构建过程会中止，从而阻止有问题的代码部署。这完全符合题目要求的&quot;确保如果没有单元测试失败，部署就会继续进行&quot;。<br><br>选项B：更新CodeBuild构建阶段命令以运行测试然后部署应用程序。在cdk deploy命令中添加--rollback true标志。<br>这个选项不正确。--rollback true标志主要用于部署失败时的回滚操作，但它不能阻止测试失败时的部署继续进行。这不能满足题目的核心需求。<br><br>选项C：更新CodeBuild构建阶段命令以运行测试然后部署应用程序。在cdk deploy命令中添加--require-approval any-change标志。<br>这个选项不正确。--require-approval any-change标志要求对任何更改进行手动批准，这与自动化测试和部署流程的目标相矛盾，也不能解决测试失败时阻止部署的问题。<br><br>选项D：创建一个使用AWS CDK断言模块的测试。使用template.hasResourceProperties断言来测试资源是否具有预期的属性。<br>这个选项是正确的。CDK断言模块提供了强大的测试功能，template.hasResourceProperties可以验证生成的CloudFormation模板中的资源是否具有正确的属性。这是实现基础设施单元测试的标准和推荐方法。<br><br>选项E：创建一个使用cdk diff命令的测试。配置测试在任何资源发生更改时失败。<br>这个选项不正确。cdk diff用于显示更改差异，但让测试在任何资源更改时都失败是不合理的，因为正常的开发过程中资源更改是必要的。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>274</td>
                    <td>A company has an application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances are in<br>multiple Availability Zones. The application was miscongured in a single Availability Zone, which caused a partial outage of the application.<br>A DevOps engineer made changes to ensure that the unhealthy EC2 instances in one Availability Zone do not affect the healthy EC2 instances<br>in the other Availability Zones. The DevOps engineer needs to test the application&#x27;s failover and shift where the ALB sends trac. During<br>failover, the ALB must avoid sending trac to the Availability Zone where the failure has occurred.</td>
                    <td>A. Turn off cross-zone load balancing on the AL<br>B. Start a zonal shift for<br>the resource set away from the Availability Zone.<br>C. Create an Amazon Route 53 Application Recovery Controller resource set that uses the DNS hostname of the AL<br>D. Create an Amazon Route 53 Application Recovery Controller resource set that uses the ARN of the ALB’s target group. Create a<br>readiness check that uses the ElbV2TargetGroupsCanServeTrac rule.</td>
                    <td>一家公司有一个应用程序运行在应用负载均衡器(ALB)后面的Amazon EC2实例上。这些EC2实例分布在多个可用区中。应用程序在单个可用区中配置错误，导致应用程序部分中断。DevOps工程师进行了更改，以确保一个可用区中的不健康EC2实例不会影响其他可用区中的健康EC2实例。DevOps工程师需要测试应用程序的故障转移并改变ALB发送流量的位置。在故障转移期间，ALB必须避免向发生故障的可用区发送流量。</td>
                    <td>A. 关闭ALB上的跨区负载均衡 - 这是正确的解决方案。当关闭跨区负载均衡时，ALB只会将流量路由到每个可用区内的目标，不会跨区域分配流量。这样可以确保当某个可用区出现问题时，ALB不会将该区域的流量分散到其他健康的可用区，从而实现区域级别的隔离和故障转移控制。<br><br>B. 为资源集启动区域转移，远离可用区 - 区域转移(zonal shift)是AWS的一个功能，但这个选项描述不完整且不够具体。虽然区域转移可以帮助将流量从有问题的可用区转移走，但题目要求的是测试故障转移机制，而不是实际执行转移。<br><br>C. 创建使用ALB的DNS主机名的Amazon Route 53应用程序恢复控制器资源集 - Route 53 ARC主要用于跨区域的故障转移和恢复，而不是单个ALB内的可用区级别的流量控制。这个方案过于复杂，不适合解决当前的可用区级别问题。<br><br>D. 创建使用ALB目标组ARN的Amazon Route 53应用程序恢复控制器资源集，并创建使用ElbV2TargetGroupsCanServeTraffic规则的就绪检查 - 虽然这提供了健康检查功能，但同样是针对更高级别的恢复场景，不是解决ALB内部可用区流量分配的最直接方法。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>275</td>
                    <td>A company sends its AWS Network Firewall ow logs to an Amazon S3 bucket. The company then analyzes the ow logs by using Amazon<br>Athena.<br>The company needs to transform the ow logs and add additional data before the ow logs are delivered to the existing S3 bucket.</td>
                    <td>A. Create an AWS Lambda function to transform the data and to write a new object to the existing S3 bucket. Congure the Lambda<br>function with an S3 trigger for the existing S3 bucket. Specify all object create events for the event type. Acknowledge the recursive<br>invocation.<br>B. Enable Amazon EventBridge notications on the existing S3 bucket. Create a custom EventBridge event bus. Create an EventBridge rule<br>that is associated with the custom event bus. Congure the rule to react to all object create events for the existing S3 bucket and to<br>invoke an AWS Step Functions workow. Congure a Step Functions task to transform the data and to write the data into a new S3 bucket.<br>C. Create an Amazon EventBridge rule that is associated with the default EventBridge event bus. Congure the rule to react to all object<br>create events for the existing S3 bucket. Dene a new S3 bucket as the target for the rule. Create an EventBridge input transformation to<br>customize the event before passing the event to the rule target.<br>D. Create an Amazon Kinesis Data Firehose delivery stream that is congured with an AWS Lambda transformer. Specify the existing S3<br>bucket as the destination. Change the Network Firewall logging destination from Amazon S3 to Kinesis Data Firehose.</td>
                    <td>一家公司将其AWS网络防火墙流日志发送到Amazon S3存储桶。然后公司使用Amazon Athena分析这些流日志。<br>公司需要在流日志传送到现有S3存储桶之前对流日志进行转换并添加额外数据。</td>
                    <td>A. 创建AWS Lambda函数来转换数据并向现有S3存储桶写入新对象。为现有S3存储桶配置Lambda函数的S3触发器，指定所有对象创建事件作为事件类型，并确认递归调用。<br>这个选项存在严重问题：会导致无限递归调用。当Lambda函数向同一个S3存储桶写入新对象时，会再次触发S3事件，从而再次调用Lambda函数，形成无限循环。虽然题目提到&quot;确认递归调用&quot;，但这并不是一个好的解决方案。<br><br>B. 在现有S3存储桶上启用Amazon EventBridge通知。创建自定义EventBridge事件总线。创建与自定义事件总线关联的EventBridge规则。配置规则响应现有S3存储桶的所有对象创建事件并调用AWS Step Functions工作流。配置Step Functions任务来转换数据并将数据写入新的S3存储桶。<br>这个方案过于复杂，使用了不必要的自定义事件总线和Step Functions，增加了架构复杂性和成本。<br><br>C. 创建与默认EventBridge事件总线关联的Amazon EventBridge规则。配置规则响应现有S3存储桶的所有对象创建事件。定义新的S3存储桶作为规则的目标。创建EventBridge输入转换来在将事件传递给规则目标之前自定义事件。<br>这个选项使用EventBridge的输入转换功能来转换数据，并将转换后的数据发送到新的S3存储桶，避免了递归问题，架构相对简单。<br><br>D. 创建配置了AWS Lambda转换器的Amazon Kinesis Data Firehose传输流。指定现有S3存储桶作为目标。将网络防火墙日志目标从Amazon S3更改为Kinesis Data Firehose。<br>这个方案需要更改现有的网络防火墙配置，可能会影响现有的日志流程，不是最佳选择。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>276</td>
                    <td>A DevOps engineer needs to implement integration tests into an existing AWS CodePipeline CI/CD workow for an Amazon Elastic Container<br>Service (Amazon ECS) service. The CI/CD workow retrieves new application code from an AWS CodeCommit repository and builds a container<br>image. The Cl/CD workow then uploads the container image to Amazon Elastic Container Registry (Amazon ECR) with a new image tag<br>version.<br>The integration tests must ensure that new versions of the service endpoint are reachable and that various API methods return successful<br>response data. The DevOps engineer has already created an ECS cluster to test the service.</td>
                    <td>A. Add a deploy stage to the pipeline. Congure Amazon ECS as the action provider.<br>B. Add a deploy stage to the pipeline. Congure AWS CodeDeploy as the action provider.<br>C. Add an appspec.yml le to the CodeCommit repository.<br>D. Update the image build pipeline stage to output an imagedenitions.json le that references the new image tag.<br>E. Create an AWS Lambda function that runs connectivity checks and API calls against the service. Integrate the Lambda function with<br>CodePipeline by using a Lambda action stage.</td>
                    <td>一名DevOps工程师需要将集成测试实施到现有的AWS CodePipeline CI/CD工作流中，该工作流用于Amazon Elastic Container Service (Amazon ECS)服务。CI/CD工作流从AWS CodeCommit存储库检索新的应用程序代码并构建容器镜像。然后CI/CD工作流将容器镜像上传到Amazon Elastic Container Registry (Amazon ECR)，并使用新的镜像标签版本。<br><br>集成测试必须确保服务端点的新版本是可达的，并且各种API方法返回成功的响应数据。DevOps工程师已经创建了一个ECS集群来测试该服务。</td>
                    <td>A. 向管道添加部署阶段，配置Amazon ECS作为操作提供者。这个选项是正确的，因为需要将新构建的容器镜像部署到ECS集群中进行集成测试。通过添加ECS部署阶段，可以将镜像部署到测试环境中，为后续的集成测试做准备。<br><br>B. 向管道添加部署阶段，配置AWS CodeDeploy作为操作提供者。这个选项不是最佳选择，因为CodeDeploy主要用于EC2实例和本地服务器的部署，而题目明确提到使用的是ECS服务，直接使用ECS作为操作提供者更合适。<br><br>C. 向CodeCommit存储库添加appspec.yml文件。这个选项不正确，appspec.yml文件主要用于CodeDeploy部署配置，而在ECS环境中通常使用任务定义和服务配置，不需要appspec.yml文件。<br><br>D. 更新镜像构建管道阶段以输出引用新镜像标签的imagedefinitions.json文件。这个选项是正确的，imagedefinitions.json文件是ECS部署所必需的，它告诉ECS使用哪个镜像版本进行部署，这是连接构建阶段和部署阶段的关键文件。<br><br>E. 创建一个AWS Lambda函数来运行连接检查和对服务的API调用，通过使用Lambda操作阶段将Lambda函数与CodePipeline集成。这个选项是正确的，Lambda函数可以执行集成测试，验证服务端点的可达性和API方法的响应，完全符合题目要求的集成测试需求。</td>
                    <td>ADE</td>
                </tr>
                <tr>
                    <td>277</td>
                    <td>A company runs applications on Windows and Linux Amazon EC2 instances. The instances run across multiple Availability Zones in an AWS<br>Region. The company uses Auto Scaling groups for each application.<br>The company needs a durable storage solution for the instances. The solution must use SMB for Windows and must use NFS for Linux. The<br>solution must also have sub-millisecond latencies. All instances will read and write the data.</td>
                    <td>A. Create an Amazon Elastic File System (Amazon EFS) le system that has targets in multiple Availability Zones.<br>B. Create an Amazon FSx for NetApp ONTAP Multi-AZ le system.<br>C. Create a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume to use for shared storage.<br>D. Update the user data for each application’s launch template to mount the le system.<br>E. Perform an instance refresh on each Auto Scaling group.</td>
                    <td>一家公司在Windows和Linux Amazon EC2实例上运行应用程序。这些实例运行在AWS区域的多个可用区中。公司为每个应用程序使用Auto Scaling组。<br>公司需要为实例提供持久的存储解决方案。该解决方案必须为Windows使用SMB协议，为Linux使用NFS协议。该解决方案还必须具有亚毫秒级延迟。所有实例都将读写数据。</td>
                    <td>A. 创建一个在多个可用区中有目标的Amazon Elastic File System (Amazon EFS)文件系统 - 这个选项部分正确。EFS支持NFS协议，适合Linux实例，并且可以跨多个可用区提供持久存储。但是EFS不支持SMB协议，无法满足Windows实例的需求。<br><br>B. 创建一个Amazon FSx for NetApp ONTAP Multi-AZ文件系统 - 这个选项正确。FSx for NetApp ONTAP支持多协议访问，包括SMB和NFS，可以同时满足Windows和Linux实例的需求。它提供Multi-AZ部署以确保高可用性和持久性，并且能够提供亚毫秒级延迟。<br><br>C. 创建一个通用SSD (gp3) Amazon Elastic Block Store (Amazon EBS)卷用于共享存储 - 这个选项不正确。EBS卷一次只能挂载到一个实例，不支持多实例共享访问，无法满足所有实例读写数据的需求。<br><br>D. 更新每个应用程序启动模板的用户数据以挂载文件系统 - 这个选项是实施步骤，不是存储解决方案本身。<br><br>E. 对每个Auto Scaling组执行实例刷新 - 这个选项是部署步骤，不是存储解决方案。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>278</td>
                    <td>A company uses an organization in AWS Organizations that a security team and a DevOps team manage. Both teams access the accounts by<br>using AWS IAM Identity Center.<br>A dedicated group has been created for each team. The DevOps team&#x27;s group has been assigned a permission set named DevOps. The<br>permission set has the AdministratorAccess managed IAM policy attached. The permission set has been applied to all accounts in the<br>organization.<br>The security team wants to ensure that the DevOps team does not have access to IAM Identity Center in the organization&#x27;s management<br>account. The security team has attached the following SCP to the organization root:<br>After implementing the policy, the security team discovers that the DevOps team can still access IAM Identity Center.</td>
                    <td>A. In the organization&#x27;s management account, create a new OU. Move the organization&#x27;s management account to the new OU. Detach the<br>SCP from the organization root. Attach the SCP to the new OU.<br>B. In the organization&#x27;s management account, update the SCP condition reference to the ARN of the DevOps team&#x27;s group role to include<br>the AWS account ID of the organization&#x27;s management account.<br>C. In IAM Identity Center, create a new permission set. Ensure that the assigned policy has full access but explicitly denies permission for<br>the sso:* action and the sso-directory:* action. Update the assigned permission set for the DevOps team&#x27;s group role in the organization&#x27;s<br>management account. Delete the SCP.<br>D. In IAM Identity Center, update the DevOps permission set. Ensure that the assigned policy has full access but explicitly denies<br>permission for the sso:* action and the sso-directory:* action. In the Deny statement, add a StringEquals condition that compares the<br>aws:SourceAccount global condition context key with the organization&#x27;s management account IDelete the SCP.</td>
                    <td>一家公司使用AWS Organizations中的组织，由安全团队和DevOps团队管理。两个团队都通过AWS IAM Identity Center访问账户。<br>为每个团队创建了专用组。DevOps团队的组被分配了名为DevOps的权限集。该权限集附加了AdministratorAccess托管IAM策略。该权限集已应用于组织中的所有账户。<br>安全团队希望确保DevOps团队无法访问组织管理账户中的IAM Identity Center。安全团队已将以下SCP附加到组织根：<br>实施策略后，安全团队发现DevOps团队仍然可以访问IAM Identity Center。</td>
                    <td>A. 在组织的管理账户中创建新的OU，将组织的管理账户移动到新的OU中，从组织根分离SCP，将SCP附加到新的OU。这个方案存在概念错误，因为管理账户不能移动到OU中，管理账户始终位于组织根级别，不受SCP限制。<br><br>B. 在组织的管理账户中，更新SCP条件引用，将DevOps团队组角色的ARN包含组织管理账户的AWS账户ID。这个方案试图通过修改SCP条件来解决问题，但SCP对管理账户不生效，所以这种方法无法解决根本问题。<br><br>C. 在IAM Identity Center中创建新的权限集，确保分配的策略具有完全访问权限但明确拒绝sso:*和sso-directory:*操作。为DevOps团队在管理账户中的组角色更新分配的权限集，删除SCP。这个方案通过在权限集级别限制访问，可以有效控制DevOps团队对IAM Identity Center的访问。<br><br>D. 在IAM Identity Center中更新DevOps权限集，确保分配的策略具有完全访问权限但明确拒绝sso:*和sso-directory:*操作，在拒绝语句中添加StringEquals条件比较aws:SourceAccount，删除SCP。这个方案类似于C，但只针对管理账户进行限制。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>279</td>
                    <td>An Amazon EC2 Auto Scaling group manages EC2 instances that were created from an AMI. The AMI has the AWS Systems Manager Agent<br>installed. When an EC2 instance is launched into the Auto Scaling group, tags are applied to the EC2 instance.<br>EC2 instances that are launched by the Auto Scaling group must have the correct operating system conguration.</td>
                    <td>A. Create a Systems Manager Run Command document that congures the desired instance conguration. Set up Systems Manager<br>Compliance to invoke the Run Command document when the EC2 instances are not in compliance with the most recent patches.<br>B. Create a Systems Manager State Manager association that links to the Systems Manager command document. Create a tag query that<br>runs immediately.<br>C. Create a Systems Manager Run Command task that species the desired instance conguration. Create a maintenance window in<br>Systems Manager Maintenance Windows that runs daily. Register the Run Command task against the maintenance window. Designate the<br>targets.<br>D. Create a Systems Manager Patch Manager patch baseline and a patch group that use the same tags that the Auto Scaling group<br>applies. Register the patch group with the patch baseline. Dene a Systems Manager command document to patch the instances Invoke<br>the document by using Systems Manager Run Command.</td>
                    <td>一个Amazon EC2 Auto Scaling组管理着从AMI创建的EC2实例。该AMI已安装了AWS Systems Manager Agent。当EC2实例启动到Auto Scaling组中时，会对EC2实例应用标签。由Auto Scaling组启动的EC2实例必须具有正确的操作系统配置。</td>
                    <td>A. 创建一个Systems Manager Run Command文档来配置所需的实例配置。设置Systems Manager Compliance在EC2实例不符合最新补丁时调用Run Command文档。这个选项的问题是它是被动的，只有在实例不合规时才会执行配置，而且主要关注补丁合规性，不能确保新启动的实例立即获得正确配置。<br><br>B. 创建一个Systems Manager State Manager关联，链接到Systems Manager命令文档。创建一个立即运行的标签查询。这个选项是最佳的，因为State Manager可以确保实例持续保持所需的配置状态，并且可以通过标签查询自动识别新启动的实例，立即应用配置。这是主动的配置管理方式。<br><br>C. 创建一个Systems Manager Run Command任务来指定所需的实例配置。在Systems Manager Maintenance Windows中创建每日运行的维护窗口。将Run Command任务注册到维护窗口并指定目标。这个选项的问题是它依赖于计划的维护窗口，新启动的实例可能需要等待最多24小时才能获得正确配置，不能满足立即配置的需求。<br><br>D. 创建一个Systems Manager Patch Manager补丁基线和使用Auto Scaling组应用的相同标签的补丁组。将补丁组注册到补丁基线。定义一个Systems Manager命令文档来修补实例，使用Systems Manager Run Command调用文档。这个选项主要关注补丁管理，而不是完整的操作系统配置，且需要手动调用，不是自动化的解决方案。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>280</td>
                    <td>A company uses AWS Organizations to manage its AWS accounts. The organization root has a child OU that is named Department. The<br>Department OU has a child OU that is named Engineering. The default FullAWSAccess policy is attached to the root, the Department OU, and<br>the Engineering OU.<br>The company has many AWS accounts in the Engineering OU. Each account has an administrative IAM role with the AdministratorAccess IAM<br>policy attached. The default FullAWSAccessPolicy is also attached to each account.<br>A DevOps engineer plans to remove the FullAWSAccess policy from the Department OU. The DevOps engineer will replace the policy with a<br>policy that contains an Allow statement for all Amazon EC2 API operations.</td>
                    <td>A. All API actions on all resources will be allowed.<br>B. All API actions on EC2 resources will be allowed. All other API actions will be denied.<br>C. All API actions on all resources will be denied.<br>D. All API actions on EC2 resources will be denied. All other API actions will be allowed.</td>
                    <td>一家公司使用AWS Organizations来管理其AWS账户。组织根有一个名为Department的子OU。Department OU有一个名为Engineering的子OU。默认的FullAWSAccess策略附加到根、Department OU和Engineering OU上。<br><br>公司在Engineering OU中有许多AWS账户。每个账户都有一个管理IAM角色，附加了AdministratorAccess IAM策略。默认的FullAWSAccess策略也附加到每个账户上。<br><br>一名DevOps工程师计划从Department OU中移除FullAWSAccess策略。DevOps工程师将用一个包含允许所有Amazon EC2 API操作的Allow语句的策略来替换该策略。</td>
                    <td>A. 所有资源上的所有API操作都将被允许 - 这个选项不正确。虽然根OU和Engineering OU仍然有FullAWSAccess策略，但由于SCP（服务控制策略）的工作原理是取交集，Department OU的限制性策略会影响其子OU。<br><br>B. EC2资源上的所有API操作将被允许，所有其他API操作将被拒绝 - 这个选项正确。在AWS Organizations中，SCP策略是通过层级继承的，并且采用最严格的限制。当Department OU的策略被替换为只允许EC2操作的策略时，Engineering OU中的账户将受到这个限制，即使Engineering OU本身仍有FullAWSAccess策略。<br><br>C. 所有资源上的所有API操作都将被拒绝 - 这个选项不正确。新策略明确允许EC2 API操作，所以不是完全拒绝。<br><br>D. EC2资源上的所有API操作将被拒绝，所有其他API操作将被允许 - 这个选项不正确，与实际的SCP行为相反。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>281</td>
                    <td>A company manages AWS accounts in AWS Organizations. The company needs a solution to send Amazon CloudWatch Logs data to an<br>Amazon S3 bucket in a dedicated AWS account. The solution must support all existing and future CloudWatch Logs log groups.</td>
                    <td>A. Enable Organizations backup policies to back up all log groups to a dedicated S3 bucket. Add an S3 bucket policy that allows access<br>from all accounts that belong to the company.<br>B. Create a backup plan in AWS Backup. Specify a dedicated S3 bucket as a backup vault. Assign all CloudWatch Logs log group resources<br>to the backup plan. Create resource assignments in the backup plan for all accounts that belong to the company.<br>C. Create a backup plan in AWS Backup. Specify a dedicated S3 bucket as a backup vault. Assign all existing log groups to the backup<br>plan. Create resource assignments in the backup plan for all accounts that belong to the company. Create an AWS Systems Manager<br>Automation runbook to assign log groups to a backup plan. Create an AWS Cong rule that has an automatic remediation action for all<br>noncompliant log groups. Specify the runbook as the rule&#x27;s target.<br>D. Create a CloudWatch Logs destination and an Amazon Kinesis Data Firehose delivery stream in the dedicated AWS account. Specify the<br>S3 bucket as the destination of the delivery stream. Create subscription lters for all existing log groups in all accounts. Create an AWS<br>Lambda function to call the CloudWatch Logs PutSubscriptionFilter API operation. Create an Amazon EventBridge rule to invoke the<br>Lambda function when a CreateLogGroup event occurs.</td>
                    <td>一家公司在AWS Organizations中管理AWS账户。该公司需要一个解决方案将Amazon CloudWatch Logs数据发送到专用AWS账户中的Amazon S3存储桶。该解决方案必须支持所有现有和未来的CloudWatch Logs日志组。</td>
                    <td>选项A：启用Organizations备份策略将所有日志组备份到专用S3存储桶。添加允许公司所有账户访问的S3存储桶策略。<br>这个选项不正确。Organizations备份策略主要用于管理AWS Backup服务的策略，而不是直接处理CloudWatch Logs的实时流式传输。此外，这种方法无法自动处理未来创建的日志组，也不是CloudWatch Logs数据传输的标准方法。<br><br>选项B：在AWS Backup中创建备份计划。指定专用S3存储桶作为备份保管库。将所有CloudWatch Logs日志组资源分配给备份计划。为公司所有账户在备份计划中创建资源分配。<br>这个选项也不正确。AWS Backup主要用于定期备份，而不是实时或近实时的日志流式传输。CloudWatch Logs不是AWS Backup直接支持的资源类型，且这种方法无法满足持续的日志传输需求。<br><br>选项C：在AWS Backup中创建备份计划，指定专用S3存储桶作为备份保管库。将所有现有日志组分配给备份计划。为公司所有账户创建资源分配。创建AWS Systems Manager自动化运行手册来分配日志组到备份计划。创建带有自动修复操作的AWS Config规则处理不合规的日志组，指定运行手册作为规则目标。<br>虽然这个选项考虑了自动化处理新日志组的需求，但仍然基于AWS Backup，这不是处理CloudWatch Logs流式传输的正确方法。过于复杂且不是最佳实践。<br><br>选项D：在专用AWS账户中创建CloudWatch Logs目标和Amazon Kinesis Data Firehose传输流。指定S3存储桶作为传输流的目标。为所有账户中的所有现有日志组创建订阅过滤器。创建AWS Lambda函数调用CloudWatch Logs PutSubscriptionFilter API操作。创建Amazon EventBridge规则在CreateLogGroup事件发生时调用Lambda函数。<br>这是正确的方法。使用CloudWatch Logs订阅过滤器和Kinesis Data Firehose是将日志数据流式传输到S3的标准架构。EventBridge规则监听CreateLogGroup事件，Lambda函数自动为新日志组创建订阅过滤器，确保支持未来的日志组。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>282</td>
                    <td>A DevOps engineer manages a Java-based application that runs in an Amazon Elastic Container Service (Amazon ECS) cluster on AWS<br>Fargate. Auto scaling has not been congured for the application.<br>The DevOps engineer has determined that the Java Virtual Machine (JVM) thread count is a good indicator of when to scale the application.<br>The application serves customer trac on port 8080 and makes JVM metrics available on port 9404.<br>Application use has recently increased. The DevOps engineer needs to congure auto scaling for the application.</td>
                    <td>A. Deploy the Amazon CloudWatch agent as a container sidecar. Congure the CloudWatch agent to retrieve JVM metrics from port 9404.<br>Create CloudWatch alarms on the JVM thread count metric to scale the application. Add a step scaling policy in Fargate to scale up and<br>scale down based on the CloudWatch alarms.<br>B. Deploy the Amazon CloudWatch agent as a container sidecar. Congure a metric lter for the JVM thread count metric on the<br>CloudWatch log group for the CloudWatch agent. Add a target tracking policy in Fargate. Select the metric from the metric lter as a scale<br>target.<br>C. Create an Amazon Managed Service for Prometheus workspace. Deploy AWS Distro for OpenTelemetry as a container sidecar to publish<br>the JVM metrics from port 9404 to the Prometheus workspace. Congure rules for the workspace to use the JVM thread count metric to<br>scale the application. Add a step scaling policy in Fargate. Select the Prometheus rules to scale up and scaling down.<br>D. Create an Amazon Managed Service for Prometheus workspace. Deploy AWS Distro for OpenTelemetry as a container sidecar to retrieve<br>JVM metrics from port 9404 to publish the JVM metrics from port 9404 to the Prometheus workspace. Add a target tracking policy in<br>Fargate. Select the Prometheus metric as a scale target.</td>
                    <td>一名DevOps工程师管理着一个基于Java的应用程序，该应用程序运行在AWS Fargate上的Amazon Elastic Container Service (Amazon ECS)集群中。该应用程序尚未配置自动扩缩容。<br>DevOps工程师已确定Java虚拟机(JVM)线程数是决定何时扩缩容应用程序的良好指标。<br>该应用程序在端口8080上为客户流量提供服务，并在端口9404上提供JVM指标。<br>应用程序使用量最近有所增加。DevOps工程师需要为应用程序配置自动扩缩容。</td>
                    <td>选项A：将Amazon CloudWatch代理部署为容器边车。配置CloudWatch代理从端口9404检索JVM指标。在JVM线程数指标上创建CloudWatch告警来扩缩容应用程序。在Fargate中添加步进扩缩容策略，基于CloudWatch告警进行扩容和缩容。这个方案是可行的，CloudWatch代理可以收集自定义指标，并通过告警触发扩缩容策略。<br><br>选项B：将Amazon CloudWatch代理部署为容器边车。在CloudWatch代理的CloudWatch日志组上为JVM线程数指标配置指标过滤器。在Fargate中添加目标跟踪策略。选择指标过滤器中的指标作为扩缩容目标。这个方案有问题，因为指标过滤器主要用于从日志中提取指标，而不是直接从端口获取指标数据。<br><br>选项C：创建Amazon Managed Service for Prometheus工作空间。部署AWS Distro for OpenTelemetry作为容器边车，将端口9404的JVM指标发布到Prometheus工作空间。为工作空间配置规则，使用JVM线程数指标来扩缩容应用程序。在Fargate中添加步进扩缩容策略。选择Prometheus规则进行扩容和缩容。这个方案的问题是Prometheus规则不能直接与Fargate的扩缩容策略集成。<br><br>选项D：创建Amazon Managed Service for Prometheus工作空间。部署AWS Distro for OpenTelemetry作为容器边车，从端口9404检索JVM指标并发布到Prometheus工作空间。在Fargate中添加目标跟踪策略。选择Prometheus指标作为扩缩容目标。这个方案也有问题，因为Fargate的目标跟踪策略不能直接使用Prometheus指标。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>283</td>
                    <td>A company has an application that runs in a single AWS Region. The application runs on an Amazon Elastic Kubernetes Service (Amazon EKS)<br>cluster and connects to an Amazon Aurora MySQL cluster. The application is built in an AWS CodeBuild project. The container images are<br>published to Amazon Elastic Container Registry (Amazon ECR).<br>The company needs to replicate the state of the application for the container images and the database to a second Region.</td>
                    <td>A. Turn on Amazon S3 Cross-Region Replication (CRR) on the bucket that holds the ECR container images. Deploy the application to an EKS<br>cluster in the second Region by referencing the new S3 bucket object URL for the container image in a Kubernetes deployment le.<br>Congure a cross-Region Aurora Replica in the second Region. Congure the new application deployment to use the endpoints for the<br>cross-Region Aurora Replica.<br>B. Create an Amazon EventBridge rule that reacts to image pushes to the ECR repository. Congure the EventBridge rule to invoke an AWS<br>Lambda function to replicate the image to a new ECR repository in the second Region. Deploy the application to an EKS cluster in the<br>second Region by referencing the new ECR repository in a Kubernetes deployment le. Congure a cross-Region Aurora Replica in the<br>second Region. Congure the new application deployment to use the endpoints for the cross-Region Aurora Replica.<br>C. Turn on Cross-Region Replication to replicate the ECR repository to the second Region. Deploy the application to an EKS cluster in the<br>second Region by referencing the new ECR repository in a Kubernetes deployment le. Congure an Aurora global database with clusters<br>in the initial Region and the second Region. Congure the new application deployment to use the endpoints for the second Region&#x27;s<br>cluster in the Aurora global database.<br>D. Congure the CodeBuild project to also push the container image to an ECR repository in the second Region. Deploy the application to<br>an EKS cluster in the second Region by referencing the new ECR repository in a Kubernetes deployment le. Congure an Aurora MySQL<br>cluster in the second Region as the target for binary log replication from the Aurora MySQL cluster in the initial Region. Congure the new<br>application deployment to use the endpoints for the second Region&#x27;s cluster.</td>
                    <td>一家公司有一个在单个AWS区域运行的应用程序。该应用程序运行在Amazon Elastic Kubernetes Service (Amazon EKS)集群上，并连接到Amazon Aurora MySQL集群。该应用程序在AWS CodeBuild项目中构建。容器镜像发布到Amazon Elastic Container Registry (Amazon ECR)。<br>公司需要将应用程序的容器镜像和数据库状态复制到第二个区域。</td>
                    <td>选项A：建议使用S3跨区域复制来复制ECR容器镜像，这是错误的。ECR不是基于S3存储的，不能使用S3的跨区域复制功能。ECR有自己的复制机制。此外，在Kubernetes部署文件中引用S3存储桶对象URL作为容器镜像也是不正确的做法。对于数据库部分，使用跨区域Aurora副本是可行的，但整体方案存在根本性错误。<br><br>选项B：使用EventBridge规则监听ECR镜像推送事件，然后通过Lambda函数复制镜像到第二个区域的ECR仓库。这种方法技术上可行，但过于复杂，需要自定义开发Lambda函数来处理镜像复制逻辑。对于数据库使用跨区域Aurora副本是合理的，但这不是最优解决方案，因为AWS提供了更简单直接的ECR复制功能。<br><br>选项C：使用ECR的跨区域复制功能将仓库复制到第二个区域，这是ECR的原生功能，简单高效。对于数据库，配置Aurora全球数据库，在初始区域和第二个区域都有集群，这提供了更好的性能和灾难恢复能力。新应用部署使用第二个区域集群的端点。这是最佳实践的组合。<br><br>选项D：配置CodeBuild项目同时推送容器镜像到第二个区域的ECR仓库，这需要修改构建流程。对于数据库，使用二进制日志复制从初始区域的Aurora MySQL集群复制到第二个区域，这种方法相对复杂，且不如Aurora全球数据库功能强大和易于管理。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>284</td>
                    <td>A company is building a serverless application that uses AWS Lambda functions to process data.<br>A BeginResponse Lambda function initializes data in response to specic application events. The company needs to ensure that a large<br>number of Lambda functions are invoked after the BeginResponse Lambda function runs. Each Lambda function must be invoked in parallel<br>and depends on only the outputs of the BeginResponse Lambda function. Each Lambda function has retry logic for invocation and must be<br>able to ne-tune concurrency without losing data.</td>
                    <td>A. Create an Amazon Simple Notication Service (Amazon SNS) topic. Modify the BeginResponse Lambda function to publish to the SNS<br>topic before the BeginResponse Lambda function nishes running. Subscribe all Lambda functions that need to invoke after the<br>BeginResponse Lambda function runs to the SNS topic. Subscribe any new Lambda functions to the SNS topic.<br>B. Create an Amazon Simple Queue Service (Amazon SQS) queue for each Lambda function that needs to run after the BeginResponse<br>Lambda function runs. Subscribe each Lambda function to its own SQS queue. Create an Amazon Simple Notication Service (Amazon<br>SNS) topic. Subscribe each SQS queue to the SNS topic. Modify the BeginResponse function to publish to the SNS topic when it nishes<br>running.<br>C. Create an Amazon Simple Queue Service (Amazon SQS) queue for each Lambda function that needs to run after the BeginResponse<br>Lambda function runs. Subscribe the Lambda function to the SQS queue. Create an Amazon Simple Notication Service (Amazon SNS)<br>topic for each SQS queue. Subscribe the SQS queues to the SNS topics. Modify the BeginResponse function to publish to the SNS topics<br>when the function nishes running.<br>D. Create an AWS Step Functions Standard Workow. Congure states in the workow to invoke the Lambda functions sequentially. Create<br>an Amazon Simple Notication Service (Amazon SNS) topic. Modify the BeginResponse Lambda function to publish to the SNS topic<br>before the Lambda function nishes running. Create a new Lambda function that is subscribed to the SNS topic and that invokes the Step<br>Functions workow.</td>
                    <td>一家公司正在构建一个使用AWS Lambda函数处理数据的无服务器应用程序。BeginResponse Lambda函数响应特定应用程序事件来初始化数据。公司需要确保在BeginResponse Lambda函数运行后调用大量Lambda函数。每个Lambda函数必须并行调用，并且只依赖于BeginResponse Lambda函数的输出。每个Lambda函数都有调用重试逻辑，并且必须能够在不丢失数据的情况下微调并发性。</td>
                    <td>选项A：创建Amazon SNS主题，修改BeginResponse Lambda函数在完成运行前发布到SNS主题，所有需要在BeginResponse Lambda函数运行后调用的Lambda函数都订阅SNS主题。这种方案虽然能实现并行调用，但SNS是推送模式，无法提供重试逻辑和并发控制，也无法保证数据不丢失。如果Lambda函数处理失败，消息可能会丢失。<br><br>选项B：为每个需要在BeginResponse Lambda函数运行后运行的Lambda函数创建单独的Amazon SQS队列，每个Lambda函数订阅自己的SQS队列。创建Amazon SNS主题，每个SQS队列订阅SNS主题。修改BeginResponse函数在完成运行时发布到SNS主题。这种方案结合了SNS的扇出能力和SQS的可靠性，每个Lambda函数都有独立的队列，可以独立控制并发性和重试逻辑，确保数据不丢失。<br><br>选项C：为每个Lambda函数创建单独的SQS队列和SNS主题，这种设计过于复杂且不必要。一对一的SNS-SQS映射没有意义，增加了系统复杂性而没有带来额外好处。<br><br>选项D：使用AWS Step Functions标准工作流配置状态来顺序调用Lambda函数。这与题目要求的并行执行相矛盾，Step Functions的顺序执行不符合并行处理的需求。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>285</td>
                    <td>A company operates a globally deployed product out of multiple AWS Regions. The company&#x27;s DevOps team needs to use Amazon API<br>Gateway to deploy an API to support the product.<br>The API must be deployed redundantly. The deployment must provide independent availability from each company location. The deployment<br>also must respond to a custom domain URL and must optimize performance for the API user requests.</td>
                    <td>A. Deploy an API Gateway edge-optimized API endpoint in the us-east-1 Region. Create an API Gateway custom domain for the API. Create<br>an Amazon Route 53 record set with a geoproximity routing policy for the API&#x27;s custom domain. Increase the geographic bias to the<br>maximum allowed value.<br>B. Deploy an API Gateway regional API endpoint in the us-east-1 Region. Integrate the API Gateway API with a public Application Load<br>Balancer (ALB). Create an AWS Global Accelerator standard accelerator. Associate the endpoint with the ALCreate an Amazon Route 53<br>alias record set that points the custom domain name to the DNS name that is assigned to the accelerator.<br>C. Deploy an API Gateway regional API endpoint in every AWS Region where the company&#x27;s product is deployed. Create an API Gateway<br>custom domain in each Region for the deployed API Gateway API. Create an Amazon Route 53 record set that has a latency routing policy<br>for every deployed API Gateway custom domain.<br>D. Deploy an API Gateway edge-optimized API endpoint in the us-east-1 Region. Create an Amazon CloudFront distribution. Congure the<br>CloudFront distribution with an alternate domain name. Specify the API Gateway Invoke URL as the origin domain. Create an Amazon<br>Route 53 alias record set with a simple routing policy. Point the routing policy to the CloudFront distribution domain name.</td>
                    <td>一家公司在多个AWS区域运营全球部署的产品。公司的DevOps团队需要使用Amazon API Gateway来部署一个API以支持该产品。<br>该API必须冗余部署。部署必须为每个公司位置提供独立的可用性。部署还必须响应自定义域名URL，并且必须为API用户请求优化性能。</td>
                    <td>A. 在us-east-1区域部署边缘优化的API Gateway端点，创建自定义域名和Route 53地理邻近路由策略。这种方案只在单个区域部署，不能提供真正的冗余性和独立可用性，如果us-east-1区域出现问题，整个API服务将不可用。虽然边缘优化可以提供一定的性能优化，但不满足冗余部署的核心要求。<br><br>B. 在us-east-1区域部署区域性API端点，集成ALB和Global Accelerator。虽然Global Accelerator可以提供性能优化，但仍然只在单个区域部署API Gateway，不能提供真正的区域级冗余。如果us-east-1区域出现故障，API服务仍会中断，不满足独立可用性要求。<br><br>C. 在公司产品部署的每个AWS区域都部署区域性API Gateway端点，为每个区域创建自定义域名，使用Route 53延迟路由策略。这种方案提供了真正的多区域冗余部署，每个区域都有独立的API Gateway实例，确保了独立可用性。延迟路由策略可以将用户请求路由到延迟最低的区域，优化性能。<br><br>D. 部署边缘优化的API Gateway并配置CloudFront分发。虽然CloudFront可以提供全球性能优化，但底层仍然只有单个区域的API Gateway，不能提供区域级的冗余和独立可用性。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>286</td>
                    <td>A DevOps engineer uses AWS CodeBuild to frequently produce software packages. The CodeBuild project builds large Docker images that the<br>DevOps engineer can use across multiple builds.<br>The DevOps engineer wants to improve build performance and minimize costs.</td>
                    <td>A. Store the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Implement a local Docker layer cache for<br>CodeBuild.<br>B. Cache the Docker images in an Amazon S3 bucket that is available across multiple build hosts. Expire the cache by using an S3<br>Lifecycle policy.<br>C. Store the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Modify the CodeBuild project runtime<br>conguration to always use the most recent image version.<br>D. Create custom AMIs that contain the cached Docker images. In the CodeBuild build, launch Amazon EC2 instances from the custom<br>AMIs.</td>
                    <td>一位DevOps工程师使用AWS CodeBuild频繁生成软件包。CodeBuild项目构建大型Docker镜像，DevOps工程师可以在多个构建中使用这些镜像。DevOps工程师希望提高构建性能并最小化成本。</td>
                    <td>A. 将Docker镜像存储在Amazon Elastic Container Registry (Amazon ECR)仓库中，为CodeBuild实施本地Docker层缓存。这个选项非常合理，ECR是AWS原生的容器镜像仓库服务，与CodeBuild集成良好。本地Docker层缓存可以避免重复下载相同的镜像层，显著提高构建速度并减少网络传输成本。Docker的分层架构使得缓存机制能够有效复用已有的镜像层。<br><br>B. 将Docker镜像缓存在可跨多个构建主机使用的Amazon S3存储桶中，使用S3生命周期策略使缓存过期。虽然S3可以存储Docker镜像，但这不是最佳实践。S3不是专门为容器镜像设计的，缺乏镜像管理功能，且需要额外的脚本来处理镜像的上传下载，增加了复杂性。<br><br>C. 将Docker镜像存储在Amazon ECR仓库中，修改CodeBuild项目运行时配置以始终使用最新的镜像版本。虽然使用ECR是正确的，但始终使用最新版本并不能解决性能问题，反而可能因为每次都要下载完整镜像而降低性能，没有利用缓存机制。<br><br>D. 创建包含缓存Docker镜像的自定义AMI，在CodeBuild构建中从自定义AMI启动Amazon EC2实例。这个方案过于复杂且成本高昂，CodeBuild本身就是托管服务，不需要管理EC2实例，这违背了使用托管服务的初衷。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>287</td>
                    <td>A large company recently acquired a small company. The large company invited the small company to join the large company&#x27;s existing<br>organization in AWS Organizations as a new OU.<br>A DevOps engineer determines that the small company needs to launch t3.small Amazon EC2 instance types for the company&#x27;s application<br>workloads. The small company needs to deploy the instances only within US-based AWS Regions.<br>The DevOps engineer needs to use an SCP in the small company&#x27;s new OU to ensure that the small company can launch only the required<br>instance types.</td>
                    <td>A. Congure a statement to deny the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is not<br>equal to t3.small.<br>Congure another statement to deny the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition<br>is not equal to us-*.<br>B. Congure a statement to allow the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is not<br>equal to t3.small.<br>Congure another statement to allow the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion<br>condition is not equal to us-*.<br>C. Congure a statement to deny the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is equal<br>to t3.small.<br>Congure another statement to deny the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion condition<br>is equal to us-*.<br>D. Congure a statement to allow the ec2:RunInstances action for all EC2 instance resources when the ec2:InstanceType condition is<br>equal to t3.small.<br>Congure another statement to allow the ec2:RunInstances action for all EC2 instance resources when the aws:RequestedRegion<br>condition is equal to us-*.</td>
                    <td>一家大公司最近收购了一家小公司。大公司邀请小公司作为新的组织单位(OU)加入大公司在AWS Organizations中的现有组织。<br>一名DevOps工程师确定小公司需要为其应用程序工作负载启动t3.small Amazon EC2实例类型。小公司需要仅在美国的AWS区域内部署实例。<br>DevOps工程师需要在小公司的新OU中使用服务控制策略(SCP)来确保小公司只能启动所需的实例类型。</td>
                    <td>选项A：配置一个语句拒绝所有EC2实例资源的ec2:RunInstances操作，当ec2:InstanceType条件不等于tsmall时。配置另一个语句拒绝所有EC2实例资源的ec2:RunInstances操作，当aws:RequestedRegion条件不等于us-*时。这个选项正确使用了拒绝策略，当实例类型不是tsmall或区域不是美国时拒绝启动实例，符合SCP的最佳实践。<br><br>选项B：配置允许语句，当条件不满足时允许操作。这个逻辑是错误的，因为它会在实例类型不是tsmall或区域不是美国时允许操作，这与需求相反。<br><br>选项C：配置拒绝语句，当实例类型等于tsmall或区域等于us-*时拒绝操作。这个逻辑完全相反，会阻止用户启动所需的tsmall实例和在美国区域的操作。<br><br>选项D：配置允许语句，当条件满足时允许操作。虽然逻辑看似正确，但SCP主要用于设置权限边界，通常使用拒绝策略而不是允许策略来限制权限。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>288</td>
                    <td>A DevOps team manages infrastructure for an application. The application uses long-running processes to process items from an Amazon<br>Simple Queue Service (Amazon SQS) queue. The application is deployed to an Auto Scaling group.<br>The application recently experienced an issue where items were taking signicantly longer to process. The queue exceeded the expected size,<br>which prevented various business processes from functioning properly. The application records all logs to a third-party tool.<br>The team is currently subscribed to an Amazon Simple Notication Service (Amazon SNS) topic that the team uses for alerts. The team needs<br>to be alerted if the queue exceeds the expected size.</td>
                    <td>A. Create an Amazon CloudWatch metric alarm with a period of 1 hour and a static threshold to alarm if the average of the<br>ApproximateNumberOfMessagesDelayed metric is greater than the expected value. Congure the alarm to notify the SNS topic.<br>B. Create an Amazon CloudWatch metric alarm with a period of 1 hour and a static threshold to alarm if the sum of the<br>ApproximateNumberOfMessagesVisible metric is greater than the expected value. Congure the alarm to notify the SNS topic.<br>C. Create an AWS Lambda function that retrieves the ApproximateNumberOfMessages SQS queue attribute value and publishes the value<br>as a new CloudWatch custom metric. Create an Amazon EventBridge rule that is scheduled to run every 5 minutes and that invokes the<br>Lambda function. Congure a CloudWatch metrics alarm with a period of 1 hour and a static threshold to alarm if the sum of the new<br>custom metric is greater than the expected value.<br>D. Create an AWS Lambda function that checks the ApproximateNumberOfMessagesDelayed SQS queue attribute and compares the value<br>to a dened expected size in the function. Create an Amazon EventBridge rule that is scheduled to run every 5 minutes and that invokes<br>the Lambda function. When the ApproximateNumberOfMessagesDelayed SQS queue attribute exceeds the expected size, send a<br>notication the SNS topic.</td>
                    <td>一个DevOps团队管理应用程序的基础设施。该应用程序使用长时间运行的进程来处理来自Amazon Simple Queue Service (Amazon SQS)队列的项目。应用程序部署到Auto Scaling组中。<br>该应用程序最近遇到了一个问题，项目处理时间显著延长。队列超过了预期大小，这阻止了各种业务流程正常运行。应用程序将所有日志记录到第三方工具中。<br>团队目前订阅了一个Amazon Simple Notification Service (Amazon SNS)主题，用于接收警报。团队需要在队列超过预期大小时收到警报。</td>
                    <td>选项A：使用ApproximateNumberOfMessagesDelayed指标创建CloudWatch警报。这个指标表示延迟消息的数量（即那些在可见性超时期间或飞行中的消息），而不是队列中可见消息的总数。对于监控队列大小来说，这不是最合适的指标，因为它不能准确反映队列中等待处理的消息总数。<br><br>选项B：使用ApproximateNumberOfMessagesVisible指标创建CloudWatch警报。这个指标表示队列中可见且可供消费者检索的消息数量，这正是我们需要监控的队列大小指标。使用1小时周期和静态阈值来监控队列大小是合理的，当队列中可见消息数量超过预期值时触发警报并通知SNS主题。<br><br>选项C：创建Lambda函数获取ApproximateNumberOfMessages属性并发布为自定义CloudWatch指标。虽然技术上可行，但这种方法过于复杂，因为SQS已经原生提供了CloudWatch指标，无需创建自定义指标。这增加了不必要的复杂性和成本。<br><br>选项D：创建Lambda函数检查ApproximateNumberOfMessagesDelayed属性。与选项A类似，使用延迟消息指标不如使用可见消息指标准确。此外，这种方法也比直接使用CloudWatch警报更复杂，需要额外的Lambda函数和EventBridge规则。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>289</td>
                    <td>A large company runs critical workloads in multiple AWS accounts. The AWS accounts are managed under AWS Organizations with all features<br>enabled. The company stores condential customer data in an Amazon S3 bucket. Access to the S3 bucket requires multiple levels of<br>approval.<br>The company wants to monitor when the S3 bucket is accessed by using the AWS CLI. The company also wants insights into the various<br>activities performed by other users on all other S3 buckets in the AWS accounts to detect any issues.</td>
                    <td>A. Create an AWS CloudTrail trail that is delivered to Amazon CloudWatch in each AWS account. Enable data events logs for all S3 buckets.<br>Use Amazon GuardDuty for anomaly detection in all the AWS accounts. Use Amazon Athena to perform SQL queries on the custom metrics<br>created from the CloudTrail logs.<br>B. Create an AWS CloudTrail organization trail that is delivered to Amazon CloudWatch in the Organizations management account. Enable<br>data events logs for all S3 buckets. Use Amazon CloudWatch anomaly detection in all the AWS accounts. Use Amazon Athena to perform<br>SQL queries on the custom metrics created from the CloudTrail logs.<br>C. Create an AWS CloudTrail organization trail that is delivered to Amazon CloudWatch in the Organizations management account. Enable<br>data events logs for all S3 buckets. Use Amazon CloudWatch anomaly detection in all the AWS accounts. Use Amazon CloudWatch Metrics<br>Insights to perform SQL queries on the custom metrics created from the CloudTrail logs.<br>D. Create an AWS CloudTrail trail that is delivered to Amazon CloudWatch in each AWS account. Enable data events logs for all S3 buckets.<br>Use a custom solution for anomaly detection in all the AWS accounts. Use Amazon CloudWatch Metrics Insights to perform SQL queries on<br>the custom metrics created from the CloudTrail logs.</td>
                    <td>一家大型公司在多个AWS账户中运行关键工作负载。这些AWS账户通过启用了所有功能的AWS Organizations进行管理。该公司在Amazon S3存储桶中存储机密客户数据。访问S3存储桶需要多级审批。<br><br>公司希望监控何时通过AWS CLI访问S3存储桶。公司还希望深入了解其他用户在所有AWS账户中的其他S3存储桶上执行的各种活动，以检测任何问题。</td>
                    <td>选项A：在每个AWS账户中创建CloudTrail跟踪并传送到CloudWatch，为所有S3存储桶启用数据事件日志，使用GuardDuty进行异常检测，使用Athena查询CloudTrail日志的自定义指标。这个方案的问题是在多账户环境中为每个账户单独创建跟踪会增加管理复杂性和成本，而且使用Athena查询自定义指标不如直接查询日志数据有效。<br><br>选项B：创建组织级CloudTrail跟踪并传送到管理账户的CloudWatch，为所有S3存储桶启用数据事件日志，在所有账户中使用CloudWatch异常检测，使用Athena查询CloudTrail日志的自定义指标。这个方案利用了组织级跟踪的优势，可以集中管理所有账户的日志，CloudWatch异常检测适合监控访问模式，Athena适合查询大量日志数据。<br><br>选项C：与选项B类似，但使用CloudWatch Metrics Insights而不是Athena进行查询。CloudWatch Metrics Insights主要用于查询指标数据，而不是日志数据，对于CloudTrail日志分析来说不如Athena合适。<br><br>选项D：在每个账户创建单独的跟踪，使用自定义异常检测解决方案，使用CloudWatch Metrics Insights查询。这个方案管理复杂度高，自定义解决方案增加了开发和维护成本，且Metrics Insights不适合查询日志数据。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>290</td>
                    <td>A DevOps team is deploying microservices for an application on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The cluster uses<br>managed node groups. The DevOps team wants to enable auto scaling for the microservice Pods based on a specic CPU utilization<br>percentage. The DevOps team has already installed the Kubernetes Metrics Server on the cluster.</td>
                    <td>A. Edit the Auto Scaling group that is associated with the worker nodes of the EKS cluster. Congure the Auto Scaling group to use a<br>target tracking scaling policy to scale when the average CPU utilization of the Auto Scaling group reaches a specic percentage.<br>B. Deploy the Kubernetes Horizontal Pod Autoscaler (HPA) and the Kubernetes Vertical Pod Autoscaler (VPA) in the cluster. Congure the<br>HPA to scale based on the target CPU utilization percentage. Congure the VPA to use the recommender mode setting.<br>C. Run the AWS Systems Manager AWS-UpdateEKSManagedNodeGroup Automation document. Modify the values for<br>NodeGroupDesiredSize, NodeGroupMaxSize, and NodeGroupMinSize to be based on an estimate for the required node size.<br>D. Deploy the Kubernetes Horizontal Pod Autoscaler (HPA) and the Kubernetes Cluster Autoscaler in the cluster. Congure the HPA to<br>scale based on the target CPU utilization percentage. Congure the Cluster Autoscaler to use the auto-discovery setting.</td>
                    <td>一个DevOps团队正在Amazon Elastic Kubernetes Service (Amazon EKS)集群上为应用程序部署微服务。该集群使用托管节点组。DevOps团队希望基于特定的CPU利用率百分比为微服务Pod启用自动扩缩容。DevOps团队已经在集群上安装了Kubernetes Metrics Server。</td>
                    <td>选项A：编辑与EKS集群工作节点关联的Auto Scaling组，配置Auto Scaling组使用目标跟踪扩缩容策略在平均CPU利用率达到特定百分比时进行扩缩容。这个选项只是在节点级别进行扩缩容，而不是在Pod级别，无法满足题目要求的基于CPU利用率对微服务Pod进行自动扩缩容的需求。<br><br>选项B：在集群中部署Kubernetes水平Pod自动扩缩容器(HPA)和Kubernetes垂直Pod自动扩缩容器(VPA)，配置HPA基于目标CPU利用率百分比进行扩缩容，配置VPA使用推荐器模式设置。虽然HPA可以满足Pod级别的水平扩缩容需求，但VPA主要用于调整Pod的资源请求和限制，而且同时使用HPA和VPA可能会产生冲突。更重要的是，这个方案没有考虑节点级别的扩缩容。<br><br>选项C：运行AWS Systems Manager的AWS-UpdateEKSManagedNodeGroup自动化文档，修改NodeGroupDesiredSize、NodeGroupMaxSize和NodeGroupMinSize的值，基于所需节点大小的估算进行配置。这个选项只是手动调整节点组大小，不能实现基于CPU利用率的自动扩缩容，也不是动态的解决方案。<br><br>选项D：在集群中部署Kubernetes水平Pod自动扩缩容器(HPA)和Kubernetes集群自动扩缩容器，配置HPA基于目标CPU利用率百分比进行扩缩容，配置集群自动扩缩容器使用自动发现设置。这个选项提供了完整的两层自动扩缩容解决方案：HPA负责Pod级别的扩缩容，集群自动扩缩容器负责节点级别的扩缩容，能够完全满足需求。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>291</td>
                    <td>A company has multiple AWS accounts. The company uses AWS IAM Identity Center that is integrated with a third-party SAML 2.0 identity<br>provider (IdP).<br>The attributes for access control feature is enabled in IAM Identity Center. The attribute mapping list maps the department key from the IdP<br>to the ${path:enterprise.department} attribute. All existing Amazon EC2 instances have a d1, d2, d3 department tag that corresponds to three<br>company’s departments.<br>A DevOps engineer must create policies based on the matching attributes. The policies must grant each user access to only the EC2 instances<br>that are tagged with the user’s respective department name.</td>
                    <td>A. <br>B. <br>C. <br>D.</td>
                    <td>一家公司有多个AWS账户。该公司使用与第三方SAML 0身份提供商(IdP)集成的AWS IAM Identity Center。<br><br>IAM Identity Center中启用了访问控制属性功能。属性映射列表将IdP中的department键映射到${path:enterprise.department}属性。所有现有的Amazon EC2实例都有d1、d2、d3部门标签，对应公司的三个部门。<br><br>DevOps工程师必须基于匹配属性创建策略。这些策略必须授予每个用户仅访问标记有用户各自部门名称的EC2实例的权限。</td>
                    <td>由于题目中没有提供具体的选项A、B、C、D的内容，我无法对每个选项进行详细分析。但基于题目描述，正确的解决方案应该包含以下关键要素：<br><br>- 需要创建基于属性的访问控制(ABAC)策略<br>- 策略应该使用${path:enterprise.department}属性来匹配EC2实例的部门标签<br>- 策略条件应该确保用户只能访问与其部门属性匹配的EC2实例<br>- 可能需要使用aws:RequestedRegion和ec2:ResourceTag条件键<br>- 策略应该在IAM Identity Center的权限集中配置<br><br>由于参考答案是B，选项B应该包含了正确配置ABAC策略的方法，使用适当的条件语句来匹配用户的部门属性与EC2实例标签。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>292</td>
                    <td>A security team wants to use AWS CloudTrail to monitor all actions and API calls in multiple accounts that are in the same organization in<br>AWS Organizations. The security team needs to ensure that account users cannot turn off CloudTrail in the accounts.</td>
                    <td>A. Apply an SCP to all OUs to deny the cloudtrail:StopLogging action and the cloudtrail:DeleteTrail action.<br>B. Create IAM policies in each account to deny the cloudtrail:StopLogging action and the cloudtrail:DeleteTrail action.<br>C. Set up Amazon CloudWatch alarms to notify the security team when a user disables CloudTrail in an account.<br>D. Use AWS Cong to automatically re-enable CloudTrail if a user disables CloudTrail in an account.</td>
                    <td>一个安全团队希望使用AWS CloudTrail来监控AWS Organizations中同一组织内多个账户中的所有操作和API调用。安全团队需要确保账户用户无法在这些账户中关闭CloudTrail。</td>
                    <td>A. 对所有组织单位(OU)应用服务控制策略(SCP)来拒绝cloudtrail:StopLogging操作和cloudtrail:DeleteTrail操作。这是最有效的解决方案，因为SCP在组织级别强制执行策略，无论账户内的IAM权限如何设置，都无法绕过SCP的限制。SCP具有最高优先级，可以从根本上阻止用户执行被拒绝的操作。<br><br>B. 在每个账户中创建IAM策略来拒绝cloudtrail:StopLogging操作和cloudtrail:DeleteTrail操作。这种方法存在缺陷，因为具有管理员权限的用户可以修改或删除这些IAM策略，从而绕过限制。此外，需要在每个账户中单独配置，管理复杂度较高。<br><br>C. 设置Amazon CloudWatch告警，当用户在账户中禁用CloudTrail时通知安全团队。这只是一个监控和通知机制，并不能阻止用户关闭CloudTrail，只能在事后发现问题，不符合题目要求的&quot;确保用户无法关闭&quot;的需求。<br><br>D. 使用AWS Config自动重新启用CloudTrail，如果用户在账户中禁用了CloudTrail。这种方法虽然可以自动恢复，但仍然允许用户暂时关闭CloudTrail，存在监控盲区，不能完全满足安全要求。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>293</td>
                    <td>A DevOps engineer needs to congure a blue/green deployment for an existing three-tier application. The application runs on Amazon EC2<br>instances and uses an Amazon RDS database. The EC2 instances run behind an Application Load Balancer (ALB) and are in an Auto Scaling<br>group.<br>The DevOps engineer has created launch templates, Auto Scaling groups, and ALB target groups for the blue environment and the green<br>environment. Each target group species which application version, blue or green, will be loaded on the EC2 instances. An Amazon Route 53<br>record for www.example.com points to the AL</td>
                    <td>B. <br>A. Starta rolling restart of the Auto Scaling group for the green environment to deploy the new application version to the green<br>environment&#x27;s EC2 instances. When the rolling restart is complete, use an AWS CLI command to update the ALB to send trac to the<br>green environment&#x27;s target group.<br>C. Update the launch template to deploy the green environment&#x27;s application version to the blue environment&#x27;s EC2 instances. Do not<br>change the target groups or the Auto Scaling groups in either environment. Perform a rolling restart of the blue environments EC2<br>instances.<br>D. Starta rolling restart of the Auto Scaling group for the green environment to deploy the new application version to the green<br>environment&#x27;s EC2 instances. When the rolling restart is complete, update Route 53 to point to the green environment&#x27;s endpoint on the<br>AL</td>
                    <td>一名DevOps工程师需要为现有的三层应用程序配置蓝绿部署。该应用程序运行在Amazon EC2实例上，并使用Amazon RDS数据库。EC2实例运行在应用负载均衡器(ALB)后面，并位于Auto Scaling组中。<br><br>DevOps工程师已经为蓝色环境和绿色环境创建了启动模板、Auto Scaling组和ALB目标组。每个目标组指定将在EC2实例上加载哪个应用程序版本（蓝色或绿色）。Amazon Route 53记录www.example.com指向ALB。</td>
                    <td>选项A：启动绿色环境Auto Scaling组的滚动重启，将新应用程序版本部署到绿色环境的EC2实例。当滚动重启完成后，使用AWS CLI命令更新ALB将流量发送到绿色环境的目标组。这个方案符合蓝绿部署的原理，通过在ALB层面切换目标组来实现流量切换，可以实现快速回滚，是正确的蓝绿部署实施方式。<br><br>选项C：更新启动模板以将绿色环境的应用程序版本部署到蓝色环境的EC2实例，不更改任一环境中的目标组或Auto Scaling组，对蓝色环境的EC2实例执行滚动重启。这种方法实际上是就地更新而不是蓝绿部署，违背了蓝绿部署保持两个独立环境的核心原则。<br><br>选项D：启动绿色环境Auto Scaling组的滚动重启，将新应用程序版本部署到绿色环境的EC2实例。当滚动重启完成后，更新Route 53指向ALB上的绿色环境端点。这个方案在DNS层面进行切换，但由于DNS缓存的存在，切换时间较长且不够精确，不如在ALB层面切换效率高。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>294</td>
                    <td>A company has an application that runs on Amazon EC2 instances in an Auto Scaling group. The application processes a high volume of<br>messages from an Amazon Simple Queue Service (Amazon SQS) queue.<br>A DevOps engineer noticed that the application took several hours to process a group of messages from the SQS queue. The average CPU<br>utilization of the Auto Scaling group did not cross the threshold of a target tracking scaling policy when processing the messages. The<br>application that processes the SQS queue publishes logs to ‘Amazon CloudWatch Logs.<br>The DevOps engineer needs to ensure that the queue is processed quickly.</td>
                    <td>A. Create an AWS Lambda function. Congure the Lambda function to publish a custom metric by using the<br>ApproximateNumberOfMessagesVisible SQS queue attribute and the GroupInServiceInstances Auto Scaling group attribute to publish the<br>queue messages for each instance. Schedule an Amazon EventBridge rule to run the Lambda function every hour. Create a target tracking<br>scaling policy for the Auto Scaling group that uses the custom metric to scale in and out.<br>B. Create an AWS Lambda function. Congure the Lambda function to publish a custom metric by using the<br>ApproximateNumberOfMessagesVisible SQS queue attribute and the GroupInServiceInstances Auto Scaling group attribute to publish the<br>queue messages for each instance. Create a CloudWatch subscription lter for the application logs with the Lambda function as the<br>target. Create a target tracking scaling policy for the Auto Scaling group that uses the custom metric to scale in and out.<br>C. Create a target tracking scaling policy for the Auto Scaling group. In the target tracking policy, use the<br>ApproximateNumberOfMessagesVisible SQS queue attribute and the GroupInServiceInstances Auto Scaling group attribute to calculate<br>how many messages are in the queue for each number of instances by using metric math. Use the calculated attribute to scale in and out.<br>D. Create an AWS Lambda function that logs the ApproximateNumberOfMessagesVisible attribute of the SQS queue to a CloudWatch Logs<br>log group. Schedule an Amazon EventBridge rule to run the Lambda function every 5 minutes. Create a metric ler to count the number of<br>log events from a CloudWatch logs group. Create a target tracking scaling policy for the Auto Scaling group that uses the custom metric to<br>scale in and out.</td>
                    <td>一家公司有一个运行在Auto Scaling组中Amazon EC2实例上的应用程序。该应用程序处理来自Amazon Simple Queue Service (Amazon SQS)队列的大量消息。<br>DevOps工程师注意到应用程序花费了几个小时来处理SQS队列中的一组消息。在处理消息时，Auto Scaling组的平均CPU利用率没有超过目标跟踪扩展策略的阈值。处理SQS队列的应用程序将日志发布到Amazon CloudWatch Logs。<br>DevOps工程师需要确保队列能够快速处理。</td>
                    <td>选项A：创建Lambda函数使用ApproximateNumberOfMessagesVisible和GroupInServiceInstances属性发布自定义指标，通过EventBridge每小时运行一次。这种方法的问题是每小时运行一次频率太低，无法及时响应队列消息的变化，不能满足快速处理的需求。<br><br>选项B：创建Lambda函数发布自定义指标，通过CloudWatch订阅过滤器触发。虽然响应更及时，但增加了不必要的复杂性，需要额外的Lambda函数和订阅过滤器，而CloudWatch本身就支持直接的指标数学计算。<br><br>选项C：直接在目标跟踪扩展策略中使用ApproximateNumberOfMessagesVisible和GroupInServiceInstances属性，通过指标数学计算每个实例的队列消息数。这是最直接、最高效的方法，无需额外的Lambda函数，能够实时响应队列变化。<br><br>选项D：创建Lambda函数将队列属性记录到CloudWatch Logs，然后使用指标过滤器计数。这种方法过于复杂，增加了不必要的日志记录步骤，而且5分钟的间隔仍然可能不够及时。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>295</td>
                    <td>A company has a single AWS account that runs hundreds of Amazon EC2 instances in a single AWS Region. The company launches and<br>terminates new EC2 instances every hour. The account includes existing EC2 instances that have been running for longer than a week.<br>The company&#x27;s security policy requires all running EC2 instances to have an EC2 instance prole attached. The company has created a default<br>EC2 instance prole. The default EC2 instance prole must be attached to any EC2 instances that do not have a prole attached.</td>
                    <td>A. Congure an Amazon EventBridge rule that matches the Amazon EC2 RunInstances API calls. Congure the rule to invoke an AWS<br>Lambda function to attach the default instance prole to the EC2 instances.<br>B. Congure AWS Cong. Deploy an AWS Cong ec2-instance-prole-attached managed rule. Congure an automatic remediation action<br>that invokes an AWS Systems Manager Automation runbook to attach the default instance prole to the EC2 instances.<br>C. Congure an Amazon EventBridge rule that matches the Amazon EC2 StartInstances API calls. Congure the rule to invoke an AWS<br>Systems Manager Automation runbook to attach the default instance prole to the EC2 instances.<br>D. Congure AWS Cong. Deploy an AWS Cong iam-role-managed-policy-check managed rule. Congure an automatic remediation action<br>that invokes an AWS Lambda function to attach the default instance prole to the EC2 instances.</td>
                    <td>一家公司有一个单独的AWS账户，在单个AWS区域中运行数百个Amazon EC2实例。该公司每小时都会启动和终止新的EC2实例。该账户包括已经运行超过一周的现有EC2实例。<br>公司的安全策略要求所有正在运行的EC2实例都必须附加EC2实例配置文件。公司已经创建了一个默认的EC2实例配置文件。默认的EC2实例配置文件必须附加到任何没有配置文件附加的EC2实例上。</td>
                    <td>A. 配置Amazon EventBridge规则匹配Amazon EC2 RunInstances API调用。配置规则调用AWS Lambda函数将默认实例配置文件附加到EC2实例。这个选项的问题是EventBridge只能处理新启动的实例，无法处理已经存在但没有实例配置文件的EC2实例。而且RunInstances事件触发时机可能不够准确。<br><br>B. 配置AWS Config。部署AWS Config的ec2-instance-profile-attached托管规则。配置自动修复操作，调用AWS Systems Manager自动化运行手册将默认实例配置文件附加到EC2实例。这个选项很全面，AWS Config可以持续监控所有EC2实例的合规性状态，包括新的和现有的实例，并且可以通过自动修复功能自动附加缺失的实例配置文件。<br><br>C. 配置Amazon EventBridge规则匹配Amazon EC2 StartInstances API调用。配置规则调用AWS Systems Manager自动化运行手册将默认实例配置文件附加到EC2实例。这个选项的问题是StartInstances API只在实例从停止状态启动时触发，不会在实例首次创建时触发，而且同样无法处理已存在的实例。<br><br>D. 配置AWS Config。部署AWS Config的iam-role-managed-policy-check托管规则。配置自动修复操作，调用AWS Lambda函数将默认实例配置文件附加到EC2实例。这个选项使用了错误的Config规则，iam-role-managed-policy-check是用于检查IAM角色策略的，不是用于检查EC2实例配置文件的。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>296</td>
                    <td>A company uses AWS Organizations to manage hundreds of AWS accounts. The company has a team that is responsible for AWS Identity and<br>Access Management (IAM).<br>The IAM team wants to implement AWS IAM Identity Center. The IAM team must have only the minimum required permissions to manage IAM<br>Identity Center. The IAM team must not be able to gain unnecessary access to the Organizations management account. The IAM team must<br>be able to provision new IAM Identity Center permission sets and assignments for new and existing member accounts.</td>
                    <td>A. Create a new AWS account for the IAM team. Enable IAM Identity Center in the new account. In the Organizations management<br>account, register the new account as a delegated administrator for IAM Identity Center.<br>B. Create a new AWS account for the IAM team. Enable IAM Identity Center in the Organizations management account. In the<br>Organizations management account, register the new account as a delegated administrator for IAM Identity Center.<br>C. Create an SCP in Organizations. Create a new OU for the Organizations management account, and link the new SCP to the OU.<br>Congure the SCP to deny all access to IAM Identity Center.<br>D. Create IAM users and an IAM group for the IAM team in IAM Identity Center. Add the users to the group. Create a new permission set.<br>Attach the AWSSSOMemberAccountAdministrator managed IAM policy to the group.<br>E. Assign the new permission set to the Organizations management account. Allow the IAM team&#x27;s group to use the permission set.<br>F. Assign the new permission set to the new AWS account. Allow the IAM team&#x27;s group to use the permission set.</td>
                    <td>一家公司使用AWS Organizations来管理数百个AWS账户。该公司有一个负责AWS身份和访问管理(IAM)的团队。<br>IAM团队希望实施AWS IAM Identity Center。IAM团队必须只拥有管理IAM Identity Center所需的最小权限。IAM团队不能获得对Organizations管理账户的不必要访问权限。IAM团队必须能够为新的和现有的成员账户配置新的IAM Identity Center权限集和分配。</td>
                    <td>A. 为IAM团队创建一个新的AWS账户。在新账户中启用IAM Identity Center。在Organizations管理账户中，将新账户注册为IAM Identity Center的委托管理员。<br>- 这个选项不正确。IAM Identity Center只能在Organizations管理账户中启用，不能在成员账户中启用。委托管理员功能允许成员账户管理IAM Identity Center，但服务本身必须在管理账户中启用。<br><br>B. 为IAM团队创建一个新的AWS账户。在Organizations管理账户中启用IAM Identity Center。在Organizations管理账户中，将新账户注册为IAM Identity Center的委托管理员。<br>- 这个选项是正确的。IAM Identity Center必须在Organizations管理账户中启用，然后可以将成员账户设置为委托管理员，这样IAM团队就可以在不直接访问管理账户的情况下管理IAM Identity Center。<br><br>C. 在Organizations中创建SCP。为Organizations管理账户创建新的OU，并将新的SCP链接到OU。配置SCP以拒绝对IAM Identity Center的所有访问。<br>- 这个选项不正确。这样做会阻止对IAM Identity Center的访问，而不是提供管理权限。<br><br>D. 在IAM Identity Center中为IAM团队创建IAM用户和IAM组。将用户添加到组中。创建新的权限集。将AWSSSOMemberAccountAdministrator托管IAM策略附加到组。<br>- 这个选项是正确的。这创建了必要的用户、组和权限集，AWSSSOMemberAccountAdministrator策略提供了管理IAM Identity Center所需的权限。<br><br>E. 将新权限集分配给Organizations管理账户。允许IAM团队的组使用该权限集。<br>- 这个选项不正确，因为它会给IAM团队访问管理账户的权限，违反了最小权限原则。<br><br>F. 将新权限集分配给新的AWS账户。允许IAM团队的组使用该权限集。<br>- 这个选项在委托管理员场景下是合理的，但不是必需的，因为委托管理员权限是在组织级别授予的。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>297</td>
                    <td>A company uses an Amazon Aurora PostgreSQL global database that has two secondary AWS Regions. A DevOps engineer has congured the<br>database parameter group to guarantee an RPO of 60 seconds. Write operations on the primary cluster are occasionally blocked because of<br>the RPO setting.<br>The DevOps engineer needs to reduce the frequency of blocked write operations.</td>
                    <td>A. Add an additional secondary cluster to the global database.<br>B. Enable write forwarding for the global database.<br>C. Remove one of the secondary clusters from the global database.<br>D. Congure synchronous replication for the global database.</td>
                    <td>一家公司使用Amazon Aurora PostgreSQL全球数据库，该数据库有两个辅助AWS区域。DevOps工程师已配置数据库参数组以保证60秒的RPO（恢复点目标）。由于RPO设置，主集群上的写操作偶尔会被阻塞。DevOps工程师需要减少写操作被阻塞的频率。</td>
                    <td>A. 向全球数据库添加额外的辅助集群：这个选项会增加更多的复制目标，实际上会增加写操作的延迟和被阻塞的可能性。因为主集群需要等待更多的辅助集群完成数据同步才能满足RPO要求，这会使问题变得更严重而不是改善。<br><br>B. 为全球数据库启用写转发：写转发功能允许辅助集群接收写请求并转发到主集群，但这不会解决主集群因RPO设置而阻塞写操作的根本问题。写转发主要是为了减少跨区域的延迟，而不是解决RPO相关的阻塞问题。<br><br>C. 从全球数据库中移除一个辅助集群：这是正确的解决方案。当前有两个辅助集群，主集群需要确保数据复制到所有辅助集群以满足60秒RPO要求。移除一个辅助集群可以减少复制的复杂性和等待时间，从而减少写操作被阻塞的频率。<br><br>D. 为全球数据库配置同步复制：同步复制会要求主集群等待所有辅助集群确认数据写入后才能提交事务，这会显著增加写操作的延迟和被阻塞的频率，使问题变得更严重。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>298</td>
                    <td>A company has a web application that is hosted on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster runs on AWS<br>Fargate that is available through an internet-facing Application Load Balancer.<br>The application is experiencing stability issues that lead to longer response times. A DevOps engineer needs to congure observability in<br>Amazon CloudWatch to troubleshoot the issue. The solution must provide only the minimum necessary permissions.</td>
                    <td>A. Deploy the CloudWatch agent as a Kubernetes StatefulSet to the EKS cluster.<br>B. Deploy the AWS Distro for OpenTelemetry Collector as a Kubernetes DaemonSet to the EKS cluster.<br>C. Associate a Kubernetes service account with an IAM role by using IAM roles for service accounts in Amazon EKS. Use the<br>CloudWatchAgentServerPolicy AWS managed policy.<br>D. Associate a Kubernetes service account with an IAM role by using IAM roles for service accounts in Amazon EKS. Use the<br>CloudWatchAgentAdminPolicy AWS managed policy.<br>E. Congure an IAM OpenID Connect (OIDC) provider for the EKS cluster.<br>F. Enable EKS control plane logging for the EKS cluster.</td>
                    <td>一家公司有一个托管在Amazon Elastic Kubernetes Service (Amazon EKS)集群上的Web应用程序。该EKS集群运行在AWS Fargate上，通过面向互联网的应用负载均衡器提供服务。应用程序正在经历稳定性问题，导致响应时间延长。DevOps工程师需要在Amazon CloudWatch中配置可观测性来排查问题。解决方案必须仅提供最小必要权限。</td>
                    <td>A. 将CloudWatch代理作为Kubernetes StatefulSet部署到EKS集群：这个选项不正确，因为在Fargate环境中，StatefulSet不是部署监控代理的最佳方式。Fargate有特定的限制，且StatefulSet通常用于有状态应用。<br><br>B. 将AWS Distro for OpenTelemetry Collector作为Kubernetes DaemonSet部署到EKS集群：这个选项正确。AWS Distro for OpenTelemetry (ADOT)是AWS推荐的可观测性解决方案，可以收集指标、日志和跟踪数据发送到CloudWatch。DaemonSet确保每个节点上都运行一个收集器实例。<br><br>C. 使用Amazon EKS中的IAM角色服务账户功能，将Kubernetes服务账户与IAM角色关联，使用CloudWatchAgentServerPolicy AWS托管策略：这个选项正确。IRSA (IAM Roles for Service Accounts)是最佳实践，CloudWatchAgentServerPolicy提供了写入CloudWatch所需的最小权限。<br><br>D. 使用Amazon EKS中的IAM角色服务账户功能，将Kubernetes服务账户与IAM角色关联，使用CloudWatchAgentAdminPolicy AWS托管策略：这个选项不正确，因为CloudWatchAgentAdminPolicy提供了过多的权限，违反了最小权限原则。<br><br>E. 为EKS集群配置IAM OpenID Connect (OIDC)提供程序：这个选项正确。OIDC提供程序是使用IRSA功能的前提条件，必须先配置OIDC提供程序才能实现Kubernetes服务账户与IAM角色的关联。<br><br>F. 为EKS集群启用控制平面日志记录：这个选项虽然有助于可观测性，但主要针对EKS控制平面组件的日志，对于应用程序稳定性问题的排查帮助有限。</td>
                    <td>BCE</td>
                </tr>
                <tr>
                    <td>299</td>
                    <td>A company stores its Python-based application code in AWS CodeCommit. The company uses AWS CodePipeline to deploy the application. The<br>CodeCommit repository and the CodePipeline pipeline are deployed to the same AWS account.<br>The company&#x27;s security team requires all code to be scanned for vulnerabilities before the code is deployed to production. If any<br>vulnerabilities are found, the deployment must stop.</td>
                    <td>A. Create a new CodeBuild project. Congure the project to run a security scan on the code by using Amazon CodeGuru Security. Congure<br>the CodeBuild project to raise an error if CodeGuru Security nds vulnerabilities. Create a new IAM role that has sucient permissions to<br>run CodeGuru Security scans. Assign the role to the CodeBuild project. In the CodePipeline pipeline, add a new stage before the<br>deployment stage. Select AWS CodeBuild as the action provider for the new stage. Use the source artifact from the CodeCommit<br>repository. Congure the action to use the CodeBuild project.<br>B. Create a new CodeBuild project. Congure the project to run a security scan on the code by using Amazon Inspector. Congure the<br>CodeBuild project to raise an error if Amazon Inspector nds vulnerabilities. Create a new IAM role that has sucient permissions to run<br>Amazon Inspector scans. Assign the role to the CodeBuild project. In the CodePipeline pipeline, add a new stage before the deployment<br>stage. Select AWS CodeBuild as the action provider for the new stage. Use the source artifact from the CodeCommit repository. Congure<br>the action to use the CodeBuild project.<br>C. Update the IAM role that is attached to CodePipeline to include sucient permissions to invoke Amazon DevOps Guru. In the<br>CodePipeline pipeline, add a new stage before the deployment stage. Select DevOps Guru as the action provider for the new stage. Use<br>the source artifact from the CodeCommit repository.<br>D. Update the IAM role that is attached to CodePipeline to include sucient permissions to invoke Amazon DevOps Guru. In the<br>CodePipeline pipeline, add a new stage before the deployment stage. Select CodeGuru Security as the action provider for the new stage.<br>Use the source artifact from the CodeCommit repository.</td>
                    <td>一家公司将其基于Python的应用程序代码存储在AWS CodeCommit中。该公司使用AWS CodePipeline来部署应用程序。CodeCommit存储库和CodePipeline管道部署在同一个AWS账户中。<br>公司的安全团队要求在代码部署到生产环境之前，必须扫描所有代码的漏洞。如果发现任何漏洞，部署必须停止。</td>
                    <td>选项A：创建一个新的CodeBuild项目，配置项目使用Amazon CodeGuru Security对代码进行安全扫描。如果CodeGuru Security发现漏洞，配置CodeBuild项目抛出错误。创建具有足够权限运行CodeGuru Security扫描的新IAM角色，并将角色分配给CodeBuild项目。在CodePipeline管道中，在部署阶段之前添加新阶段，选择AWS CodeBuild作为操作提供者，使用来自CodeCommit存储库的源工件。这个方案是正确的，因为CodeGuru Security专门用于代码安全扫描，通过CodeBuild集成可以在管道中实现自动化扫描和错误处理。<br><br>选项B：与选项A类似，但使用Amazon Inspector进行安全扫描。这个方案不正确，因为Amazon Inspector主要用于EC2实例和容器镜像的安全评估，不是专门用于源代码漏洞扫描的服务。<br><br>选项C：更新附加到CodePipeline的IAM角色以包含调用Amazon DevOps Guru的权限，在部署阶段之前添加新阶段，选择DevOps Guru作为操作提供者。这个方案不正确，因为DevOps Guru主要用于应用程序性能监控和异常检测，不是代码漏洞扫描工具，且DevOps Guru不能作为CodePipeline的直接操作提供者。<br><br>选项D：更新CodePipeline的IAM角色权限以调用Amazon DevOps Guru，但选择CodeGuru Security作为操作提供者。这个方案存在逻辑矛盾，权限和操作提供者不匹配，且CodeGuru Security不能直接作为CodePipeline的操作提供者使用。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>300</td>
                    <td>A DevOps engineer deploys an application to a eet of Amazon Linux EC2 instances. The DevOps engineer needs to monitor system metrics<br>across the eet. The DevOps engineer wants to monitor the relationship between network trac and memory utilization for the application<br>code. The DevOps engineer wants to track the data on a 60 second interval.</td>
                    <td>A. Use Amazon CloudWatch basic monitoring to collect the NetworkIn metric and the MemoryBytesUsed metric. Graph the metrics in<br>CloudWatch.<br>B. Use Amazon CloudWatch detailed monitoring to collect the NetworkIn metric and the MemoryBytesUsed metric. Graph the metrics in<br>CloudWatch.<br>C. Use Amazon CloudWatch detailed monitoring to collect the NetworkIn metric. Install the CloudWatch agent on the EC2 instances to<br>collect the mem_used metric. Graph the metrics in CloudWatch.<br>D. Use Amazon CloudWatch basic monitoring to collect the built-in NetworkIn metric. Install the CloudWatch agent on the EC2 instances to<br>collect the mem_used metric. Graph the metrics in CloudWatch.</td>
                    <td>一名DevOps工程师将应用程序部署到一组Amazon Linux EC2实例上。该DevOps工程师需要监控整个实例组的系统指标。DevOps工程师希望监控应用程序代码的网络流量和内存利用率之间的关系。DevOps工程师希望以60秒的间隔跟踪数据。</td>
                    <td>A. 使用Amazon CloudWatch基础监控收集NetworkIn指标和MemoryBytesUsed指标，在CloudWatch中绘制指标图表。这个选项错误，因为CloudWatch默认不提供MemoryBytesUsed指标，内存指标需要通过CloudWatch代理收集。基础监控的数据间隔是5分钟，不满足60秒间隔的要求。<br><br>B. 使用Amazon CloudWatch详细监控收集NetworkIn指标和MemoryBytesUsed指标，在CloudWatch中绘制指标图表。这个选项错误，虽然详细监控可以提供1分钟间隔的数据，但CloudWatch仍然不提供默认的MemoryBytesUsed指标，内存指标必须通过CloudWatch代理安装后才能收集。<br><br>C. 使用Amazon CloudWatch详细监控收集NetworkIn指标，在EC2实例上安装CloudWatch代理收集mem_used指标，在CloudWatch中绘制指标图表。这个选项正确，详细监控提供1分钟间隔的NetworkIn指标，CloudWatch代理可以收集内存使用率指标，满足60秒间隔的监控需求。<br><br>D. 使用Amazon CloudWatch基础监控收集内置的NetworkIn指标，在EC2实例上安装CloudWatch代理收集mem_used指标，在CloudWatch中绘制指标图表。这个选项错误，因为基础监控的数据收集间隔是5分钟，不能满足60秒间隔的要求。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>301</td>
                    <td>A company uses AWS Systems Manager to manage a eet of Amazon Linux EC2 instances that have SSM Agent installed. All EC2 instances<br>are congured to use Instance Metadata Service Version 2 (IMDSv2) and are running in the same AWS account and AWS Region. Company<br>policy requires developers to use only Amazon Linux.<br>The company wants to ensure that all new EC2 instances are automatically managed by Systems Manager after creation.</td>
                    <td>A. Create an IAM role that has a trust policy that allows Systems Manager to assume the role. Attach the<br>AmazonSSMManagedEC2InstanceDefaultPolicy policy to the role. Congure the default-ec2-instance-management-role SSM service<br>setting to use the role.<br>B. Ensure that AWS Cong is set up. Create an AWS Cong rule that validates if an EC2 instance has SSM Agent installed. Congure the<br>rule to run on EC2 conguration changes. Congure automatic remediation for the rule to run the AWS-InstallSSMAgent SSM document to<br>install SSM Agent.<br>C. Congure Systems Manager Patch Manager. Create a patch baseline that automatically installs SSM Agent on all new EC2 instances.<br>Create a patch group for all EC2 instances. Attach the patch baseline to the patch group. Create a maintenance window and maintenance<br>window task to start installing SSM Agent daily.<br>D. Create an EC2 instance role that has a trust policy that allows Amazon EC2 to assume the role. Attach the<br>AmazonSSMManagedInstanceCore policy to the role. Ensure that AWS Cong is set up. Use the ec2-instance-prole-attached managed<br>AWS Cong rule to validate if an EC2 instance has the role attached. Congure the rule to run on EC2 conguration changes. Congure<br>automatic remediation for the rule to run the AWS-SetupManagedRoleOnEc2Instance SSM document to attach the role to the EC2<br>instance.</td>
                    <td>一家公司使用AWS Systems Manager来管理一组已安装SSM Agent的Amazon Linux EC2实例。所有EC2实例都配置为使用实例元数据服务版本2（IMDSv2），并在同一个AWS账户和AWS区域中运行。公司政策要求开发人员只能使用Amazon Linux。<br>公司希望确保所有新的EC2实例在创建后都能自动被Systems Manager管理。</td>
                    <td>选项A：创建一个IAM角色，该角色具有允许Systems Manager承担该角色的信任策略。将AmazonSSMManagedEC2InstanceDefaultPolicy策略附加到该角色。配置default-ec2-instance-management-role SSM服务设置以使用该角色。这是正确的方法，因为default-ec2-instance-management-role是Systems Manager的一个服务设置，可以自动为新创建的EC2实例分配IAM角色，使其能够被Systems Manager管理。这种方法直接解决了自动管理新实例的需求。<br><br>选项B：确保设置AWS Config。创建一个AWS Config规则来验证EC2实例是否安装了SSM Agent。配置规则在EC2配置更改时运行。为规则配置自动修复以运行AWS-InstallSSMAgent SSM文档来安装SSM Agent。这个方法有问题，因为题目已经说明所有实例都已安装SSM Agent，问题不在于安装Agent，而在于权限配置。<br><br>选项C：配置Systems Manager Patch Manager。创建一个补丁基线，自动在所有新EC2实例上安装SSM Agent。为所有EC2实例创建补丁组。将补丁基线附加到补丁组。创建维护窗口和维护窗口任务以每天开始安装SSM Agent。这个方法不合适，因为Patch Manager主要用于补丁管理，不是用来安装SSM Agent的正确工具。<br><br>选项D：创建一个EC2实例角色，该角色具有允许Amazon EC2承担该角色的信任策略。将AmazonSSMManagedInstanceCore策略附加到该角色。确保设置AWS Config。使用ec2-instance-profile-attached托管AWS Config规则来验证EC2实例是否附加了该角色。配置规则在EC2配置更改时运行。为规则配置自动修复以运行AWS-SetupManagedRoleOnEc2Instance SSM文档将角色附加到EC2实例。虽然这个方法在技术上可行，但比选项A复杂，需要额外的Config规则和修复配置。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>302</td>
                    <td>A company congured an Amazon S3 event source for an AWS Lambda function. The company needs the Lambda function to run when a new<br>object is created or an existing object is modied in a specic S3 bucket. The Lambda function will use the S3 bucket name and the S3 object<br>key of the incoming event to read the contents of the new or modied S3 object. The Lambda function will parse the contents and save the<br>parsed contents to an Amazon DynamoDB table.<br>The Lambda function&#x27;s execution role has permissions to read from the S3 bucket and to write to the DynamoDB table. During testing, a<br>DevOps engineer discovers that the Lambda function does not run when objects are added to the S3 bucket or when existing objects are<br>modied.</td>
                    <td>A. Create an S3 bucket policy for the S3 bucket that grants the S3 bucket permission to invoke the Lambda function.<br>B. Create a resource policy for the Lambda function to grant Amazon S3 permission to invoke the Lambda function on the S3 bucket.<br>C. Congure an Amazon Simple Queue Service (Amazon SQS) queue as an OnFailure destination for the Lambda function. Update the<br>Lambda function to process messages from the SQS queue and the S3 event notications.<br>D. Congure an Amazon Simple Queue Service (Amazon SQS) queue as the destination for the S3 bucket event notications. Update the<br>Lambda function&#x27;s execution role to have permission to read from the SQS queue. Update the Lambda function to consume messages<br>from the SQS queue.</td>
                    <td>一家公司为AWS Lambda函数配置了Amazon S3事件源。公司需要Lambda函数在特定S3存储桶中创建新对象或修改现有对象时运行。Lambda函数将使用传入事件的S3存储桶名称和S3对象键来读取新建或修改的S3对象内容。Lambda函数将解析内容并将解析后的内容保存到Amazon DynamoDB表中。<br><br>Lambda函数的执行角色具有从S3存储桶读取和写入DynamoDB表的权限。在测试过程中，DevOps工程师发现当对象添加到S3存储桶或修改现有对象时，Lambda函数不会运行。</td>
                    <td>A. 为S3存储桶创建S3存储桶策略，授予S3存储桶调用Lambda函数的权限。这个选项是错误的，因为S3存储桶策略主要用于控制对存储桶资源的访问权限，而不是用于授予调用Lambda函数的权限。S3事件通知需要的是Lambda函数的资源策略来允许S3服务调用该函数。<br><br>B. 为Lambda函数创建资源策略，授予Amazon S3在S3存储桶上调用Lambda函数的权限。这是正确的选项。当S3事件触发Lambda函数时，需要Lambda函数的资源策略明确允许S3服务调用该函数。这是S3事件通知机制正常工作的必要条件。<br><br>C. 配置Amazon SQS队列作为Lambda函数的OnFailure目标，更新Lambda函数以处理来自SQS队列和S3事件通知的消息。这个选项混淆了概念，OnFailure目标是用于处理Lambda函数执行失败的情况，而不是解决S3事件通知无法触发Lambda函数的问题。<br><br>D. 配置Amazon SQS队列作为S3存储桶事件通知的目标，更新Lambda函数的执行角色以具有从SQS队列读取的权限，更新Lambda函数以消费SQS队列中的消息。虽然这是一个可行的架构模式，但题目要求的是直接的S3到Lambda的事件触发，而不是通过SQS的间接方式。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>303</td>
                    <td>A company recently congured AWS Control Tower in its organization in AWS Organizations. The company enrolled all existing AWS accounts<br>in AWS Control Tower. The company wants to ensure that all new AWS accounts are automatically enrolled in AWS Control Tower.<br>The company has an existing AWS Step Functions workow that creates new AWS accounts and performs any actions required as part of<br>account creation. The Step Functions workow is dened in the same AWS account as AWS Control Tower.</td>
                    <td>A. Create an Amazon EventBridge event that has an aws.controltower source and a CreateManagedAccount detail-type. Add the details of<br>the new AWS account to the detail eld of the event.<br>B. Create an Amazon EventBridge event that has an aws.controltower source and a SetupLandingZone detail-type. Add the details of the<br>new AWS account to the detail eld of the event.<br>C. Create an AWSControlTowerExecution role in the new AWS account. Congure the role to allow the AWS Control Tower administrator<br>account to assume the role.<br>D. Call the AWS Service Catalog ProvisionProduct API operation with the details of the new AWS account.<br>E. Call the Organizations EnableAWSServiceAccess API operation with the controltower.amazonaws.com service name and the details of<br>the new AWS account.</td>
                    <td>一家公司最近在其AWS Organizations组织中配置了AWS Control Tower。该公司将所有现有的AWS账户注册到了AWS Control Tower中。该公司希望确保所有新的AWS账户都能自动注册到AWS Control Tower中。<br>该公司有一个现有的AWS Step Functions工作流，用于创建新的AWS账户并执行账户创建过程中所需的任何操作。该Step Functions工作流定义在与AWS Control Tower相同的AWS账户中。</td>
                    <td>A. 创建一个具有aws.controltower源和CreateManagedAccount详细类型的Amazon EventBridge事件。将新AWS账户的详细信息添加到事件的detail字段中。<br>这个选项不正确。EventBridge事件是用于监听和响应事件的，而不是用于主动创建或注册账户到Control Tower的。CreateManagedAccount是Control Tower发出的事件类型，而不是我们应该创建的事件。<br><br>B. 创建一个具有aws.controltower源和SetupLandingZone详细类型的Amazon EventBridge事件。将新AWS账户的详细信息添加到事件的detail字段中。<br>这个选项也不正确。SetupLandingZone是用于设置Landing Zone的事件类型，与注册新账户到Control Tower无关。同样，这也是试图创建事件而不是调用正确的API。<br><br>C. 在新AWS账户中创建AWSControlTowerExecution角色。配置该角色以允许AWS Control Tower管理员账户承担该角色。<br>这个选项不完整。虽然AWSControlTowerExecution角色确实是Control Tower管理账户所需的，但仅创建角色并不能自动将账户注册到Control Tower中。<br><br>D. 使用新AWS账户的详细信息调用AWS Service Catalog ProvisionProduct API操作。<br>这个选项是正确的。AWS Control Tower使用Service Catalog来管理账户创建。通过调用ProvisionProduct API，可以触发Control Tower的账户创建和注册流程。<br><br>E. 使用controltower.amazonaws.com服务名称和新AWS账户的详细信息调用Organizations EnableAWSServiceAccess API操作。<br>这个选项是正确的。EnableAWSServiceAccess API用于在Organizations中启用AWS服务访问，这是将账户集成到Control Tower所必需的步骤。</td>
                    <td>DE</td>
                </tr>
                <tr>
                    <td>304</td>
                    <td>A company&#x27;s web application uses an Application Load Balancer (ALB) to direct trac to Amazon EC2 instances across three Availability<br>Zones.<br>The company has deployed a newer version of the application to one Availability Zone for testing. If a problem is detected with the<br>application, the company wants to direct trac away from the affected Availability Zone until the deployment has been rolled back. The<br>application must remain available and maintain static stability during the rollback.</td>
                    <td>A. Disable cross-zone load balancing on the ALB&#x27;s target group. Initiate a zonal shift on the ALB to direct trac away from the affected<br>Availability Zone.<br>B. Remove the subnet that is associated<br>with the affected Availability Zone.<br>C. Congure cross-zone load balancing on the ALB&#x27;s target group to inherit settings from the AL<br>D. Congure cross-zone load balancing on the ALB&#x27;s target group to inherit settings from the AL</td>
                    <td>一家公司的Web应用程序使用应用负载均衡器(ALB)将流量分发到跨三个可用区的Amazon EC2实例。该公司已将应用程序的新版本部署到一个可用区进行测试。如果检测到应用程序存在问题，公司希望将流量从受影响的可用区转移开，直到部署回滚完成。在回滚过程中，应用程序必须保持可用并维持静态稳定性。</td>
                    <td>A. 在ALB的目标组上禁用跨区负载均衡，在ALB上启动区域转移以将流量从受影响的可用区转移开。这个选项是正确的，因为区域转移(zonal shift)是AWS专门为这种场景设计的功能，可以快速将流量从有问题的可用区转移到健康的可用区，同时禁用跨区负载均衡确保流量不会意外路由回问题区域。<br><br>B. 移除与受影响可用区关联的子网。这个选项不合适，因为移除子网是一个破坏性操作，可能会影响应用程序的稳定性，并且不是为临时故障转移设计的解决方案。<br><br>C. 配置ALB目标组上的跨区负载均衡以继承ALB的设置。这个选项不完整且不能解决问题，跨区负载均衡的配置调整无法实现将流量从特定可用区转移的需求。<br><br>D. 与选项C相同，配置跨区负载均衡继承ALB设置，同样无法解决流量转移问题。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>305</td>
                    <td>A company has several AWS accounts. An Amazon Connect instance runs in each account. The company uses an Amazon EventBridge default<br>event bus in each account for event handling.<br>A DevOps team needs to receive all the Amazon Connect events in a single DevOps account.</td>
                    <td>A. Update the resource-based policy of the default event bus in each account to allow the DevOps account to replay events. Congure an<br>EventBridge rule in the DevOps account that matches Amazon Connect events and has a target of the default event bus in the other<br>accounts.<br>B. Update the resource-based policy of the default event bus in each account to allow the DevOps account to receive events. Congure an<br>EventBridge rule in the DevOps account that matches Amazon Connect events and has a target of the default event bus in the other<br>accounts.<br>C. Update the resource-based policy of the default event bus in the DevOps account. Update the policy to allow events to be received from<br>the accounts. Congure an EventBridge rule in each account that matches Amazon Connect events and has a target of the DevOps<br>account&#x27;s default event bus.<br>D. Update the resource-based policy of the default event bus in the DevOps account. Update the policy to allow events to be replayed by<br>the accounts. Congure an EventBridge rule in each account that matches Amazon Connect events and has a target of the DevOps<br>account&#x27;s default event bus.</td>
                    <td>一家公司有多个AWS账户。每个账户中都运行着一个Amazon Connect实例。该公司在每个账户中使用Amazon EventBridge默认事件总线进行事件处理。<br>DevOps团队需要在单个DevOps账户中接收所有的Amazon Connect事件。</td>
                    <td>A. 更新每个账户中默认事件总线的基于资源的策略，允许DevOps账户重放事件。在DevOps账户中配置EventBridge规则，匹配Amazon Connect事件并将其他账户的默认事件总线作为目标。这个选项的配置方向是错误的，DevOps账户应该是接收方而不是发送方，且重放事件不是正确的概念。<br><br>B. 更新每个账户中默认事件总线的基于资源的策略，允许DevOps账户接收事件。在DevOps账户中配置EventBridge规则，匹配Amazon Connect事件并将其他账户的默认事件总线作为目标。这个选项的配置逻辑有误，规则应该在源账户中配置，目标应该是DevOps账户的事件总线。<br><br>C. 更新DevOps账户中默认事件总线的基于资源的策略，允许从其他账户接收事件。在每个账户中配置EventBridge规则，匹配Amazon Connect事件并将DevOps账户的默认事件总线作为目标。这个选项正确地配置了跨账户事件传递：目标账户（DevOps）允许接收，源账户配置规则发送到目标。<br><br>D. 更新DevOps账户中默认事件总线的基于资源的策略，允许账户重放事件。在每个账户中配置EventBridge规则，匹配Amazon Connect事件并将DevOps账户的默认事件总线作为目标。虽然规则配置正确，但&quot;重放事件&quot;的概念不适用于此场景。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>306</td>
                    <td>A company has deployed an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 node groups. The company&#x27;s DevOps<br>team uses the Kubernetes Horizontal Pod Autoscaler and recently installed a supported EKS cluster Autoscaler.<br>The DevOps team needs to implement a solution to collect metrics and logs of the EKS cluster to establish a baseline for performance. The<br>DevOps team will create an initial set of thresholds for specic metrics and will update the thresholds over time as the cluster is used. The<br>DevOps team must receive an Amazon Simple Notication Service (Amazon SNS) email notication if the initial set of thresholds is exceeded<br>or if the EKS cluster Autoscaler is not functioning properly.<br>The solution must collect cluster, node, and pod metrics. The solution also must capture logs in Amazon CloudWatch.</td>
                    <td>A. Deploy the CloudWatch agent and Fluent Bit to the cluster. Ensure that the EKS cluster has appropriate permissions to send metrics<br>and logs to CloudWatch.<br>B. Deploy AWS Distro for OpenTelemetry to the cluster. Ensure that the EKS cluster has appropriate permissions to send metrics and logs<br>to CloudWatch.<br>C. Create CloudWatch alarms to monitor the CPU, memory, and node failure metrics of the cluster. Congure the alarms to send an SNS<br>email notication to the DevOps team if thresholds are exceeded.<br>D. Create a CloudWatch composite alarm to monitor a metric log lter of the CPU, memory, and node metrics of the cluster. Congure the<br>alarm to send an SNS email notication to the DevOps team when anomalies are detected.<br>E. Create a CloudWatch alarm to monitor the logs of the Autoscaler deployments for errors. Congure the alarm to send an SNS email<br>notication to the DevOps team if thresholds are exceeded.<br>F. Create a CloudWatch alarm to monitor a metric log lter of the Autoscaler deployments for errors. Congure the alarm to send an SNS<br>email notication to the DevOps team if thresholds are exceeded.</td>
                    <td>一家公司已经部署了一个带有Amazon EC2节点组的Amazon Elastic Kubernetes Service (Amazon EKS)集群。公司的DevOps团队使用Kubernetes水平Pod自动扩缩器，并且最近安装了一个受支持的EKS集群自动扩缩器。<br>DevOps团队需要实施一个解决方案来收集EKS集群的指标和日志，以建立性能基线。DevOps团队将为特定指标创建一组初始阈值，并随着集群的使用逐步更新这些阈值。如果超过初始阈值或EKS集群自动扩缩器无法正常工作，DevOps团队必须收到Amazon Simple Notification Service (Amazon SNS)电子邮件通知。<br>该解决方案必须收集集群、节点和Pod指标。该解决方案还必须在Amazon CloudWatch中捕获日志。</td>
                    <td>A. 将CloudWatch代理和Fluent Bit部署到集群。确保EKS集群具有向CloudWatch发送指标和日志的适当权限。<br>这个选项正确。CloudWatch代理可以收集集群、节点和Pod级别的指标，Fluent Bit是一个轻量级的日志处理器，专门用于收集和转发日志到CloudWatch。这个组合能够满足题目要求的指标和日志收集需求。<br><br>B. 将AWS Distro for OpenTelemetry部署到集群。确保EKS集群具有向CloudWatch发送指标和日志的适当权限。<br>这个选项虽然技术上可行，但AWS Distro for OpenTelemetry主要用于分布式追踪和可观测性，对于基本的指标和日志收集来说过于复杂，不是最佳选择。<br><br>C. 创建CloudWatch告警来监控集群的CPU、内存和节点故障指标。配置告警在超过阈值时向DevOps团队发送SNS电子邮件通知。<br>这个选项正确。它直接满足了题目要求的监控特定指标并在超过阈值时发送通知的需求。<br><br>D. 创建CloudWatch复合告警来监控集群CPU、内存和节点指标的指标日志过滤器。配置告警在检测到异常时向DevOps团队发送SNS电子邮件通知。<br>这个选项过于复杂，使用复合告警和异常检测不符合题目要求的简单阈值监控需求。<br><br>E. 创建CloudWatch告警来监控自动扩缩器部署的日志错误。配置告警在超过阈值时向DevOps团队发送SNS电子邮件通知。<br>这个选项不正确，因为直接监控日志内容不如监控基于日志的指标有效。<br><br>F. 创建CloudWatch告警来监控自动扩缩器部署错误的指标日志过滤器。配置告警在超过阈值时向DevOps团队发送SNS电子邮件通知。<br>这个选项正确，通过指标日志过滤器可以有效监控自动扩缩器的错误情况。</td>
                    <td>AC</td>
                </tr>
                <tr>
                    <td>307</td>
                    <td>A company discovers that its production environment and disaster recovery (DR) environment are deployed to the same AWS Region. All the<br>production applications run on Amazon EC2 instances and are deployed by AWS CloudFormation. The applications use an Amazon FSx for<br>NetApp ONTAP volume for application storage. No application data resides on the EC2 instances.<br>A DevOps engineer copies the required AMIs to a new DR Region. The DevOps engineer also updates the CloudFormation code to accept a<br>Region as a parameter. The storage needs to have an RPO of 10 minutes in the DR Region.</td>
                    <td>A. Create an Amazon S3 bucket in both Regions. Congure S3 Cross-Region Replication (CRR) for the S3 buckets. Create a scheduled AWS<br>Lambda function to copy any new content from the FSx for ONTAP volume to the S3 bucket in the production Region.<br>B. Use AWS Backup to create a backup vault and a custom backup plan that has a 10-minute frequency. Specify the DR Region as the<br>target Region. Assign the EC2 instances in the production Region to the backup plan.<br>C. Create an AWS Lambda function to create snapshots of the instance store volumes that are attached to the EC2 instances. Congure<br>the Lambda function to copy the snapshots to the DR Region and to remove the previous copies. Create an Amazon EventBridge<br>scheduled rule that invokes the Lambda function every 10 minutes.<br>D. Create an FSx for ONTAP instance in the DR Region. Congure a 5-minute schedule for a volume-level NetApp SnapMirror to replicate<br>the volume from the production Region to the DR Region.</td>
                    <td>一家公司发现其生产环境和灾难恢复(DR)环境部署在同一个AWS区域。所有生产应用程序都运行在Amazon EC2实例上，并通过AWS CloudFormation部署。应用程序使用Amazon FSx for NetApp ONTAP卷作为应用程序存储。EC2实例上没有应用程序数据。<br>DevOps工程师将所需的AMI复制到新的DR区域，并更新CloudFormation代码以接受区域作为参数。存储需要在DR区域具有10分钟的RPO(恢复点目标)。</td>
                    <td>选项A：创建S3存储桶并配置跨区域复制，使用Lambda函数将FSx ONTAP卷的内容复制到S3。这种方案虽然可行，但增加了不必要的复杂性，需要额外的Lambda函数来处理数据传输，而且S3不是FSx ONTAP的原生复制解决方案，可能无法保证数据一致性和10分钟RPO要求。<br><br>选项B：使用AWS Backup创建备份计划，频率为10分钟，目标为DR区域，并将生产区域的EC2实例分配给备份计划。但题目明确说明应用程序数据不在EC2实例上，而是在FSx ONTAP卷上，因此备份EC2实例无法满足数据保护需求。<br><br>选项C：创建Lambda函数为EC2实例的实例存储卷创建快照。这个选项存在根本性错误，因为题目明确指出没有应用程序数据驻留在EC2实例上，所有数据都在FSx ONTAP卷中，因此对实例存储卷进行快照是无意义的。<br><br>选项D：在DR区域创建FSx ONTAP实例，配置5分钟频率的卷级NetApp SnapMirror来将卷从生产区域复制到DR区域。这是最合适的解决方案，因为SnapMirror是NetApp ONTAP的原生复制技术，专门设计用于数据复制和灾难恢复，5分钟的复制频率可以满足10分钟RPO的要求。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>308</td>
                    <td>During a security audit, a company discovered that some security groups allow SSH trac from 0.0.0.0/0. A security team must implement a<br>solution to detect and remediate this issue as soon as possible. The company uses one organization in AWS Organizations to manage all the<br>company&#x27;s AWS accounts.</td>
                    <td>A. Enable AWS Cong for all AWS accounts. Use a periodic trigger to activate the vpe-sg-port-restriction-check AWS Cong rule. Create an<br>AWS Lambda function to remediate any noncompliant rules.<br>B. Create an AWS Lambda function in each AWS account to delete all the security group rules. Create an Amazon EventBridge rule to<br>match security group update events or creation events. Set the Lambda function in each account as a target for the rule.<br>C. Enable AWS Cong for all AWS accounts. Create a custom AWS Cong rule to run on the restricted-ssh conguration change trigger.<br>Congure the rule to invoke an AWS Lambda function to remediate any noncompliant resources.<br>D. Create an AWS Systems Manager Automation document in each account to inspect all security groups and to delete noncompliant<br>rules. Use an Amazon EventBridge rule to run the Automation document every hour.</td>
                    <td>在安全审计期间，一家公司发现某些安全组允许来自0.0.0.0/0的SSH流量。安全团队必须尽快实施一个解决方案来检测和修复此问题。该公司在AWS Organizations中使用一个组织来管理公司的所有AWS账户。</td>
                    <td>选项A：为所有AWS账户启用AWS Config。使用周期性触发器激活vpc-sg-port-restriction-check AWS Config规则。创建AWS Lambda函数来修复任何不合规的规则。<br>这个选项使用了AWS Config的内置规则vpc-sg-port-restriction-check，专门用于检查安全组端口限制。周期性触发器可以定期检查合规性，Lambda函数可以自动修复不合规的安全组规则。这是一个完整且有效的解决方案。<br><br>选项B：在每个AWS账户中创建AWS Lambda函数来删除所有安全组规则。创建Amazon EventBridge规则来匹配安全组更新事件或创建事件。将每个账户中的Lambda函数设置为规则的目标。<br>这个选项过于激进，删除所有安全组规则会导致网络连接问题。而且没有使用AWS Config来进行合规性检查，只是简单地删除规则，不够精确和安全。<br><br>选项C：为所有AWS账户启用AWS Config。创建自定义AWS Config规则在restricted-ssh配置变更触发器上运行。配置规则调用AWS Lambda函数来修复任何不合规的资源。<br>虽然使用了AWS Config，但创建自定义规则比使用现有的内置规则更复杂，而且restricted-ssh触发器的描述不够清晰，可能不是标准的AWS Config触发器。<br><br>选项D：在每个账户中创建AWS Systems Manager自动化文档来检查所有安全组并删除不合规规则。使用Amazon EventBridge规则每小时运行自动化文档。<br>这个选项没有使用AWS Config进行持续的合规性监控，而是依赖于定时执行，可能会错过在执行间隔期间创建的不合规资源。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>309</td>
                    <td>A company&#x27;s DevOps engineer must install a software package on 30 on-premises VMs and 15 Amazon EC2 instances.<br>The DevOps engineer needs to ensure that all VMs receive the package in a process that is auditable and that any conguration drift on the<br>VMs is automatically identied and alerted on. The company uses AWS Direct Connect to connect its on-premises data center to AWS.</td>
                    <td>A. Write a script that iterates through the list of VMs once a week. Congure the script to check for the package and install the package if<br>the package is not found. Congure the script to send an email message notication to the system administrator if the package is not<br>found.<br>B. Install the AWS Systems Manager Agent (SSM Agent) on all VMs. Use the SSM Agent to install the package. Use AWS Cong to monitor<br>for conguration drift. Use Amazon Simple Notication Service (Amazon SNS) to notify the system administrator if any drift is found.<br>C. Write a script that checks if the package is installed across the environment. Congure the script to create a list of all VMs that are<br>noncompliant. Congure the script to send the list to the system administrator, who will install the package on the noncompliant VMs.<br>D. Log in to each VM. Use a local package manager to install the package. Use AWS Cong to monitor the AWS resources for conguration<br>changes. Write a script to monitor the on-premises resources.</td>
                    <td>一家公司的DevOps工程师必须在30台本地虚拟机和15台Amazon EC2实例上安装软件包。DevOps工程师需要确保所有虚拟机都能通过可审计的流程接收到软件包，并且能够自动识别和警报虚拟机上的任何配置漂移。该公司使用AWS Direct Connect将其本地数据中心连接到AWS。</td>
                    <td>选项A：编写脚本每周遍历虚拟机列表一次，检查软件包并在未找到时安装。这种方法缺乏实时性和专业的配置管理能力，无法提供完整的审计功能，也不能有效监控配置漂移。手动脚本方式容易出错且维护困难，不符合现代DevOps最佳实践。<br><br>选项B：在所有虚拟机上安装AWS Systems Manager Agent，使用SSM Agent安装软件包，使用AWS Config监控配置漂移，使用Amazon SNS通知系统管理员。这是最佳解决方案，SSM提供统一的混合云管理能力，支持本地和云端资源，具备完整的审计日志功能，AWS Config可以持续监控配置变化，SNS提供自动化通知机制。<br><br>选项C：编写脚本检查软件包安装情况，创建不合规虚拟机列表并发送给管理员手动安装。这种方法仍然依赖手动操作，缺乏自动化程度，无法提供持续的配置漂移监控，不符合DevOps自动化原则。<br><br>选项D：手动登录每台虚拟机使用本地包管理器安装软件包。这是最原始的方法，完全缺乏自动化，无法扩展，且AWS Config无法监控本地资源，需要额外编写脚本监控本地资源，整体方案复杂且效率低下。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>310</td>
                    <td>A company has an AWS CodePipeline pipeline in the eu-west-1 Region. The pipeline stores the build artifacts in an Amazon S3 bucket. The<br>pipeline builds and deploys an AWS Lambda function by using an AWS CloudFormation deploy action.<br>A DevOps engineer needs to update the existing pipeline to also deploy the Lambda function to the us-east-1 Region. The pipeline has<br>already been updated to create an additional artifact to deploy to us-east-1.</td>
                    <td>A. Modify the CloudFormation template to include a parameter for the Lambda function code&#x27;s .zip le location. Create a new<br>CloudFormation deploy action for us-east-1 in the pipeline. Congure the new deploy action to pass in the us-east-1 artifact location as a<br>parameter override.<br>B. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Congure the new deploy action to use the CloudFormation<br>template from the additional artifact that was created for us-east-1.<br>C. Create an S3 bucket in us-east-1. Congure the S3 bucket policy to allow CodePipeline to have read and write access.<br>D. Create an S3 bucket in us-east-1. Congure S3 Cross-Region Replication (CRR) from the S3 bucket in eu-west-1 to the S3 bucket in us-<br>east-1.<br>E. Modify the pipeline to include the S3 bucket for us-east-1 as an artifact store. Create a new CloudFormation deploy action for us-east-1<br>in the pipeline. Congure the new deploy action to use the CloudFormation template from the us-east-1 artifact.</td>
                    <td>一家公司在eu-west-1区域有一个AWS CodePipeline管道。该管道将构建工件存储在Amazon S3存储桶中。管道使用AWS CloudFormation部署操作来构建和部署AWS Lambda函数。<br>DevOps工程师需要更新现有管道，以便也将Lambda函数部署到us-east-1区域。管道已经更新为创建一个额外的工件来部署到us-east-1。</td>
                    <td>A. 修改CloudFormation模板以包含Lambda函数代码zip文件位置的参数。在管道中为us-east-1创建新的CloudFormation部署操作。配置新的部署操作以将us-east-1工件位置作为参数覆盖传入。这个选项是正确的，因为它通过参数化的方式让CloudFormation模板能够接受不同区域的工件位置，实现了跨区域部署的灵活性。<br><br>B. 在管道中为us-east-1创建新的CloudFormation部署操作。配置新的部署操作使用为us-east-1创建的额外工件中的CloudFormation模板。这个选项不够完整，因为它没有解决跨区域工件存储的问题。<br><br>C. 在us-east-1创建S3存储桶。配置S3存储桶策略以允许CodePipeline具有读写访问权限。这个选项只是创建了存储桶但没有配置为工件存储，不完整。<br><br>D. 在us-east-1创建S3存储桶。配置从eu-west-1的S3存储桶到us-east-1的S3存储桶的S3跨区域复制(CRR)。这个选项使用复制而不是直接的工件存储配置，不是最佳实践。<br><br>E. 修改管道以包含us-east-1的S3存储桶作为工件存储。在管道中为us-east-1创建新的CloudFormation部署操作。配置新的部署操作使用来自us-east-1工件的CloudFormation模板。这个选项正确地配置了跨区域工件存储，这是跨区域部署的必要条件。</td>
                    <td>AE</td>
                </tr>
                <tr>
                    <td>311</td>
                    <td>A company uses an AWS Cloud Development Kit (AWS CDK) application for its infrastructure. The AWS CDK application creates AWS Lambda<br>functions and the IAM roles that are attached to the functions. The company also uses AWS Organizations. The company&#x27;s developers can<br>assume the AWS CDK application deployment role.<br>The company&#x27;s security team discovered that the developers and the role used to deploy the AWS CDK application have more permissions<br>than necessary. The security team also discovered that the roles attached to the Lambda functions that the CDK application creates have<br>more permissions than necessary. The developers must not have the ability to grant additional permissions.</td>
                    <td>A. Create an SCP that denies the iam:CreateRole action and the iam:UpdateRole action for the developer role and the AWS CDK<br>application deployment role. Centrally create new IAM roles to attach to the Lambda functions for the developers to use to provision<br>Lambda functions.<br>B. Create an IAM permission boundary policy. Dene the maximum actions that the AWS CDK application requires in the policy. Update the<br>account&#x27;s AWS CDK bootstrapping to use the permission boundary. Update the conguration in the AWS CDK application for the default<br>permissions boundary to use the policy.<br>C. Create an IAM permission boundary policy. Dene the maximum actions that the AWS CDK application requires in the policy. Instruct<br>the developers to use the permission boundary policy name when they create a role in the AWS CDK application code.<br>D. Create an SCP that denies the iam:CreateRole action and the iam:UpdateRole action for the developer role. Give the AWS CDK<br>deployment role access to create roles associated with Lambda functions. Run AWS Identity and Access Management Access Analyzer to<br>verify that the Lambda functions role does not have permissions.</td>
                    <td>一家公司使用AWS云开发工具包(AWS CDK)应用程序来管理其基础设施。AWS CDK应用程序创建AWS Lambda函数和附加到这些函数的IAM角色。该公司还使用AWS Organizations。公司的开发人员可以担任AWS CDK应用程序部署角色。<br><br>公司的安全团队发现开发人员和用于部署AWS CDK应用程序的角色拥有超出必要的权限。安全团队还发现CDK应用程序创建的附加到Lambda函数的角色也拥有超出必要的权限。开发人员不得具有授予额外权限的能力。</td>
                    <td>选项A：创建一个SCP来拒绝开发人员角色和AWS CDK应用程序部署角色的iam:CreateRole和iam:UpdateRole操作，并集中创建新的IAM角色供开发人员使用。这种方法过于严格，完全阻止了角色创建，会影响CDK的正常功能，且需要额外的集中管理开销。<br><br>选项B：创建IAM权限边界策略，定义AWS CDK应用程序所需的最大操作权限。更新账户的AWS CDK引导程序以使用权限边界，并更新CDK应用程序配置以使用该策略作为默认权限边界。这是最佳实践，既限制了权限又保持了CDK的功能性，通过权限边界自动应用到所有创建的角色。<br><br>选项C：创建IAM权限边界策略并指示开发人员在CDK应用程序代码中创建角色时使用该权限边界策略名称。这种方法依赖于开发人员的手动操作，容易出错且无法强制执行，不能保证一致性。<br><br>选项D：创建SCP拒绝开发人员角色的iam:CreateRole和iam:UpdateRole操作，但给CDK部署角色创建Lambda函数相关角色的权限。这种方法部分解决了问题，但仍然没有限制CDK部署角色创建的角色权限，且使用IAM Access Analyzer只是验证工具，不能主动限制权限。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>312</td>
                    <td>A company uses Amazon Elastic Container Registry (Amazon ECR) private registries to store container images.<br>A DevOps team needs to ensure that the container images are regularly scanned for software package vulnerabilities.</td>
                    <td>A. Enable enhanced scanning for private registries in Amazon ECR.<br>B. Enable basic continuous scanning for private registries in Amazon ECR.<br>C. Create an AWS System Manager Automation document to scan images by using the AWS SDK. Congure the Automation document to<br>run when a new image is pushed to an ECR registry.<br>D. Create an AWS Lambda function that scans all images in Amazon ECR by using the AWS SDK. Create an Amazon EventBridge rule to<br>invoke the Lambda function each day.</td>
                    <td>一家公司使用Amazon Elastic Container Registry (Amazon ECR)私有注册表来存储容器镜像。DevOps团队需要确保容器镜像定期扫描软件包漏洞。</td>
                    <td>A. 为Amazon ECR中的私有注册表启用增强扫描。<br>这是正确的选择。Amazon ECR的增强扫描功能提供了更全面的漏洞检测能力，使用Inspector v2引擎，可以检测操作系统和编程语言包的漏洞。增强扫描支持持续监控，当新的漏洞数据库更新时会自动重新扫描镜像，提供更准确和及时的安全评估。<br><br>B. 为Amazon ECR中的私有注册表启用基本持续扫描。<br>这个选项部分正确，但不是最佳选择。基本扫描功能相对有限，主要检测操作系统级别的漏洞，覆盖范围不如增强扫描全面。虽然也能提供漏洞检测，但检测能力和准确性都不如增强扫描。<br><br>C. 创建AWS Systems Manager自动化文档，使用AWS SDK扫描镜像。配置自动化文档在新镜像推送到ECR注册表时运行。<br>这是一个过度复杂的自定义解决方案。虽然技术上可行，但需要额外的开发和维护工作，而ECR本身已经提供了内置的扫描功能，没有必要重新发明轮子。<br><br>D. 创建AWS Lambda函数使用AWS SDK扫描Amazon ECR中的所有镜像。创建Amazon EventBridge规则每天调用Lambda函数。<br>同样是不必要的复杂解决方案。这种方法需要自己实现扫描逻辑，维护成本高，而且可能无法达到ECR内置扫描功能的专业水平和准确性。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>313</td>
                    <td>A security team sets up a workow that invokes an AWS Step Functions workow when Amazon EventBridge matches specic events. The<br>events can be generated by several AWS services. AWS CloudTrail records user activities.<br>The security team notices that some important events do not invoke the workow as expected. The CloudTrail logs do not indicate any direct<br>errors related to the missing events.</td>
                    <td>A. Enable EventBridge schema discovery on the event bus to determine whether the event patterns match the expected schema.<br>B. Congure Amazon CloudWatch to monitor EventBridge metrics and Step Functions metrics. Set up alerts for anomalies in event<br>patterns and workow invocations.<br>C. Congure an AWS Lambda logging function to monitor and log events from EventBridge to provide more details about the processed<br>events.<br>D. Review the Step Functions execution history for patterns of failures or timeouts that could correlate to the missing event invocations.<br>E. Review metrics for the EventBridge failed invocations to ensure that the IAM execution role that is attached to the rule has sucient<br>permissions.<br>F. Verify that the Step Functions workow has the correct permissions to be invoked by EventBridge.</td>
                    <td>安全团队设置了一个工作流，当Amazon EventBridge匹配特定事件时会调用AWS Step Functions工作流。这些事件可以由多个AWS服务生成。AWS CloudTrail记录用户活动。<br>安全团队注意到一些重要事件没有按预期调用工作流。CloudTrail日志没有显示与缺失事件相关的任何直接错误。</td>
                    <td>A. 在事件总线上启用EventBridge架构发现以确定事件模式是否匹配预期架构 - 这个选项有一定价值，但架构发现主要用于了解事件结构，不是排查调用失败的最佳方法。<br><br>B. 配置Amazon CloudWatch监控EventBridge指标和Step Functions指标，为事件模式和工作流调用中的异常设置警报 - 这是正确的方法，通过监控相关指标可以发现调用失败的模式和原因，CloudWatch提供了详细的指标数据来分析问题。<br><br>C. 配置AWS Lambda日志记录功能来监控和记录来自EventBridge的事件，提供有关已处理事件的更多详细信息 - 虽然可以提供更多信息，但这不是最直接的排查方法，而且增加了系统复杂性。<br><br>D. 查看Step Functions执行历史记录，寻找可能与缺失事件调用相关的失败或超时模式 - 这是正确的方法，执行历史记录可以显示工作流是否被调用以及调用后的执行情况，有助于确定问题是在EventBridge层面还是Step Functions层面。<br><br>E. 查看EventBridge失败调用的指标，确保附加到规则的IAM执行角色具有足够的权限 - 这是正确的方法，权限问题是导致调用失败的常见原因，检查失败调用指标和IAM权限是必要的排查步骤。<br><br>F. 验证Step Functions工作流具有被EventBridge调用的正确权限 - 虽然权限很重要，但这更多是Step Functions资源策略的问题，不如选项E全面。</td>
                    <td>BDE</td>
                </tr>
                <tr>
                    <td>314</td>
                    <td>A company&#x27;s DevOps engineer uses AWS Systems Manager to perform maintenance tasks. The company has a few Amazon EC2 instances that<br>require a restart after notications from AWS Health.<br>The DevOps engineer must implement an automated solution that uses Amazon EventBridge to remediate the notications during the<br>company&#x27;s scheduled maintenance windows.<br>How should the DevOps engineer congure an EventBridge rule to meet these requirements?</td>
                    <td>A. Congure an event source of AWS Health. Congure event types that indicate scheduled instance termination and retirement. Target<br>the AWS-RestartEC2Instance Systems Manager Automation runbook to restart the EC2 instances.<br>B. Congure an event source of Systems Manager. Congure an event type that indicates a maintenance window. Target the AWS-<br>RestartEC2Instance Systems Manager Automation runbook to restart the EC2 instances.<br>C. Congure an event source of AWS Health. Congure event types that indicate scheduled instance termination and retirement. Target a<br>newly created AWS Lambda function that registers a Systems Manager maintenance window task to restart the EC2 instances.<br>D. Congure an event source of EC2. Congure an event type that indicates instance state notication. Target a newly created AWS<br>Lambda function that registers a Systems Manager maintenance window task to restart the EC2 instances.</td>
                    <td>一家公司的DevOps工程师使用AWS Systems Manager来执行维护任务。该公司有一些Amazon EC2实例，在收到AWS Health通知后需要重启。DevOps工程师必须实现一个自动化解决方案，使用Amazon EventBridge在公司的计划维护窗口期间修复这些通知。DevOps工程师应该如何配置EventBridge规则来满足这些要求？</td>
                    <td>选项A：配置AWS Health作为事件源，配置指示计划实例终止和退役的事件类型，目标为AWS-RestartEC2Instance Systems Manager自动化运行手册来重启EC2实例。这个选项正确识别了事件源（AWS Health），并且直接使用了现有的Systems Manager自动化运行手册来重启实例，是最直接和高效的解决方案。<br><br>选项B：配置Systems Manager作为事件源，配置指示维护窗口的事件类型，目标为AWS-RestartEC2Instance Systems Manager自动化运行手册来重启EC2实例。这个选项的问题在于事件源不正确，应该是AWS Health而不是Systems Manager，因为通知来自AWS Health。<br><br>选项C：配置AWS Health作为事件源，配置指示计划实例终止和退役的事件类型，目标为新创建的AWS Lambda函数，该函数注册一个Systems Manager维护窗口任务来重启EC2实例。虽然事件源正确，但这种方法过于复杂，需要额外创建Lambda函数和维护窗口任务，而直接使用现有的自动化运行手册更简单。<br><br>选项D：配置EC2作为事件源，配置指示实例状态通知的事件类型，目标为新创建的AWS Lambda函数，该函数注册一个Systems Manager维护窗口任务来重启EC2实例。这个选项的事件源不正确，应该是AWS Health而不是EC2，并且同样过于复杂。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>315</td>
                    <td>A DevOps engineer manages an AWS CodePipeline pipeline that builds and deploys a web application on AWS. The pipeline has a source<br>stage, a build stage, and a deploy stage. When deployed properly, the web application responds with a 200 OK HTTP response code when the<br>URL of the home page is requested.<br>The home page recently returned a 503 HTTP response code after CodePipeline deployed the application. The DevOps engineer needs to add<br>an automated test into the pipeline. The automated test must ensure that the application returns a 200 OK HTTP response code after the<br>application is deployed. The pipeline must fail if the response code is not present during the test. The DevOps engineer has added a<br>CheckURL stage after the deploy stage in the pipeline.</td>
                    <td>A. Congure the CheckURL stage to use an Amazon CloudWatch action. Congure the action to use a canary synthetic monitoring check<br>on the application URL and to report a success or failure to CodePipeline.<br>B. Create an AWS Lambda function to check the response code status of the URL and to report a success or failure to CodePipeline.<br>Congure an action in the CheckURL stage to invoke the Lambda function.<br>C. Congure the CheckURL stage to use an AWS CodeDeploy action. Congure the action with an input artifact that is the URL of the<br>application and to report a success or failure to CodePipeline.<br>D. Deploy an Amazon API Gateway HTTP API that checks the response code status of the URL and that reports success or failure to<br>CodePipeline. Congure the CheckURL stage to use the AWS Device Farm test action and to provide the API Gateway HTTP API as an input<br>artifact.</td>
                    <td>一名DevOps工程师管理着一个AWS CodePipeline流水线，该流水线在AWS上构建和部署Web应用程序。该流水线包含源代码阶段、构建阶段和部署阶段。当正确部署时，Web应用程序在请求主页URL时会返回200 OK HTTP响应代码。<br>最近，在CodePipeline部署应用程序后，主页返回了503 HTTP响应代码。DevOps工程师需要在流水线中添加自动化测试。该自动化测试必须确保应用程序在部署后返回200 OK HTTP响应代码。如果测试期间没有出现该响应代码，流水线必须失败。DevOps工程师已经在流水线的部署阶段后添加了一个CheckURL阶段。</td>
                    <td>选项A：配置CheckURL阶段使用Amazon CloudWatch操作，配置该操作使用金丝雀综合监控检查应用程序URL并向CodePipeline报告成功或失败。虽然CloudWatch Synthetics可以进行URL检查，但CloudWatch操作并不是CodePipeline的标准操作类型，且这种集成方式不够直接和简单。<br><br>选项B：创建AWS Lambda函数来检查URL的响应代码状态并向CodePipeline报告成功或失败，在CheckURL阶段配置操作来调用Lambda函数。这是一个完全可行的解决方案，Lambda函数可以轻松执行HTTP请求检查响应代码，并且CodePipeline原生支持Lambda操作，可以根据函数执行结果决定流水线是否继续或失败。<br><br>选项C：配置CheckURL阶段使用AWS CodeDeploy操作，配置该操作以应用程序URL作为输入工件并向CodePipeline报告成功或失败。CodeDeploy主要用于应用程序部署，不是用于URL健康检查的工具，这种用法不合适且不符合CodeDeploy的设计目的。<br><br>选项D：部署Amazon API Gateway HTTP API来检查URL响应代码状态并向CodePipeline报告成功或失败，配置CheckURL阶段使用AWS Device Farm测试操作并提供API Gateway HTTP API作为输入工件。这个方案过于复杂，Device Farm主要用于移动应用测试，不适合简单的URL检查任务。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>316</td>
                    <td>A company has an application that uploads access logs to an Amazon CloudWatch Logs log group. The elds in the log lines include the<br>response code and the application name.<br>The company wants to create a CloudWatch metric to track the number of requests by response code in a specic range and with a specic<br>application name.</td>
                    <td>A. Create a CloudWatch Logs log event lter on the CloudWatch Logs log stream to match the response code range. Congure the log<br>event lter to increment a metric. Set the response code and application name as dimensions.<br>B. Create a CloudWatch Logs metric lter on the CloudWatch Logs log group to match the response code range. Congure the metric lter<br>to increment a metric. Set the response code and application name as dimensions.<br>C. Create a CloudWatch Contributor Insights rule on the CloudWatch Logs log stream with a lter to match the response code range.<br>Congure the Contributor Insights rule to increment a CloudWatch metric with the response code and application name as dimensions.<br>D. Create a CloudWatch Logs Insights query on the CloudWatch Logs log group to match the response code range. Congure the Logs<br>Insights query to increment a CloudWatch metric with the response code and application name as dimensions.</td>
                    <td>一家公司有一个应用程序，它将访问日志上传到Amazon CloudWatch Logs日志组。日志行中的字段包括响应代码和应用程序名称。<br>该公司希望创建一个CloudWatch指标来跟踪特定范围内响应代码和特定应用程序名称的请求数量。</td>
                    <td>A. 在CloudWatch Logs日志流上创建CloudWatch Logs日志事件过滤器来匹配响应代码范围。配置日志事件过滤器来增加指标。将响应代码和应用程序名称设置为维度。<br>这个选项的问题在于日志事件过滤器是在日志流级别创建的，而不是在日志组级别。通常我们需要在日志组级别创建指标过滤器来处理整个日志组的数据，而不是单个日志流。<br><br>B. 在CloudWatch Logs日志组上创建CloudWatch Logs指标过滤器来匹配响应代码范围。配置指标过滤器来增加指标。将响应代码和应用程序名称设置为维度。<br>这是正确的方法。指标过滤器应该在日志组级别创建，可以解析日志内容并根据特定模式创建自定义指标。它可以提取响应代码和应用程序名称作为维度，并跟踪匹配特定条件的请求数量。<br><br>C. 在CloudWatch Logs日志流上创建CloudWatch Contributor Insights规则，使用过滤器匹配响应代码范围。配置Contributor Insights规则来增加CloudWatch指标，将响应代码和应用程序名称作为维度。<br>Contributor Insights主要用于分析和可视化日志数据中的贡献者模式，虽然可以创建指标，但不是这种场景的最佳选择，而且通常不在日志流级别操作。<br><br>D. 在CloudWatch Logs日志组上创建CloudWatch Logs Insights查询来匹配响应代码范围。配置Logs Insights查询来增加CloudWatch指标，将响应代码和应用程序名称作为维度。<br>Logs Insights是一个交互式查询服务，用于搜索和分析日志数据，但它不能直接配置为自动增加CloudWatch指标。它主要用于临时查询和分析。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>317</td>
                    <td>A DevOps engineer provisioned an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with managed node groups. The DevOps<br>engineer associated an OpenID Connect (OIDC) issuer with the cluster.<br>The DevOps engineer is conguring Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp3) volumes for the cluster. The DevOps<br>engineer attempts to initiate a PersistentVolumeClaim (PVC) request but is unable to provision a volume. To troubleshoot the issue, the<br>DevOps engineer runs the kubectl describe pyc command. The DevOps engineer receives a failed to provision volume with StorageClass error<br>and a could not create volume in EC2:UnauthorizedOperation error.</td>
                    <td>A. Create a Kubernetes cluster role that allows the persistent volumes to perform get, list, watch, create, and delete operations. Congure<br>the cluster role to allow get, list, and watch operations for storage in the cluster.<br>B. Create an Amazon EBS Container Storage Interface (CSI) driver IAM role that has the required permissions and trust relationships.<br>Attach the IAM role to the Amazon EBS CSI driver add-on in the cluster.<br>C. Add the ebs.csi.aws.com/volumeType:gp3 annotation to the PersistentVolumeClaim object in the cluster.<br>D. Create a Kubernetes storage class object. Set the provisioner value to ebs.csi.aws.com. Set the volumeBindingMode value to<br>WaitForFirstConsumer in the luster.</td>
                    <td>一名DevOps工程师使用托管节点组配置了Amazon Elastic Kubernetes Service (Amazon EKS)集群。该DevOps工程师将OpenID Connect (OIDC)发行者与集群关联。<br>该DevOps工程师正在为集群配置Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp3)卷。DevOps工程师尝试发起PersistentVolumeClaim (PVC)请求，但无法配置卷。为了排查问题，DevOps工程师运行kubectl describe pvc命令。DevOps工程师收到&quot;failed to provision volume with StorageClass&quot;错误和&quot;could not create volume in EC2:UnauthorizedOperation&quot;错误。</td>
                    <td>A. 创建一个Kubernetes集群角色，允许持久卷执行get、list、watch、create和delete操作。配置集群角色以允许对集群中的存储执行get、list和watch操作。<br>这个选项涉及的是Kubernetes RBAC权限配置，但错误信息显示的是EC2:UnauthorizedOperation，这表明问题出在AWS IAM权限层面，而不是Kubernetes内部的RBAC权限问题。<br><br>B. 创建一个具有所需权限和信任关系的Amazon EBS Container Storage Interface (CSI)驱动程序IAM角色。将IAM角色附加到集群中的Amazon EBS CSI驱动程序附加组件。<br>这个选项直接针对错误信息中的&quot;EC2:UnauthorizedOperation&quot;问题。EBS CSI驱动程序需要适当的IAM权限来在EC2中创建和管理EBS卷。错误表明CSI驱动程序缺少必要的AWS权限。<br><br>C. 将ebs.csi.aws.com/volumeType:gp3注释添加到集群中的PersistentVolumeClaim对象。<br>这个选项是关于指定卷类型的注释，但这不会解决权限问题。错误信息表明问题是权限不足，而不是卷类型配置问题。<br><br>D. 创建一个Kubernetes存储类对象。将provisioner值设置为ebs.csi.aws.com。将volumeBindingMode值设置为WaitForFirstConsumer。<br>虽然正确配置StorageClass很重要，但错误信息显示的是权限问题，而不是StorageClass配置问题。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>318</td>
                    <td>A company runs a eet of Amazon EC2 instances in a VP</td>
                    <td>C. Create a ow log in VPC Flow Logs.<br>A. Create an Amazon EventBridge rule that reacts to EC2 Instance State-change Notication events.<br>B. Create an Amazon CloudWatch Logs log group. Specify the log group as a target for the EventBridge rule.<br>D. Create an Amazon CloudWatch Logs log group. Specify the log group as a destination for the ow log.<br>E. Create a log group metric lter.<br>F. Create a log group subscription lter. Use EventBridge as the destination.</td>
                    <td>一家公司在VPC中运行一组Amazon EC2实例（题目似乎不完整，但根据选项可以推断这是关于监控和日志记录的问题）</td>
                    <td>A. 创建一个Amazon EventBridge规则来响应EC2实例状态变更通知事件<br>这个选项用于监控EC2实例的状态变化，如启动、停止、终止等。EventBridge可以捕获这些状态变更事件并触发相应的操作。虽然这是一个有效的监控方案，但根据题目上下文和参考答案，这可能不是主要需求。<br><br>B. 创建一个Amazon CloudWatch Logs日志组，将该日志组指定为EventBridge规则的目标<br>这个选项是将EventBridge规则的输出发送到CloudWatch Logs。这通常用于记录事件处理的结果或状态。但这需要先有EventBridge规则（选项A），且不在参考答案中。<br><br>C. 在VPC Flow Logs中创建流日志<br>VPC Flow Logs用于捕获进出VPC网络接口的IP流量信息。这对于网络监控、安全分析和故障排除非常重要。流日志可以帮助分析网络流量模式、检测异常活动和优化网络性能。<br><br>D. 创建一个Amazon CloudWatch Logs日志组，将该日志组指定为流日志的目标<br>这个选项是为VPC Flow Logs配置目标存储位置。流日志需要一个目标来存储收集的数据，CloudWatch Logs是一个常用的目标，便于后续分析和查询。<br><br>E. 创建日志组指标过滤器<br>指标过滤器用于从日志数据中提取特定的指标，并将这些指标发送到CloudWatch Metrics。这允许基于日志内容创建自定义指标，用于监控和告警。<br><br>F. 创建日志组订阅过滤器，使用EventBridge作为目标<br>订阅过滤器用于实时处理日志数据，将匹配特定模式的日志事件发送到其他服务进行处理。虽然技术上可行，但不在参考答案中。</td>
                    <td>CDE</td>
                </tr>
                <tr>
                    <td>319</td>
                    <td>A company is using Amazon Elastic Kubernetes Service (Amazon EKS) to run its applications. The EKS cluster is successfully running multiple<br>pods. The company stores the pod images in Amazon Elastic Container Registry (Amazon ECR).<br>The company needs to congure Pod Identity access for the EKS cluster. The company has already updated the node IAM role by using the<br>permissions for Pod Identity access.</td>
                    <td>A. Create an IAM OpenID Connect (OIDC) provider for the EKS cluster.<br>B. Ensure that the nodes can reach the EKS Auth API. Add and congure the EKS Pod Identity Agent add-on for the EKS cluster.<br>C. Create an EKS access entry that uses the API_AND-CONFIG_MAP cluster authentication mode.<br>D. Congure the AWS Security Token Service (AWS STS) endpoint for the Kubernetes service account that the pods in the EKS cluster use.</td>
                    <td>一家公司正在使用Amazon Elastic Kubernetes Service (Amazon EKS)来运行其应用程序。EKS集群成功运行着多个pod。该公司将pod镜像存储在Amazon Elastic Container Registry (Amazon ECR)中。<br>该公司需要为EKS集群配置Pod Identity访问。该公司已经通过使用Pod Identity访问权限更新了节点IAM角色。</td>
                    <td>A. 为EKS集群创建IAM OpenID Connect (OIDC)提供程序。这是配置Pod Identity访问的关键步骤。OIDC提供程序允许Kubernetes服务账户与AWS IAM角色建立信任关系，使pod能够安全地访问AWS服务。这是实现Pod Identity功能的基础组件，必须首先配置才能让pod获得适当的AWS权限。<br><br>B. 确保节点可以访问EKS Auth API，并为EKS集群添加和配置EKS Pod Identity Agent插件。虽然Pod Identity Agent是新的功能，但这不是传统Pod Identity配置的标准步骤。Pod Identity Agent是AWS较新推出的功能，而题目描述的场景更符合传统的IRSA（IAM Roles for Service Accounts）配置方式。<br><br>C. 创建使用API_AND_CONFIG_MAP集群身份验证模式的EKS访问条目。这个选项涉及的是集群访问控制配置，主要用于管理用户和角色对EKS集群的访问权限，而不是配置pod对AWS服务的访问权限，与Pod Identity配置无直接关系。<br><br>D. 为EKS集群中pod使用的Kubernetes服务账户配置AWS Security Token Service (AWS STS)端点。STS端点配置通常是自动处理的，不需要手动配置。这不是配置Pod Identity访问的主要步骤，而且STS端点配置通常在OIDC提供程序创建后自动生效。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>320</td>
                    <td>A company has multiple AWS accounts in an organization in AWS Organizations that has all features enabled. The company’s DevOps<br>administrator needs to improve security across all the company&#x27;s AWS accounts. The administrator needs to identify the top users and roles in<br>use across all accounts.</td>
                    <td>A. Create a new organization trail in AWS CloudTrail. Congure the trail to send log events to Amazon CloudWatch Logs. Create a<br>CloudWatch Contributor Insights rule for the userIdentity.arn log eld. View the results in CloudWatch Contributor Insights.<br>B. Create an unused access analysis for the organization by using AWS Identity and Access Management Access Analyzer. Review the<br>analyzer results and determine if each nding has the intended level of permissions required for the workload.<br>C. Create a new organization trail in AWS CloudTrail. Create a table in Amazon Athena that uses partition projection. Load the Athena<br>table with CloudTrail data. Query the Athena table to nd the top users and roles.<br>D. Generate a Service access report for each account by using Organizations. From the results, pull the last accessed date and last<br>accessed by account elds to nd the top users and roles.</td>
                    <td>一家公司在AWS Organizations中拥有多个AWS账户，该组织启用了所有功能。公司的DevOps管理员需要提高所有公司AWS账户的安全性。管理员需要识别所有账户中使用最频繁的顶级用户和角色。</td>
                    <td>选项A：创建新的组织级AWS CloudTrail跟踪，配置跟踪将日志事件发送到Amazon CloudWatch Logs。为userIdentity.arn日志字段创建CloudWatch Contributor Insights规则，在CloudWatch Contributor Insights中查看结果。这个方案能够有效捕获所有账户的API调用活动，通过Contributor Insights可以统计和分析最活跃的用户和角色，直接满足题目要求。<br><br>选项B：使用AWS Identity and Access Management Access Analyzer为组织创建未使用访问分析。审查分析器结果并确定每个发现是否具有工作负载所需的预期权限级别。这个方案主要用于识别未使用的权限和过度权限，而不是识别最活跃的用户和角色，不符合题目要求。<br><br>选项C：创建新的组织级AWS CloudTrail跟踪，在Amazon Athena中创建使用分区投影的表，将CloudTrail数据加载到Athena表中，查询Athena表以找到顶级用户和角色。虽然技术上可行，但相比选项A更复杂，需要额外的数据处理和查询工作。<br><br>选项D：使用Organizations为每个账户生成服务访问报告，从结果中提取最后访问日期和最后访问账户字段来找到顶级用户和角色。服务访问报告主要显示服务级别的访问信息，而不是用户和角色的详细活动统计，无法准确识别最活跃的用户和角色。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>321</td>
                    <td>A company has an organization in AWS Organizations with many Oils that contain many AWS accounts. The organization has a dedicated<br>delegated administrator AWS account.<br>The company needs the accounts in one OU to have server-side encryption enforced for all Amazon Elastic Block Store (Amazon EBS) volumes<br>and Amazon Simple Queue Service (Amazon SQS) queues that are created or updated on an AWS CloudFormation stack.</td>
                    <td>A. Activate trusted access to CloudFormation StackSets. Create a CloudFormation Hook that enforces server-side encryption on EBS<br>volumes and SQS queues. Deploy the Hook across the accounts in the OU by using StackSets.<br>B. Set up AWS Cong in all the accounts in the OU. Use AWS Systems Manager to deploy AWS Cong rules that enforce server-side<br>encryption for EBS volumes and SQS queues across the accounts in the OU.<br>C. Write an SCP to deny the creation of EBS volumes and SQS queues unless the EBS volumes and SQS queues have server-side<br>encryption. Attach the SCP to the OU.<br>D. Create an AWS Lambda function in the delegated administrator account that checks whether server-side encryption is enforced for EBS<br>volumes and SQS queues. Create an IAM role to provide the Lambda function access to the accounts in the OU.</td>
                    <td>一家公司在AWS Organizations中有一个组织，包含多个OU（组织单位），这些OU包含许多AWS账户。该组织有一个专门的委托管理员AWS账户。<br>公司需要在一个OU中的账户对所有在AWS CloudFormation堆栈中创建或更新的Amazon Elastic Block Store (Amazon EBS) 卷和Amazon Simple Queue Service (Amazon SQS) 队列强制执行服务器端加密。</td>
                    <td>A. 激活CloudFormation StackSets的可信访问。创建一个CloudFormation Hook来强制EBS卷和SQS队列的服务器端加密。使用StackSets在OU中的账户间部署Hook。这个选项利用了CloudFormation Hooks的预防性控制机制，可以在资源创建或更新时强制执行加密策略，并通过StackSets实现跨账户部署，符合题目要求的在CloudFormation堆栈中强制加密的需求。<br><br>B. 在OU中的所有账户设置AWS Config。使用AWS Systems Manager部署AWS Config规则，在OU的账户间强制执行EBS卷和SQS队列的服务器端加密。Config规则是检测性控制，只能发现不合规资源但无法阻止创建，不能满足&quot;强制执行&quot;的要求。<br><br>C. 编写SCP来拒绝创建没有服务器端加密的EBS卷和SQS队列。将SCP附加到OU。虽然SCP可以实现预防性控制，但题目特别强调是针对CloudFormation堆栈中的资源，SCP的粒度可能过于宽泛，且难以精确控制CloudFormation特定的资源创建。<br><br>D. 在委托管理员账户中创建AWS Lambda函数来检查EBS卷和SQS队列是否强制执行服务器端加密。创建IAM角色为Lambda函数提供访问OU中账户的权限。这是事后检查机制，无法在资源创建时阻止不合规的资源，不符合&quot;强制执行&quot;的要求。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>322</td>
                    <td>A company is running an internal application in an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. The ECS cluster<br>instances can connect to the public internet. The ECS tasks that run on the cluster instances are congured to use images from both private<br>Amazon Elastic Container Registry (Amazon ECR) repositories and a public ECR registry repository.<br>A new security policy requires the company to remove the ECS cluster&#x27;s direct access to the internet. The company must remove any NAT<br>gateways and internet gateways from the VPC that hosts the cluster. A DevOps engineer needs to ensure the ECS cluster can still download<br>images from both the public ECR registry and the private ECR repositories. Images from the public ECR registry must remain up-to-date. New<br>versions of the images must be available to the ECS cluster within 24 hours of publication.</td>
                    <td>A. Create an AWS CodeBuild project and a new private ECR repository for each image that is downloaded from the public ECR registry.<br>Congure each project to pull the image from the public ECR repository and push the image to the new private ECR repository. Create an<br>Amazon EventBridge rule that invokes the CodeBuild project once every 24 hours. Update each task denition in the ECS cluster to refer<br>to the new private ECR repository.<br>B. Create a new Amazon ECR pull through cache rule for each image that is downloaded from the public ECR registry. Create an AWS<br>Lambda function that invokes each pull through cache rule. Create an Amazon EventBridge rule that invokes the Lambda function once<br>every 24 hours. Update each task denition in the ECS cluster to refer to the image from the pull through cache.<br>C. <br>D. Create an Amazon ECR interface VPC endpoint for the public ECR repositories that are in the VP<br>E. Create an Amazon ECR interface VPC endpoint for the private ECR repositories that are in the VP</td>
                    <td>一家公司在Amazon EC2上的Amazon Elastic Container Service (Amazon ECS)集群中运行内部应用程序。ECS集群实例可以连接到公共互联网。在集群实例上运行的ECS任务配置为使用来自私有Amazon Elastic Container Registry (Amazon ECR)存储库和公共ECR注册表存储库的镜像。<br><br>新的安全策略要求公司移除ECS集群对互联网的直接访问。公司必须从托管集群的VPC中移除任何NAT网关和互联网网关。DevOps工程师需要确保ECS集群仍然可以从公共ECR注册表和私有ECR存储库下载镜像。来自公共ECR注册表的镜像必须保持最新。新版本的镜像必须在发布后24小时内对ECS集群可用。</td>
                    <td>选项A：创建AWS CodeBuild项目和新的私有ECR存储库来处理公共ECR镜像。这种方法可以工作，但增加了复杂性和成本，需要维护额外的基础设施。虽然可以满足24小时更新要求，但不是最优解决方案，因为它需要创建和管理多个CodeBuild项目和私有存储库。<br><br>选项B：创建Amazon ECR拉取缓存规则和Lambda函数来处理公共ECR镜像。这是一个更优雅的解决方案，ECR拉取缓存可以自动缓存公共镜像到私有存储库中，Lambda函数定期触发更新确保镜像保持最新。这种方法减少了管理开销，同时满足24小时更新要求。<br><br>选项C和D（题目中显示不完整）：从描述来看，这些选项涉及创建Amazon ECR接口VPC端点。VPC端点允许在没有互联网网关或NAT网关的情况下私密地访问AWS服务。为公共和私有ECR存储库创建接口VPC端点是解决网络连接问题的关键组件。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>323</td>
                    <td>A company has a continuous integration pipeline where the company creates container images by using AWS CodeBuild. The created images<br>are stored in Amazon Elastic Container Registry (Amazon ECR).<br>Checking for and xing the vulnerabilities in the images takes the company too much time. The company wants to identify the image<br>vulnerabilities quickly and notify the security team of the vulnerabilities.</td>
                    <td>A. Activate Amazon Inspector enhanced scanning for Amazon ECR. Congure the enhanced scanning to use continuous scanning. Set up a<br>topic in Amazon Simple Notication Service (Amazon SNS).<br>B. Create an Amazon EventBridge rule for Amazon Inspector ndings. Set an Amazon Simple Notication Service (Amazon SNS) topic as<br>the rule target.<br>C. Activate AWS Lambda enhanced scanning for Amazon ECR. Congure the enhanced scanning to use continuous scanning. Set up a<br>topic in Amazon Simple Email Service (Amazon SES).<br>D. Create a new AWS Lambda function. Invoke the new Lambda function when scan ndings are detected.<br>E. Activate default basic scanning for Amazon ECR for all container images. Congure the default basic scanning to use continuous<br>scanning. Set up a topic in Amazon Simple Notication Service (Amazon SNS).</td>
                    <td>一家公司有一个持续集成管道，该公司使用AWS CodeBuild创建容器镜像。创建的镜像存储在Amazon弹性容器注册表(Amazon ECR)中。<br>检查和修复镜像中的漏洞耗费了公司太多时间。公司希望快速识别镜像漏洞并通知安全团队相关漏洞。</td>
                    <td>A. 为Amazon ECR激活Amazon Inspector增强扫描。配置增强扫描使用持续扫描。在Amazon简单通知服务(Amazon SNS)中设置主题。<br>这个选项是正确的。Amazon Inspector增强扫描可以对ECR中的容器镜像进行深度漏洞扫描，持续扫描模式可以在镜像推送时自动触发扫描，SNS可以用于通知安全团队。这是AWS推荐的容器镜像安全扫描解决方案。<br><br>B. 为Amazon Inspector发现创建Amazon EventBridge规则。设置Amazon简单通知服务(Amazon SNS)主题作为规则目标。<br>这个选项也是正确的。当Inspector发现漏洞时，会生成事件，通过EventBridge规则可以捕获这些事件并触发SNS通知，实现自动化的漏洞通知机制。<br><br>C. 为Amazon ECR激活AWS Lambda增强扫描。配置增强扫描使用持续扫描。在Amazon简单邮件服务(Amazon SES)中设置主题。<br>这个选项是错误的。AWS Lambda不提供增强扫描功能，Lambda是计算服务，不是扫描服务。增强扫描是Amazon Inspector的功能。<br><br>D. 创建新的AWS Lambda函数。当检测到扫描发现时调用新的Lambda函数。<br>这个选项不完整。虽然Lambda可以用于处理扫描结果，但没有说明如何激活扫描功能本身，也没有提到通知机制。<br><br>E. 为所有容器镜像激活Amazon ECR的默认基础扫描。配置默认基础扫描使用持续扫描。在Amazon简单通知服务(Amazon SNS)中设置主题。<br>这个选项部分正确但不是最佳选择。基础扫描功能有限，不如增强扫描全面，而且基础扫描的通知机制不如增强扫描完善。</td>
                    <td>AB</td>
                </tr>
                <tr>
                    <td>324</td>
                    <td>A DevOps administrator is conguring a repository to store a company&#x27;s container images. The administrator needs to congure a lifecycle<br>rule that automatically deletes container images that have a specic tag and that are older than 15 days.</td>
                    <td>A. Create a repository in Amazon Elastic Container Registry (Amazon ECR). Add a lifecycle policy to the repository to expire images that<br>have the matching tag after 15 days.<br>B. Create a repository in AWS CodeArtifact. Add a repository policy to the CodeArtifact repository to expire old assets that have the<br>matching tag after 15 days.<br>C. Create a bucket in Amazon S3. Add a bucket lifecycle policy to expire old objects that have the matching tag after 15 days<br>D. Create an EC2 Image Builder container recipe. Add a build component to expire the container that has the matching tag after 15 days.</td>
                    <td>一位DevOps管理员正在配置一个存储库来存储公司的容器镜像。管理员需要配置一个生命周期规则，自动删除具有特定标签且超过15天的容器镜像。</td>
                    <td>A. 在Amazon Elastic Container Registry (Amazon ECR)中创建存储库。向存储库添加生命周期策略，使具有匹配标签的镜像在15天后过期。这个选项是正确的，因为ECR是AWS专门用于存储容器镜像的服务，支持生命周期策略来自动管理镜像的生命周期，可以基于标签和时间条件自动删除镜像。<br><br>B. 在AWS CodeArtifact中创建存储库。向CodeArtifact存储库添加存储库策略，使具有匹配标签的旧资产在15天后过期。这个选项不正确，因为CodeArtifact主要用于存储软件包和依赖项（如Maven、npm、Python包等），而不是容器镜像。<br><br>C. 在Amazon S3中创建存储桶。添加存储桶生命周期策略，使具有匹配标签的旧对象在15天后过期。虽然技术上可以在S3中存储容器镜像，但这不是最佳实践，且缺乏容器镜像管理的专门功能。<br><br>D. 创建EC2 Image Builder容器配方。添加构建组件，使具有匹配标签的容器在15天后过期。这个选项不正确，因为EC2 Image Builder主要用于构建和管理AMI和容器镜像，而不是用于存储和生命周期管理。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>325</td>
                    <td>A company uses Amazon Redshift as its data warehouse solution. The company wants to create a dashboard to view changes to the Redshift<br>users and the queries the users perform.</td>
                    <td>A. Create an Amazon CloudWatch log group. Create an AWS CloudTrail trail that writes to the CloudWatch log group.<br>B. Create a new Amazon S3 bucket. Congure default audit logging on the Redshift cluster. Congure the S3 bucket as the target.<br>C. Congure the Redshift cluster database audit logging to include user activity logs. Congure Amazon CloudWatch as the target.<br>D. Create an Amazon CloudWatch dashboard that has a log widget. Congure the widget to display user details from the Redshift logs.<br>E. Create an AWS Lambda function that uses Amazon Athena to query the Redshift logs. Create an Amazon CloudWatch dashboard that<br>has a custom widget type that uses the Lambda function.</td>
                    <td>一家公司使用Amazon Redshift作为其数据仓库解决方案。该公司希望创建一个仪表板来查看Redshift用户的变更以及用户执行的查询。</td>
                    <td>A. 创建Amazon CloudWatch日志组。创建AWS CloudTrail跟踪，将其写入CloudWatch日志组。<br>这个选项不正确。CloudTrail主要记录AWS API调用和管理事件，虽然可以捕获一些Redshift的管理操作，但无法记录具体的用户查询活动和数据库内部的用户操作。CloudTrail不能提供Redshift集群内部的详细用户活动日志，因此不能满足监控用户查询的需求。<br><br>B. 创建新的Amazon S3存储桶。在Redshift集群上配置默认审计日志记录。将S3存储桶配置为目标。<br>这个选项正确。Redshift支持审计日志记录功能，可以记录用户活动、连接日志和用户日志等。通过配置审计日志记录并将日志存储到S3存储桶中，可以捕获用户的登录、查询执行等详细信息。这些日志包含了用户变更和查询活动的完整记录，为后续的仪表板创建提供了数据源。<br><br>C. 配置Redshift集群数据库审计日志记录以包含用户活动日志。将Amazon CloudWatch配置为目标。<br>这个选项正确。Redshift的审计日志记录功能可以配置为将日志直接发送到CloudWatch Logs。用户活动日志包含了用户连接、查询执行、权限变更等详细信息。将这些日志发送到CloudWatch后，可以利用CloudWatch的监控和可视化功能来创建仪表板，实时监控用户活动。<br><br>D. 创建Amazon CloudWatch仪表板，包含日志小部件。配置小部件以显示来自Redshift日志的用户详细信息。<br>这个选项不完整。虽然创建CloudWatch仪表板是正确的方向，但仅仅创建仪表板和小部件是不够的。首先需要配置Redshift的审计日志记录功能来收集用户活动数据，然后才能在仪表板中显示这些信息。这个选项缺少了数据收集的关键步骤。<br><br>E. 创建AWS Lambda函数，使用Amazon Athena查询Redshift日志。创建Amazon CloudWatch仪表板，包含使用Lambda函数的自定义小部件类型。<br>这个选项过于复杂且不是最佳实践。虽然技术上可行，但增加了不必要的复杂性。直接使用CloudWatch Logs Insights或其他原生功能就可以实现相同的目标，无需额外的Lambda函数和Athena查询。</td>
                    <td>BC</td>
                </tr>
                <tr>
                    <td>326</td>
                    <td>A company uses an organization in AWS Organizations to manage its 500 AWS accounts. The organization has all features enabled. The AWS<br>accounts are in a single OU. The developers need to use the CostCenter tag key for all resources in the organization&#x27;s member accounts.<br>Some teams do not use the CostCenter tag key to tag their Amazon EC2 instances.<br>The cloud team wrote a script that scans all EC2 instances in the organization&#x27;s member accounts. If the EC2 instances do not have a<br>CostCenter tag key, the script will notify AWS account administrators. To avoid this notication, some developers use the CostCenter tag key<br>with an arbitrary string in the tag value.<br>The cloud team needs to ensure that all EC2 instances in the organization use a CostCenter tag key with the appropriate cost center value.</td>
                    <td>A. Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Create a tag policy that requires the<br>CostCenter tag to be values from a known list of cost centers for all EC2 instances. Attach the policy to the OU. Update the script to scan<br>the tag keys and tag values. Modify the script to update noncompliant resources with a default approved tag value for the CostCenter tag<br>key.<br>B. Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Attach the policy to the OU. Update the<br>script to scan the tag keys and tag values and notify the administrators when the tag values are not valid.<br>C. Create an SCP that prevents the creation of EC2 instances without the CostCenter tag key. Attach the policy to the OU. Create an IAM<br>permission boundary in the organization&#x27;s member accounts that restricts the CostCenter tag values to a list of valid cost centers.<br>D. Create a tag policy that requires the CostCenter tag to be values from a known list of cost centers for all EC2 instances. Attach the<br>policy to the OU. Congure an AWS Lambda function that adds an empty CostCenter tag key to an EC2 instance. Create an Amazon<br>EventBridge rule that matches events to the RunInstances API action with the Lambda function as the target.</td>
                    <td>一家公司使用AWS Organizations中的组织来管理其500个AWS账户。该组织已启用所有功能。这些AWS账户位于单个OU中。开发人员需要为组织成员账户中的所有资源使用CostCenter标签键。一些团队没有使用CostCenter标签键来标记他们的Amazon EC2实例。<br>云团队编写了一个脚本，扫描组织成员账户中的所有EC2实例。如果EC2实例没有CostCenter标签键，脚本将通知AWS账户管理员。为了避免此通知，一些开发人员使用CostCenter标签键，但在标签值中使用任意字符串。<br>云团队需要确保组织中的所有EC2实例都使用带有适当成本中心值的CostCenter标签键。</td>
                    <td>选项A：创建SCP防止创建没有CostCenter标签键的EC2实例，创建标签策略要求CostCenter标签值来自已知的成本中心列表，将策略附加到OU，更新脚本扫描标签键和值，修改脚本为不合规资源更新默认批准的标签值。这个方案综合使用了SCP和标签策略，既防止了创建不合规实例，又确保了标签值的有效性，还提供了自动修复机制。<br><br>选项B：创建SCP防止创建没有CostCenter标签键的EC2实例，附加到OU，更新脚本扫描标签键和值并在标签值无效时通知管理员。这个方案只使用了SCP，虽然能防止创建没有标签的实例，但无法强制标签值的有效性，仍然依赖通知机制。<br><br>选项C：创建SCP防止创建没有CostCenter标签键的EC2实例，附加到OU，在成员账户中创建IAM权限边界限制CostCenter标签值为有效成本中心列表。这个方案使用SCP和IAM权限边界，但IAM权限边界主要用于限制用户权限，不是标签值验证的最佳实践。<br><br>选项D：创建标签策略要求CostCenter标签值来自已知成本中心列表，附加到OU，配置Lambda函数为EC2实例添加空的CostCenter标签键，创建EventBridge规则匹配RunInstances API操作。这个方案只使用标签策略，无法防止创建不合规实例，且添加空标签值不能解决问题。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>327</td>
                    <td>A DevOps engineer uses a pipeline in AWS CodePipeline. The pipeline has a build action and a deploy action for a single-page web application<br>that is delivered to an Amazon S3 bucket. Amazon CloudFront serves the web application. The build action creates an artifact for the web<br>application.<br>The DevOps engineer has created an AWS CloudFormation template that denes the S3 bucket and congures the S3 bucket to host the<br>application. The DevOps engineer has congured a CloudFormation deploy action before the S3 action. The CloudFormation deploy action<br>creates the S3 bucket. The DevOps engineer needs to congure the S3 deploy action to use the S3 bucket from the CloudFormation template.</td>
                    <td>A. Add an output named BucketName to the CloudFormation template. Set the output&#x27;s value to refer to the S3 bucket from the<br>CloudFormation template. Congure the output value to export to an AWS::SSM::Parameter resource named Stackvariables.<br>B. Add an output named BucketName to the CloudFormation template. Set the output&#x27;s value to refer to the S3 bucket from the<br>CloudFormation template. Set the CloudFormation action&#x27;s namespace to StackVariables in the pipeline.<br>C. Congure the output artifacts of the CloudFormation action in the pipeline to be an AWS Systems Manager Parameter Store parameter<br>named StackVariables. Name the artifact BucketName.<br>D. Congure the build artifact from the build action as the input to the CodePipeline S3 deploy action. Congure the deploy action to<br>deploy to the S3 bucket by using the StackVariables.BucketName variable.<br>E. Congure the build artifact from the build action and the AWS Systems Manager parameter as the inputs to the deploy action. Congure<br>the deploy action to deploy to the S3 bucket by using the StackVariables.BucketName variable.</td>
                    <td>一个DevOps工程师在AWS CodePipeline中使用管道。该管道有一个构建操作和一个部署操作，用于将单页Web应用程序交付到Amazon S3存储桶。Amazon CloudFront为Web应用程序提供服务。构建操作为Web应用程序创建一个构件。<br><br>DevOps工程师已经创建了一个AWS CloudFormation模板，该模板定义了S3存储桶并配置S3存储桶来托管应用程序。DevOps工程师在S3操作之前配置了CloudFormation部署操作。CloudFormation部署操作创建S3存储桶。DevOps工程师需要配置S3部署操作以使用CloudFormation模板中的S3存储桶。</td>
                    <td>A. 在CloudFormation模板中添加名为BucketName的输出，将输出值设置为引用CloudFormation模板中的S3存储桶，并配置输出值导出到名为Stackvariables的AWS::SSM::Parameter资源。这种方法过于复杂，不是CodePipeline中传递变量的标准方式，需要额外的SSM参数资源。<br><br>B. 在CloudFormation模板中添加名为BucketName的输出，将输出值设置为引用CloudFormation模板中的S3存储桶，在管道中将CloudFormation操作的命名空间设置为StackVariables。这是正确的方法，通过设置命名空间，可以在后续操作中使用StackVariables.BucketName变量引用输出值。<br><br>C. 将管道中CloudFormation操作的输出构件配置为名为StackVariables的AWS Systems Manager Parameter Store参数，将构件命名为BucketName。这种描述不准确，CloudFormation操作的输出不是通过Parameter Store传递的。<br><br>D. 将构建操作的构建构件配置为CodePipeline S3部署操作的输入，配置部署操作使用StackVariables.BucketName变量部署到S3存储桶。这是正确的，S3部署操作需要构建构件作为输入，并且可以使用命名空间变量引用存储桶名称。<br><br>E. 将构建操作的构建构件和AWS Systems Manager参数配置为部署操作的输入，配置部署操作使用StackVariables.BucketName变量部署到S3存储桶。这种方法不必要地引入了Systems Manager参数，增加了复杂性。</td>
                    <td>BD</td>
                </tr>
                <tr>
                    <td>328</td>
                    <td>A company used a lift and shift strategy to migrate a workload to AWS. The company has an Auto Scaling group of Amazon EC2 instances.<br>Each EC2 instance runs a web application, a database, and a Redis cache.<br>Users are experiencing large variations in the web application&#x27;s response times. Requests to the web application go to a single EC2 instance<br>that is under signicant load. The company wants to separate the application components to improve availability and performance.</td>
                    <td>A. Create a Network Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora<br>Serverless database. Create an Application Load Balancer and an Auto Scaling group for the Redis cache.<br>B. Create an Application Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora<br>database that has a Multi-AZ deployment. Create a Network Load Balancer and an Auto Scaling group in a single Availability Zone for the<br>Redis cache.<br>C. Create a Network Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora<br>Serverless database. Create an Amazon ElastiCache (Redis OSS) cluster for the cache. Create a target group that has a DNS target type<br>that contains the ElastiCache (Redis OSS) cluster hostname.<br>D. Create an Application Load Balancer and an Auto Scaling group for the web application. Migrate the database to an Amazon Aurora<br>database that has a Multi-AZ deployment. Create an Amazon ElastiCache (Redis OSS) cluster for the cache.</td>
                    <td>一家公司使用直接迁移策略将工作负载迁移到AWS。该公司有一个Amazon EC2实例的Auto Scaling组。每个EC2实例运行一个Web应用程序、一个数据库和一个Redis缓存。用户在Web应用程序的响应时间上遇到很大的变化。对Web应用程序的请求会发送到承受重大负载的单个EC2实例。该公司希望分离应用程序组件以提高可用性和性能。</td>
                    <td>A. 为Web应用程序创建Network Load Balancer和Auto Scaling组，将数据库迁移到Amazon Aurora Serverless数据库，为Redis缓存创建Application Load Balancer和Auto Scaling组。这个选项的问题在于为Web应用程序使用了Network Load Balancer（适用于TCP/UDP流量），而Web应用程序通常需要Application Load Balancer（适用于HTTP/HTTPS）。另外，为Redis缓存创建负载均衡器和Auto Scaling组是不合适的，应该使用托管的ElastiCache服务。<br><br>B. 为Web应用程序创建Application Load Balancer和Auto Scaling组，将数据库迁移到具有Multi-AZ部署的Amazon Aurora数据库，为Redis缓存在单个可用区创建Network Load Balancer和Auto Scaling组。虽然Web应用程序部分配置正确，但为Redis缓存创建负载均衡器和仅在单个AZ部署违背了高可用性原则，且不如使用托管ElastiCache服务。<br><br>C. 为Web应用程序创建Network Load Balancer和Auto Scaling组，将数据库迁移到Amazon Aurora Serverless数据库，为缓存创建Amazon ElastiCache (Redis OSS)集群，创建包含ElastiCache集群主机名的DNS目标类型目标组。这个选项为Web应用程序错误地使用了Network Load Balancer，且创建DNS目标类型目标组的配置过于复杂且不必要。<br><br>D. 为Web应用程序创建Application Load Balancer和Auto Scaling组，将数据库迁移到具有Multi-AZ部署的Amazon Aurora数据库，为缓存创建Amazon ElastiCache (Redis OSS)集群。这个选项正确地为Web应用程序使用了Application Load Balancer，使用了高可用的Aurora Multi-AZ数据库，并使用了托管的ElastiCache服务来替代自建Redis缓存。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>329</td>
                    <td>A company is using AWS Organizations and wants to implement a governance strategy with the following requirements:<br>• AWS resource access is restricted to the same two Regions for all accounts.<br>• AWS services are limited to a specic group of authorized services for all accounts.<br>• Authentication is provided by Active Directory.<br>• Access permissions are organized by job function and are identical in each account.</td>
                    <td>A. Establish an organizational unit (OU) with group policies in the management account to restrict Regions and authorized services. Use<br>AWS CloudFormation StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity<br>provider authentication in each account.<br>B. Establish a permission boundary in the management account to restrict Regions and authorized services. Use AWS CloudFormation<br>StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication<br>in each account.<br>C. Establish a service control policy in the management account to restrict Regions and authorized services. Use AWS Resource Access<br>Manager (AWS RAM) to share management account roles with permissions for each job function, including AWS IAM Identity Center for<br>authentication in each account.<br>D. Establish a service control policy in the management account to restrict Regions and authorized services. Use AWS CloudFormation<br>StackSets to provision roles with permissions for each job function, including an IAM trust policy for IAM identity provider authentication<br>in each account.</td>
                    <td>一家公司正在使用AWS Organizations，并希望实施一个治理策略，具有以下要求：<br>• 对于所有账户，AWS资源访问仅限于相同的两个区域。<br>• 对于所有账户，AWS服务仅限于特定的一组授权服务。<br>• 身份验证由Active Directory提供。<br>• 访问权限按工作职能组织，并且在每个账户中都相同。</td>
                    <td>选项A：建议在管理账户中建立带有组策略的组织单元(OU)来限制区域和授权服务。这里的&quot;组策略&quot;概念不正确，AWS Organizations使用的是服务控制策略(SCP)而不是组策略。使用CloudFormation StackSets配置角色是正确的方法，但第一部分的实现方式有误。<br><br>选项B：建议在管理账户中建立权限边界来限制区域和授权服务。权限边界主要用于限制单个IAM实体的最大权限，而不是用于跨账户的区域和服务限制。对于组织级别的限制，应该使用服务控制策略。CloudFormation StackSets部分是正确的。<br><br>选项C：正确地建议使用服务控制策略来限制区域和授权服务，这是AWS Organizations中实现组织级别控制的正确方法。但是建议使用AWS RAM共享管理账户角色是不合适的，因为IAM角色不能通过RAM共享。此外，提到使用IAM Identity Center进行身份验证，但题目要求使用Active Directory。<br><br>选项D：正确地建议使用服务控制策略在管理账户中限制区域和授权服务，这是AWS Organizations的标准做法。使用CloudFormation StackSets在每个账户中配置具有工作职能权限的角色是正确的方法，可以确保跨账户的一致性。IAM信任策略配合IAM身份提供商可以实现与Active Directory的集成。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>330</td>
                    <td>A company detects unusual login attempts in many of its AWS accounts. A DevOps engineer must implement a solution that sends a<br>notication to the company&#x27;s security team when multiple failed login attempts occur. The DevOps engineer has already created an Amazon<br>Simple Notication Service (Amazon SNS) topic and has subscribed the security team to the SNS topic.</td>
                    <td>A. Congure AWS CloudTrail to send management events to an Amazon CloudWatch Logs log group. Create a CloudWatch Logs metric<br>lter to match failed ConsoleLogin events. Create a CloudWatch alarm that is based on the metric lter. Congure an alarm action to send<br>messages to the SNS topic.<br>B. Congure AWS CloudTrail to send management events to an Amazon S3 bucket. Create an Amazon Athena query that returns a failure<br>if the query nds failed logins in the logs in the S3 bucket. Create an Amazon EventBridge rule to periodically run the query. Create a<br>second EventBridge rule to detect when the query fails and to send a message to the SNS topic.<br>C. Congure AWS CloudTrail to send data events to an Amazon CloudWatch Logs log group. Create a CloudWatch logs metric lter to<br>match failed ConsoleLogin events. Create a CloudWatch alarm that is based on the metric lter. Congure an alarm action to send<br>messages to the SNS topic.<br>D. Congure AWS CloudTrail to send data events to an Amazon S3 bucket. Congure an Amazon S3 event notication for the<br>s3:ObjectCreated event type. Filter the event type by ConsoleLogin failed events. Congure the event notication to forward to the SNS<br>topic.</td>
                    <td>一家公司在其多个AWS账户中检测到异常登录尝试。DevOps工程师必须实施一个解决方案，当发生多次登录失败尝试时向公司安全团队发送通知。DevOps工程师已经创建了Amazon Simple Notification Service (Amazon SNS)主题，并让安全团队订阅了该SNS主题。</td>
                    <td>选项A：配置AWS CloudTrail将管理事件发送到Amazon CloudWatch Logs日志组。创建CloudWatch Logs指标过滤器来匹配失败的ConsoleLogin事件。基于指标过滤器创建CloudWatch告警。配置告警操作向SNS主题发送消息。这个方案是正确的，因为登录事件属于管理事件，CloudWatch Logs可以实时监控和过滤日志，CloudWatch告警可以基于指标触发并发送通知到SNS。<br><br>选项B：配置AWS CloudTrail将管理事件发送到Amazon S3存储桶。创建Amazon Athena查询，如果在S3存储桶的日志中发现登录失败则返回失败。创建Amazon EventBridge规则定期运行查询。创建第二个EventBridge规则检测查询失败并向SNS主题发送消息。这个方案过于复杂，且不是实时的，需要定期查询，响应延迟较大。<br><br>选项C：配置AWS CloudTrail将数据事件发送到Amazon CloudWatch Logs日志组。创建CloudWatch logs指标过滤器来匹配失败的ConsoleLogin事件。基于指标过滤器创建CloudWatch告警。配置告警操作向SNS主题发送消息。这个方案错误，因为登录事件是管理事件而不是数据事件，数据事件主要记录对S3对象或Lambda函数的操作。<br><br>选项D：配置AWS CloudTrail将数据事件发送到Amazon S3存储桶。为s3:ObjectCreated事件类型配置Amazon S3事件通知。通过ConsoleLogin失败事件过滤事件类型。配置事件通知转发到SNS主题。这个方案完全错误，混淆了S3对象创建事件和CloudTrail登录事件，且登录事件不是数据事件。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>331</td>
                    <td>A company has deployed a new REST API by using Amazon API Gateway. The company uses the API to access condential data. The API must<br>be accessed from only specic VPCs in the company.</td>
                    <td>A. Create and attach a resource policy to the API Gateway API. Congure the resource policy to allow only the specic VPC IDs.<br>B. Add a security group to the API Gateway API. Congure the inbound rules to allow only the specic VPC IP address ranges.<br>C. Create and attach an IAM role to the API Gateway API. Congure the IAM role to allow only the specic VPC IDs.<br>D. Add an ACL to the API Gateway API. Congure the outbound rules to allow only the specic VPC IP address ranges.</td>
                    <td>一家公司使用Amazon API Gateway部署了一个新的REST API。该公司使用这个API来访问机密数据。这个API必须只能从公司内的特定VPC中访问。</td>
                    <td>A. 为API Gateway API创建并附加资源策略。配置资源策略以仅允许特定的VPC ID。<br>这是正确的方法。API Gateway资源策略可以基于VPC或VPC端点来控制访问权限。通过在资源策略中指定特定的VPC ID或VPC端点，可以确保只有来自这些VPC的请求才能访问API。这是AWS推荐的限制API Gateway访问特定VPC的标准做法。<br><br>B. 为API Gateway API添加安全组。配置入站规则以仅允许特定的VPC IP地址范围。<br>这个选项是错误的。API Gateway是托管服务，不能直接附加安全组。安全组主要用于EC2实例、RDS等资源，而不是API Gateway。即使可以配置，通过IP地址范围限制也不如VPC级别的控制精确和安全。<br><br>C. 为API Gateway API创建并附加IAM角色。配置IAM角色以仅允许特定的VPC ID。<br>这个选项是错误的。IAM角色主要用于身份验证和授权，而不是基于网络位置的访问控制。IAM角色无法直接限制来自特定VPC的访问，它更多关注的是&quot;谁&quot;可以访问，而不是&quot;从哪里&quot;访问。<br><br>D. 为API Gateway API添加ACL。配置出站规则以仅允许特定的VPC IP地址范围。<br>这个选项是错误的。首先，API Gateway不支持直接添加网络ACL。其次，出站规则控制的是从API Gateway出去的流量，而题目要求的是控制进入API的流量。网络ACL通常用于子网级别的访问控制，不适用于API Gateway。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>332</td>
                    <td>A company runs a website by using an Amazon Elastic Container Service (Amazon ECS) service that is connected to an Application Load<br>Balancer (ALB). The service was in a steady state with tasks responding to requests successfully.<br>A DevOps engineer updated the task denition with a new container image and deployed the new task denition to the service. The DevOps<br>engineer noticed that the service is frequently stopping and starting new tasks because the ALB healtth checks are failing.</td>
                    <td>A. Ensure that a security group associated with the service allows trac from the AL<br>B. Increase the ALB health check grace period for the service.<br>C. Increase the service minimum healthy percent setting.<br>D. Decrease the ALB health check interval.</td>
                    <td>一家公司使用连接到应用负载均衡器（ALB）的Amazon弹性容器服务（Amazon ECS）服务来运行网站。该服务之前处于稳定状态，任务能够成功响应请求。一名DevOps工程师使用新的容器镜像更新了任务定义，并将新的任务定义部署到服务中。DevOps工程师注意到由于ALB健康检查失败，服务频繁地停止和启动新任务。</td>
                    <td>A. 确保与服务关联的安全组允许来自ALB的流量 - 这是最可能的原因。当更新容器镜像后，如果安全组配置不正确，ALB无法访问ECS任务的健康检查端点，导致健康检查失败。新镜像可能使用了不同的端口或网络配置，需要确保安全组规则允许ALB到ECS任务的通信。<br><br>B. 增加服务的ALB健康检查宽限期 - 虽然这可能暂时缓解问题，但如果根本问题是网络连接（如安全组阻止流量），增加宽限期并不能解决实际问题，只是延迟发现失败的时间。<br><br>C. 增加服务的最小健康百分比设置 - 这个设置控制部署期间保持运行的任务百分比，但不会解决健康检查失败的根本原因。如果健康检查本身失败，调整这个参数不会有帮助。<br><br>D. 减少ALB健康检查间隔 - 这实际上会使问题更严重，因为会更频繁地进行失败的健康检查，导致任务更快地被标记为不健康并被替换。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>333</td>
                    <td>A company that uses electronic patient health records runs a eet of Amazon EC2 instances with an Amazon Linux operating system. The<br>company must continuously ensure that the EC2 instances are running operating system patches and application patches that are in<br>compliance with current privacy regulations. The company uses a custom repository to store application patches.<br>A DevOps engineer needs to automate the deployment of operating system patches and application patches. The DevOps engineer wants to<br>use both the default operating system patch repository and the custom patch repository.</td>
                    <td>A. Use AWS Systems Manager to create a new custom patch baseline that includes the default operating system repository and the<br>custom repository. Run the AWS-RunPatchBaseline document by using the Run command to verify and install patches. Use the<br>BaselineOverride API to congure the new custom patch baseline.<br>B. Use AWS Direct Connect to integrate the custom repository with the EC2 instances. Use Amazon EventBridge events to deploy the<br>patches.<br>C. Use the yum-cong-manager command to add the custom repository to the /etc/yum.repos.d conguration. Run the yum-cong-<br>manager-enable command to activate the new repository.<br>D. Use AWS Systems Manager to create a patch baseline for the default operating system repository and a second patch baseline for the<br>custom repository. Run the AWS-RunPatchBaseline document by using the Run command to verify and install patches. Use the<br>BaselineOverride API to congure the default patch baseline and the custom patch baseline.</td>
                    <td>一家使用电子患者健康记录的公司运行着一组使用Amazon Linux操作系统的Amazon EC2实例。该公司必须持续确保EC2实例运行的操作系统补丁和应用程序补丁符合当前隐私法规的合规要求。该公司使用自定义存储库来存储应用程序补丁。<br>DevOps工程师需要自动化部署操作系统补丁和应用程序补丁。DevOps工程师希望同时使用默认的操作系统补丁存储库和自定义补丁存储库。</td>
                    <td>选项A：使用AWS Systems Manager创建一个新的自定义补丁基线，包含默认操作系统存储库和自定义存储库。通过Run命令运行AWS-RunPatchBaseline文档来验证和安装补丁。使用BaselineOverride API配置新的自定义补丁基线。这是正确的方法，因为Systems Manager Patch Manager支持创建包含多个存储库的单一补丁基线，可以同时管理操作系统和应用程序补丁，BaselineOverride API允许灵活配置补丁基线。<br><br>选项B：使用AWS Direct Connect将自定义存储库与EC2实例集成，使用Amazon EventBridge事件部署补丁。这个方案过于复杂且不必要，Direct Connect主要用于网络连接，不是补丁管理的最佳实践，EventBridge也不是专门的补丁管理工具。<br><br>选项C：使用yum-config-manager命令将自定义存储库添加到/etc/yum.repos.d配置，运行yum-config-manager-enable命令激活新存储库。这是手动配置方法，不符合自动化要求，且无法提供集中管理和合规性报告功能。<br><br>选项D：使用AWS Systems Manager为默认操作系统存储库创建一个补丁基线，为自定义存储库创建第二个补丁基线。这种方法创建了两个独立的补丁基线，增加了管理复杂性，不如单一补丁基线方案高效。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>334</td>
                    <td>A company use an organization in AWS Organizations to manage multiple AWS accounts. The company has enabled all features enabled for<br>the organization. The company congured the organization as a hierarchy of OUs under the root OU. The company recently registered all its<br>OUs and enrolled all its AWS accounts in AWS Control Tower.<br>The company needs to customize the AWS Control Tower managed AWS Cong conguration recorder in each of the company&#x27;s AWS accounts.<br>The company needs to apply the customizations to both the existing AWS accounts and to any new AWS accounts that the company enrolls in<br>AWS Control Tower in the future.</td>
                    <td>A. Create a new AWS account. Create an AWS Lambda function in the new account to apply the customizations to the AWS Cong<br>conguration recorder in each AWS account in the organization.<br>B. Create a new AWS account as an AWS Cong delegated administrator. Create an AWS Lambda function in the delegated administrator<br>account to apply the customizations to the AWS Cong conguration recorder in the delegated administrator account.<br>C. Congure an Amazon EventBridge rule in the AWS Control Tower management account to invoke an AWS Lambda function when the<br>Organizations OU is registered or reregistered. Re-register the root Organizations OU.<br>D. Congure the AWSControlTowerExecution IAM role in each AWS account in the organization to be assumable by an AWS Lambda<br>function. Congure the Lambda function to assume the AWSControlTowerExecution IAM role.<br>E. Create an IAM role in the AWS Control Tower management account that an AWS Lambda function can assume. Grant the IAM role<br>permission to assume the AWSControlTowerExecution IAM role in any account in the organization. Congure the Lambda function to use<br>the new IAM role.<br>F. Congure an Amazon EventBridge rule in the AWS Control Tower management account to invoke an AWS Lambda function when an AWS<br>account is updated or enrolled in AWS Control Tower or when the landing zone is updated. Re-register each Organizations OU in the<br>organization.</td>
                    <td>一家公司使用AWS Organizations中的组织来管理多个AWS账户。该公司已为组织启用了所有功能。公司将组织配置为根OU下的OU层次结构。公司最近注册了所有OU，并将所有AWS账户注册到AWS Control Tower中。<br><br>公司需要自定义每个AWS账户中由AWS Control Tower管理的AWS Config配置记录器。公司需要将这些自定义应用到现有的AWS账户以及将来注册到AWS Control Tower的任何新AWS账户。</td>
                    <td>A. 创建新AWS账户，在新账户中创建Lambda函数来对组织中每个AWS账户的AWS Config配置记录器应用自定义。这种方法缺乏适当的权限管理和自动化触发机制，无法有效管理跨账户操作，也不能自动处理新账户的注册。<br><br>B. 创建新AWS账户作为AWS Config委托管理员，在委托管理员账户中创建Lambda函数来对委托管理员账户的AWS Config配置记录器应用自定义。这个选项只处理委托管理员账户本身，不能解决对所有账户应用自定义的需求，范围过于局限。<br><br>C. 在AWS Control Tower管理账户中配置Amazon EventBridge规则，当Organizations OU被注册或重新注册时调用Lambda函数，并重新注册根Organizations OU。这提供了自动化触发机制，能够响应OU级别的变化，确保在OU注册时应用自定义配置。<br><br>D. 配置组织中每个AWS账户的AWSControlTowerExecution IAM角色，使其可被Lambda函数承担，并配置Lambda函数承担该角色。这建立了必要的跨账户权限机制，允许Lambda函数在每个账户中执行操作，是实现跨账户自定义的关键组件。<br><br>E. 在AWS Control Tower管理账户中创建IAM角色供Lambda函数承担，授予该角色权限来承担组织中任何账户的AWSControlTowerExecution角色，并配置Lambda函数使用新角色。这提供了集中化的权限管理方式，通过角色链实现跨账户访问，是安全且可扩展的权限管理方案。<br><br>F. 在AWS Control Tower管理账户中配置Amazon EventBridge规则，当AWS账户更新、注册到Control Tower或着陆区更新时调用Lambda函数，并重新注册组织中的每个OU。这提供了更全面的事件触发机制，能够响应账户级别和着陆区级别的变化，确保新账户和现有账户都能应用自定义。</td>
                    <td>CE</td>
                </tr>
                <tr>
                    <td>335</td>
                    <td>A company runs an application in an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2<br>instances run Docker containers that make requests to a MySQL database that runs on separate EC2 instances.<br>A DevOps engineer needs to update the application to use a serverless architecture.</td>
                    <td>A. Replace the containers that run on EC2 instances and the ALB with AWS Lambda functions. Replace the MySQL database with an<br>Amazon Aurora Serverless v2 database that is compatible with MySQL.<br>B. Replace the containers that run on EC2 instances with AWS Fargate. Replace the MySQL database with an Amazon Aurora Serverless<br>v2 database that is compatible with MySQL.<br>C. Replace the containers that run on EC2 instances and the ALB with AWS Lambda functions. Replace the MySQL database with Amazon<br>DynamoDB tables.<br>D. Replace the containers that run on EC2 instances with AWS Fargate. Replace the MySQL database with Amazon DynamoDB tables.</td>
                    <td>一家公司在应用负载均衡器(ALB)后面的Amazon EC2实例自动扩展组中运行应用程序。EC2实例运行Docker容器，这些容器向运行在独立EC2实例上的MySQL数据库发出请求。DevOps工程师需要将应用程序更新为使用无服务器架构。</td>
                    <td>A. 用AWS Lambda函数替换运行在EC2实例上的容器和ALB。用与MySQL兼容的Amazon Aurora Serverless v2数据库替换MySQL数据库。<br>这个选项将容器直接迁移到Lambda函数，但Lambda有执行时间限制(最长15分钟)和内存限制，可能不适合所有容器化应用。同时移除ALB可能会影响负载分发和路由功能。Aurora Serverless v2是很好的无服务器数据库选择。<br><br>B. 用AWS Fargate替换运行在EC2实例上的容器。用与MySQL兼容的Amazon Aurora Serverless v2数据库替换MySQL数据库。<br>这个选项使用Fargate运行容器，这是真正的无服务器容器服务，无需管理底层EC2实例。保留ALB用于负载均衡。Aurora Serverless v2提供无服务器数据库功能，与MySQL兼容，迁移成本较低。<br><br>C. 用AWS Lambda函数替换运行在EC2实例上的容器和ALB。用Amazon DynamoDB表替换MySQL数据库。<br>Lambda的限制可能不适合复杂的容器化应用，且从MySQL迁移到DynamoDB需要重新设计数据模型和应用逻辑，迁移复杂度很高。<br><br>D. 用AWS Fargate替换运行在EC2实例上的容器。用Amazon DynamoDB表替换MySQL数据库。<br>虽然Fargate是好选择，但从MySQL迁移到DynamoDB需要大量的应用程序重构和数据模型重新设计。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>336</td>
                    <td>A company uses an organization in AWS Organizations to manage 10 AWS accounts. All features are enabled, and trusted access for AWS<br>CloudFormation is enabled.<br>A DevOps engineer needs to use CloudFormation to deploy an IAM role to the Organizations management account and all member accounts<br>in the organization.</td>
                    <td>A. Create a CloudFormation StackSet that has service-managed permissions. Set the root OU as a deployment target.<br>B. Create a CloudFormation StackSet that has service-managed permissions. Set the root OU as a deployment target. Deploy a separate<br>CloudFormation stack in the Organizations management account.<br>C. Create a CloudFormation StackSet that has self-managed permissions. Set the root OU as a deployment target.<br>D. Create a CloudFormation StackSet that has self-managed permissions. Set the root OU as a deployment target. Deploy a separate<br>CloudFormation stack in the Organizations management account.</td>
                    <td>一家公司使用AWS Organizations中的组织来管理10个AWS账户。所有功能都已启用，并且已启用AWS CloudFormation的可信访问。<br>DevOps工程师需要使用CloudFormation将IAM角色部署到Organizations管理账户和组织中的所有成员账户。</td>
                    <td>A. 创建具有服务管理权限的CloudFormation StackSet，将根OU设置为部署目标。这个选项是正确的，因为服务管理权限允许StackSet自动管理跨账户部署所需的IAM角色和权限，并且当将根OU设置为目标时，会自动包含管理账户和所有成员账户。<br><br>B. 创建具有服务管理权限的CloudFormation StackSet，将根OU设置为部署目标，并在Organizations管理账户中部署单独的CloudFormation堆栈。这个选项不必要，因为当使用服务管理权限并将根OU作为目标时，管理账户已经自动包含在内，无需额外的单独堆栈。<br><br>C. 创建具有自管理权限的CloudFormation StackSet，将根OU设置为部署目标。这个选项不正确，因为自管理权限需要手动在每个目标账户中预先创建IAM角色，这会增加复杂性，而且题目中已经启用了CloudFormation的可信访问，应该使用服务管理权限。<br><br>D. 创建具有自管理权限的CloudFormation StackSet，将根OU设置为部署目标，并在Organizations管理账户中部署单独的CloudFormation堆栈。这个选项结合了C选项的问题，既使用了不必要的自管理权限，又添加了不必要的单独堆栈部署。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>337</td>
                    <td>A company runs an application that stores artifacts in an Amazon S3 bucket. The application has a large user base. The application writes a<br>high volume of objects to the S3 bucket. The company has enabled event notications for the S3 bucket.<br>When the application writes an object to the S3 bucket, several processing tasks need to be performed simultaneously. The company&#x27;s DevOps<br>team needs to create an AWS Step Functions workow to orchestrate the processing tasks.</td>
                    <td>A. Create a Standard workow that contains a parallel state that denes the processing tasks. Create an Asynchronous Express workow<br>that contains a parallel state that denes the processing tasks.<br>B. Create a Synchronous Express workow that contains a map state that denes the processing tasks.<br>C. Create an Amazon EventBridge rule to match when a new S3 object is created. Congure the EventBridge rule to invoke an AWS<br>Lambda function. Congure the Lambda function to start the processing workow.<br>D. Create an Amazon EventBridge rule to match when a new S3 object is created. Congure the EventBridge rule to start the processing<br>workow.</td>
                    <td>一家公司运行一个应用程序，该应用程序将工件存储在Amazon S3存储桶中。该应用程序拥有大量用户基础。应用程序向S3存储桶写入大量对象。公司已为S3存储桶启用了事件通知。<br>当应用程序向S3存储桶写入对象时，需要同时执行多个处理任务。公司的DevOps团队需要创建一个AWS Step Functions工作流来编排这些处理任务。</td>
                    <td>选项A：创建一个包含并行状态定义处理任务的标准工作流。创建一个包含并行状态定义处理任务的异步Express工作流。<br>这个选项提到了两种工作流类型。标准工作流适合长时间运行的任务，支持并行状态，可以处理多个同时执行的任务。异步Express工作流适合高频率、短时间的任务。由于题目提到需要同时执行多个处理任务，并行状态是合适的选择。这个选项在技术上是可行的。<br><br>选项B：创建一个包含映射状态定义处理任务的同步Express工作流。<br>映射状态主要用于对数组中的每个项目执行相同的处理步骤，而不是同时执行不同的处理任务。同步Express工作流需要等待响应，可能不适合高频率的S3事件处理。这个选项不太适合题目描述的需求。<br><br>选项C：创建Amazon EventBridge规则来匹配新S3对象创建时的事件。配置EventBridge规则调用AWS Lambda函数。配置Lambda函数启动处理工作流。<br>这是一个完整的事件驱动架构方案。通过EventBridge捕获S3事件，然后通过Lambda函数启动Step Functions工作流。这种方式提供了更好的解耦和错误处理能力，是一个很好的实践方案。<br><br>选项D：创建Amazon EventBridge规则来匹配新S3对象创建时的事件。配置EventBridge规则直接启动处理工作流。<br>这个选项提供了最直接的方式，EventBridge可以直接启动Step Functions工作流，减少了中间层，提高了效率。对于简单的工作流启动需求，这是一个优雅的解决方案。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>338</td>
                    <td>A DevOps team supports an application that runs in an Amazon Elastic Container Service (Amazon ECS) cluster behind an Application Load<br>Balancer (ALB). Currently, the DevOps team uses AWS CodeDeploy to deploy the application by using a blue/green all-at-once strategy.<br>Recently, the DevOps team had to roll back a deployment when a new version of the application dramatically increased response times for<br>requests.<br>The DevOps team needs use to a deployment strategy that will allow the team to monitor a new version of the application before the team<br>shifts all trac to the new version. If a new version of the application increases response times, the deployment should be rolled back as<br>quickly as possible.</td>
                    <td>A. Modify the CodeDeploy deployment to use the CodeDeployDefault.ECSCanary10Percent5Minutes conguration.<br>B. Set the alarm to activate if the<br>metric is higher than the desired value. Associate the alarm with the CodeDeploy deployment group. Modify the deployment group to roll<br>back when alarm thresholds are met.<br>C. Create an Amazon CloudWatch alarm to monitor the UnHealthyHostCount metric for the AL<br>D. Create an Amazon CloudWatch alarm to monitor the TargetResponseTime metric for the AL<br>E. Create an Amazon CloudWatch alarm to monitor the TargetConnectionErrorCount metric for the AL</td>
                    <td>一个DevOps团队支持运行在Amazon弹性容器服务(Amazon ECS)集群中的应用程序，该集群位于应用程序负载均衡器(ALB)后面。目前，DevOps团队使用AWS CodeDeploy通过蓝/绿一次性部署策略来部署应用程序。最近，当应用程序的新版本显著增加了请求的响应时间时，DevOps团队不得不回滚部署。DevOps团队需要使用一种部署策略，允许团队在将所有流量转移到新版本之前监控应用程序的新版本。如果应用程序的新版本增加了响应时间，部署应该尽快回滚。</td>
                    <td>A. 修改CodeDeploy部署以使用CodeDeployDefault.ECSCanary10Percent5Minutes配置 - 这是正确的选择。金丝雀部署策略允许先将10%的流量路由到新版本，在5分钟内监控其性能，然后再决定是否继续部署。这完全符合题目要求的渐进式部署和监控需求。<br><br>B. 设置告警在指标高于期望值时激活，将告警与CodeDeploy部署组关联，修改部署组在满足告警阈值时回滚 - 这个选项描述不完整，没有指定具体的监控指标，而且语句似乎被截断了。<br><br>C. 创建Amazon CloudWatch告警监控ALB的UnHealthyHostCount指标 - 虽然监控不健康主机数量有用，但这个指标主要反映主机健康状况，不能直接监控响应时间问题，不是最佳选择。<br><br>D. 创建Amazon CloudWatch告警监控ALB的TargetResponseTime指标 - 这是正确的选择。响应时间指标直接对应题目中提到的问题（响应时间显著增加），是监控应用程序性能的关键指标。<br><br>E. 创建Amazon CloudWatch告警监控ALB的TargetConnectionErrorCount指标 - 连接错误计数主要监控连接问题，而不是响应时间问题，不符合题目的具体需求。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>339</td>
                    <td>A security team must record the conguration of AWS resources, detect issues, and send notications for ndings. The main workload in the<br>AWS account consists of an Amazon EC2 Auto Scaling group that scales in and out several times during the day.<br>The team wants to be notied within 2 days if any Amazon EC2 security group allows trac on port 22 for 0.0.0.0/0. The team also needs a<br>snapshot of the conguration of the AWS resources to be taken routinely.<br>The security team has already created and subscribed to an Amazon Simple Notication Service (Amazon SNS) topic.</td>
                    <td>A. Congure AWS Cong to use periodic recording for the AWS account. Deploy the vpc-sg-port-restriction-check AWS Cong managed rule.<br>Congure AWS Cong to use the SNS topic as the target for notications.<br>B. Congure AWS Cong to use conguration change recording for the AWS account. Deploy the vpc-sg-open-only-to-authorized-ports AWS<br>Cong managed rule. Congure AWS Cong to use the SNS topic as the target for notications.<br>C. Congure AWS Cong to use conguration change recording for the AWS account. Deploy the ssh-restricted AWS Cong managed rule.<br>Congure AWS Cong to use the SNS topic as the target for notications.<br>D. Create an AWS Lambda function to evaluate security groups and publish a message to the SNS topic. Use an Amazon EventBridge rule<br>to schedule the Lambda function to run once a day.</td>
                    <td>一个安全团队必须记录AWS资源的配置，检测问题，并发送发现结果的通知。AWS账户中的主要工作负载包含一个Amazon EC2 Auto Scaling组，该组在一天中会多次进行扩展和收缩。<br>团队希望在2天内收到通知，如果任何Amazon EC2安全组允许来自0.0.0.0/0的22端口流量。团队还需要定期获取AWS资源配置的快照。<br>安全团队已经创建并订阅了一个Amazon Simple Notification Service (Amazon SNS)主题。</td>
                    <td>选项A：配置AWS Config使用周期性记录，部署vpc-sg-port-restriction-check托管规则，配置AWS Config使用SNS主题作为通知目标。这个选项使用周期性记录可以定期获取配置快照，vpc-sg-port-restriction-check规则专门检查安全组端口限制，能够检测22端口对0.0.0.0/0开放的情况，完全符合需求。<br><br>选项B：配置AWS Config使用配置变更记录，部署vpc-sg-open-only-to-authorized-ports托管规则。配置变更记录只在资源配置发生变化时触发，无法满足定期快照的需求。虽然规则可以检测端口开放情况，但不是最适合的规则类型。<br><br>选项C：配置AWS Config使用配置变更记录，部署ssh-restricted托管规则。同样使用配置变更记录无法满足定期快照需求，ssh-restricted规则虽然相关但不如vpc-sg-port-restriction-check规则精确。<br><br>选项D：创建Lambda函数评估安全组并发布消息到SNS，使用EventBridge规则调度Lambda每天运行一次。这个方案需要自定义开发，复杂度高，且无法提供AWS Config的完整配置管理功能。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>340</td>
                    <td>A company has proprietary data available by using an Amazon CloudFront distribution. The company needs to ensure that the distribution is<br>accessible by only users from the corporate oce that have a known set of IP address ranges. An AWS WAF web ACL is associated with the<br>distribution and has a default action set to Count.</td>
                    <td>A. Create a new regex pattern set. Add the regex pattern set to a new rule group. Create a new web ACL that has a default action set to<br>Block. Associate the web ACL with the CloudFront distribution. Add a rule that allows trac based on the new rule group.<br>B. Create an AWS WAF IP address set that matches the corporate oce IP address range. Create a new web ACL that has a default action<br>set to Allow. Associate the web ACL with the CloudFront distribution. Add a rule that allows trac from the IP address set.<br>C. Create a new regex pattern set. Add the regex pattern set to a new rule group. Set the default action on the existing web ACL to Allow.<br>Add a rule that has priority 0 that allows trac based on the regex pattern set.<br>D. Create a WAF IP address set that matches the corporate oce IP address range. Set the default action on the existing web ACL to<br>Block. Add a rule that has priority 0 that allows trac from the IP address set.</td>
                    <td>一家公司通过Amazon CloudFront分发拥有专有数据。公司需要确保该分发只能被来自企业办公室的用户访问，这些用户具有已知的IP地址范围集合。AWS WAF web ACL与该分发关联，并且默认操作设置为Count（计数）。</td>
                    <td>A. 创建新的正则表达式模式集，将其添加到新规则组中，创建默认操作为Block的新web ACL，与CloudFront分发关联，添加基于新规则组允许流量的规则。这个选项错误，因为正则表达式模式集用于匹配字符串模式，不适用于IP地址匹配。而且创建新的web ACL是不必要的复杂操作。<br><br>B. 创建匹配企业办公室IP地址范围的AWS WAF IP地址集，创建默认操作为Allow的新web ACL，与CloudFront分发关联，添加允许来自IP地址集流量的规则。这个选项的问题在于默认操作设置为Allow，这意味着所有流量都会被允许，包括不在IP地址集中的流量，这与安全要求相矛盾。<br><br>C. 创建新的正则表达式模式集，添加到新规则组，将现有web ACL的默认操作设置为Allow，添加优先级为0的规则允许基于正则表达式模式集的流量。这个选项同样错误，因为使用正则表达式模式集来处理IP地址不合适，而且默认操作为Allow会允许所有流量。<br><br>D. 创建匹配企业办公室IP地址范围的WAF IP地址集，将现有web ACL的默认操作设置为Block，添加优先级为0的规则允许来自IP地址集的流量。这个选项正确，使用IP地址集来匹配特定IP范围，默认阻止所有流量，只允许来自指定IP地址集的流量通过。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>341</td>
                    <td>A company runs several applications in the same AWS account. The applications send logs to Amazon CloudWatch.<br>A data analytics team needs to collect performance metrics and custom metrics from the applications. The analytics team needs to transform<br>the metrics data before storing the data in an Amazon S3 bucket. The analytics team must automatically collect any new metrics that are<br>added to the CloudWatch namespace.</td>
                    <td>A. Congure a CloudWatch metric stream to include metrics from the application and the CloudWatch namespace. Congure the metric<br>stream to deliver the metrics to an Amazon Data Firehose delivery stream. Congure the Firehose delivery stream to invoke an AWS<br>Lambda function to transform the data. Congure the delivery stream to send the transformed data to the S3 bucket.<br>B. Congure a CloudWatch metrics stream to include all the metrics and to deliver the metrics to an Amazon Data Firehose delivery<br>stream. Congure the Firehose delivery stream to invoke an AWS Lambda function to transform the data. Congure the delivery stream to<br>send the transformed data to the S3 bucket.<br>C. Congure metric lters for the CloudWatch logs to create custom metrics. Congure a CloudWatch metric stream to deliver the<br>application metrics to the S3 bucket.<br>D. Congure subscription lters on the application log groups to target an Amazon Data Firehose delivery stream. Congure the Firehose<br>delivery stream to invoke an AWS Lambda function to transform the data. Congure the delivery stream to send the transformed data to<br>the S3 bucket.</td>
                    <td>一家公司在同一个AWS账户中运行多个应用程序。这些应用程序将日志发送到Amazon CloudWatch。<br>数据分析团队需要从应用程序中收集性能指标和自定义指标。分析团队需要在将数据存储到Amazon S3存储桶之前转换指标数据。分析团队必须自动收集添加到CloudWatch命名空间的任何新指标。</td>
                    <td>选项A：配置CloudWatch指标流以包含来自应用程序和CloudWatch命名空间的指标。配置指标流将指标传递到Amazon Data Firehose传输流。配置Firehose传输流调用AWS Lambda函数来转换数据。配置传输流将转换后的数据发送到S3存储桶。这个选项的问题在于它只包含特定应用程序和命名空间的指标，无法自动收集新添加到CloudWatch命名空间的所有新指标，不满足题目要求的自动收集任何新指标的需求。<br><br>选项B：配置CloudWatch指标流以包含所有指标并将指标传递到Amazon Data Firehose传输流。配置Firehose传输流调用AWS Lambda函数来转换数据。配置传输流将转换后的数据发送到S3存储桶。这个选项通过包含&quot;所有指标&quot;来满足自动收集任何新指标的要求，同时提供了完整的数据转换和存储流程，完全符合题目需求。<br><br>选项C：为CloudWatch日志配置指标过滤器以创建自定义指标。配置CloudWatch指标流将应用程序指标传递到S3存储桶。这个选项的问题是它混淆了日志和指标的概念，而且没有提供数据转换功能，不能满足题目要求的转换数据需求。<br><br>选项D：在应用程序日志组上配置订阅过滤器以目标Amazon Data Firehose传输流。配置Firehose传输流调用AWS Lambda函数来转换数据。配置传输流将转换后的数据发送到S3存储桶。这个选项处理的是日志而不是指标，不符合题目要求收集性能指标和自定义指标的需求。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>342</td>
                    <td>A company uses an HPC platform to run analysis jobs for data. The company uses AWS CodeBuild to create container images and store the<br>images on Amazon Elastic Container Registry (Amazon ECR). The images are then deployed on Amazon Elastic Kubernetes Service (Amazon<br>EKS).<br>To maintain compliance, the company needs to ensure that the images are signed before the images are deployed on Amazon EKS. The<br>signing keys must be rotated periodically and must be managed automatically. The company needs to track who generates the signatures.</td>
                    <td>A. Use CodeBuild to retrieve the image that was previously pushed to Amazon ECR. Use AWS Signer to sign the image. Use AWS CloudTrail<br>to track who generates the signatures.<br>B. Use AWS Lambda to retrieve the image that was previously pushed to Amazon ECR. Use a Lambda function to sign the image. Use<br>Amazon CloudWatch to track who generates the signatures.<br>C. Use AWS Lambda to retrieve the image that was previously pushed to Amazon ECR. Use AWS Signer to sign the image. Use Amazon<br>CloudWatch to track who generates the signatures.<br>D. Use CodeBuild to build the image. Sign the image by using AWS Signer before pushing the image to Amazon ECR. Use AWS CloudTrail<br>to track who generates the signatures.</td>
                    <td>一家公司使用HPC平台来运行数据分析作业。该公司使用AWS CodeBuild创建容器镜像并将镜像存储在Amazon弹性容器注册表(Amazon ECR)上。然后将镜像部署在Amazon弹性Kubernetes服务(Amazon EKS)上。<br>为了保持合规性，公司需要确保镜像在部署到Amazon EKS之前进行签名。签名密钥必须定期轮换并且必须自动管理。公司需要跟踪谁生成了签名。</td>
                    <td>选项A：使用CodeBuild检索之前推送到Amazon ECR的镜像，使用AWS Signer签名镜像，使用AWS CloudTrail跟踪签名生成者。这个方案的问题是需要先推送未签名的镜像到ECR，然后再检索回来签名，这违反了安全最佳实践，因为未签名的镜像已经存在于注册表中。<br><br>选项B：使用AWS Lambda检索之前推送到ECR的镜像，使用Lambda函数签名镜像，使用Amazon CloudWatch跟踪签名生成者。这个方案有多个问题：首先Lambda函数不是专门的签名服务，需要自己实现签名逻辑；其次CloudWatch不是最佳的审计跟踪工具；最后同样存在先推送未签名镜像的问题。<br><br>选项C：使用AWS Lambda检索之前推送到ECR的镜像，使用AWS Signer签名镜像，使用Amazon CloudWatch跟踪签名生成者。虽然使用了AWS Signer这个专门的签名服务，但仍然存在先推送未签名镜像的问题，且CloudWatch不如CloudTrail适合审计跟踪。<br><br>选项D：使用CodeBuild构建镜像，在推送到Amazon ECR之前使用AWS Signer签名镜像，使用AWS CloudTrail跟踪签名生成者。这个方案在构建流程中就完成签名，确保只有签名后的镜像才会推送到ECR，符合安全最佳实践。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>343</td>
                    <td>A company uses an AWS CodeArtifact repository to store Python packages that the company developed internally. A DevOps engineer needs<br>to use AWS CodeDeploy to deploy an application to an Amazon EC2 instance. The application uses a Python package that is stored in the<br>CodeArtifact repository. A BeforeInstall lifecycle event hook will install the package.<br>The DevOps engineer needs to grant the EC2 instance access to the CodeArtifact repository.</td>
                    <td>A. Create a service-linked role for CodeArtifact. Associate the role with the EC2 instance. Use the aws codeartifact get-authorization-token<br>CLI command on the instance.<br>B. Congure a resource-based policy for the CodeArtifact repository that allows the ReadFromRepository action for the EC2 instance<br>principal.<br>C. Congure ACLs on the CodeArtifact repository to allow the EC2 instance to access the Python package.<br>D. Create an instance prole that contains an IAM role that has access to CodeArtifact. Associate the instance prole with the EC2<br>instance. Use the aws codeartifact login CLI command on the instance.</td>
                    <td>一家公司使用AWS CodeArtifact存储库来存储公司内部开发的Python包。DevOps工程师需要使用AWS CodeDeploy将应用程序部署到Amazon EC2实例。该应用程序使用存储在CodeArtifact存储库中的Python包。BeforeInstall生命周期事件钩子将安装该包。<br>DevOps工程师需要授予EC2实例访问CodeArtifact存储库的权限。</td>
                    <td>A. 为CodeArtifact创建服务链接角色，将角色与EC2实例关联，在实例上使用aws codeartifact get-authorization-token CLI命令。这个选项不正确，因为服务链接角色是AWS服务自动创建和管理的，不能手动关联到EC2实例。而且get-authorization-token命令只是获取令牌，不能完成完整的认证配置。<br><br>B. 为CodeArtifact存储库配置基于资源的策略，允许EC2实例主体执行ReadFromRepository操作。这个选项部分正确，但仅配置资源策略还不够，EC2实例还需要有相应的IAM权限才能访问CodeArtifact，而且没有处理认证令牌的获取问题。<br><br>C. 在CodeArtifact存储库上配置ACL以允许EC2实例访问Python包。这个选项不正确，因为CodeArtifact不使用传统的ACL（访问控制列表）机制，而是使用IAM策略和资源策略进行访问控制。<br><br>D. 创建包含具有CodeArtifact访问权限的IAM角色的实例配置文件，将实例配置文件与EC2实例关联，在实例上使用aws codeartifact login CLI命令。这是正确的方法，通过实例配置文件为EC2实例提供必要的IAM权限，然后使用login命令配置pip以使用CodeArtifact进行包安装。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>344</td>
                    <td>A company has a le-reading application that saves les to a database that runs on Amazon EC2 instances. Regulations require the company<br>to delete les from EC2 instances every day at a specic time. The company must delete database records that are older than 60 days.<br>The database record deletion must occur after the le deletions. The company has created scripts to delete les and database records. The<br>company needs to receive an email notication for any failure of the deletion scripts.</td>
                    <td>A. Use AWS Systems Manager State Manager to automatically invoke a Systems Manager Automation document at the specied time<br>each day. Congure the Automation document to use a run command to run the deletion scripts in sequential order. Create an Amazon<br>EventBridge rule to use Amazon Simple Notication Service (Amazon SNS) to send failure notications to the company.<br>B. Use AWS Systems Manager State Manager to automatically invoke a Systems Manager Automation document at the specied time<br>each day. Congure the Automation document to use a run command to run the deletion scripts in sequential order. Create a conditional<br>statement inside the Automation document as the last step to check for errors. Use Amazon Simple Email Service (Amazon SES) to send<br>failure notications as email messages to the company.<br>C. Create an Amazon EventBridge rule that invokes an AWS Lambda function at the specied time. Add the necessary permissions for the<br>invocation to the Lambda function&#x27;s resource-based policy. Congure the Lambda function to run the deletion scripts in sequential order.<br>Congure the Lambda function to use Amazon Simple Notication Service (Amazon SNS) to send failure notications to the company.<br>D. Create an Amazon EventBridge rule that invokes an AWS Lambda function at the specied time. Add the necessary permissions for the<br>invocation to the Lambda function&#x27;s resource-based policy. Congure the Lambda function to run the deletion scripts in sequential order.<br>Congure the Lambda function to use Amazon Simple Email Service (Amazon SES) to send failure notications as email messages to the<br>company.</td>
                    <td>一家公司有一个文件读取应用程序，将文件保存到运行在Amazon EC2实例上的数据库中。法规要求公司每天在特定时间从EC2实例删除文件。公司必须删除超过60天的数据库记录。数据库记录删除必须在文件删除之后进行。公司已经创建了删除文件和数据库记录的脚本。公司需要在删除脚本失败时收到电子邮件通知。</td>
                    <td>选项A：使用AWS Systems Manager State Manager在每天指定时间自动调用Systems Manager自动化文档。配置自动化文档使用运行命令按顺序运行删除脚本。创建Amazon EventBridge规则使用Amazon SNS向公司发送失败通知。这个方案使用了合适的调度服务（State Manager），能够按顺序执行脚本，并通过EventBridge和SNS提供可靠的通知机制。SNS可以轻松配置邮件通知，并且EventBridge能够捕获自动化文档的执行状态。<br><br>选项B：与选项A类似，但使用Amazon SES发送失败通知，并在自动化文档内部添加条件语句检查错误。虽然SES可以发送邮件，但在自动化文档内部处理错误检查和通知不如使用EventBridge的事件驱动方式灵活和可靠。这种方式增加了复杂性，且错误处理逻辑耦合在自动化文档中。<br><br>选项C：使用Amazon EventBridge规则在指定时间调用AWS Lambda函数，配置Lambda函数按顺序运行删除脚本，使用SNS发送失败通知。Lambda函数适合轻量级任务，但对于需要在EC2实例上执行脚本的场景，Systems Manager更为合适，因为它专门设计用于管理EC2实例上的操作。<br><br>选项D：与选项C类似，但使用Amazon SES发送通知。同样面临Lambda不是最佳选择来管理EC2实例操作的问题，且SES相比SNS在这种场景下配置更复杂。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>345</td>
                    <td>A company uses an organization in AWS Organizations that has all features enabled to manage its AWS accounts. Amazon EQ instances run in<br>the AWS accounts.<br>The company requires that all current EC2 instances must use Instance Metadata Service Version 2 (IMDSv2). The company needs to block<br>AWS API calls that originate from EC2 instances that do not use IMDSv2.</td>
                    <td>A. Create a new SCP statement that denies the ec2:RunInstances action when the ec2:MetadataHttpTokens condition key is not equal to<br>the value of required. Attach the SCP to the root of the organization.<br>B. Create a new SCP statement that denies the ec2:RunInstances action when the ec2:MetadataHttpPutResponseHopLimit condition key<br>value is greater than two. Attach the SCP to the root of the organization.<br>C. Create a new SCP statement that denies &quot;*&quot; when the ec2:RoleDelivery condition key value is less than two. Attach the SCP to the root<br>of the organization.<br>D. Create a new SCP statement that denies when the ec2:MetadataHttpTokens condition key value is not equal to required. Attach the<br>SCP to the root of the organization.</td>
                    <td>一家公司使用AWS Organizations中启用了所有功能的组织来管理其AWS账户。Amazon EC2实例在AWS账户中运行。<br>公司要求所有当前的EC2实例必须使用实例元数据服务版本2（IMDSv2）。公司需要阻止来自未使用IMDSv2的EC2实例的AWS API调用。</td>
                    <td>选项A：创建一个新的SCP语句，当ec2:MetadataHttpTokens条件键不等于required值时拒绝ec2:RunInstances操作。将SCP附加到组织的根部。这个选项正确地使用了IMDSv2的关键条件键ec2:MetadataHttpTokens，当其值不为&quot;required&quot;时阻止启动新实例，但这只能防止创建新的非IMDSv2实例，不能阻止现有非IMDSv2实例的API调用。<br><br>选项B：创建一个新的SCP语句，当ec2:MetadataHttpPutResponseHopLimit条件键值大于2时拒绝ec2:RunInstances操作。这个条件键控制的是元数据响应的跳数限制，与IMDSv2的强制使用没有直接关系，不能有效解决问题。<br><br>选项C：创建一个新的SCP语句，当ec2:RoleDelivery条件键值小于2时拒绝&quot;*&quot;操作。ec2:RoleDelivery不是一个有效的AWS条件键，这个选项在技术上是错误的，无法实现所需的功能。<br><br>选项D：创建一个新的SCP语句，当ec2:MetadataHttpTokens条件键值不等于required时拒绝&quot;*&quot;操作。这个选项使用了正确的条件键，并且通过拒绝所有操作(&quot;*&quot;)来阻止来自未使用IMDSv2实例的所有AWS API调用，完全符合题目要求。</td>
                    <td>D</td>
                </tr>
                <tr>
                    <td>346</td>
                    <td>A DevOps team supports an application that runs on a large number of Amazon EC2 instances in an Auto Scaling group. The DevOps team<br>uses AWS CloudFormation to deploy the EC2 instances. The application recently experienced an issue. A single instance returned errors to a<br>large percentage of requests. The EC2 instance responded as healthy to both Amazon EC2 and Elastic Load Balancing health checks.<br>The DevOps team collects application logs in Amazon CloudWatch by using the embedded metric format. The DevOps team needs to receive<br>an alert if any EC2 instance is responsible for more than half of all errors.</td>
                    <td>A. Create a CloudWatch Contributor Insights rule that groups logs from the CloudWatch application logs based on instance ID and errors.<br>B. Create a resource group in AWS Resource Groups. Use the CloudFormation stack to group the resources for the application. Add the<br>application to CloudWatch Application Insights. Use the resource group to identify the application.<br>C. Create a metric lter for the application logs to count the occurrence of the term &quot;Error.&#x27;&#x27; Create a CloudWatch alarm that uses the<br>METRIC_COUNT function to determine whether errors have occurred. Congure the CloudWatch alarm to send a notication to an Amazon<br>Simple Notication Service (Amazon SNS) topic to notify the DevOps team.<br>D. Create a CloudWatch alarm that uses the INSIGHT_RULE_METRIC function to determine whether a specic instance is responsible for<br>more than half of all errors reported by EC2 instances. Congure the CloudWatch alarm to send a notication to an Amazon Simple<br>Notication Service (Amazon SNS) topic to notify the DevOps team.<br>E. Create a CloudWatch subscription lter for the application logs that lters for errors and invokes an AWS Lambda function. Congure<br>the Lambda function to send the instance ID and error and in a notication to an Amazon Simple Notication Service (Amazon SNS) topic<br>to notify the DevOps team.</td>
                    <td>一个DevOps团队支持运行在Auto Scaling组中大量Amazon EC2实例上的应用程序。DevOps团队使用AWS CloudFormation来部署EC2实例。应用程序最近遇到了一个问题：单个实例对大部分请求返回错误。该EC2实例对Amazon EC2和Elastic Load Balancing健康检查的响应都是健康的。DevOps团队使用嵌入式指标格式在Amazon CloudWatch中收集应用程序日志。DevOps团队需要在任何EC2实例负责超过一半的所有错误时收到警报。</td>
                    <td>A. 创建CloudWatch Contributor Insights规则，根据实例ID和错误对CloudWatch应用程序日志进行分组。这个选项是正确的，因为Contributor Insights专门用于分析日志数据并识别顶级贡献者，可以按实例ID分组并统计错误数量，从而识别出负责大部分错误的实例。<br><br>B. 在AWS Resource Groups中创建资源组，使用CloudFormation堆栈对应用程序资源进行分组，将应用程序添加到CloudWatch Application Insights。虽然Application Insights可以提供应用程序监控，但它主要用于应用程序性能监控，不是专门用于识别特定实例错误贡献率的最佳解决方案。<br><br>C. 为应用程序日志创建指标过滤器来计算&quot;Error&quot;术语的出现次数，创建使用METRIC_COUNT函数的CloudWatch警报。这个方案只能统计总错误数，无法识别哪个特定实例负责超过一半的错误，不符合需求。<br><br>D. 创建使用INSIGHT_RULE_METRIC函数的CloudWatch警报，以确定特定实例是否负责EC2实例报告的超过一半错误。这个选项正确，因为它与选项A配合使用，通过INSIGHT_RULE_METRIC函数可以基于Contributor Insights规则创建警报。<br><br>E. 为应用程序日志创建CloudWatch订阅过滤器，过滤错误并调用AWS Lambda函数。虽然可以实现通知功能，但需要在Lambda中编写复杂逻辑来计算错误比例，不如使用现有的CloudWatch功能高效。</td>
                    <td>AD</td>
                </tr>
                <tr>
                    <td>347</td>
                    <td>A company is using AWS CloudFormation to perform deployments of its application environment. A deployment failed during a recent update<br>to the existing CloudFormation stack. A DevOps engineer discovered that some resources in the stack were manually modied.<br>The DevOps engineer needs a solution that detects manual modication of resources and sends an alert to the DevOps lead.</td>
                    <td>A. Create an Amazon Simple Notication Service (Amazon SNS) topic. Subscribe the DevOps lead to the topic by using an email address.<br>Create an AWS Cong managed rule that has the CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK identier. Create an Amazon<br>EventBridge rule that is invoked on the NON_COMPLIANT resources status. Set the SNS topic as the rule target.<br>B. Tag all CloudFormation resources with a specic tag. Create an AWS Cong custom rule by using the AWS Cong Rules Development Kit<br>Library (RDKlib) that checks all resource changes that have the specic tag. Congure the custom rule to mark all the tagged resource<br>changes as NON_COMPLIANT when the change is not performed by CloudFormation. Create an Amazon EventBridge rule that is invoked<br>on the NON_COMPUANT resources status. Create an AWS Lambda function that sends an email message to the DevOps lead. Set the<br>Lambda function as the rule target.<br>C. Create an Amazon Simple Notication Service (Amazon SNS) topic. Subscribe the DevOps lead to the topic by using an email address.<br>Create an AWS Cong managed rule that has the CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK identier. Create an Amazon<br>EventBridge rule that is invoked on the COMPLIANT resources status. Set the SNS topic as the rule target.<br>D. Create an AWS Cong managed rule that has the CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK identier. Create an Amazon<br>EventBridge rule that is invoked on the NON_COMPLIANT resources status. Create an AWS Lambda function that sends an email message<br>to the DevOps lead. Set the Lambda function as the rule target.</td>
                    <td>一家公司正在使用AWS CloudFormation来执行其应用程序环境的部署。在最近对现有CloudFormation堆栈进行更新时，部署失败了。DevOps工程师发现堆栈中的一些资源被手动修改了。DevOps工程师需要一个解决方案来检测资源的手动修改并向DevOps负责人发送警报。</td>
                    <td>选项A：创建Amazon SNS主题，让DevOps负责人通过邮箱地址订阅该主题。创建一个具有CLOUDFORMATION_STACK_DRIFT_DETECTION_CHECK标识符的AWS Config托管规则。创建一个Amazon EventBridge规则，当资源状态为NON_COMPLIANT时触发。将SNS主题设置为规则目标。这个方案使用了正确的AWS Config托管规则来检测CloudFormation堆栈漂移，当检测到不合规资源时通过EventBridge触发SNS通知，逻辑完整且实用。<br><br>选项B：为所有CloudFormation资源打上特定标签，使用AWS Config Rules Development Kit Library创建自定义规则检查带有特定标签的资源变更。当变更不是由CloudFormation执行时，将标记为NON_COMPLIANT。创建EventBridge规则在NON_COMPLIANT状态时触发Lambda函数发送邮件。虽然功能完整，但过于复杂，需要自定义开发，而AWS已提供现成的托管规则。<br><br>选项C：与选项A类似，但EventBridge规则是在COMPLIANT状态时触发，这是错误的，因为我们需要在检测到问题（NON_COMPLIANT）时发送警报，而不是在正常状态时。<br><br>选项D：使用正确的AWS Config托管规则和EventBridge规则配置，但缺少通知机制的具体实现（如SNS主题订阅），只有Lambda函数发送邮件，相比SNS的邮件订阅机制更复杂。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>348</td>
                    <td>A DevOps engineer deployed multiple AWS accounts by using AWS Control Tower to support different business, technical, and administrative<br>units in a company. A security team needs the DevOps engineer to automate AWS Control Tower guardrails for the company. The guardrails<br>must be applied to all accounts in an OU of the company&#x27;s organization in AWS Organizations.<br>The security team needs a solution that has version control and can be reviewed and rolled back if necessary. The security team will maintain<br>the management of the solution in its OU. The security team wants to limit the type of guardrails that are allowed and allow only new<br>guardrails that are approved by the security team.</td>
                    <td>A. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in an AWS CodeCommit repository.<br>Create an AWS::ControlTower::EnableControl logical resource in the template for each OU in the organization. Congure an AWS Code<br>Build project that an Amazon EventBridge rule will invoke for the security team&#x27;s AWS CodeCommit changes.<br>B. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in an AWS CodeCommit repository.<br>Create an AWS::ControlTower::EnableControl logical resource in the template for each account in the organization. Congure an AWS<br>CodePipeline pipeline in the security team&#x27;s account. Advise the security team to invoke the pipeline and provide these parameters when<br>starting the pipeline.<br>C. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in an AWS CodeCommit repository.<br>Create an AWS::ControlTower::EnableControl logical resource in the template for each OU in the organization. Congure an AWS<br>CodePipeline pipeline in the security team&#x27;s account that an Amazon EventBridge rule will invoke for the security team&#x27;s CodeCommit<br>changes.<br>D. Congure an AWS CodePipeline pipeline in the security team&#x27;s account that an Amazon EventBridge rule will invoke for PutObject<br>events to an Amazon S3 bucket. Create individual AWS CloudFormation templates that align to a guardrail. Store the templates in the S3<br>bucket. Create an AWS::ControlTower::EnableControl logical resource in the template for each OU in the organization.</td>
                    <td>一名DevOps工程师使用AWS Control Tower部署了多个AWS账户，以支持公司中不同的业务、技术和管理单元。安全团队需要DevOps工程师为公司自动化AWS Control Tower护栏。这些护栏必须应用到AWS Organizations组织中公司OU的所有账户。<br><br>安全团队需要一个具有版本控制功能的解决方案，可以进行审查，并在必要时回滚。安全团队将在其OU中维护解决方案的管理。安全团队希望限制允许的护栏类型，只允许经过安全团队批准的新护栏。</td>
                    <td>选项A：使用CloudFormation模板存储在CodeCommit中，为每个OU创建EnableControl资源，通过EventBridge规则触发CodeBuild项目。这个方案缺少完整的CI/CD流水线，只有CodeBuild项目无法提供完整的部署和回滚能力，不符合安全团队对版本控制和回滚的要求。<br><br>选项B：使用CloudFormation模板存储在CodeCommit中，但为每个账户而不是OU创建EnableControl资源，配置CodePipeline流水线。这种方法会导致管理复杂性增加，因为需要为每个账户单独管理，而不是在OU级别统一管理，不符合题目要求的OU级别应用护栏。<br><br>选项C：使用CloudFormation模板存储在CodeCommit中，为每个OU创建EnableControl资源，在安全团队账户中配置CodePipeline流水线，通过EventBridge规则响应CodeCommit变更自动触发。这个方案提供了完整的版本控制、自动化部署、审查和回滚能力，符合所有要求。<br><br>选项D：使用S3存储模板而不是CodeCommit，通过PutObject事件触发CodePipeline。虽然技术上可行，但S3不如CodeCommit提供更好的版本控制功能，特别是对于代码审查和协作方面，不是最佳实践。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>349</td>
                    <td>A company runs a web application on Amazon Elastic Kubernetes Service (Amazon EKS). The company uses Amazon CloudFront to distribute<br>the application. The company recently enabled AWS WAF. The company set up Amazon CloudWatch Logs to send logs to an aws-waf-logs log<br>group.<br>The company wants a DevOps engineer to receive alerts if there are sudden changes in blocked trac. The company does not want to receive<br>alerts for other changes in AWS WAF log behavior. The company will tune AWS WAF rules over time.<br>The DevOps engineer is currently subscribed to an Amazon Simple Notication Service (Amazon SNS) topic in the environment.</td>
                    <td>A. Create a CloudWatch Logs metrics lter for blocked requests on the AWS WAF log group to create a custom metric. Create a<br>CloudWatch alarm by using CloudWatch anomaly detection and the published custom metric. Congure the alarm to notify the SNS topic<br>to alert the DevOps engineer.<br>B. Create a CloudWatch anomaly detector for the log group. Create a CloudWatch alarm by using metrics that the CloudWatch anomaly<br>detector publishes. Use the high setting for the LogAnomalyPriority metric. Congure the alarm to go into alarm state if a static threshold<br>of one anomaly is detected. Congure the alarm to notify the SNS topic to alert the DevOps engineer.<br>C. Create a CloudWatch metrics lter for counted requests on the AWS WAF log group to create a custom metric. Create a CloudWatch<br>alarm that activates when the sum of blocked requests in the custom metric during a period of 1 hour is greater than a static estimate for<br>the acceptable number of blocked requests in 1 hour. Congure the alarm to notify the SNS topic to alert the DevOps engineer.<br>D. Create a CloudWatch anomaly detector for the log group. Create a CloudWatch alarm by using metrics that the CloudWatch anomaly<br>detector publishes. Use the medium setting for the LogAnomalyPriority metric. Congure the alarm to go into alarm state if a sum of<br>anomalies over 1 hour is greater than an expected value. Congure the alarm to notify the SNS topic to alert the DevOps engineer.</td>
                    <td>一家公司在Amazon Elastic Kubernetes Service (Amazon EKS)上运行Web应用程序。该公司使用Amazon CloudFront来分发应用程序。该公司最近启用了AWS WAF。该公司设置了Amazon CloudWatch Logs将日志发送到aws-waf-logs日志组。<br>该公司希望DevOps工程师在被阻止流量突然变化时收到警报。该公司不希望收到AWS WAF日志行为其他变化的警报。该公司将随时间调整AWS WAF规则。<br>DevOps工程师目前已订阅环境中的Amazon Simple Notification Service (Amazon SNS)主题。</td>
                    <td>选项A：为AWS WAF日志组上的被阻止请求创建CloudWatch Logs指标过滤器来创建自定义指标。使用CloudWatch异常检测和发布的自定义指标创建CloudWatch警报。配置警报通知SNS主题以提醒DevOps工程师。这个方案针对性强，专门监控被阻止的请求，使用异常检测可以自动适应WAF规则调整，避免误报。<br><br>选项B：为日志组创建CloudWatch异常检测器。使用CloudWatch异常检测器发布的指标创建CloudWatch警报。对LogAnomalyPriority指标使用高设置。配置警报在检测到一个异常的静态阈值时进入警报状态。这个方案监控所有日志异常，不够精确，可能会产生不需要的警报。<br><br>选项C：为AWS WAF日志组上的计数请求创建CloudWatch指标过滤器来创建自定义指标。创建CloudWatch警报，当1小时内自定义指标中被阻止请求的总和大于1小时内可接受被阻止请求数量的静态估计时激活。这个方案使用静态阈值，无法适应WAF规则调整带来的正常变化。<br><br>选项D：为日志组创建CloudWatch异常检测器。使用CloudWatch异常检测器发布的指标创建CloudWatch警报。对LogAnomalyPriority指标使用中等设置。配置警报在1小时内异常总和大于预期值时进入警报状态。这个方案同样监控所有异常，不够精确。</td>
                    <td>A</td>
                </tr>
                <tr>
                    <td>350</td>
                    <td>A video platform company is migrating its video catalog to AWS. The company will host MP4 videos les in an Amazon S3 bucket. The<br>company will use Amazon CloudFront and Amazon EC2 instances to serve the video les.<br>Users rst connect to a frontend application that redirects to a video URL. The video URL contains an authorization token in CloudFront. The<br>cache is activated on the CloudFront distribution. Authorization token check activity needs to be logged in Amazon CloudWatch.<br>The company wants to prevent direct access to video les on CloudFront and Amazon S3 and wants to implement checks of the authorization<br>token that the frontend application provides. The company also wants to perform regular rolling updates of the code that checks the<br>authorization token signature.</td>
                    <td>A. Implement an authorization token check in Lambda@Edge as a trigger on the CloudFront distribution. Enable CloudWatch logging for<br>the Lambda@Edge function. Attach the Lambda@Edge function to the CloudFront distribution. Implement CloudFront continuous<br>deployment to perform updates.<br>B. Implement an authorization token check in CloudFront Functions. Enable CloudWatch logging for the CloudFront function. Attach the<br>CloudFront function to the CloudFront distribution. Implement CloudFront continuous deployment to perform updates.<br>C. Implement an authorization token check in the application code that is installed on the EC2 instances. Install the CloudWatch agent on<br>the EC2 instances. Congure the application to log to the CloudWatch agent. Implement a second CloudFront distribution. Migrate the<br>trac from the rst CloudFront distribution by using Amazon Route 53 weighted routing.<br>D. Implement an authorization token check in CloudFront Functions. Enable CloudWatch logging for the CloudFront function. Attach the<br>CloudFront function to the CloudFront distribution. Implement a second CloudFront distribution. Migrate the trac from the rst<br>CloudFront distribution by using Amazon Route 53 weighted routing.</td>
                    <td>一家视频平台公司正在将其视频目录迁移到AWS。该公司将在Amazon S3存储桶中托管MP4视频文件。公司将使用Amazon CloudFront和Amazon EC2实例来提供视频文件服务。<br>用户首先连接到前端应用程序，该应用程序重定向到视频URL。视频URL在CloudFront中包含授权令牌。CloudFront分发上启用了缓存。授权令牌检查活动需要记录在Amazon CloudWatch中。<br>公司希望防止直接访问CloudFront和Amazon S3上的视频文件，并希望实现对前端应用程序提供的授权令牌的检查。公司还希望对检查授权令牌签名的代码执行定期滚动更新。</td>
                    <td>A. 在Lambda@Edge中实现授权令牌检查作为CloudFront分发的触发器。为Lambda@Edge函数启用CloudWatch日志记录。将Lambda@Edge函数附加到CloudFront分发。实施CloudFront持续部署来执行更新。这个选项技术上可行，但Lambda@Edge比CloudFront Functions更复杂且成本更高，对于简单的令牌验证来说过于复杂。<br><br>B. 在CloudFront Functions中实现授权令牌检查。为CloudFront函数启用CloudWatch日志记录。将CloudFront函数附加到CloudFront分发。实施CloudFront持续部署来执行更新。这个选项最适合，CloudFront Functions专为轻量级边缘计算设计，成本低，延迟小，非常适合令牌验证，且支持持续部署进行滚动更新。<br><br>C. 在安装在EC2实例上的应用程序代码中实现授权令牌检查。在EC2实例上安装CloudWatch代理。配置应用程序记录到CloudWatch代理。实施第二个CloudFront分发。使用Amazon Route 53加权路由从第一个CloudFront分发迁移流量。这个方案将验证逻辑放在EC2上，无法防止直接访问CloudFront，不符合安全要求。<br><br>D. 在CloudFront Functions中实现授权令牌检查。为CloudFront函数启用CloudWatch日志记录。将CloudFront函数附加到CloudFront分发。实施第二个CloudFront分发。使用Amazon Route 53加权路由从第一个CloudFront分发迁移流量。虽然使用了正确的CloudFront Functions，但部署策略过于复杂，不如CloudFront持续部署高效。</td>
                    <td>B</td>
                </tr>
                <tr>
                    <td>351</td>
                    <td>A company uses an organization in AWS Organizations to manage multiple AWS accounts in a hierarchical structure. An SCP that is associated<br>with the organization root allows IAM users to be created.<br>A DevOps team must be able to create IAM users with any level of permissions. Developers must also be able to create IAM users. However,<br>developers must not be able to grant new IAM users excessive permissions. The developers have the CreateAndManageUsers role in each<br>account. The DevOps team must be able to prevent other users from creating IAM users.</td>
                    <td>A. Create an SCP in the organization to deny users the ability to create and modify IAM users. Attach the SCP to the root of the<br>organization. Attach the CreateAndManageUsers role to developers.<br>B. Create an SCP in the organization to grant users that have the DeveloperBoundary policy attached the ability to create new IAM users<br>and to modify IAM users. Congure the SCP to require users to attach the PermissionBoundaries policy to any new IAM user. Attach the<br>SCP to the root of the organization.<br>C. Create an IAM permissions policy named PermissionBoundaries within each account. Congure the PermissionBoundaries policy to<br>specify the maximum permissions that a developer can grant to a new IAM user.<br>D. Create an IAM permissions policy named PermissionBoundaries within each account. Congure PermissionsBoundaries to allow users<br>who have the PermissionBoundaries policy to create new IAM users.<br>E. Create an IAM permissions policy named DeveloperBoundary within each account. Congure the DeveloperBoundary policy to allow<br>developers to create IAM users and to assign policies to IAM users of only if the developer includes the PermissionBoundaries policy as<br>the permissions boundary. Attach the DeveloperBoundary policy to the CreateAndManageUsers role within each account.</td>
                    <td>一家公司使用AWS Organizations中的组织来管理分层结构中的多个AWS账户。与组织根关联的SCP允许创建IAM用户。<br>DevOps团队必须能够创建具有任何级别权限的IAM用户。开发人员也必须能够创建IAM用户。但是，开发人员不能向新IAM用户授予过度权限。开发人员在每个账户中都有CreateAndManageUsers角色。DevOps团队必须能够阻止其他用户创建IAM用户。</td>
                    <td>A. 在组织中创建SCP以拒绝用户创建和修改IAM用户的能力，将SCP附加到组织根，将CreateAndManageUsers角色附加给开发人员。这个选项通过SCP在组织级别限制IAM用户创建，但这会阻止所有用户（包括DevOps团队）创建IAM用户，不符合DevOps团队需要完全权限的要求。<br><br>B. 创建SCP授予具有DeveloperBoundary策略的用户创建和修改IAM用户的能力，配置SCP要求用户为任何新IAM用户附加PermissionBoundaries策略。这个选项试图通过SCP控制权限边界，但SCP主要用于拒绝权限而不是授予权限，且没有解决DevOps团队的完全访问需求。<br><br>C. 在每个账户内创建名为PermissionBoundaries的IAM权限策略，配置该策略指定开发人员可以授予新IAM用户的最大权限。这个选项正确使用了权限边界概念，通过在账户级别设置权限边界来限制开发人员创建的IAM用户的最大权限，符合限制开发人员过度授权的要求。<br><br>D. 在每个账户内创建PermissionBoundaries策略，配置为允许具有该策略的用户创建新IAM用户。这个选项混淆了权限边界的概念，权限边界应该是限制最大权限而不是授予创建权限。<br><br>E. 在每个账户内创建DeveloperBoundary策略，配置为允许开发人员创建IAM用户并仅在包含PermissionBoundaries策略作为权限边界时分配策略。这个选项部分正确但过于复杂，且没有解决DevOps团队需要完全控制的要求。</td>
                    <td>CE</td>
                </tr>
                <tr>
                    <td>352</td>
                    <td>A company has deployed a landing zone that has a well-dened AWS Organizations structure and an SCP. The company&#x27;s development team<br>can create their AWS resources only by using AWS CloudFormation and the AWS Cloud Development Kit (AWS CDK).<br>A DevOps engineer notices that Amazon Simple Queue Service (Amazon SQS) queues that are deployed in different CloudFormation stacks<br>have different congurations. The DevOps engineer also notices that the application cost allocation tag is not always set.<br>The DevOps engineer needs a solution that will enforce tagging and promote the reuse of code. The DevOps engineer needs to avoid different<br>congurations for the deployed SQS queues.</td>
                    <td>A. Create an Organizations tag policy to enforce the cost allocation tag in CloudFormation stacks. Instruct the development team to use<br>CloudFormation to dene SQS queues. Instruct the development team to deploy the SQS queues by using CloudFormation StackSets.<br>B. Update the SCP to enforce the cost allocation tag in CloudFormation stacks. Instruct the development team to use CloudFormation<br>modules to dene SQS queues. Instruct the development team to deploy the SQS queues by using CloudFormation stacks.<br>C. Use AWS CDK tagging to enforce the cost allocation tag in CloudFormation StackSets. Instruct the development team to use the AWS<br>CDK to dene SQS queues. Instruct the development team to deploy the SQS queues by using CDK stacks.<br>D. Use AWS CDK tagging to enforce the cost allocation tag in CloudFormation stacks. Instruct the development team to use the AWS CDK<br>to dene SQS queues. Instruct the development team to deploy the SQS queues by using CDK feature ags.</td>
                    <td>一家公司已经部署了一个具有良好定义的AWS Organizations结构和SCP（服务控制策略）的着陆区。公司的开发团队只能通过使用AWS CloudFormation和AWS云开发工具包（AWS CDK）来创建他们的AWS资源。<br>一名DevOps工程师注意到在不同CloudFormation堆栈中部署的Amazon简单队列服务（Amazon SQS）队列具有不同的配置。DevOps工程师还注意到应用程序成本分配标签并不总是被设置。<br>DevOps工程师需要一个解决方案来强制执行标签并促进代码重用。DevOps工程师需要避免已部署的SQS队列出现不同配置。</td>
                    <td>A. 创建Organizations标签策略来在CloudFormation堆栈中强制执行成本分配标签。指导开发团队使用CloudFormation定义SQS队列。指导开发团队使用CloudFormation StackSets部署SQS队列。<br>- 这个选项使用Organizations标签策略可以强制标签，但使用普通CloudFormation定义SQS队列无法很好地促进代码重用和标准化配置。StackSets主要用于跨账户/区域部署，不是解决配置一致性的最佳方案。<br><br>B. 更新SCP来在CloudFormation堆栈中强制执行成本分配标签。指导开发团队使用CloudFormation模块定义SQS队列。指导开发团队使用CloudFormation堆栈部署SQS队列。<br>- SCP主要用于权限控制，不是强制标签的最佳工具。CloudFormation模块可以促进重用，但整体方案不如CDK方案优雅和现代化。<br><br>C. 使用AWS CDK标签功能在CloudFormation StackSets中强制执行成本分配标签。指导开发团队使用AWS CDK定义SQS队列。指导开发团队使用CDK堆栈部署SQS队列。<br>- CDK提供了优秀的标签管理功能，可以在应用级别自动应用标签。使用CDK定义SQS队列可以通过构造函数实现标准化配置和代码重用。CDK堆栈是标准的部署方式。<br><br>D. 使用AWS CDK标签功能在CloudFormation堆栈中强制执行成本分配标签。指导开发团队使用AWS CDK定义SQS队列。指导开发团队使用CDK功能标志部署SQS队列。<br>- CDK标签功能很好，但CDK功能标志主要用于控制CDK行为和新功能的启用，不是部署资源的方式。</td>
                    <td>C</td>
                </tr>
                <tr>
                    <td>353</td>
                    <td>A DevOps team manages a company&#x27;s AWS account. The company wants to ensure that specic AWS resource conguration changes are<br>automatically reverted.</td>
                    <td>A. Use AWS Cong rules to detect changes in resource congurations. Congure remediation action that uses AWS Systems Manager<br>Automation documents to revert the conguration changes.<br>B. Use Amazon CloudWatch alarms to monitor resource metrics. When an alarm is activated, use an Amazon Simple Notication Service<br>(Amazon SNS) topic to notify an administrator to manually reverts the conguration changes.<br>C. Use AWS CloudFormation to create a stack that deploys the necessary conguration changes. Update the stack when conguration<br>changes need to be reverted.<br>D. Use AWS Trusted Advisor to check for noncompliant congurations. Manually apply necessary changes based on Trusted Advisor<br>recommendations.</td>
                    <td>一个DevOps团队管理公司的AWS账户。公司希望确保特定的AWS资源配置更改能够自动回滚。</td>
                    <td>A. 使用AWS Config规则来检测资源配置的变更。配置修复操作，使用AWS Systems Manager自动化文档来回滚配置更改。<br>这个选项提供了完整的自动化解决方案。AWS Config可以持续监控资源配置并检测不合规的更改，当检测到违规时，可以触发自动修复操作。通过AWS Systems Manager Automation文档，可以定义具体的回滚步骤并自动执行，完全满足题目要求的&quot;自动回滚&quot;需求。<br><br>B. 使用Amazon CloudWatch告警来监控资源指标。当告警被激活时，使用Amazon SNS主题通知管理员手动回滚配置更改。<br>这个选项虽然能够检测到问题，但需要人工干预来执行回滚操作，不符合题目要求的&quot;自动回滚&quot;。CloudWatch主要监控性能指标而非配置更改，且依赖手动操作。<br><br>C. 使用AWS CloudFormation创建堆栈来部署必要的配置更改。当需要回滚配置更改时更新堆栈。<br>CloudFormation适合基础设施即代码的部署和管理，但它不能自动检测配置偏移并触发回滚。需要人工识别问题并手动更新堆栈，不满足自动化要求。<br><br>D. 使用AWS Trusted Advisor检查不合规配置。根据Trusted Advisor建议手动应用必要的更改。<br>Trusted Advisor主要提供最佳实践建议和成本优化建议，不是实时配置监控工具，且需要手动操作，完全不符合自动回滚的需求。</td>
                    <td>A</td>
                </tr>
            </tbody>
        </table>
    </div>
</body>
</html>
